/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/StaticObstacle/TwoDim/SingleObstacle/DDPGHER_CNN.py
episode index:0
target Thresh 1.8999999999999995
target distance 1.0
model initialize at round 0
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.87155603, 4.873745  , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 2.0654666046193566}
done in step count: 99
reward sum = -0.2428622149104694
running average episode reward sum: -0.2428622149104694
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([16.87007127, 11.87007127,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 15.947316358631477}
episode index:1
target Thresh 1.9085478628562047
target distance 1.0
model initialize at round 1
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.8674032 , 5.99505776, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 3.1181339431255806}
done in step count: 99
reward sum = -0.23994618741795823
running average episode reward sum: -0.2414042011642138
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 1.13477157, 11.87063955,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.321897872739491}
episode index:2
target Thresh 1.9170914528492862
target distance 1.0
model initialize at round 2
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.2360679774997645}
done in step count: 99
reward sum = -0.2503918239540168
running average episode reward sum: -0.24440007542748146
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 8.35545953, 11.89769067,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 5.955003528696977}
episode index:3
target Thresh 1.9256307721151433
target distance 1.0
model initialize at round 3
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.13259695, 6.99505755, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 3.5295274256865987}
done in step count: 93
reward sum = 0.19725504484445544
running average episode reward sum: -0.13398629535949724
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.26924027, 4.01193292, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7308571500888057}
episode index:4
target Thresh 1.9341658227886036
target distance 1.0
model initialize at round 4
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([6.08048308, 9.74266136, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 2.058176644810408}
done in step count: 99
reward sum = -0.20987945166893668
running average episode reward sum: -0.1491649266213851
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([1.12582113, 8.18142547, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 6.9227450776473445}
episode index:5
target Thresh 1.9426966070034315
target distance 1.0
model initialize at round 5
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.09926869, 2.62181132, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9769051010219946}
done in step count: 0
reward sum = 0.9967011118738712
running average episode reward sum: 0.0418127464611576
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.09926869, 2.62181132, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9769051010219946}
episode index:6
target Thresh 1.9512231268923217
target distance 1.0
model initialize at round 6
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([13.11102695,  3.70070541,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.9727350002250437}
done in step count: 99
reward sum = -0.24716966411949437
running average episode reward sum: 0.0005295449496358946
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 1.12977696, 11.86882629,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 17.314144878290914}
episode index:7
target Thresh 1.9597453845869062
target distance 1.0
model initialize at round 7
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 7.00494245, 11.86740305,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.0351506972012094}
done in step count: 99
reward sum = -0.23781608226042233
running average episode reward sum: -0.029263658451621384
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 1.12992873, 11.87007127,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 7.445481075455877}
episode index:8
target Thresh 1.9682633822177449
target distance 1.0
model initialize at round 8
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 2.236067977499766}
done in step count: 39
reward sum = 0.5425022374743571
running average episode reward sum: 0.034265885540154
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.42916895, 10.20492076,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.60649863899614}
episode index:9
target Thresh 1.9767771219143433
target distance 1.0
model initialize at round 9
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.86778212, 6.40239847, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.423563566848812}
done in step count: 0
reward sum = 0.9997190431460437
running average episode reward sum: 0.13081120130074297
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.86778212, 6.40239847, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.423563566848812}
episode index:10
target Thresh 1.9852866058051313
target distance 1.0
model initialize at round 10
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.05887544, 10.11167812,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.9477275124096184}
done in step count: 0
reward sum = 0.9990705290898738
running average episode reward sum: 0.20974386746339124
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.05887544, 10.11167812,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.9477275124096184}
episode index:11
target Thresh 1.9937918360174836
target distance 1.0
model initialize at round 11
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.66744241, 11.88353105,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.9126640539710649}
done in step count: 99
reward sum = -0.23813504431765758
running average episode reward sum: 0.17242062481497047
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 1.13343395, 11.84420439,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.408561395584223}
episode index:12
target Thresh 2.002292814677706
target distance 2.0
model initialize at round 12
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([9.57303262, 9.79885423, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.2747753927818304}
done in step count: 3
reward sum = 0.9594605447912447
running average episode reward sum: 0.23296215712083773
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.89313107, 11.86946633,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.2464568999952756}
episode index:13
target Thresh 2.0107895439110446
target distance 2.0
model initialize at round 13
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([4.55510949, 5.        , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 2.743826619190316}
done in step count: 99
reward sum = -0.2718381495999446
running average episode reward sum: 0.19690499235506756
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 1.12949787, 11.86644461,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 7.9144630096097455}
episode index:14
target Thresh 2.0192820258416786
target distance 2.0
model initialize at round 14
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([12.11798656,  8.26070166,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.941555127449364}
done in step count: 99
reward sum = -0.24043656410772987
running average episode reward sum: 0.16774888859088105
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 1.12992873, 11.87007127,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 14.145545991334139}
episode index:15
target Thresh 2.027770262592733
target distance 2.0
model initialize at round 15
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([ 7.02619884, 11.86775028,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 3.48136218546156}
done in step count: 99
reward sum = -0.22615954660246737
running average episode reward sum: 0.1431296113912968
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([ 1.12877169, 11.85970561,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 8.374613499957068}
episode index:16
target Thresh 2.0362542562862624
target distance 2.0
model initialize at round 16
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.01979937, 11.86209838,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.0252934467005796}
done in step count: 99
reward sum = -0.2762753025242664
running average episode reward sum: 0.11845873410214601
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 1.12739773, 11.84522754,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 13.183292795707892}
episode index:17
target Thresh 2.044734009043269
target distance 2.0
model initialize at round 17
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 2.        , 10.20919919,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.1506663899535052}
done in step count: 99
reward sum = -0.2234483655935567
running average episode reward sum: 0.09946389523016252
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([1.11474151, 8.6887533 , 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 3.696833490248786}
episode index:18
target Thresh 2.05320952298369
target distance 1.0
model initialize at round 18
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([2., 7., 0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.236067977499764}
done in step count: 31
reward sum = 0.6355857193260936
running average episode reward sum: 0.1276808333404747
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.95196258, 7.3358596 , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1607390893348404}
episode index:19
target Thresh 2.0616808002264015
target distance 2.0
model initialize at round 19
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.89520824,  5.78678441,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9202492487274967}
done in step count: 0
reward sum = 0.9975973149403206
running average episode reward sum: 0.171176657420467
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.89520824,  5.78678441,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9202492487274967}
episode index:20
target Thresh 2.0701478428892255
target distance 1.0
model initialize at round 20
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([1.13804319, 8.8649892 , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.053068310959081}
done in step count: 99
reward sum = -0.2641760145452414
running average episode reward sum: 0.15044557780305232
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 1.12992873, 11.87007127,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.298211051806369}
episode index:21
target Thresh 2.078610653088921
target distance 2.0
model initialize at round 21
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([ 8.02619884, 11.86775028,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 3.48136218546156}
done in step count: 99
reward sum = -0.2828071951071524
running average episode reward sum: 0.13075226994349756
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([ 1.15312529, 11.87245333,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 9.30151494751662}
episode index:22
target Thresh 2.087069232941195
target distance 1.0
model initialize at round 22
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.57214866, 11.90503183,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.9524863715149983}
done in step count: 99
reward sum = -0.26052406560933494
running average episode reward sum: 0.11374025535424397
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 1.12992873, 11.87007127,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 13.995572290679055}
episode index:23
target Thresh 2.095523584560687
target distance 2.0
model initialize at round 23
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([13.13224963,  5.97380131,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.028577421940211}
done in step count: 99
reward sum = -0.27740233558935556
running average episode reward sum: 0.09744264739826065
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 1.12198762, 11.79699908,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 16.357091698120424}
episode index:24
target Thresh 2.1039737100609868
target distance 2.0
model initialize at round 24
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([1.12926547, 8.8875883 , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 4.071766563079829}
done in step count: 99
reward sum = -0.21987172345276287
running average episode reward sum: 0.08475007256421971
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([ 1.13260997, 11.870393  ,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 6.533256430899263}
episode index:25
target Thresh 2.112419611554628
target distance 2.0
model initialize at round 25
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([11., 11.,  0.]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 4.1231056256176375}
done in step count: 99
reward sum = -0.20886443513853734
running average episode reward sum: 0.07345720688334444
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.41852466, 11.90428509,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.9497344962860799}
episode index:26
target Thresh 2.120861291153082
target distance 2.0
model initialize at round 26
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.        , 8.84636629, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.8463662862777497}
done in step count: 99
reward sum = -0.2270544093323905
running average episode reward sum: 0.0623271470235024
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 6.8933352 , 11.86454113,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 7.449383434718423}
episode index:27
target Thresh 2.129298750966771
target distance 2.0
model initialize at round 27
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.99505755, 11.86740305,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.867417127752727}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.09560117749428343
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.99505755, 11.86740305,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.867417127752727}
episode index:28
target Thresh 2.1377319931050613
target distance 2.0
model initialize at round 28
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([15.50609242, 11.91301406,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 2.66722587750833}
done in step count: 10
reward sum = 0.8566061741612436
running average episode reward sum: 0.12184272910348895
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.65880876, 11.89136201,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9544305653307452}
episode index:29
target Thresh 2.146161019676261
target distance 1.0
model initialize at round 29
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([12.80303743,  7.62706727,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.886901773689515}
done in step count: 18
reward sum = 0.7761647201559618
running average episode reward sum: 0.1436534621385714
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.1896251 ,  5.38133067,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.42587646262780354}
episode index:30
target Thresh 2.154585832787629
target distance 2.0
model initialize at round 30
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.33537575,  5.36548646,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.332939862981652}
done in step count: 2
reward sum = 0.9748803193198224
running average episode reward sum: 0.17046723172506337
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.50868843,  7.58856751,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7779303518332044}
episode index:31
target Thresh 2.1630064345453666
target distance 2.0
model initialize at round 31
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([10.43638158,  9.64443135,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.6940878049190924}
done in step count: 51
reward sum = 0.4798770595499537
running average episode reward sum: 0.18013628884459118
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.86022132, 11.8705656 ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8817156817102793}
episode index:32
target Thresh 2.171422827054623
target distance 2.0
model initialize at round 32
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([12.01828678, 11.86334976,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 4.904366631863979}
done in step count: 99
reward sum = -0.24139172637637793
running average episode reward sum: 0.16736271262577393
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.54873778, 11.89734007,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.948845981374443}
episode index:33
target Thresh 2.1798350124195034
target distance 2.0
model initialize at round 33
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.84184003, 2.39232266, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.6279222297928542}
done in step count: 0
reward sum = 0.9973402809879651
running average episode reward sum: 0.1917738175776031
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.84184003, 2.39232266, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.6279222297928542}
episode index:34
target Thresh 2.188242992743045
target distance 1.0
model initialize at round 34
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([4.        , 3.48220301, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.7879948974205715}
done in step count: 99
reward sum = -0.24256419747682226
running average episode reward sum: 0.1793641600046195
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([15.41450383, 11.90700468,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 15.882967203507572}
episode index:35
target Thresh 2.196646770127249
target distance 2.0
model initialize at round 35
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.23010594, 11.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.23010593652741562}
done in step count: 0
reward sum = 0.9969602886245532
running average episode reward sum: 0.20207516357739544
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.23010594, 11.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.23010593652741562}
episode index:36
target Thresh 2.2050463466730568
target distance 2.0
model initialize at round 36
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.47448885,  6.        ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 4.0343725613955135}
done in step count: 99
reward sum = -0.2527972387061418
running average episode reward sum: 0.18978131486702957
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.25591489, 11.88323607,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 9.886548823116726}
episode index:37
target Thresh 2.213441724480367
target distance 2.0
model initialize at round 37
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.75098035, 6.        , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 4.190471344919748}
done in step count: 14
reward sum = 0.8226493996946875
running average episode reward sum: 0.20643573815196792
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.39958133, 1.60718403, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7175004970971268}
episode index:38
target Thresh 2.2218329056480166
target distance 1.0
model initialize at round 38
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.36515629, 6.88919461, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.2794198497407445}
done in step count: 99
reward sum = -0.22227918417657686
running average episode reward sum: 0.1954430478358514
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 9.17545591, 10.82401121,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 6.7905298051755105}
episode index:39
target Thresh 2.2302198922738077
target distance 2.0
model initialize at round 39
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.7041055 , 10.97923112,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.29662249353059583}
done in step count: 0
reward sum = 0.997821695025245
running average episode reward sum: 0.21550251401558623
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.7041055 , 10.97923112,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.29662249353059583}
episode index:40
target Thresh 2.238602686454484
target distance 2.0
model initialize at round 40
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.        , 5.52513719, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 3.525137186050456}
done in step count: 8
reward sum = 0.8884952155703937
running average episode reward sum: 0.23191697015106935
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.09813457, 2.37320949, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9760361567647605}
episode index:41
target Thresh 2.2469812902857433
target distance 2.0
model initialize at round 41
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 1.10161899, 10.36286442,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.9675839062771496}
done in step count: 24
reward sum = 0.7357690644119335
running average episode reward sum: 0.24391344858585182
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.29710299, 11.88957521,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.1337584633523425}
episode index:42
target Thresh 2.2553557058622413
target distance 2.0
model initialize at round 42
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([2.        , 6.24853981, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.658498225168694}
done in step count: 99
reward sum = -0.19805379204640475
running average episode reward sum: 0.23363514066417143
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.79987913, 3.89426049, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.110613733319729}
episode index:43
target Thresh 2.2637259352775763
target distance 1.0
model initialize at round 43
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.89730732, 1.35584343, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.8730753496883596}
done in step count: 8
reward sum = 0.8954583745440843
running average episode reward sum: 0.2486765777978058
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.86541326, 3.09005325, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8700860280187641}
episode index:44
target Thresh 2.2720919806243076
target distance 2.0
model initialize at round 44
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([16.        ,  4.64166093,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0622649810670326}
done in step count: 0
reward sum = 0.9949787120231842
running average episode reward sum: 0.2652610696694809
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([16.        ,  4.64166093,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0622649810670326}
episode index:45
target Thresh 2.280453843993948
target distance 2.0
model initialize at round 45
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.5757174171303632e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.2811032203333046
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.5757174171303632e-14}
episode index:46
target Thresh 2.2888115274769616
target distance 2.0
model initialize at round 46
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.93273628, 11.        ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.06726372241975298}
done in step count: 0
reward sum = 0.994198397803714
running average episode reward sum: 0.29627545815182393
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.93273628, 11.        ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.06726372241975298}
episode index:47
target Thresh 2.297165033162771
target distance 2.0
model initialize at round 47
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([12.1897331 , 11.87437479,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 3.613441778351181}
done in step count: 99
reward sum = -0.1970821967698767
running average episode reward sum: 0.2859971736742885
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([13.08751198,  4.52526025,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 5.43654541299644}
episode index:48
target Thresh 2.3055143631397508
target distance 1.0
model initialize at round 48
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.35596496,  9.81147599,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.4028056023880879}
done in step count: 0
reward sum = 0.9994110967290639
running average episode reward sum: 0.3005566414917329
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.35596496,  9.81147599,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.4028056023880879}
episode index:49
target Thresh 2.3138595194952343
target distance 2.0
model initialize at round 49
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([5.44471455, 9.63473272, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.812926616294261}
done in step count: 99
reward sum = -0.19940572451429808
running average episode reward sum: 0.29055739417161225
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.79443535, 1.1216878 , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 6.9240382920386265}
episode index:50
target Thresh 2.3222005043155125
target distance 1.0
model initialize at round 50
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([2.45594889, 8.        , 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 3.048932864315351}
done in step count: 99
reward sum = -0.28058449426616344
running average episode reward sum: 0.2793585336140088
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([1.12932233, 1.13511399, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 10.040687768355259}
episode index:51
target Thresh 2.330537319685827
target distance 2.0
model initialize at round 51
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.27786517, 7.09576416, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 2.454621520999837}
done in step count: 2
reward sum = 0.9742287920265219
running average episode reward sum: 0.29272142319886485
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.9779588 , 5.27253962, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.2734294363376876}
episode index:52
target Thresh 2.3388699676903846
target distance 2.0
model initialize at round 52
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.12779926, 2.82686391, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.4618421285493552}
done in step count: 90
reward sum = 0.22202898666867812
running average episode reward sum: 0.2913876036416915
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.74518248, 3.32623703, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.0046160795315529}
episode index:53
target Thresh 2.34719845041235
target distance 1.0
model initialize at round 53
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.08212519, 11.18060863,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 2.3659138129913746}
done in step count: 2
reward sum = 0.9727385646911533
running average episode reward sum: 0.30400521403149633
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.5729869 ,  8.38675964,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.7472642909656347}
episode index:54
target Thresh 2.3555227699338404
target distance 2.0
model initialize at round 54
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.41033876, 5.        , 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 4.3043028308652955}
done in step count: 99
reward sum = -0.2060714829780043
running average episode reward sum: 0.29473109226768723
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([1.80741233, 1.12362145, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 8.922731111578448}
episode index:55
target Thresh 2.3638429283359366
target distance 1.0
model initialize at round 55
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.75789136, 8.11826311, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.9143722043635159}
done in step count: 0
reward sum = 0.9965527583566929
running average episode reward sum: 0.30726362201927665
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.75789136, 8.11826311, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.9143722043635159}
episode index:56
target Thresh 2.3721589276986785
target distance 2.0
model initialize at round 56
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.02462101,  7.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 4.117203440986532}
done in step count: 99
reward sum = -0.24905381912899058
running average episode reward sum: 0.2975036669114123
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.12006603,  3.69525462,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 7.542775139210846}
episode index:57
target Thresh 2.3804707701010637
target distance 1.0
model initialize at round 57
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 6.00007141, 11.81391484,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 3.108380430834205}
done in step count: 7
reward sum = 0.9060573078513616
running average episode reward sum: 0.3079959710655493
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.08803133, 11.86882847,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.2595831745363941}
episode index:58
target Thresh 2.388778457621058
target distance 2.0
model initialize at round 58
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.99505755, 11.86740305,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.8674095874256116}
done in step count: 99
reward sum = -0.26119060727304644
running average episode reward sum: 0.2983487409242172
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([1.12992873, 1.12992873, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 11.219449351176882}
episode index:59
target Thresh 2.397081992335578
target distance 2.0
model initialize at round 59
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([7.0433155 , 8.13206337, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.3571371496986986}
done in step count: 99
reward sum = -0.24234906648675264
running average episode reward sum: 0.289337110800701
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([2.60537284, 1.09581952, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 8.602299842286412}
episode index:60
target Thresh 2.405381376320511
target distance 1.0
model initialize at round 60
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.02619884, 1.13224972, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.3043309558476481}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.30088896144667926
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.02619884, 1.13224972, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.3043309558476481}
episode index:61
target Thresh 2.4136766116507005
target distance 1.0
model initialize at round 61
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.13998153,  2.02673791,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.298795923233911}
done in step count: 0
reward sum = 0.995655193410353
running average episode reward sum: 0.31209486841383527
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.13998153,  2.02673791,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.298795923233911}
episode index:62
target Thresh 2.421967700399957
target distance 2.0
model initialize at round 62
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([16.06776953,  7.        ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.296882848251003}
done in step count: 99
reward sum = -0.2745357488981727
running average episode reward sum: 0.30278327131364463
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.95681621,  1.13233937,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 6.867796400625113}
episode index:63
target Thresh 2.4302546446410505
target distance 1.0
model initialize at round 63
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.35931933,  5.        ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.187632822827656}
done in step count: 0
reward sum = 0.9966921458101823
running average episode reward sum: 0.31362559747765306
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.35931933,  5.        ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.187632822827656}
episode index:64
target Thresh 2.43853744644572
target distance 2.0
model initialize at round 64
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.17696381,  9.        ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 2.007813783369319}
done in step count: 99
reward sum = -0.21659314063215163
running average episode reward sum: 0.3054683861221176
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([1.13614342, 1.14490087, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 13.2548456160372}
episode index:65
target Thresh 2.446816107884667
target distance 1.0
model initialize at round 65
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.13558301, 2.08542142, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.8663728247591957}
done in step count: 36
reward sum = 0.6165801799334538
running average episode reward sum: 0.3101822011798651
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.62434473, 1.09860895, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9765360793934411}
episode index:66
target Thresh 2.455090631027552
target distance 2.0
model initialize at round 66
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([3.41068596, 6.        , 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 3.3949844077603233}
done in step count: 99
reward sum = -0.19767791988125524
running average episode reward sum: 0.3026021993729827
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([2.79865698, 1.12195131, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 8.179826539477281}
episode index:67
target Thresh 2.4633610179430114
target distance 1.0
model initialize at round 67
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.07751393,  6.78465688,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.2166987930342783}
done in step count: 99
reward sum = -0.1988279939989858
running average episode reward sum: 0.29522822594104203
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([2.35833685, 1.10616908, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 14.903832184843127}
episode index:68
target Thresh 2.4716272706986366
target distance 1.0
model initialize at round 68
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.69325292,  9.        ,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.2167989203725689}
done in step count: 0
reward sum = 0.9969294296455785
running average episode reward sum: 0.30539780860342663
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.69325292,  9.        ,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.2167989203725689}
episode index:69
target Thresh 2.4798893913609947
target distance 1.0
model initialize at round 69
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.74124239, 8.12033764, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.8973892775179095}
done in step count: 84
reward sum = 0.2718131942212466
running average episode reward sum: 0.3049180283979669
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.45045168, 9.92214291, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.5550361076244312}
episode index:70
target Thresh 2.488147381995614
target distance 2.0
model initialize at round 70
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.50390744, 8.53353262, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.614201042371935}
done in step count: 1
reward sum = 0.9866691306630929
running average episode reward sum: 0.3145201565988841
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.40520489, 6.53353262, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.6178857675345204}
episode index:71
target Thresh 2.4964012446669916
target distance 2.0
model initialize at round 71
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.48123813,  5.30034494,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5672717338533125}
done in step count: 0
reward sum = 0.9961878298873644
running average episode reward sum: 0.3239877631723352
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.48123813,  5.30034494,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5672717338533125}
episode index:72
target Thresh 2.504650981438596
target distance 2.0
model initialize at round 72
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.59396541,  5.        ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 4.305894301078223}
done in step count: 99
reward sum = -0.22574483653884372
running average episode reward sum: 0.3164571796146478
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([2.54461577, 1.0890426 , 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 13.921532779922936}
episode index:73
target Thresh 2.5128965943728585
target distance 1.0
model initialize at round 73
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([7.02619869, 8.13224963, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 3.51169271361777}
done in step count: 37
reward sum = 0.5648862549101343
running average episode reward sum: 0.31981432928080306
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.29826158, 10.18119139,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.724753153983652}
episode index:74
target Thresh 2.5211380855311845
target distance 2.0
model initialize at round 74
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([16.88402959,  6.33799539,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 3.0026304990610284}
done in step count: 14
reward sum = 0.8411346664472608
running average episode reward sum: 0.3267652671096892
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.10836132,  4.94137015,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.2966099228501025}
episode index:75
target Thresh 2.529375456973947
target distance 2.0
model initialize at round 75
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.06030631,  1.9424814 ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 2.058402207121805}
done in step count: 90
reward sum = 0.19675898010296586
running average episode reward sum: 0.32505465807012707
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.7440541 ,  3.27211793,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0408788681029457}
episode index:76
target Thresh 2.537608710760488
target distance 1.0
model initialize at round 76
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.74465811, 6.38062596, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.404039718024864}
done in step count: 2
reward sum = 0.9752651016745832
running average episode reward sum: 0.3334989495455096
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.77068663, 5.84629786, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8768150839162171}
episode index:77
target Thresh 2.54583784894912
target distance 2.0
model initialize at round 77
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.        , 8.78131628, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.7813162803649796}
done in step count: 99
reward sum = -0.22593975503518662
running average episode reward sum: 0.3263266584611417
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.12992873,  1.12992873,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.91630224420082}
episode index:78
target Thresh 2.554062873597128
target distance 2.0
model initialize at round 78
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2., 4., 0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.9999999999999805}
done in step count: 82
reward sum = 0.20469348889182623
running average episode reward sum: 0.32478699808684663
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.66097127, 7.1548786 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.6788743623785507}
episode index:79
target Thresh 2.5622837867607693
target distance 2.0
model initialize at round 79
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.19000119, 7.21528816, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.8606987494279474}
done in step count: 1
reward sum = 0.9859894454275514
running average episode reward sum: 0.33305202867860545
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.87175483, 5.85015738, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8597757822982864}
episode index:80
target Thresh 2.5705005904952736
target distance 2.0
model initialize at round 80
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.52512807,  3.34817517,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.248230614246487}
done in step count: 89
reward sum = 0.2468611737892945
running average episode reward sum: 0.3319879440503423
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.14027975,  4.19275578,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1793057892038616}
episode index:81
target Thresh 2.5787132868548364
target distance 1.0
model initialize at round 81
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([6.99505019, 8.13259643, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.5295214641399792}
done in step count: 99
reward sum = -0.19264265198421898
running average episode reward sum: 0.32559000995235987
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([2.92038383, 1.16154452, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 8.904148830693458}
episode index:82
target Thresh 2.586921877892639
target distance 1.0
model initialize at round 82
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.12474513, 9.05286694, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.1354852777487564}
done in step count: 0
reward sum = 0.9983784155887993
running average episode reward sum: 0.3336958943576182
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.12474513, 9.05286694, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.1354852777487564}
episode index:83
target Thresh 2.595126365660823
target distance 1.0
model initialize at round 83
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4., 2., 0.]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 3.1622776601683555}
done in step count: 99
reward sum = -0.1561192408721792
running average episode reward sum: 0.32786476179535873
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([1.12900315, 1.13806457, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 4.291290540960306}
episode index:84
target Thresh 2.603326752210515
target distance 2.0
model initialize at round 84
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2., 2., 0.]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.999999999999981}
done in step count: 99
reward sum = -0.20011551368876876
running average episode reward sum: 0.3216532291426043
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.58483734, 1.09615687, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 2.9621511830449365}
episode index:85
target Thresh 2.6115230395918054
target distance 1.0
model initialize at round 85
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.13225265,  5.02619426,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.7174232800836355}
done in step count: 99
reward sum = -0.20888909118097088
running average episode reward sum: 0.3154841323946558
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([16.87025638,  1.13145196,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 6.159359952308533}
episode index:86
target Thresh 2.619715229853775
target distance 2.0
model initialize at round 86
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.        , 4.76011896, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.2560974625321886}
done in step count: 0
reward sum = 0.9958470209256323
running average episode reward sum: 0.32330439548121875
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.        , 4.76011896, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.2560974625321886}
episode index:87
target Thresh 2.627903325044465
target distance 2.0
model initialize at round 87
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([ 9.       , 10.7955395,  0.       ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 2.687742936007884}
done in step count: 99
reward sum = -0.20441137000170456
running average episode reward sum: 0.31730762541891283
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([14.0805834 ,  6.15140686,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 7.6321126922765155}
episode index:88
target Thresh 2.6360873272109018
target distance 2.0
model initialize at round 88
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.18303424,  6.        ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8169657588004213}
done in step count: 0
reward sum = 0.9964994253517755
running average episode reward sum: 0.3249389939574843
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.18303424,  6.        ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8169657588004213}
episode index:89
target Thresh 2.644267238399084
target distance 2.0
model initialize at round 89
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.46184283,  5.        ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.523078965152695}
done in step count: 24
reward sum = 0.7191376321824365
running average episode reward sum: 0.32931897882665045
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.81574167,  3.85518699,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.1818541591118472}
episode index:90
target Thresh 2.65244306065399
target distance 1.0
model initialize at round 90
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([6.63066653, 8.09758213, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.246475034371834}
done in step count: 99
reward sum = -0.2364678327478811
running average episode reward sum: 0.32310154133682045
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.61999182, 1.11688073, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 8.90472895857515}
episode index:91
target Thresh 2.6606147960195794
target distance 2.0
model initialize at round 91
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.61718178, 9.        , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.6171817779540465}
done in step count: 0
reward sum = 0.9955658603134043
running average episode reward sum: 0.3304109361083051
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.61718178, 9.        , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.6171817779540465}
episode index:92
target Thresh 2.6687824465387786
target distance 2.0
model initialize at round 92
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([9.78547829, 9.        , 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.4291236964804415}
done in step count: 38
reward sum = 0.5788653010628427
running average episode reward sum: 0.3330824884196442
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.94452828, 11.86779726,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.2826557434612358}
episode index:93
target Thresh 2.676946014253508
target distance 1.0
model initialize at round 93
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.05859637, 10.27422833,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.1886905822048224}
done in step count: 0
reward sum = 0.9999410239668906
running average episode reward sum: 0.3401767281595085
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.05859637, 10.27422833,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.1886905822048224}
episode index:94
target Thresh 2.685105501204654
target distance 1.0
model initialize at round 94
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([7.98724943, 9.58174527, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 2.0307873681552886}
done in step count: 99
reward sum = -0.2585788641468164
running average episode reward sum: 0.33387403771417873
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([16.84589095,  1.14873844,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 13.99922075094022}
episode index:95
target Thresh 2.6932609094320896
target distance 1.0
model initialize at round 95
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.87155778, 1.14357628, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6361027214618806}
done in step count: 99
reward sum = -0.22975193926242682
running average episode reward sum: 0.3280029337873391
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.14943578, 1.1278853 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.632375617506072}
episode index:96
target Thresh 2.701412240974669
target distance 2.0
model initialize at round 96
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.43858957,  1.71780312,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.311370889604411}
done in step count: 23
reward sum = 0.7483441675111748
running average episode reward sum: 0.3323363485679972
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.81824085,  5.29488793,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8697568536015623}
episode index:97
target Thresh 2.7095594978702238
target distance 2.0
model initialize at round 97
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.94904494,  3.50577474,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.7798999579623023}
done in step count: 18
reward sum = 0.8007831166296512
running average episode reward sum: 0.33711641762985084
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.07587986,  1.13062589,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.872679261454309}
episode index:98
target Thresh 2.7177026821555668
target distance 2.0
model initialize at round 98
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.52475486, 1.08747146, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 2.415390990883049}
done in step count: 17
reward sum = 0.7771739790416147
running average episode reward sum: 0.3415614435026969
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.85345746, 2.19391828, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.1739494764440606}
episode index:99
target Thresh 2.725841795866497
target distance 2.0
model initialize at round 99
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.55762088,  5.6468749 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.38218083318307}
done in step count: 33
reward sum = 0.6146728562330182
running average episode reward sum: 0.34429255763000016
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.9013685 ,  8.62204407,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9774026045714995}
episode index:100
target Thresh 2.7339768410377894
target distance 2.0
model initialize at round 100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([3.4006809, 2.3761611, 0.       ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.4503117439262065}
done in step count: 5
reward sum = 0.9342822582176213
running average episode reward sum: 0.350134039814036
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.72361773, 1.1392713 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.12448944330406}
episode index:101
target Thresh 2.7421078197032096
target distance 2.0
model initialize at round 101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.38791478,  9.16701746,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.42234193014973753}
done in step count: 0
reward sum = 0.9973672728768339
running average episode reward sum: 0.35647946366759287
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.38791478,  9.16701746,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.42234193014973753}
episode index:102
target Thresh 2.7502347338954993
target distance 1.0
model initialize at round 102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4.36563015, 7.        , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.8204981358244434}
done in step count: 25
reward sum = 0.711237666449921
running average episode reward sum: 0.3599237180635378
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.47550858, 9.99230925, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.475570765442808}
episode index:103
target Thresh 2.7583575856463876
target distance 1.0
model initialize at round 103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([1.13090029, 5.07979984, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.8708024319883974}
done in step count: 99
reward sum = -0.26324671577310543
running average episode reward sum: 0.35393169466126234
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.87007127, 1.12992873, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 4.298211051806369}
episode index:104
target Thresh 2.766476376986588
target distance 2.0
model initialize at round 104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.99505755, 8.13259695, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.8674095874256116}
done in step count: 11
reward sum = 0.8492507540851393
running average episode reward sum: 0.35864901903672786
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.16117176, 9.25891676, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.7584066845541282}
episode index:105
target Thresh 2.774591109945798
target distance 1.0
model initialize at round 105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.12611894, 7.82101372, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.0198413488511084}
done in step count: 13
reward sum = 0.8201903473369722
running average episode reward sum: 0.36300318251125846
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.1384067 , 6.21122236, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8871064815921951}
episode index:106
target Thresh 2.782701786552701
target distance 2.0
model initialize at round 106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.70408356,  1.49007308,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.6660170921347346}
done in step count: 43
reward sum = 0.5478023296093965
running average episode reward sum: 0.36473027734395136
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.53699688,  2.94055176,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5402774633621052}
episode index:107
target Thresh 2.790808408834966
target distance 2.0
model initialize at round 107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 4.36563073, 11.6598854 ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.5167056544797612}
done in step count: 1
reward sum = 0.9851909850716916
running average episode reward sum: 0.370475283896986
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.5093519 , 11.90501455,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0294595170552674}
episode index:108
target Thresh 2.798910978819247
target distance 2.0
model initialize at round 108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 8.88340545, 10.96536255,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.1171316613538385}
done in step count: 1
reward sum = 0.987882606035137
running average episode reward sum: 0.3761395712560516
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.5601218 , 11.78023474,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.895689162869072}
episode index:109
target Thresh 2.807009498531191
target distance 1.0
model initialize at round 109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.23725009, 7.9477917 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9770346479400229}
done in step count: 0
reward sum = 0.9995616151425059
running average episode reward sum: 0.381807044382292
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.23725009, 7.9477917 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9770346479400229}
episode index:110
target Thresh 2.8151039699954272
target distance 2.0
model initialize at round 110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.065005  , 5.23049617, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.9170645639104267}
done in step count: 99
reward sum = -0.19394796412983537
running average episode reward sum: 0.37662006232362427
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.67980124, 1.10788954, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 8.068899539406019}
episode index:111
target Thresh 2.8231943952355674
target distance 1.0
model initialize at round 111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.62460116, 8.09637564, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.0984824049199802}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.38213238319756837
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.62460116, 8.09637564, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.0984824049199802}
episode index:112
target Thresh 2.8312807762742236
target distance 2.0
model initialize at round 112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.61521161, 6.        , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.3847883939742842}
done in step count: 0
reward sum = 0.9968889531698307
running average episode reward sum: 0.3875727068256415
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.61521161, 6.        , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.3847883939742842}
episode index:113
target Thresh 2.83936311513299
target distance 2.0
model initialize at round 113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.88831723, 9.        , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 3.513171844058489}
done in step count: 61
reward sum = 0.36002744386308416
running average episode reward sum: 0.3873310817119348
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.4381848 , 10.03638537,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.0585646252827579}
episode index:114
target Thresh 2.847441413832451
target distance 2.0
model initialize at round 114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.98033609, 11.86216915,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 3.025404285825971}
done in step count: 99
reward sum = -0.24581330754360706
running average episode reward sum: 0.38182547832710406
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([4.87007127, 1.12992873, 0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 11.315200526160314}
episode index:115
target Thresh 2.8555156743921817
target distance 2.0
model initialize at round 115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.22608329,  7.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 4.3757034253772344}
done in step count: 99
reward sum = -0.21214698381557762
running average episode reward sum: 0.3767050260672533
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.8713199 ,  1.14119652,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.03483154573637}
episode index:116
target Thresh 2.8635858988307454
target distance 1.0
model initialize at round 116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.94475937, 1.58702481, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4140546031642487}
done in step count: 6
reward sum = 0.9270544364538853
running average episode reward sum: 0.381408867181669
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.74026811, 3.20337357, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7676963471338366}
episode index:117
target Thresh 2.871652089165702
target distance 1.0
model initialize at round 117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.78093749,  7.        ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.1470592371275754}
done in step count: 77
reward sum = 0.3028956541635416
running average episode reward sum: 0.380743500969651
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.36023626,  5.40101418,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5390570793992606}
episode index:118
target Thresh 2.8797142474135953
target distance 2.0
model initialize at round 118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.13471913,  2.        ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 4.002268012627052}
done in step count: 95
reward sum = 0.23516086999767766
running average episode reward sum: 0.379520117516105
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.24874986,  5.66059985,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.4207956209088009}
episode index:119
target Thresh 2.887772375589965
target distance 2.0
model initialize at round 119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.01928174, 11.86238947,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 2.862454410593232}
done in step count: 6
reward sum = 0.9265223099512857
running average episode reward sum: 0.3840784691197315
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.58436731, 8.09363315, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.9971215558836615}
episode index:120
target Thresh 2.895826475709346
target distance 1.0
model initialize at round 120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.4743027 ,  1.08783798,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0281063154806174}
done in step count: 0
reward sum = 0.9968806777238668
running average episode reward sum: 0.3891429501825756
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.4743027 ,  1.08783798,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0281063154806174}
episode index:121
target Thresh 2.9038765497852626
target distance 2.0
model initialize at round 121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.        ,  2.78699243,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.365028624018378}
done in step count: 99
reward sum = -0.20780230627372043
running average episode reward sum: 0.3842499562771961
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.13349021,  1.12950533,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 5.215896581976571}
episode index:122
target Thresh 2.9119225998302345
target distance 2.0
model initialize at round 122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([14.84319758,  5.        ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.1568024158478245}
done in step count: 99
reward sum = -0.2513566567510941
running average episode reward sum: 0.37908242283794175
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.87007127,  1.12992873,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.966670605948408}
episode index:123
target Thresh 2.919964627855771
target distance 2.0
model initialize at round 123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.86716726, 5.00359263, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.8671707160808788}
done in step count: 67
reward sum = 0.33133111384462566
running average episode reward sum: 0.37869733163638275
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.86373072, 4.75316631, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.898308205118782}
episode index:124
target Thresh 2.9280026358723803
target distance 2.0
model initialize at round 124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([ 8.36330077, 11.87723457,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 3.1838762208523277}
done in step count: 99
reward sum = -0.21994293794306805
running average episode reward sum: 0.3739082094797471
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([4.86658479, 1.12951392, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 8.154508666269958}
episode index:125
target Thresh 2.9360366258895634
target distance 2.0
model initialize at round 125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.98517085,  1.13259755,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.3350150625278667}
done in step count: 1
reward sum = 0.9817406771523504
running average episode reward sum: 0.37873227668349796
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.78717494,  1.12383169,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1778434937001998}
episode index:126
target Thresh 2.944066599915822
target distance 2.0
model initialize at round 126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([16.86740305,  8.00494245,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.8674073064180114}
done in step count: 99
reward sum = -0.17748533195805408
running average episode reward sum: 0.374352610473722
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.55804508,  1.09075725,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 7.082735335055669}
episode index:127
target Thresh 2.9520925599586447
target distance 2.0
model initialize at round 127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.17214608, 7.        , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.168204222128599}
done in step count: 99
reward sum = -0.18096644719497235
running average episode reward sum: 0.37001418033568534
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.87007127, 1.12992873, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 10.278891773644544}
episode index:128
target Thresh 2.960114508024526
target distance 1.0
model initialize at round 128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.72207583,  5.00000015,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.2334476902564397}
done in step count: 0
reward sum = 0.9947758147251863
running average episode reward sum: 0.3748572937805652
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.72207583,  5.00000015,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.2334476902564397}
episode index:129
target Thresh 2.9681324461189496
target distance 1.0
model initialize at round 129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([12.8334069 ,  7.60146466,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 3.0189986136465743}
done in step count: 99
reward sum = -0.2239566314577793
running average episode reward sum: 0.3702510328171933
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([14.81960577,  1.92480769,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 8.932979292323848}
episode index:130
target Thresh 2.976146376246401
target distance 1.0
model initialize at round 130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.67842865,  5.15978837,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.9612863828896454}
done in step count: 47
reward sum = 0.5181752940444396
running average episode reward sum: 0.3713802256509891
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.91654039,  6.33796886,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.1306332395256402}
episode index:131
target Thresh 2.984156300410362
target distance 2.0
model initialize at round 131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.        , 10.10378683,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.099170410340514}
done in step count: 82
reward sum = 0.23627813329603853
running average episode reward sum: 0.3703567249513304
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.44493777, 11.91146071,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0671713569549133}
episode index:132
target Thresh 2.992162220613318
target distance 2.0
model initialize at round 132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.9999999999999822}
done in step count: 5
reward sum = 0.9280693013295286
running average episode reward sum: 0.37455005259327173
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.59188752,  8.77405651,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.46648285888937807}
episode index:133
target Thresh 3.000164138856743
target distance 3.0
model initialize at round 133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.68309939, 5.        , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 5.046446747500278}
done in step count: 99
reward sum = -0.26945969490804217
running average episode reward sum: 0.36974400970147087
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.85058921, 1.12788762, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.06306010587424}
episode index:134
target Thresh 3.008162057141119
target distance 3.0
model initialize at round 134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.59872997,  7.        ,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.1655374617172567}
done in step count: 99
reward sum = -0.23145208382560545
running average episode reward sum: 0.365290705304974
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.65718101,  1.10389515,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 4.90809205388527}
episode index:135
target Thresh 3.01615597746593
target distance 3.0
model initialize at round 135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.24331582, 6.87744796, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 3.3712973913575306}
done in step count: 99
reward sum = -0.1934571538804635
running average episode reward sum: 0.36118226516390456
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.15973606,  1.12695461,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.59974762803843}
episode index:136
target Thresh 3.0241459018296477
target distance 2.0
model initialize at round 136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([5.66337677, 7.65846446, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 2.9821625446092286}
done in step count: 99
reward sum = -0.2149703198107059
running average episode reward sum: 0.35697677184292204
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.12868706,  1.14112761,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 12.819991299467867}
episode index:137
target Thresh 3.032131832229759
target distance 3.0
model initialize at round 137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([11.9521395 ,  9.94442248,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 4.156574127881658}
done in step count: 13
reward sum = 0.8387643611316801
running average episode reward sum: 0.3604679862580579
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.91272214,  8.46484975,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0580394636723411}
episode index:138
target Thresh 3.0401137706627455
target distance 2.0
model initialize at round 138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.95063406, 9.24552059, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.2504343362485351}
done in step count: 0
reward sum = 0.9970135726845636
running average episode reward sum: 0.36504745090860835
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.95063406, 9.24552059, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.2504343362485351}
episode index:139
target Thresh 3.0480917191240917
target distance 2.0
model initialize at round 139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.01244056,  7.12044573,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 3.8795742182150517}
done in step count: 13
reward sum = 0.8264739085800717
running average episode reward sum: 0.3683433541776902
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.89211285, 11.35003664,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9583271815859695}
episode index:140
target Thresh 3.0560656796082837
target distance 3.0
model initialize at round 140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.8674032 , 5.99505776, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.3275154321449871}
done in step count: 99
reward sum = -0.1780486885162789
running average episode reward sum: 0.36446823330752026
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([16.85218776,  1.12803706,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 14.130062947517521}
episode index:141
target Thresh 3.064035654108813
target distance 3.0
model initialize at round 141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.88462973,  4.        ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.006633150670246}
done in step count: 1
reward sum = 0.9832703362893408
running average episode reward sum: 0.36882599459612464
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.36695844,  2.        ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.065203499905142}
episode index:142
target Thresh 3.072001644618174
target distance 2.0
model initialize at round 142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.57866085, 8.        , 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 3.029443295871126}
done in step count: 59
reward sum = 0.36866864360865914
running average episode reward sum: 0.36882489423956893
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.49739141, 11.91251961,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.0417808902126566}
episode index:143
target Thresh 3.0799636531278636
target distance 1.0
model initialize at round 143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 6.41180747, 11.90263438,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.575182345556534}
done in step count: 27
reward sum = 0.684573942614854
running average episode reward sum: 0.3710175959643973
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.85025461, 11.87192103,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.217858442970303}
episode index:144
target Thresh 3.087921681628382
target distance 3.0
model initialize at round 144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 3.55564594, 11.90735851,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.297388022201673}
done in step count: 80
reward sum = 0.2645700317790102
running average episode reward sum: 0.3702834748320843
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.11372431, 9.5941398 , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.066998925339726}
episode index:145
target Thresh 3.0958757321092363
target distance 3.0
model initialize at round 145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([ 9.5857048 , 10.65617442,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 3.476776748692956}
done in step count: 99
reward sum = -0.20199625310475036
running average episode reward sum: 0.3663637506681333
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([15.0647423 , 11.86696138,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 2.7836496855986765}
episode index:146
target Thresh 3.1038258065589446
target distance 2.0
model initialize at round 146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.97454826, 11.85650503,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.8566794829335536}
done in step count: 99
reward sum = -0.18865920445839532
running average episode reward sum: 0.36258808430672834
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.34705415, 11.89573525,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.9272412234600782}
episode index:147
target Thresh 3.1117719069650205
target distance 3.0
model initialize at round 147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([14.06071072, 11.86873893,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 2.1487885960588065}
done in step count: 99
reward sum = -0.19594084376103504
running average episode reward sum: 0.3588142401981623
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([11.74502102, 11.07924438,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.655216196520118}
episode index:148
target Thresh 3.119714035313989
target distance 2.0
model initialize at round 148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([14.95655779, 11.8598675 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.044275574208863}
done in step count: 99
reward sum = -0.19919725756553694
running average episode reward sum: 0.35506919658901
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 1.47917726, 11.89510898,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 14.806618417021603}
episode index:149
target Thresh 3.127652193591383
target distance 2.0
model initialize at round 149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.        , 6.42855012, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 3.1460857753084546}
done in step count: 99
reward sum = -0.24242928609548378
running average episode reward sum: 0.35108587337111336
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 1.12992873, 11.87007127,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 8.377071743440359}
episode index:150
target Thresh 3.135586383781746
target distance 3.0
model initialize at round 150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.16589916, 6.        , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.5360080877116118}
done in step count: 1
reward sum = 0.9837946297291073
running average episode reward sum: 0.3552759975854047
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.62812912, 8.        , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.1809090554410657}
episode index:151
target Thresh 3.14351660786862
target distance 1.0
model initialize at round 151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.00494245, 11.86740305,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 3.0351506972012103}
done in step count: 99
reward sum = -0.206779580954033
running average episode reward sum: 0.35157826351606625
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([16.30498103, 11.88889036,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 4.38959988202233}
episode index:152
target Thresh 3.1514428678345636
target distance 1.0
model initialize at round 152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.47969988, 11.91137002,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0299064499358321}
done in step count: 0
reward sum = 0.9953814935143881
running average episode reward sum: 0.3557861277644213
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.47969988, 11.91137002,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0299064499358321}
episode index:153
target Thresh 3.15936516566114
target distance 2.0
model initialize at round 153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.10625565,  9.        ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 4.098631352320753}
done in step count: 19
reward sum = 0.7669707218069367
running average episode reward sum: 0.3584561575958662
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.11405362,  4.4185134 ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.05972999281963}
episode index:154
target Thresh 3.1672835033289295
target distance 1.0
model initialize at round 154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.36497084,  1.09984736,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.101606469534096}
done in step count: 0
reward sum = 0.9949602472056024
running average episode reward sum: 0.3625626355933484
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.36497084,  1.09984736,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.101606469534096}
episode index:155
target Thresh 3.17519788281751
target distance 2.0
model initialize at round 155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.38056517,  8.1698935 ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.9978037913777937}
done in step count: 44
reward sum = 0.5637564961620831
running average episode reward sum: 0.3638523398277634
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.13310119,  7.10079213,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8727385590947345}
episode index:156
target Thresh 3.1831083061054772
target distance 1.0
model initialize at round 156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.13607454,  3.62767315,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.3964133765993783}
done in step count: 0
reward sum = 0.9993362541017067
running average episode reward sum: 0.36790000807154644
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.13607454,  3.62767315,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.3964133765993783}
episode index:157
target Thresh 3.1910147751704425
target distance 3.0
model initialize at round 157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.47851783,  7.12729526,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.9196534385758435}
done in step count: 2
reward sum = 0.9756462786941064
running average episode reward sum: 0.3717465034552335
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.57049572, 10.4507587 ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6226213328990746}
episode index:158
target Thresh 3.1989172919890168
target distance 1.0
model initialize at round 158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([14.48966146, 11.        ,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.7941826136078638}
done in step count: 43
reward sum = 0.5476384835789099
running average episode reward sum: 0.3728527423239359
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.936619  , 10.07128808,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.9393280254046228}
episode index:159
target Thresh 3.206815858536829
target distance 3.0
model initialize at round 159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.37265968,  7.0938611 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.155598265958467}
done in step count: 1
reward sum = 0.9853838385352466
running average episode reward sum: 0.3766810616752566
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.32276583,  6.29306769,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.43596611369603244}
episode index:160
target Thresh 3.214710476788528
target distance 2.0
model initialize at round 160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 8.02815568, 10.18110907,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 3.2503454410982973}
done in step count: 99
reward sum = -0.18013196228686265
running average episode reward sum: 0.3732225956879142
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([14.17115828, 11.87317535,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 9.610685761294082}
episode index:161
target Thresh 3.2226011487177617
target distance 1.0
model initialize at round 161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 3.95656145, 11.        ,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 2.216982138028365}
done in step count: 99
reward sum = -0.20625811636326835
running average episode reward sum: 0.36964555425549955
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 4.79536208, 11.8788855 ,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 3.3928316643234497}
episode index:162
target Thresh 3.230487876297203
target distance 1.0
model initialize at round 162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.91143888,  6.        ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 3.0013068939312357}
done in step count: 99
reward sum = -0.20435916232222193
running average episode reward sum: 0.3661240529268019
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 1.14735788, 11.87192011,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 17.300634220990908}
episode index:163
target Thresh 3.238370661498529
target distance 3.0
model initialize at round 163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.131097169678476}
done in step count: 99
reward sum = -0.19567677041860626
running average episode reward sum: 0.3626984381503055
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([16.87059045,  1.13433377,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.29447133631135}
episode index:164
target Thresh 3.246249506292436
target distance 1.0
model initialize at round 164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.99505755,  1.13259695,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.1754637807056256}
done in step count: 2
reward sum = 0.9667634091452221
running average episode reward sum: 0.36635943797451714
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.73202282,  1.1178326 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.1463318623237873}
episode index:165
target Thresh 3.2541244126486424
target distance 2.0
model initialize at round 165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.49354382, 1.08808867, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 2.955626071933443}
done in step count: 99
reward sum = -0.20077278630672366
running average episode reward sum: 0.3629429787921
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 2.24576194, 11.85591708,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 7.8920408161220905}
episode index:166
target Thresh 3.261995382535867
target distance 2.0
model initialize at round 166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.04757452,  3.70147181,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7030832304762629}
done in step count: 0
reward sum = 0.9967719787041678
running average episode reward sum: 0.3667383620251064
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.04757452,  3.70147181,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7030832304762629}
episode index:167
target Thresh 3.269862417921857
target distance 1.0
model initialize at round 167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.47137472,  4.97762764,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 2.047060270003041}
done in step count: 99
reward sum = -0.16911906864773943
running average episode reward sum: 0.3635487344615776
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.39493581, 11.87582944,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 8.984775686083664}
episode index:168
target Thresh 3.277725520773368
target distance 3.0
model initialize at round 168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.95587838,  7.50771833,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.4925603808345187}
done in step count: 24
reward sum = 0.7241899460338543
running average episode reward sum: 0.3656827061276857
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.69681293, 11.89120508,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.1312801350232242}
episode index:169
target Thresh 3.2855846930561774
target distance 3.0
model initialize at round 169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.24414718,  9.        ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.005957235758161}
done in step count: 31
reward sum = 0.6789073230170531
running average episode reward sum: 0.36752520387409376
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.16227489,  4.38131156,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.41440517029834684}
episode index:170
target Thresh 3.293439936735078
target distance 3.0
model initialize at round 170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([4.12441194, 3.31302261, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.7286788584742423}
done in step count: 3
reward sum = 0.9640168633706805
running average episode reward sum: 0.3710134591927872
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.69646377, 1.86559802, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7093135224016914}
episode index:171
target Thresh 3.3012912537738823
target distance 3.0
model initialize at round 171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.65973312,  3.17539251,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.8560641287233977}
done in step count: 7
reward sum = 0.9193996271799936
running average episode reward sum: 0.3742017508671315
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.90178948,  5.48829718,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0255039744296934}
episode index:172
target Thresh 3.3091386461354197
target distance 2.0
model initialize at round 172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([ 6.58495787, 11.90448698,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 2.962806159429911}
done in step count: 62
reward sum = 0.4210888291778052
running average episode reward sum: 0.3744727744411816
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.51959901, 8.8437053 , 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.5051862514751193}
episode index:173
target Thresh 3.316982115781532
target distance 3.0
model initialize at round 173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 5.2417297 , 11.87859529,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.072516917406165}
done in step count: 99
reward sum = -0.2100750294623023
running average episode reward sum: 0.3711133043038053
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([16.86875618, 11.85467448,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 13.433666694823128}
episode index:174
target Thresh 3.3248216646730953
target distance 2.0
model initialize at round 174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([16.88434413,  8.73668674,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.9760401602029143}
done in step count: 99
reward sum = -0.19843980616397586
running average episode reward sum: 0.36785871510113227
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([15.7485087 ,  5.40760813,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.8461340139263847}
episode index:175
target Thresh 3.332657294769992
target distance 3.0
model initialize at round 175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.162277660168369}
done in step count: 99
reward sum = -0.1828172936738434
running average episode reward sum: 0.3647298741421835
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([16.52080018, 11.91255719,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.6543030950525335}
episode index:176
target Thresh 3.3404890080311307
target distance 1.0
model initialize at round 176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.,  4.,  0.]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.4142135623730694}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.3682850725945179
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.,  4.,  0.]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.4142135623730694}
episode index:177
target Thresh 3.3483168064144415
target distance 2.0
model initialize at round 177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 5.00689244, 10.        ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.0068924427032133}
done in step count: 57
reward sum = 0.4382695216562831
running average episode reward sum: 0.36867824365666263
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.11095588, 10.68975219,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.1252366515019403}
episode index:178
target Thresh 3.3561406918768717
target distance 3.0
model initialize at round 178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 3.        , 11.54453968,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.840000771343311}
done in step count: 75
reward sum = 0.3214755785168609
running average episode reward sum: 0.3684145416167755
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.29769253, 9.41587205, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.6556113961276666}
episode index:179
target Thresh 3.3639606663743904
target distance 3.0
model initialize at round 179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 9.45688504, 11.9089704 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.7909301911484716}
done in step count: 1
reward sum = 0.9827372669070752
running average episode reward sum: 0.3718274456461661
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.08528178, 11.86588597,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.2595506892238877}
episode index:180
target Thresh 3.371776731861998
target distance 2.0
model initialize at round 180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([13.89979053,  5.        ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.1002094745635222}
done in step count: 99
reward sum = -0.18778506395073333
running average episode reward sum: 0.3687356638251888
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.45408539, 10.72475785,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 5.742738545721525}
episode index:181
target Thresh 3.3795888902937063
target distance 2.0
model initialize at round 181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.        ,  3.31759119,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0492207413982721}
done in step count: 0
reward sum = 0.9948771249904469
running average episode reward sum: 0.372176001523899
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.        ,  3.31759119,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0492207413982721}
episode index:182
target Thresh 3.3873971436225547
target distance 3.0
model initialize at round 182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([15.51974225,  9.1687839 ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 4.522892660277061}
done in step count: 12
reward sum = 0.8485194074112513
running average episode reward sum: 0.3747789709549774
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.81975032,  8.69884203,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8733193572695661}
episode index:183
target Thresh 3.3952014938006077
target distance 1.0
model initialize at round 183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.21706015, 11.        ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.1004829963617717}
done in step count: 99
reward sum = -0.13544443031436768
running average episode reward sum: 0.37200601768720926
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.51283025, 11.90892071,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.939161769244755}
episode index:184
target Thresh 3.403001942778955
target distance 1.0
model initialize at round 184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 5.79964596, 11.87828404,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.9341779692542422}
done in step count: 9
reward sum = 0.880471336822537
running average episode reward sum: 0.37475447887172453
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.86054771, 11.8568317 ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.2143734695202484}
episode index:185
target Thresh 3.4107984925077037
target distance 2.0
model initialize at round 185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.15815771,  8.87687802,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.403603014536999}
done in step count: 66
reward sum = 0.367687202544826
running average episode reward sum: 0.3747164827624402
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.89690782, 10.14559429,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.17839757410194879}
episode index:186
target Thresh 3.418591144935996
target distance 1.0
model initialize at round 186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 4.11091795, 11.74009061,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.3348680889205244}
done in step count: 2
reward sum = 0.9718346980428356
running average episode reward sum: 0.377909628298699
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.33764927, 11.89132945,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9531396622544478}
episode index:187
target Thresh 3.426379902011994
target distance 2.0
model initialize at round 187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.09285006,  4.44281467,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0646015678005178}
done in step count: 0
reward sum = 0.9954678684425163
running average episode reward sum: 0.38119451255478315
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.09285006,  4.44281467,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0646015678005178}
episode index:188
target Thresh 3.434164765682885
target distance 2.0
model initialize at round 188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.78877366, 11.        ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.21122634410860286}
done in step count: 0
reward sum = 0.9958996980301494
running average episode reward sum: 0.38444692094354166
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.78877366, 11.        ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.21122634410860286}
episode index:189
target Thresh 3.4419457378948874
target distance 1.0
model initialize at round 189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.13642085, 5.        , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 3.12182141629665}
done in step count: 75
reward sum = 0.26358509256331775
running average episode reward sum: 0.3838108060573299
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.05338256, 2.28656243, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9890412553446328}
episode index:190
target Thresh 3.449722820593242
target distance 3.0
model initialize at round 190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.31061988,  1.11073365,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.9498908494118825}
done in step count: 99
reward sum = -0.17061540411917095
running average episode reward sum: 0.38090805103022785
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 7.64110545, 11.89255308,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.082688979370888}
episode index:191
target Thresh 3.4574960157222216
target distance 1.0
model initialize at round 191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.        , 10.13705254,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0093480062034823}
done in step count: 0
reward sum = 0.996030333287617
running average episode reward sum: 0.3841118129169851
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.        , 10.13705254,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0093480062034823}
episode index:192
target Thresh 3.465265325225124
target distance 3.0
model initialize at round 192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 5.01908732, 11.86510278,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.1253581696879755}
done in step count: 99
reward sum = -0.18110997782167645
running average episode reward sum: 0.381183202602277
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 6.32628317, 11.89042091,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.9975464290874436}
episode index:193
target Thresh 3.473030751044278
target distance 2.0
model initialize at round 193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.9311235,  7.       ,  0.       ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.106944236671665}
done in step count: 99
reward sum = -0.19893119465746179
running average episode reward sum: 0.3781929222040309
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([11.68265309, 11.8684414 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.166206930767983}
episode index:194
target Thresh 3.4807922951210357
target distance 1.0
model initialize at round 194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.61550064, 11.70100747,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.8089409755474666}
done in step count: 28
reward sum = 0.7051325810016877
running average episode reward sum: 0.3798695358388907
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.92002459, 10.86866229,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.2653139629660362}
episode index:195
target Thresh 3.4885499593957894
target distance 3.0
model initialize at round 195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([10.41580904, 10.79057491,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.4312142025191052}
done in step count: 1
reward sum = 0.9827756763608553
running average episode reward sum: 0.3829455875762477
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.42753625, 11.90598111,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0716886241619776}
episode index:196
target Thresh 3.496303745807951
target distance 2.0
model initialize at round 196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2., 9., 0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.60805543562022e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.3860473866251265
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2., 9., 0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.60805543562022e-14}
episode index:197
target Thresh 3.50405365629597
target distance 2.0
model initialize at round 197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.73297095, 9.84567034, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.7490420956958925}
done in step count: 0
reward sum = 0.9982596094230278
running average episode reward sum: 0.38913936754834827
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.73297095, 9.84567034, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.7490420956958925}
episode index:198
target Thresh 3.51179969279732
target distance 2.0
model initialize at round 198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 7.18006137, 10.19408584,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.9903955022938082}
done in step count: 1
reward sum = 0.9850315684499945
running average episode reward sum: 0.3921338007187083
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.31210292, 11.88816768,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.123407421957536}
episode index:199
target Thresh 3.5195418572485124
target distance 1.0
model initialize at round 199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.80925584, 6.        , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.2864272276250328}
done in step count: 0
reward sum = 0.9969727127366532
running average episode reward sum: 0.395157995278798
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.80925584, 6.        , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.2864272276250328}
episode index:200
target Thresh 3.5272801515850927
target distance 1.0
model initialize at round 200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.25420343, 4.99999756, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6547685200864004}
done in step count: 99
reward sum = -0.21621419430397842
running average episode reward sum: 0.3921163425943066
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.01439102, 11.86732289,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.733691513408013}
episode index:201
target Thresh 3.535014577741625
target distance 3.0
model initialize at round 201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.95484889, 11.        ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.9548488855362223}
done in step count: 1
reward sum = 0.9819058062705206
running average episode reward sum: 0.39503609241448584
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.65688157, 11.89222542,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.1079528822282545}
episode index:202
target Thresh 3.5427451376517234
target distance 3.0
model initialize at round 202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([12.14940107, 11.        ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.5248711188602586}
done in step count: 18
reward sum = 0.7976391692207856
running average episode reward sum: 0.39701935880269423
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.25940291,  8.15239666,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.30085645994695764}
episode index:203
target Thresh 3.550471833248025
target distance 1.0
model initialize at round 203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.16649142,  5.0365411 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8343091793676015}
done in step count: 0
reward sum = 0.9999782090156131
running average episode reward sum: 0.39997503944099283
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.16649142,  5.0365411 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8343091793676015}
episode index:204
target Thresh 3.5581946664622075
target distance 1.0
model initialize at round 204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.41603816,  9.14215064,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.9528039354276718}
done in step count: 99
reward sum = -0.20171337713525014
running average episode reward sum: 0.3970399739942795
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 9.62946433, 11.8815967 ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 6.626420323456131}
episode index:205
target Thresh 3.565913639224971
target distance 1.0
model initialize at round 205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.01896872, 11.86260205,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.3063325242542976}
done in step count: 0
reward sum = 0.9952068084927579
running average episode reward sum: 0.39994369649184486
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.01896872, 11.86260205,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.3063325242542976}
episode index:206
target Thresh 3.5736287534660676
target distance 1.0
model initialize at round 206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 3.1622776601683547}
done in step count: 99
reward sum = -0.2323134850198482
running average episode reward sum: 0.39688931397246474
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.27809354, 11.84948565,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.854410292714538}
episode index:207
target Thresh 3.5813400111142717
target distance 2.0
model initialize at round 207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.        , 10.71435332,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.984693254894157}
done in step count: 80
reward sum = 0.2882217090127117
running average episode reward sum: 0.3963668735640044
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.01417679,  8.31078709,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6893586986148147}
episode index:208
target Thresh 3.5890474140973962
target distance 1.0
model initialize at round 208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.85970587, 1.12752198, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.8777263894457845}
done in step count: 43
reward sum = 0.5537255255862382
running average episode reward sum: 0.39711978577463714
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.18871885, 2.3822808 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0196833422132394}
episode index:209
target Thresh 3.596750964342297
target distance 3.0
model initialize at round 209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.97093964, 8.1083231 , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 4.10842587568132}
done in step count: 99
reward sum = -0.18931105325322328
running average episode reward sum: 0.3943272579697426
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 6.34225503, 11.88252695,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 8.561828060011885}
episode index:210
target Thresh 3.604450663774858
target distance 1.0
model initialize at round 210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([4.90616544, 3.58567274, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 3.3106125701394395}
done in step count: 99
reward sum = -0.24290640316887008
running average episode reward sum: 0.3913071932250098
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 1.14208435, 11.84584704,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 9.88315350276232}
episode index:211
target Thresh 3.612146514320005
target distance 3.0
model initialize at round 211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.22591794, 7.65485144, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.5519754187346193}
done in step count: 99
reward sum = -0.17973451993783518
running average episode reward sum: 0.3886136002383926
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 2.93165343, 11.86823305,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.060739328329793}
episode index:212
target Thresh 3.6198385179016985
target distance 1.0
model initialize at round 212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([ 8.93709786, 11.86847333,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 2.8691629287700957}
done in step count: 99
reward sum = -0.15495170582317774
running average episode reward sum: 0.3860616504446763
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([16.51377709, 11.91279138,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 8.05861028482374}
episode index:213
target Thresh 3.6275266764429426
target distance 3.0
model initialize at round 213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([16.85425054,  9.98897187,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.984648685692232}
done in step count: 4
reward sum = 0.9427160591469307
running average episode reward sum: 0.38866283927038775
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.84420905, 11.87209817,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.2137726838335463}
episode index:214
target Thresh 3.6352109918657765
target distance 1.0
model initialize at round 214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.20155169, 4.99996795, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.2796310394433057}
done in step count: 0
reward sum = 0.9965203330287711
running average episode reward sum: 0.3914900834274035
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.20155169, 4.99996795, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.2796310394433057}
episode index:215
target Thresh 3.6428914660912755
target distance 3.0
model initialize at round 215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([13.95395446, 11.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 2.0460455417633554}
done in step count: 2
reward sum = 0.9705163955280915
running average episode reward sum: 0.39417076079824004
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.50518015, 11.84288948,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9826849262375488}
episode index:216
target Thresh 3.650568101039563
target distance 3.0
model initialize at round 216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.66141784, 5.88079429, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.13309733565876}
done in step count: 99
reward sum = -0.20478426286341075
running average episode reward sum: 0.39141059939887757
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.66868899, 11.89358539,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.821335304202535}
episode index:217
target Thresh 3.6582408986297965
target distance 1.0
model initialize at round 217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.87007127, 11.87007127,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.42556209940782}
done in step count: 97
reward sum = 0.20809223566714438
running average episode reward sum: 0.3905696894735026
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.57714593,  9.02311414,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.42348532881692624}
episode index:218
target Thresh 3.6659098607801726
target distance 3.0
model initialize at round 218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([1.96310413, 6.0354718 , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 3.65554836109121}
done in step count: 99
reward sum = -0.2017833556741779
running average episode reward sum: 0.38786488104817074
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 2.27376685, 11.88634537,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.052459057220904}
episode index:219
target Thresh 3.673574989407937
target distance 1.0
model initialize at round 219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 1.54197472, 10.        ,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0999032475905635}
done in step count: 0
reward sum = 0.9968426594413717
running average episode reward sum: 0.3906329618590489
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 1.54197472, 10.        ,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0999032475905635}
episode index:220
target Thresh 3.6812362864293675
target distance 3.0
model initialize at round 220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([14.26254952,  8.        ,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 2.4736875971645564}
done in step count: 74
reward sum = 0.33854968170831057
running average episode reward sum: 0.3903972909081406
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.07044164,  9.26788924,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.27699579520846956}
episode index:221
target Thresh 3.6888937537597895
target distance 3.0
model initialize at round 221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.        , 10.47814822,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.1084339051890761}
done in step count: 1
reward sum = 0.9851256816108392
running average episode reward sum: 0.3930762476230176
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.36873341, 9.86510348, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.3926339198215724}
episode index:222
target Thresh 3.6965473933135686
target distance 2.0
model initialize at round 222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([9.2118628, 9.2052536, 0.       ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.229121838082053}
done in step count: 2
reward sum = 0.9776637172069742
running average episode reward sum: 0.3956977160964883
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.18068863, 8.72391325, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.32995798695517425}
episode index:223
target Thresh 3.7041972070041207
target distance 1.0
model initialize at round 223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.26084447, 10.34375775,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.706182504429356}
done in step count: 0
reward sum = 0.9998603430696849
running average episode reward sum: 0.39839487068119006
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.26084447, 10.34375775,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.706182504429356}
episode index:224
target Thresh 3.7118431967438914
target distance 2.0
model initialize at round 224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.17512198,  4.72598302,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.5118115361939517}
done in step count: 81
reward sum = 0.2778324913653768
running average episode reward sum: 0.3978590378842309
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.26691524,  3.99265798,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.2340109932224093}
episode index:225
target Thresh 3.719485364444382
target distance 3.0
model initialize at round 225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.38614333, 9.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.173379740443025}
done in step count: 1
reward sum = 0.9852069982774722
running average episode reward sum: 0.40045792266473196
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.17620605, 7.5892849 , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9205016915881674}
episode index:226
target Thresh 3.7271237120161347
target distance 3.0
model initialize at round 226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.14223874,  7.9430306 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.175030129636155}
done in step count: 6
reward sum = 0.934954386059314
running average episode reward sum: 0.4028125326356332
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.88851905, 11.85290245,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8601573055217695}
episode index:227
target Thresh 3.7347582413687355
target distance 1.0
model initialize at round 227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.28010142, 10.20608795,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.3477485391731288}
done in step count: 0
reward sum = 0.9995794553310839
running average episode reward sum: 0.4054299314193852
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.28010142, 10.20608795,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.3477485391731288}
episode index:228
target Thresh 3.742388954410818
target distance 3.0
model initialize at round 228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.20757578,  7.76413589,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.4538465206705364}
done in step count: 9
reward sum = 0.8929892558753468
running average episode reward sum: 0.4075590114388436
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.28016089, 11.03946038,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7209198699749968}
episode index:229
target Thresh 3.7500158530500594
target distance 1.0
model initialize at round 229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.       ,  8.2890327,  0.       ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 2.497933283944393}
done in step count: 77
reward sum = 0.32802012767140387
running average episode reward sum: 0.40721319020507213
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.13089403,  5.51848651,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9935795998805598}
episode index:230
target Thresh 3.7576389391931837
target distance 1.0
model initialize at round 230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.42668998, 10.05761504,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.4305622302601442}
done in step count: 0
reward sum = 0.9976345044936823
running average episode reward sum: 0.409769126630564
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.42668998, 10.05761504,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.4305622302601442}
episode index:231
target Thresh 3.765258214745965
target distance 3.0
model initialize at round 231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([ 5.20543337, 11.12188148,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 2.7790016967539053}
done in step count: 99
reward sum = -0.17304316606260634
running average episode reward sum: 0.40725700467930026
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([ 7.06083841, 11.86872845,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 2.869373489365715}
episode index:232
target Thresh 3.772873681613217
target distance 3.0
model initialize at round 232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([13.18057341, 11.87639626,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 2.2168934386576526}
done in step count: 38
reward sum = 0.581465420207069
running average episode reward sum: 0.40800468028242376
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.58924935, 10.74150797,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.8476733861077392}
episode index:233
target Thresh 3.780485341698813
target distance 1.0
model initialize at round 233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.        ,  4.71544719,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.63491158349572}
done in step count: 1
reward sum = 0.9849723725192078
running average episode reward sum: 0.4104703541808715
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.14344323,  3.7634418 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.1474026709543672}
episode index:234
target Thresh 3.788093196905666
target distance 2.0
model initialize at round 234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([15.43677127,  4.11356473,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.2242549933845885}
done in step count: 4
reward sum = 0.9573254343978735
running average episode reward sum: 0.41279739707541196
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.22753274,  6.60026872,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.45995243745906295}
episode index:235
target Thresh 3.7956972491357384
target distance 2.0
model initialize at round 235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.4668389, 4.9424938, 0.       ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 2.990406411188239}
done in step count: 99
reward sum = -0.2111520823662898
running average episode reward sum: 0.41015354334896414
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([16.85980483, 11.82391033,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 16.988331408452}
episode index:236
target Thresh 3.8032975002900424
target distance 1.0
model initialize at round 236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.86199999, 8.14802694, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 3.4059938746357403}
done in step count: 28
reward sum = 0.660022424152234
running average episode reward sum: 0.41120784242408337
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.52792868, 11.91114489,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0530402215400576}
episode index:237
target Thresh 3.8108939522686436
target distance 3.0
model initialize at round 237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.50179027, 11.91297327,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.9445624304579594}
done in step count: 99
reward sum = -0.20055082396069415
running average episode reward sum: 0.40863742785944146
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 6.90305259, 11.86862599,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 8.08287879144646}
episode index:238
target Thresh 3.818486606970652
target distance 3.0
model initialize at round 238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.0000000000000195}
done in step count: 57
reward sum = 0.4286383213872911
running average episode reward sum: 0.40872111360641994
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.2118395 ,  8.59624111,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.45595747252325397}
episode index:239
target Thresh 3.826075466294235
target distance 3.0
model initialize at round 239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.94380558, 10.11771226,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0627336630621504}
done in step count: 8
reward sum = 0.9023946159367419
running average episode reward sum: 0.4107780865327963
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.88498973, 10.26108189,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9226974427010685}
episode index:240
target Thresh 3.833660532136606
target distance 2.0
model initialize at round 240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.        , 10.45958376,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.1366836469325408}
done in step count: 0
reward sum = 0.9954022120710836
running average episode reward sum: 0.41320391277984314
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.        , 10.45958376,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.1366836469325408}
episode index:241
target Thresh 3.8412418063940312
target distance 1.0
model initialize at round 241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16., 10.,  0.]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.2360679774997645}
done in step count: 99
reward sum = -0.23748918016549622
running average episode reward sum: 0.41051509834618466
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 7.99995866, 11.86735251,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 7.997311682774638}
episode index:242
target Thresh 3.848819290961828
target distance 3.0
model initialize at round 242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 6.97380116, 11.86775028,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 2.131097169678479}
done in step count: 65
reward sum = 0.35071636316668064
running average episode reward sum: 0.41026901301622787
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.47802978, 9.82280926, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.5098127435965213}
episode index:243
target Thresh 3.856392987734369
target distance 1.0
model initialize at round 243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.24162948, 11.15747309,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.28841390304220266}
done in step count: 0
reward sum = 0.9985638476049639
running average episode reward sum: 0.41268005742028013
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.24162948, 11.15747309,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.28841390304220266}
episode index:244
target Thresh 3.8639628986050765
target distance 1.0
model initialize at round 244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.76327002, 6.81207848, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 2.91382334383615}
done in step count: 99
reward sum = -0.2033255667463272
running average episode reward sum: 0.41016574875021233
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([12.59136659, 10.25742559,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.45206042886329}
episode index:245
target Thresh 3.8715290254664314
target distance 2.0
model initialize at round 245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([15.42612636,  8.60775661,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 3.4485064468739193}
done in step count: 5
reward sum = 0.9355918136134501
running average episode reward sum: 0.41230162706266454
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.0315992,  9.6423758,  0.       ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.643152530698694}
episode index:246
target Thresh 3.879091370209961
target distance 2.0
model initialize at round 246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.95684499, 11.86503444,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.2898980216871747}
done in step count: 0
reward sum = 0.9967001097441255
running average episode reward sum: 0.4146676128225085
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.95684499, 11.86503444,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.2898980216871747}
episode index:247
target Thresh 3.8866499347262575
target distance 1.0
model initialize at round 247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.07867241, 8.82337093, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.19335763514283702}
done in step count: 0
reward sum = 0.9991039510087224
running average episode reward sum: 0.41702421096035613
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.07867241, 8.82337093, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.19335763514283702}
episode index:248
target Thresh 3.8942047209049595
target distance 3.0
model initialize at round 248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([10.90615743, 11.3474021 ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 3.1455481701271832}
done in step count: 1
reward sum = 0.9869032486613399
running average episode reward sum: 0.4193128817944966
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.08588904,  9.99550724,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.3515300625844813}
episode index:249
target Thresh 3.901755730634762
target distance 1.0
model initialize at round 249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.37095785, 10.86797976,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6427467364122839}
done in step count: 0
reward sum = 0.9979965942472488
running average episode reward sum: 0.42162761664430765
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.37095785, 10.86797976,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6427467364122839}
episode index:250
target Thresh 3.909302965803417
target distance 1.0
model initialize at round 250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.        , 4.18216026, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0164557834462773}
done in step count: 0
reward sum = 0.9969751134917331
running average episode reward sum: 0.4239198377472854
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.        , 4.18216026, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0164557834462773}
episode index:251
target Thresh 3.9168464282977373
target distance 1.0
model initialize at round 251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([2.27252108, 1.9778564 , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.7276208389151029}
done in step count: 1
reward sum = 0.9889450788346129
running average episode reward sum: 0.42616200140239385
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.15955883, 1.66342568, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9053306721811246}
episode index:252
target Thresh 3.9243861200035854
target distance 1.0
model initialize at round 252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.31174445, 7.1837604 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.36187358199953806}
done in step count: 0
reward sum = 0.9996194023014588
running average episode reward sum: 0.4284286314454732
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.31174445, 7.1837604 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.36187358199953806}
episode index:253
target Thresh 3.931922042805885
target distance 2.0
model initialize at round 253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.77763534,  5.        ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.7776353359221098}
done in step count: 0
reward sum = 0.9969629152425252
running average episode reward sum: 0.43066695539743005
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.77763534,  5.        ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.7776353359221098}
episode index:254
target Thresh 3.9394541985886162
target distance 2.0
model initialize at round 254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14., 11.,  0.]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.9999999999999822}
done in step count: 99
reward sum = -0.21994505341474993
running average episode reward sum: 0.42811553575502936
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.11428435,  2.32569311,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.74301757585691}
episode index:255
target Thresh 3.9469825892348185
target distance 2.0
model initialize at round 255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.        ,  8.79778337,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.2022166252136586}
done in step count: 1
reward sum = 0.9864528953198057
running average episode reward sum: 0.43029654106582926
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.25368255,  9.17000222,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.1161926544883254}
episode index:256
target Thresh 3.9545072166265913
target distance 1.0
model initialize at round 256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.12478482,  5.18682009,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8949320458051189}
done in step count: 0
reward sum = 0.9969765964266248
running average episode reward sum: 0.432501521825988
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.12478482,  5.18682009,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8949320458051189}
episode index:257
target Thresh 3.9620280826450878
target distance 2.0
model initialize at round 257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.       , 5.6654532, 0.       ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 3.7994140507357295}
done in step count: 96
reward sum = 0.17537766019323178
running average episode reward sum: 0.43150491771113236
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.45672803, 1.09063693, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0592854324655874}
episode index:258
target Thresh 3.969545189170528
target distance 3.0
model initialize at round 258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.36534095, 8.11855245, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.7650256314970811}
done in step count: 2
reward sum = 0.971413468784061
running average episode reward sum: 0.43358950671141394
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.61065602, 6.07393146, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.00458532803885}
episode index:259
target Thresh 3.9770585380821863
target distance 3.0
model initialize at round 259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.        , 10.30889726,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.215575173153733}
done in step count: 1
reward sum = 0.9836709349196939
running average episode reward sum: 0.43570520451221506
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 6.        , 10.27511239,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.2350959664657863}
episode index:260
target Thresh 3.984568131258402
target distance 2.0
model initialize at round 260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([5.        , 8.77364254, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.9927692092433964}
done in step count: 14
reward sum = 0.8360113478701039
running average episode reward sum: 0.4372389445250805
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.43268696, 10.48047188,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7692551926294544}
episode index:261
target Thresh 3.992073970576572
target distance 2.0
model initialize at round 261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.        , 7.78130531, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.2186946868896422}
done in step count: 1
reward sum = 0.9807891895991364
running average episode reward sum: 0.4393135637810883
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.10749721, 9.68738303, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.1265241503548329}
episode index:262
target Thresh 3.999576057913156
target distance 3.0
model initialize at round 262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([10.57751462, 11.90419607,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.191931538164386}
done in step count: 99
reward sum = -0.22158567420852712
running average episode reward sum: 0.43680063892181226
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([11.3084291 , 11.89007369,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.730457404632269}
episode index:263
target Thresh 4.007074395143675
target distance 3.0
model initialize at round 263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([ 6.03305697, 11.        ,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 3.578093195696713}
done in step count: 18
reward sum = 0.772273245735589
running average episode reward sum: 0.43807136849307654
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.51595519, 9.32072519, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.6075149440875786}
episode index:264
target Thresh 4.014568984142717
target distance 3.0
model initialize at round 264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.8445157 ,  4.00007495,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.3089525109315174}
done in step count: 1
reward sum = 0.9823757661004107
running average episode reward sum: 0.4401253473519721
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.59711605,  2.88418853,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.41919896964115455}
episode index:265
target Thresh 4.022059826783926
target distance 4.0
model initialize at round 265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([5.44547013, 7.54479685, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 2.84568802592201}
done in step count: 6
reward sum = 0.9269276934037656
running average episode reward sum: 0.4419554313596856
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.83780731, 8.17868267, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.173236228248911}
episode index:266
target Thresh 4.029546924940015
target distance 1.0
model initialize at round 266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.94401057, 11.86773944,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.8685784498320333}
done in step count: 8
reward sum = 0.9033556557304325
running average episode reward sum: 0.443683522087666
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.11176532,  9.70770531,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.935091992455813}
episode index:267
target Thresh 4.037030280482755
target distance 2.0
model initialize at round 267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.35106719, 10.28682303,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7949022286359188}
done in step count: 0
reward sum = 0.9975792992313051
running average episode reward sum: 0.44575029737551536
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.35106719, 10.28682303,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7949022286359188}
episode index:268
target Thresh 4.044509895282987
target distance 3.0
model initialize at round 268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.        ,  7.64998126,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.6499812602997466}
done in step count: 1
reward sum = 0.9828260405017359
running average episode reward sum: 0.44774686147635634
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.49934435,  5.77118444,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.550465838579283}
episode index:269
target Thresh 4.051985771210616
target distance 3.0
model initialize at round 269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.73039162,  4.        ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 5.007263591794314}
done in step count: 63
reward sum = 0.3911311624723502
running average episode reward sum: 0.4475371737022674
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.36022663,  9.4789742 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7992035095501016}
episode index:270
target Thresh 4.059457910134611
target distance 3.0
model initialize at round 270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.10901396, 4.67630334, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 2.820719002927722}
done in step count: 99
reward sum = -0.2251733599640197
running average episode reward sum: 0.44505484700977194
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 1.09512379, 11.49934308,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 9.542343527568962}
episode index:271
target Thresh 4.066926313923005
target distance 3.0
model initialize at round 271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.16048171, 6.95871925, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.045511936741136}
done in step count: 2
reward sum = 0.972281209057474
running average episode reward sum: 0.44699317922318255
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.12379951, 10.44071561,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9807943443247497}
episode index:272
target Thresh 4.074390984442901
target distance 2.0
model initialize at round 272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.00494245, 11.86740305,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.8674095874256116}
done in step count: 99
reward sum = -0.25321844480533057
running average episode reward sum: 0.4444283014794884
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.64584879, 11.22035121,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.380716379301547}
episode index:273
target Thresh 4.081851923560463
target distance 2.0
model initialize at round 273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.00573564,  8.3692981 ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.3693029824221203}
done in step count: 99
reward sum = -0.14367297670122064
running average episode reward sum: 0.44228194644963176
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([12.84898243, 11.87350526,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 6.969212010255275}
episode index:274
target Thresh 4.0893091331409295
target distance 3.0
model initialize at round 274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14., 11.,  0.]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0000000000000178}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.44423023028221015
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.97380116, 11.86775028,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.304330955847637}
episode index:275
target Thresh 4.0967626150486
target distance 1.0
model initialize at round 275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([15.82633116, 11.87502848,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.6174830153210977}
done in step count: 32
reward sum = 0.6220848319124263
running average episode reward sum: 0.44487463101275443
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.39940694,  9.54085798,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7559916774069005}
episode index:276
target Thresh 4.104212371146849
target distance 2.0
model initialize at round 276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.61473131,  3.90634561,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.498301578722135}
done in step count: 99
reward sum = -0.15690326009962324
running average episode reward sum: 0.4427021476513379
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.45003619, 11.91030456,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.920517583659201}
episode index:277
target Thresh 4.111658403298112
target distance 1.0
model initialize at round 277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.57800967, 6.75488019, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.8476199971140463}
done in step count: 99
reward sum = -0.2186864141207456
running average episode reward sum: 0.4403230521053952
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 1.14847691, 11.5706358 ,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 7.1627116881285255}
episode index:278
target Thresh 4.119100713363897
target distance 1.0
model initialize at round 278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.10667456,  3.54500017,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.612480527775207}
done in step count: 9
reward sum = 0.8913979392689628
running average episode reward sum: 0.441939807973365
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.13254646,  5.03408652,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.2982543979800916}
episode index:279
target Thresh 4.126539303204784
target distance 4.0
model initialize at round 279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4.51340294, 8.5982244 , 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 4.700481679339627}
done in step count: 4
reward sum = 0.9522784302272043
running average episode reward sum: 0.4437624459099859
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.3744081 , 10.55054295,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.8333443290412282}
episode index:280
target Thresh 4.133974174680421
target distance 2.0
model initialize at round 280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.01955116, 10.77615988,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.2504815453046647}
done in step count: 0
reward sum = 0.9987685683373531
running average episode reward sum: 0.4457375566659552
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.01955116, 10.77615988,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.2504815453046647}
episode index:281
target Thresh 4.141405329649521
target distance 4.0
model initialize at round 281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([4.43758774, 6.68218827, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 4.192115368110974}
done in step count: 99
reward sum = -0.20744966900847264
running average episode reward sum: 0.4434212899082445
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 3.11484405, 11.87069441,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 4.312068448955408}
episode index:282
target Thresh 4.1488327699698795
target distance 4.0
model initialize at round 282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 7., 11.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.0000000000000195}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.4453104726308609
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.02619884, 11.86775028,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8681456786036921}
episode index:283
target Thresh 4.156256497498351
target distance 2.0
model initialize at round 283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.44403303, 9.        , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 4.252673439400976}
done in step count: 99
reward sum = -0.19894082386595077
running average episode reward sum: 0.4430419821502383
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 1.12687078, 11.83931723,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 6.894825219561758}
episode index:284
target Thresh 4.16367651409087
target distance 4.0
model initialize at round 284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.00141541,  7.98187844,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.179919895468095}
done in step count: 4
reward sum = 0.9518435948664437
running average episode reward sum: 0.4448272509667864
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.37021862, 11.68301695,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7769002415215406}
episode index:285
target Thresh 4.171092821602441
target distance 2.0
model initialize at round 285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([ 9.45248961, 11.08888757,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 2.159448751859282}
done in step count: 11
reward sum = 0.8765361017815544
running average episode reward sum: 0.4463367224731317
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.65465408,  8.10242878,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.1109482690643124}
episode index:286
target Thresh 4.178505421887138
target distance 1.0
model initialize at round 286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.80350733,  3.56361365,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.6458522909197955}
done in step count: 4
reward sum = 0.9526560024481343
running average episode reward sum: 0.44810090114900286
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.23189682,  4.6945647 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.3834929679418617}
episode index:287
target Thresh 4.185914316798116
target distance 3.0
model initialize at round 287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.14665371, 11.87021078,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.264765902273311}
done in step count: 3
reward sum = 0.9558646854256204
running average episode reward sum: 0.44986396984440774
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.76187481, 11.87856942,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.1629004471205129}
episode index:288
target Thresh 4.193319508187596
target distance 2.0
model initialize at round 288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.53833321, 9.        , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.4616667926311342}
done in step count: 0
reward sum = 0.9968401479726136
running average episode reward sum: 0.45175662097979946
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.53833321, 9.        , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.4616667926311342}
episode index:289
target Thresh 4.2007209979068705
target distance 4.0
model initialize at round 289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([12.25912608,  8.00039304,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 7.070986793453087}
done in step count: 99
reward sum = -0.1685231245280632
running average episode reward sum: 0.44961772530563443
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 8.33766673, 10.35565094,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.33703019924598}
episode index:290
target Thresh 4.208118787806323
target distance 4.0
model initialize at round 290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 8.33046615, 11.88484487,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.738167350462597}
done in step count: 6
reward sum = 0.9214916606210276
running average episode reward sum: 0.4512392852208076
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.28239295, 11.81662113,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8640693514905579}
episode index:291
target Thresh 4.215512879735395
target distance 2.0
model initialize at round 291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([11.95594722,  8.90981221,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.0479408589477228}
done in step count: 1
reward sum = 0.9854338362729157
running average episode reward sum: 0.4530687186148216
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.95594722,  9.07147658,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.9586156630018577}
episode index:292
target Thresh 4.222903275542608
target distance 4.0
model initialize at round 292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([14.        ,  4.02040744,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 5.3662223286736355}
done in step count: 9
reward sum = 0.892132032554477
running average episode reward sum: 0.45456722821871126
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.98984966,  9.15330951,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.1536451582551822}
episode index:293
target Thresh 4.230289977075564
target distance 2.0
model initialize at round 293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([ 7.98492623, 11.8674179 ,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 3.0318583537190977}
done in step count: 24
reward sum = 0.7241218740551415
running average episode reward sum: 0.45548408075556984
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.15487226, 8.37516155, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.6437456858605055}
episode index:294
target Thresh 4.237672986180939
target distance 4.0
model initialize at round 294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 6.187096  , 10.00773287,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 4.382241198044069}
done in step count: 3
reward sum = 0.9598011378531229
running average episode reward sum: 0.45719363010166325
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.15751266, 7.12045312, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8510545598988363}
episode index:295
target Thresh 4.245052304704481
target distance 4.0
model initialize at round 295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.26382228, 1.11689707, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 5.889015395708911}
done in step count: 9
reward sum = 0.8952679854270191
running average episode reward sum: 0.4586736110318165
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.85903941, 6.24122756, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.146160686545167}
episode index:296
target Thresh 4.2524279344910205
target distance 1.0
model initialize at round 296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([16.59458828,  9.62588549,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 3.072130589028425}
done in step count: 85
reward sum = 0.2521227656155467
running average episode reward sum: 0.4579781536398425
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.70330543,  6.42242502,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6493231272806665}
episode index:297
target Thresh 4.25979987738447
target distance 1.0
model initialize at round 297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 1.30069515, 10.99999995,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.0804264289010543}
done in step count: 72
reward sum = 0.2928926938948534
running average episode reward sum: 0.457424175587007
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.16561461, 8.59507656, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.024848811711322}
episode index:298
target Thresh 4.267168135227813
target distance 2.0
model initialize at round 298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.7272613 , 6.33557725, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.5207483667569264}
done in step count: 1
reward sum = 0.9839908424736251
running average episode reward sum: 0.4591852681184003
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13207666, 4.9323154 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8705585154217939}
episode index:299
target Thresh 4.274532709863112
target distance 3.0
model initialize at round 299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.       ,  4.3167665,  0.       ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.3167665004730083}
done in step count: 26
reward sum = 0.7078960611176823
running average episode reward sum: 0.4600143040950646
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.15231339,  1.39337178,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0423868721598137}
episode index:300
target Thresh 4.281893603131512
target distance 3.0
model initialize at round 300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.2360679774997827}
done in step count: 12
reward sum = 0.8516936171027345
running average episode reward sum: 0.4613155642711698
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.88420634, 10.02110566,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8844581956733524}
episode index:301
target Thresh 4.289250816873235
target distance 4.0
model initialize at round 301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.94673574, 8.        , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.000709144634605}
done in step count: 1
reward sum = 0.9831670609276689
running average episode reward sum: 0.4630435493594363
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.15078682, 10.        ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.15078681707382913}
episode index:302
target Thresh 4.296604352927585
target distance 3.0
model initialize at round 302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7., 11.,  0.]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.4142135623730963}
done in step count: 99
reward sum = -0.2201659647829118
running average episode reward sum: 0.4607887324810787
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 1.14026142, 11.86307399,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 7.108238748250051}
episode index:303
target Thresh 4.30395421313295
target distance 4.0
model initialize at round 303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.13224972, 5.97380116, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.204194245766446}
done in step count: 1
reward sum = 0.979702118643629
running average episode reward sum: 0.46249568440924493
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.13906636, 7.94404785, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8627498917106293}
episode index:304
target Thresh 4.311300399326789
target distance 1.0
model initialize at round 304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 2.2360679774997645}
done in step count: 99
reward sum = -0.13506973375279543
running average episode reward sum: 0.46053645353002515
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([11.47372336, 11.89107616,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 9.564824217621135}
episode index:305
target Thresh 4.318642913345652
target distance 1.0
model initialize at round 305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([13.90908134, 11.86915463,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 2.1642186903028664}
done in step count: 80
reward sum = 0.2761086529049036
running average episode reward sum: 0.45993374829922407
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.91597867, 10.31944981,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9700850996796061}
episode index:306
target Thresh 4.325981757025166
target distance 4.0
model initialize at round 306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.75793946, 8.        , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 6.004880790327542}
done in step count: 12
reward sum = 0.8667916168710711
running average episode reward sum: 0.4612590182294255
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.15053695, 1.23743811, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7772785888323422}
episode index:307
target Thresh 4.333316932200042
target distance 4.0
model initialize at round 307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.97380116,  1.13224972,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 4.964199980272004}
done in step count: 61
reward sum = 0.43655442649995513
running average episode reward sum: 0.4611788085160181
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.84528293,  6.47614008,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9701611264662751}
episode index:308
target Thresh 4.340648440704076
target distance 2.0
model initialize at round 308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.07098603, 7.45657969, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.5480370930268311}
done in step count: 0
reward sum = 0.9984050023302762
running average episode reward sum: 0.4629174046125044
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.07098603, 7.45657969, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.5480370930268311}
episode index:309
target Thresh 4.347976284370144
target distance 4.0
model initialize at round 309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.96970135,  2.17348433,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.8266356262517323}
done in step count: 99
reward sum = -0.2192149381096409
running average episode reward sum: 0.4607169777004975
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.48778874, 10.1664027 ,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 4.197769865688542}
episode index:310
target Thresh 4.35530046503021
target distance 3.0
model initialize at round 310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.60374864,  8.27045703,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 2.7797454624026385}
done in step count: 10
reward sum = 0.8821662892074239
running average episode reward sum: 0.4620721201812272
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.67467628,  5.19722364,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8661901623044247}
episode index:311
target Thresh 4.362620984515311
target distance 3.0
model initialize at round 311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.60343778,  6.92635965,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.0928971726793435}
done in step count: 18
reward sum = 0.7911162125852491
running average episode reward sum: 0.4631267486825221
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.87889508, 11.75386195,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.1579138119874501}
episode index:312
target Thresh 4.369937844655583
target distance 2.0
model initialize at round 312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.,  9.,  0.]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.9999999999999822}
done in step count: 99
reward sum = -0.23037605010347534
running average episode reward sum: 0.4609110847886371
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([12.79007339, 11.86941222,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 5.832221202063771}
episode index:313
target Thresh 4.377251047280241
target distance 1.0
model initialize at round 313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([8.36749566, 9.86787713, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.6378421435038948}
done in step count: 99
reward sum = -0.2225047207692773
running average episode reward sum: 0.45873460133144633
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.7741951 , 11.87520438,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.8887507276848499}
episode index:314
target Thresh 4.384560594217584
target distance 2.0
model initialize at round 314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.14583437, 7.9894468 , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 4.399274480054063}
done in step count: 99
reward sum = -0.21050617672288227
running average episode reward sum: 0.45661002743286117
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.74306437, 4.37295864, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.3111008068782153}
episode index:315
target Thresh 4.391866487294999
target distance 4.0
model initialize at round 315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([14.        ,  6.00886071,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 3.5981820782772127}
done in step count: 21
reward sum = 0.7561041310900525
running average episode reward sum: 0.45755779358367504
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.02598257,  8.24857097,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.230185161642635}
episode index:316
target Thresh 4.399168728338964
target distance 4.0
model initialize at round 316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.16531301, 6.34961987, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.3536967114766636}
done in step count: 55
reward sum = 0.44084858672979277
running average episode reward sum: 0.4575050831519593
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.39852513, 2.57880306, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5798526904540456}
episode index:317
target Thresh 4.40646731917503
target distance 4.0
model initialize at round 317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([3.56049979, 5.78843307, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 3.195390210872635}
done in step count: 15
reward sum = 0.8222755328812231
running average episode reward sum: 0.45865216003790044
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.3864937 , 2.04598308, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.134256698805552}
episode index:318
target Thresh 4.413762261627851
target distance 1.0
model initialize at round 318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.86473358,  2.42379689,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.581996599995513}
done in step count: 1
reward sum = 0.987814997738615
running average episode reward sum: 0.4603109777109434
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.22715533,  3.50740314,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.5424492732589165}
episode index:319
target Thresh 4.42105355752116
target distance 3.0
model initialize at round 319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.46389338, 7.        , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 5.021473595617009}
done in step count: 99
reward sum = -0.17626821261383932
running average episode reward sum: 0.45832166774117844
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 1.12992873, 11.87007127,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.045669392555292}
episode index:320
target Thresh 4.428341208677785
target distance 1.0
model initialize at round 320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.05936825,  8.        ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.000587374071112}
done in step count: 99
reward sum = -0.15041417530545345
running average episode reward sum: 0.45642529439835405
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 7.64492433, 11.87983349,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.365847300046198}
episode index:321
target Thresh 4.435625216919638
target distance 1.0
model initialize at round 321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.02359951,  3.84873748,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.9821804321107146}
done in step count: 99
reward sum = -0.1917456275814695
running average episode reward sum: 0.4544123412245037
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.32134925, 11.88905385,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 8.065670399103805}
episode index:322
target Thresh 4.4429055840677165
target distance 4.0
model initialize at round 322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.236067977499815}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.4560335414077364
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.99505755, 11.86740305,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8674171277527271}
episode index:323
target Thresh 4.450182311942115
target distance 1.0
model initialize at round 323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.        , 5.92969453, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 2.1734122922338734}
done in step count: 99
reward sum = -0.22357523724772302
running average episode reward sum: 0.4539359834489232
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 1.81309082, 11.87601931,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 7.9649503220749285}
episode index:324
target Thresh 4.457455402362017
target distance 3.0
model initialize at round 324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([16.82788795, 10.99984257,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 4.318807294609021}
done in step count: 14
reward sum = 0.8252976880656563
running average episode reward sum: 0.45507863484774397
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.05817228,  9.67191985,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.6744333194771625}
episode index:325
target Thresh 4.464724857145692
target distance 3.0
model initialize at round 325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.75819299, 4.08282298, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.975287578713184}
done in step count: 99
reward sum = -0.18294371347556637
running average episode reward sum: 0.453121511079881
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 1.71185077, 11.88730904,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.684044027843222}
episode index:326
target Thresh 4.471990678110506
target distance 1.0
model initialize at round 326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.85271764,  6.        ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 3.118834296685727}
done in step count: 99
reward sum = -0.20011022886737767
running average episode reward sum: 0.4511238604990026
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.77791718, 11.87363721,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 8.876415833152622}
episode index:327
target Thresh 4.479252867072914
target distance 2.0
model initialize at round 327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([2.27326936, 4.13529754, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 2.7461052948191367}
done in step count: 31
reward sum = 0.6451265588121184
running average episode reward sum: 0.4517153321402011
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.26122884, 2.18424607, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7613996615603201}
episode index:328
target Thresh 4.486511425848464
target distance 3.0
model initialize at round 328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.16116238, 7.        , 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 2.716859180368532}
done in step count: 12
reward sum = 0.8338681489312736
running average episode reward sum: 0.4528768908538518
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.31229959, 8.59355602, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.5125697481283348}
episode index:329
target Thresh 4.493766356251795
target distance 1.0
model initialize at round 329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.43244064,  6.        ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 3.0310072427928434}
done in step count: 99
reward sum = -0.17310209135491536
running average episode reward sum: 0.4509799848471586
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([10.5265865 , 11.81667847,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.476202855526754}
episode index:330
target Thresh 4.501017660096638
target distance 4.0
model initialize at round 330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 4.03625214, 10.05577588,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 4.186064559358052}
done in step count: 99
reward sum = -0.19622603953971818
running average episode reward sum: 0.4490246796375306
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 4.31450357, 11.89174973,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 6.036607867008239}
episode index:331
target Thresh 4.508265339195822
target distance 2.0
model initialize at round 331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.45644438, 6.        , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 3.048844487528603}
done in step count: 99
reward sum = -0.25695938576817634
running average episode reward sum: 0.4468982216092002
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 7.70649655, 11.8811767 ,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.623586453413312}
episode index:332
target Thresh 4.515509395361265
target distance 1.0
model initialize at round 332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([4.48452306, 2.92111325, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 2.4857751194210462}
done in step count: 99
reward sum = -0.22718768178151896
running average episode reward sum: 0.44487393961703586
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 1.13104179, 11.8032042 ,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 8.84598737081805}
episode index:333
target Thresh 4.522749830403982
target distance 3.0
model initialize at round 333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.66241884, 6.38632023, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 3.403105311445008}
done in step count: 99
reward sum = -0.23578954774847039
running average episode reward sum: 0.442836024984205
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 1.12992873, 11.87007127,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.322846858364787}
episode index:334
target Thresh 4.529986646134081
target distance 4.0
model initialize at round 334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 6.324555320336734}
done in step count: 78
reward sum = 0.29484849145784564
running average episode reward sum: 0.442394271152783
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.37519002,  4.34402047,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.5090359839787637}
episode index:335
target Thresh 4.537219844360765
target distance 1.0
model initialize at round 335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.49537683,  9.17781091,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.357228232281298}
done in step count: 8
reward sum = 0.9042107742231786
running average episode reward sum: 0.4437687250309687
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.2334139 , 11.88086555,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9112662393638737}
episode index:336
target Thresh 4.544449426892336
target distance 2.0
model initialize at round 336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.92469835, 11.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.002831161297325}
done in step count: 0
reward sum = 0.9942216524841213
running average episode reward sum: 0.4454021165070909
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.92469835, 11.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.002831161297325}
episode index:337
target Thresh 4.551675395536187
target distance 1.0
model initialize at round 337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.79403818,  6.        ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 2.151858876831262}
done in step count: 47
reward sum = 0.5227054128755366
running average episode reward sum: 0.44563082448451224
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.25163738,  3.25970362,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7818951997634723}
episode index:338
target Thresh 4.5588977520988125
target distance 2.0
model initialize at round 338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.42410636, 9.53847194, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.6267969024522475}
done in step count: 0
reward sum = 0.9979776633431248
running average episode reward sum: 0.4472601661920598
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.42410636, 9.53847194, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.6267969024522475}
episode index:339
target Thresh 4.5661164983858
target distance 1.0
model initialize at round 339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([11.15418112, 11.12801111,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.613859697340576}
done in step count: 21
reward sum = 0.7567008071988113
running average episode reward sum: 0.44817028572443257
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.74624271,  9.88240805,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.7554508921082036}
episode index:340
target Thresh 4.573331636201838
target distance 3.0
model initialize at round 340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.05831885, 7.704669  , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.2966431627633497}
done in step count: 1
reward sum = 0.982309192969722
running average episode reward sum: 0.44973667548175017
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.13942111, 8.96382857, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8613387237330914}
episode index:341
target Thresh 4.5805431673507115
target distance 2.0
model initialize at round 341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.       , 3.6755141, 0.       ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.051328254168236}
done in step count: 0
reward sum = 0.9948944894767381
running average episode reward sum: 0.45133070417764193
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.       , 3.6755141, 0.       ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.051328254168236}
episode index:342
target Thresh 4.5877510936352985
target distance 3.0
model initialize at round 342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 3.10000701, 11.86926909,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.027472224877748}
done in step count: 11
reward sum = 0.8605552608272405
running average episode reward sum: 0.4525237786868245
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.83112661, 11.87494875,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8910968139528761}
episode index:343
target Thresh 4.5949554168575855
target distance 2.0
model initialize at round 343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 3.26290083, 10.        ,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.1315053805759288}
done in step count: 99
reward sum = -0.2133459879280471
running average episode reward sum: 0.45058811076061844
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 1.3627577 , 11.89820071,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.706486559955477}
episode index:344
target Thresh 4.602156138818652
target distance 1.0
model initialize at round 344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.07904768, 11.        ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.201852214817212}
done in step count: 30
reward sum = 0.6533391524753516
running average episode reward sum: 0.4511757949395017
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.61707609,  8.15332543,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0476835073676656}
episode index:345
target Thresh 4.6093532613186765
target distance 1.0
model initialize at round 345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.49446828,  1.11029741,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.0232951767681977}
done in step count: 0
reward sum = 0.9991956923383101
running average episode reward sum: 0.4527596674753364
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.49446828,  1.11029741,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.0232951767681977}
episode index:346
target Thresh 4.616546786156942
target distance 4.0
model initialize at round 346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([10.       ,  9.3654933,  0.       ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 2.098237058918742}
done in step count: 6
reward sum = 0.9249884041433157
running average episode reward sum: 0.454120557206368
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.37520462, 10.14716435,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.4030333171355378}
episode index:347
target Thresh 4.62373671513183
target distance 1.0
model initialize at round 347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.        , 11.04196072,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.2736762229258303}
done in step count: 99
reward sum = -0.2354833126463378
running average episode reward sum: 0.4521389368906993
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([16.83940721, 11.87312126,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 4.039437969888911}
episode index:348
target Thresh 4.630923050040822
target distance 2.0
model initialize at round 348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.74173999,  4.70417893,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 3.2165574561577586}
done in step count: 99
reward sum = -0.1990520249458133
running average episode reward sum: 0.4502730602092193
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 1.12993951, 11.87007261,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 16.219025565936686}
episode index:349
target Thresh 4.638105792680501
target distance 3.0
model initialize at round 349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([10., 11.,  0.]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 7.071067811865449}
done in step count: 99
reward sum = -0.2014878862441617
running average episode reward sum: 0.4484108860764954
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.84540242, 11.57386535,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 5.871412529068984}
episode index:350
target Thresh 4.645284944846555
target distance 3.0
model initialize at round 350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.        ,  8.88978171,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.006055699592316}
done in step count: 1
reward sum = 0.9837615869983164
running average episode reward sum: 0.4499361017486373
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.        ,  9.48073709,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.109553130682825}
episode index:351
target Thresh 4.652460508333768
target distance 4.0
model initialize at round 351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([11.00153154,  9.64542818,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 3.0224018211927492}
done in step count: 12
reward sum = 0.8532455369720673
running average episode reward sum: 0.4510818671896129
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.61649622, 10.20313716,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.43398139885245113}
episode index:352
target Thresh 4.659632484936035
target distance 3.0
model initialize at round 352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.86740305,  2.00494245,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 5.069811430677354}
done in step count: 7
reward sum = 0.9058490162168392
running average episode reward sum: 0.45237015939648895
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.89144312,  6.69517287,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9421201731613775}
episode index:353
target Thresh 4.666800876446346
target distance 1.0
model initialize at round 353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 5.99505755, 11.86740305,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.1754637807056163}
done in step count: 39
reward sum = 0.5499659252426911
running average episode reward sum: 0.4526458536502918
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.73062557, 11.88167727,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.1450626722192319}
episode index:354
target Thresh 4.673965684656806
target distance 2.0
model initialize at round 354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16., 10.,  0.]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9999999999999805}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.45417079490819345
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16., 10.,  0.]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9999999999999805}
episode index:355
target Thresh 4.681126911358607
target distance 3.0
model initialize at round 355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([ 9.64435124, 11.        ,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.6845722156434615}
done in step count: 93
reward sum = 0.17004267124582578
running average episode reward sum: 0.4533726822012767
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.92336635, 10.80366947,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.2241282745413007}
episode index:356
target Thresh 4.6882845583420645
target distance 2.0
model initialize at round 356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.92718449, 5.23590779, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.2380509565699798}
done in step count: 1
reward sum = 0.9889676472498758
running average episode reward sum: 0.45487294820981616
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.21507961, 4.39186954, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.447013396914742}
episode index:357
target Thresh 4.695438627396588
target distance 2.0
model initialize at round 357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5.86761522, 9.9733882 , 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.13503302323129657}
done in step count: 0
reward sum = 0.99659356132398
running average episode reward sum: 0.45638613427996744
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5.86761522, 9.9733882 , 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.13503302323129657}
episode index:358
target Thresh 4.702589120310692
target distance 1.0
model initialize at round 358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.48798966, 2.06160939, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0576912848589997}
done in step count: 0
reward sum = 0.9983365684630946
running average episode reward sum: 0.4578957455172463
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.48798966, 2.06160939, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0576912848589997}
episode index:359
target Thresh 4.709736038872004
target distance 4.0
model initialize at round 359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([3.55745792, 6.20011282, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 5.12740075759807}
done in step count: 99
reward sum = -0.2102925522155904
running average episode reward sum: 0.4560396669124329
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 3.49432063, 11.91297793,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 3.99365401957084}
episode index:360
target Thresh 4.716879384867249
target distance 1.0
model initialize at round 360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([13.79084015,  9.        ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.97999786033454}
done in step count: 95
reward sum = 0.1767683986968822
running average episode reward sum: 0.4552660622913372
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.867264  ,  7.98637258,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.3134221385896714}
episode index:361
target Thresh 4.724019160082266
target distance 4.0
model initialize at round 361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 5.        , 10.01066351,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 2.0000284274060047}
done in step count: 99
reward sum = -0.16685187426292142
running average episode reward sum: 0.4535475044555519
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 9.50872251, 11.9118622 ,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 3.1541886017006746}
episode index:362
target Thresh 4.731155366301999
target distance 2.0
model initialize at round 362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([4.25147653, 8.        , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.7508861017499977}
done in step count: 99
reward sum = -0.23001207810024812
running average episode reward sum: 0.4516644202060869
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 1.14008583, 11.8683401 ,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 6.92196128999639}
episode index:363
target Thresh 4.738288005310499
target distance 3.0
model initialize at round 363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 3.37218581, 11.89981069,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 4.939867959035218}
done in step count: 99
reward sum = -0.21375135941328274
running average episode reward sum: 0.4498363548774622
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 3.53665202, 11.90895416,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 4.93077299417129}
episode index:364
target Thresh 4.745417078890926
target distance 4.0
model initialize at round 364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.88577819, 10.4568305 ,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 4.593999168955919}
done in step count: 99
reward sum = -0.1916159427004877
running average episode reward sum: 0.4480789513224542
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.01382191, 11.8674393 ,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 6.194493303139613}
episode index:365
target Thresh 4.752542588825549
target distance 3.0
model initialize at round 365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 1.47153561, 11.91233043,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 5.524864015663542}
done in step count: 72
reward sum = 0.3654086184678731
running average episode reward sum: 0.44785307609607555
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.47904415, 6.78075144, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.565212285922063}
episode index:366
target Thresh 4.759664536895744
target distance 4.0
model initialize at round 366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([3.96836996, 9.58316004, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 2.0739518234979917}
done in step count: 2
reward sum = 0.9716208338200372
running average episode reward sum: 0.4492802361988656
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.61967397, 9.70267999, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.6873099833029069}
episode index:367
target Thresh 4.766782924882
target distance 3.0
model initialize at round 367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 2.60960484, 10.        ,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 5.189720485097083}
done in step count: 89
reward sum = 0.23447280086484001
running average episode reward sum: 0.44869652034197965
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.8530944 , 5.32483855, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9128472676037632}
episode index:368
target Thresh 4.773897754563912
target distance 2.0
model initialize at round 368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.20526886, 2.73355508, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.272888285594323}
done in step count: 11
reward sum = 0.8689211273059174
running average episode reward sum: 0.4498353404150527
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.52516236, 6.27264862, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.5475473074380447}
episode index:369
target Thresh 4.781009027720191
target distance 3.0
model initialize at round 369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.87656581,  7.        ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 5.001523367761946}
done in step count: 99
reward sum = -0.21980426765006936
running average episode reward sum: 0.4480255036364983
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 8.9762789 , 10.50081735,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.418690488560959}
episode index:370
target Thresh 4.788116746128651
target distance 1.0
model initialize at round 370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 7.        , 11.45889485,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 2.475555327582813}
done in step count: 99
reward sum = -0.2035178925212954
running average episode reward sum: 0.44626932197569563
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([12.86183929, 11.83874535,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 4.277240599386014}
episode index:371
target Thresh 4.795220911566225
target distance 1.0
model initialize at round 371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.88151761, 4.25021384, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.1572608122508983}
done in step count: 0
reward sum = 0.9969577681928179
running average episode reward sum: 0.44774966726122556
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.88151761, 4.25021384, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.1572608122508983}
episode index:372
target Thresh 4.802321525808952
target distance 3.0
model initialize at round 372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.87031905,  8.11553349,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 3.012905959644599}
done in step count: 1
reward sum = 0.9837776151509514
running average episode reward sum: 0.44918673950757876
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.84333933, 10.09891863,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.2341672749367651}
episode index:373
target Thresh 4.809418590631984
target distance 3.0
model initialize at round 373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.78977525,  7.37925208,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.4145639951269477}
done in step count: 47
reward sum = 0.5126040912663586
running average episode reward sum: 0.4493563046192332
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.13043332,  8.05859816,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.9503947991340055}
episode index:374
target Thresh 4.816512107809592
target distance 2.0
model initialize at round 374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.4287957 ,  6.10604095,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.9255537432806467}
done in step count: 99
reward sum = -0.19026316413573283
running average episode reward sum: 0.44765065270255333
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.29908744, 11.88513667,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.9005976846536554}
episode index:375
target Thresh 4.823602079115153
target distance 3.0
model initialize at round 375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2., 9., 0.]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 4.999999999999982}
done in step count: 87
reward sum = 0.21104439647702655
running average episode reward sum: 0.4470213807445067
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.50387442, 3.1639772 , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9761268167821804}
episode index:376
target Thresh 4.83068850632116
target distance 1.0
model initialize at round 376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.65813875, 9.16361213, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0642797049907988}
done in step count: 0
reward sum = 0.9979178551130876
running average episode reward sum: 0.4484826446022483
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.65813875, 9.16361213, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0642797049907988}
episode index:377
target Thresh 4.837771391199221
target distance 3.0
model initialize at round 377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.10853577, 5.59499574, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.821853011989938}
done in step count: 99
reward sum = -0.22182336232363448
running average episode reward sum: 0.44670934828762954
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.128558  , 11.85758408,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.053126031582554}
episode index:378
target Thresh 4.844850735520054
target distance 2.0
model initialize at round 378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([12.45582354, 10.35689545,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.5848833702911205}
done in step count: 15
reward sum = 0.8171437771404936
running average episode reward sum: 0.447686747836054
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.87038891, 10.90678894,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.9160050317165923}
episode index:379
target Thresh 4.8519265410535
target distance 2.0
model initialize at round 379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([16.86733261,  3.99548288,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.8673380748333293}
done in step count: 99
reward sum = -0.2623466254048087
running average episode reward sum: 0.4458182389591044
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.5284274, 11.9120667,  0.       ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 8.0477528162182}
episode index:380
target Thresh 4.858998809568505
target distance 3.0
model initialize at round 380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.81291509, 9.        , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.065652074339436}
done in step count: 47
reward sum = 0.5054353562414601
running average episode reward sum: 0.44597471433254887
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.2202655 , 4.74571015, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7775606249544589}
episode index:381
target Thresh 4.866067542833138
target distance 2.0
model initialize at round 381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.25235248, 8.14352989, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.29031466048438115}
done in step count: 0
reward sum = 0.9971591403889282
running average episode reward sum: 0.44741760550023574
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.25235248, 8.14352989, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.29031466048438115}
episode index:382
target Thresh 4.873132742614585
target distance 4.0
model initialize at round 382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([13.11037575,  5.6196884 ,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 4.862182174796391}
done in step count: 5
reward sum = 0.9443339654218786
running average episode reward sum: 0.44871503724937845
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.74060641,  9.93159851,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.7437584419999368}
episode index:383
target Thresh 4.8801944106791435
target distance 3.0
model initialize at round 383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.14630967,  2.54295564,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.4613966609428894}
done in step count: 6
reward sum = 0.9253082563284172
running average episode reward sum: 0.4499561654240634
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.11787547,  4.27165801,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1439518102861141}
episode index:384
target Thresh 4.887252548792231
target distance 3.0
model initialize at round 384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([ 4.55086577, 11.0164783 ,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 4.884770223538773}
done in step count: 99
reward sum = -0.1830551308173151
running average episode reward sum: 0.4483119802390209
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([11.81417769, 11.87538655,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 4.023362264783176}
episode index:385
target Thresh 4.894307158718381
target distance 2.0
model initialize at round 385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16., 11.,  0.]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.9999999999999822}
done in step count: 99
reward sum = -0.21699015886909953
running average episode reward sum: 0.44658839956775637
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.84564043, 11.87205923,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.8762042822259746}
episode index:386
target Thresh 4.901358242221251
target distance 2.0
model initialize at round 386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([13.24693529,  5.99999654,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.92905418065849}
done in step count: 62
reward sum = 0.3972972967996552
running average episode reward sum: 0.4464610323771411
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.87270803,  5.2033454 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8960851795488677}
episode index:387
target Thresh 4.908405801063603
target distance 2.0
model initialize at round 387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.98038489, 1.13455616, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 2.8655109807872408}
done in step count: 13
reward sum = 0.8484193836825987
running average episode reward sum: 0.44749700750937166
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.00790369, 4.34974007, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.3498293630944129}
episode index:388
target Thresh 4.915449837007335
target distance 2.0
model initialize at round 388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3., 11.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9999999999999822}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.44890189952144366
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3., 11.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9999999999999822}
episode index:389
target Thresh 4.922490351813454
target distance 1.0
model initialize at round 389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.53875947, 10.        ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 3.0352500434935337}
done in step count: 99
reward sum = -0.211994835610042
running average episode reward sum: 0.447207292508286
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.84506998,  9.18272067,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.3406009424038525}
episode index:390
target Thresh 4.929527347242086
target distance 1.0
model initialize at round 390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.14481327, 4.97582613, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8555283259405992}
done in step count: 0
reward sum = 0.9984105778389539
running average episode reward sum: 0.448617019580743
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.14481327, 4.97582613, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8555283259405992}
episode index:391
target Thresh 4.936560825052482
target distance 2.0
model initialize at round 391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.14116073, 5.92963398, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9402902095882302}
done in step count: 0
reward sum = 0.9967603725066958
running average episode reward sum: 0.4500153444606561
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.14116073, 5.92963398, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9402902095882302}
episode index:392
target Thresh 4.943590787003011
target distance 3.0
model initialize at round 392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.12353977,  6.28117077,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 2.7216345162940754}
done in step count: 2
reward sum = 0.9761013624996417
running average episode reward sum: 0.4513539857279309
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.84621008,  8.49504902,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.9854171634902534}
episode index:393
target Thresh 4.95061723485116
target distance 3.0
model initialize at round 393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.        , 11.09233499,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.250005486762774}
done in step count: 2
reward sum = 0.9722737819838319
running average episode reward sum: 0.4526761171905093
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.8323728 ,  8.91534013,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.237211393520687}
episode index:394
target Thresh 4.957640170353548
target distance 1.0
model initialize at round 394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 7.21821785, 10.00270796,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.4320941275908896}
done in step count: 1
reward sum = 0.9875301591055666
running average episode reward sum: 0.45403017805611706
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.9687289 , 10.51856637,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0817643096786245}
episode index:395
target Thresh 4.964659595265907
target distance 2.0
model initialize at round 395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.45820217, 11.89967114,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 2.949853825034476}
done in step count: 45
reward sum = 0.5187243375464269
running average episode reward sum: 0.454193547145739
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.5204159,  8.014249 ,  0.       ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.1146917742961682}
episode index:396
target Thresh 4.971675511343089
target distance 1.0
model initialize at round 396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.86740357, 6.00494981, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.1754572471693194}
done in step count: 28
reward sum = 0.6757055747262402
running average episode reward sum: 0.4547515119507277
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.97217567, 8.96306732, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9634691821491029}
episode index:397
target Thresh 4.978687920339078
target distance 4.0
model initialize at round 397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([5.85387008, 7.82335934, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.8591255386405487}
done in step count: 99
reward sum = -0.2232359514342314
running average episode reward sum: 0.4530480258618208
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.46254395, 11.90885451,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.653158212299504}
episode index:398
target Thresh 4.985696824006973
target distance 3.0
model initialize at round 398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([16.,  6.,  0.]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 2.2360679774997823}
done in step count: 26
reward sum = 0.7157249228401062
running average episode reward sum: 0.45370636394948566
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.22356504,  6.97766293,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.22467815464624732}
episode index:399
target Thresh 4.992702224098998
target distance 4.0
model initialize at round 399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.13224972,  3.02619884,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 5.975264873414885}
done in step count: 25
reward sum = 0.7099651062530248
running average episode reward sum: 0.45434701080524453
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.81216992,  8.54757917,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.4898619693081204}
episode index:400
target Thresh 4.999704122366511
target distance 2.0
model initialize at round 400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([ 9.02619884, 11.86775028,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 2.867869944776668}
done in step count: 55
reward sum = 0.4251761835225044
running average episode reward sum: 0.4542742656000507
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.45649899, 8.79260305, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.5014028505868177}
episode index:401
target Thresh 5.006702520559979
target distance 5.0
model initialize at round 401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([4.57859695, 7.26943517, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 4.987708780217163}
done in step count: 99
reward sum = -0.21844440064008092
running average episode reward sum: 0.45260083608204044
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 1.13994362, 11.87119239,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 8.912785842475685}
episode index:402
target Thresh 5.013697420429003
target distance 4.0
model initialize at round 402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 9.92499804, 11.        ,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 2.303395995782762}
done in step count: 4
reward sum = 0.9459911161350265
running average episode reward sum: 0.4538251295809312
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.85976008,  9.47666731,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.0065110532817525}
episode index:403
target Thresh 5.020688823722311
target distance 2.0
model initialize at round 403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.88857978,  8.13306507,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8740655819688089}
done in step count: 0
reward sum = 0.9953750456032071
running average episode reward sum: 0.45516559967009523
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.88857978,  8.13306507,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8740655819688089}
episode index:404
target Thresh 5.0276767321877545
target distance 5.0
model initialize at round 404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([10.       ,  8.6265099,  0.       ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.825369947040573}
done in step count: 41
reward sum = 0.5964954071339093
running average episode reward sum: 0.4555145621576602
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.02421934, 11.86768676,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8680247080395814}
episode index:405
target Thresh 5.0346611475723035
target distance 5.0
model initialize at round 405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([13.10347826,  4.636768  ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 4.843801636494305}
done in step count: 2
reward sum = 0.965750482924152
running average episode reward sum: 0.45677130087875994
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.33727681,  8.71003785,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.4447850033041153}
episode index:406
target Thresh 5.0416420716220705
target distance 3.0
model initialize at round 406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([13.99734676, 11.33566618,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.5417808766290277}
done in step count: 71
reward sum = 0.32445985039536573
running average episode reward sum: 0.4564462113198327
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.48670466,  9.82998794,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9758852895810907}
episode index:407
target Thresh 5.048619506082281
target distance 1.0
model initialize at round 407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.09372199, 11.02934909,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9067531130342195}
done in step count: 0
reward sum = 0.9999927661156572
running average episode reward sum: 0.45777843326786166
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.09372199, 11.02934909,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9067531130342195}
episode index:408
target Thresh 5.055593452697294
target distance 2.0
model initialize at round 408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.        , 9.50667632, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.4933236837387458}
done in step count: 0
reward sum = 0.9968074589306897
running average episode reward sum: 0.45909635264601045
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.        , 9.50667632, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.4933236837387458}
episode index:409
target Thresh 5.0625639132105995
target distance 5.0
model initialize at round 409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([2.71343172, 6.07524073, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 6.041690243794624}
done in step count: 99
reward sum = -0.19535003538047088
running average episode reward sum: 0.45750014194350686
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 8.78123915, 11.87746492,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 2.9816336090217552}
episode index:410
target Thresh 5.069530889364811
target distance 2.0
model initialize at round 410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.        , 3.37746239, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.3774623870849543}
done in step count: 99
reward sum = -0.1781230247992217
running average episode reward sum: 0.4559536135572715
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([ 1.12992873, 11.87007127,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.278891773644546}
episode index:411
target Thresh 5.076494382901671
target distance 5.0
model initialize at round 411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.5701052, 9.       , 0.       ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.860449360189157}
done in step count: 20
reward sum = 0.7581625279178187
running average episode reward sum: 0.4566871303397
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.62293996, 11.89975221,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9755656330110299}
episode index:412
target Thresh 5.083454395562056
target distance 2.0
model initialize at round 412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.9999999999999822}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.45798813002460476
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.9999999999999822}
episode index:413
target Thresh 5.090410929085966
target distance 5.0
model initialize at round 413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.14305644, 4.97190222, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.1470189806581534}
done in step count: 2
reward sum = 0.9701563612793904
running average episode reward sum: 0.45922525135613806
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.16772951, 8.95971072, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.270322332777871}
episode index:414
target Thresh 5.097363985212535
target distance 2.0
model initialize at round 414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.62965071, 11.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.370349287986727}
done in step count: 1
reward sum = 0.979651607573099
running average episode reward sum: 0.460479290768709
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.60041342, 11.90593909,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0868403336062562}
episode index:415
target Thresh 5.10431356568003
target distance 3.0
model initialize at round 415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.09154105,  7.        ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.000837906208947}
done in step count: 99
reward sum = -0.22124268793534954
running average episode reward sum: 0.45884053601220887
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 1.12992873, 11.87007127,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 16.219033312502027}
episode index:416
target Thresh 5.1112596722258425
target distance 5.0
model initialize at round 416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([10., 11.,  0.]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 9.21954445729286}
done in step count: 99
reward sum = -0.22624512731445723
running average episode reward sum: 0.45719764473324803
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.84962975, 11.87220122,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 7.9179178329364746}
episode index:417
target Thresh 5.118202306586503
target distance 5.0
model initialize at round 417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.86740305,  9.99505755,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 7.240029297175549}
done in step count: 99
reward sum = -0.1842913205306885
running average episode reward sum: 0.4556629821369228
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 8.31975169, 11.88936691,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.119647547091867}
episode index:418
target Thresh 5.125141470497665
target distance 1.0
model initialize at round 418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.00014769, 11.85471478,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.3153866302861148}
done in step count: 0
reward sum = 0.9943934804234644
running average episode reward sum: 0.456948735116127
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.00014769, 11.85471478,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.3153866302861148}
episode index:419
target Thresh 5.132077165694122
target distance 1.0
model initialize at round 419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 1.13259695, 10.99505755,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.118133697419593}
done in step count: 98
reward sum = 0.1614522466288379
running average episode reward sum: 0.4562451720483001
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.34387008, 7.29587976, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7836019061381125}
episode index:420
target Thresh 5.139009393909801
target distance 4.0
model initialize at round 420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([14.        ,  4.81531298,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 5.557065725606291}
done in step count: 66
reward sum = 0.36136715015931375
running average episode reward sum: 0.4560198085758797
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.00522907,  9.34810568,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.1893424270044202}
episode index:421
target Thresh 5.1459381568777545
target distance 1.0
model initialize at round 421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.5926708 , 4.65147364, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7683325998481478}
done in step count: 0
reward sum = 0.9996454530430293
running average episode reward sum: 0.45730802100352697
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.5926708 , 4.65147364, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7683325998481478}
episode index:422
target Thresh 5.1528634563301745
target distance 3.0
model initialize at round 422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.62354358, 7.64678884, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 2.4344217717181253}
done in step count: 13
reward sum = 0.8482527409403956
running average episode reward sum: 0.4582322401995952
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.08454167, 9.77552731, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.2398651353569984}
episode index:423
target Thresh 5.159785293998387
target distance 3.0
model initialize at round 423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.0717833, 9.       , 0.       ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.4658511013035822}
done in step count: 99
reward sum = -0.22568775965138851
running average episode reward sum: 0.45661922133202215
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 3.60888273, 11.90468248,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 4.223156277048265}
episode index:424
target Thresh 5.166703671612853
target distance 3.0
model initialize at round 424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.86775028,  7.97380116,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.3439027480342796}
done in step count: 1
reward sum = 0.9803662181806304
running average episode reward sum: 0.45785156720696
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.85433386,  9.9497342 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.2774511285974461}
episode index:425
target Thresh 5.173618590903164
target distance 5.0
model initialize at round 425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 7.3366816, 11.       ,  0.       ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 4.173715571004846}
done in step count: 98
reward sum = 0.17187678714794874
running average episode reward sum: 0.457180264906352
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.80402183,  9.9685452 ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.2587815151552137}
episode index:426
target Thresh 5.180530053598048
target distance 2.0
model initialize at round 426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.02931356,  5.41464615,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 2.4148240740052103}
done in step count: 99
reward sum = -0.1648900273304007
running average episode reward sum: 0.4557234258144627
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 3.47266563, 11.91214763,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 14.570717656646982}
episode index:427
target Thresh 5.187438061425374
target distance 1.0
model initialize at round 427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([2.27250457, 4.        , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 2.642771361135}
done in step count: 99
reward sum = -0.2507382577055182
running average episode reward sum: 0.4540728144043693
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([ 1.14875054, 11.87205088,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.275554102013356}
episode index:428
target Thresh 5.194342616112147
target distance 4.0
model initialize at round 428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([11.99505755, 11.86740305,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.739888388158521}
done in step count: 45
reward sum = 0.5280233854095957
running average episode reward sum: 0.4542451933577614
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.05905805, 10.95418231,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.956008231710755}
episode index:429
target Thresh 5.201243719384499
target distance 3.0
model initialize at round 429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 1.33428381, 11.88418266,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.940818823538603}
done in step count: 99
reward sum = -0.19908946361228883
running average episode reward sum: 0.45272581043457527
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 1.14642066, 11.87183124,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.964804511606914}
episode index:430
target Thresh 5.2081413729677095
target distance 1.0
model initialize at round 430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.99396908,  5.        ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 3.000006061987504}
done in step count: 72
reward sum = 0.3538827106851069
running average episode reward sum: 0.4524964760964094
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.7984117 ,  1.12146178,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1871354759293777}
episode index:431
target Thresh 5.215035578586194
target distance 4.0
model initialize at round 431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([12.08777067,  7.87348483,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.2193822150827036}
done in step count: 90
reward sum = 0.19958274811255794
running average episode reward sum: 0.45191102765200236
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.82330119,  9.59821334,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0176856375365537}
episode index:432
target Thresh 5.221926337963497
target distance 5.0
model initialize at round 432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.89556882, 3.64526002, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.429114456488176}
done in step count: 11
reward sum = 0.8765775458347287
running average episode reward sum: 0.4528917817355652
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.83156575, 8.67821215, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8916552074980576}
episode index:433
target Thresh 5.228813652822314
target distance 3.0
model initialize at round 433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([7.35882032, 9.93853712, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.4874813065682466}
done in step count: 42
reward sum = 0.5432315243209294
running average episode reward sum: 0.45309993782447155
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.5829379 , 9.24003182, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6304219793616539}
episode index:434
target Thresh 5.235697524884475
target distance 1.0
model initialize at round 434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.98743042,  2.16161275,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.838481470710902}
done in step count: 0
reward sum = 0.9974651305641447
running average episode reward sum: 0.45435135206065475
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.98743042,  2.16161275,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.838481470710902}
episode index:435
target Thresh 5.242577955870943
target distance 5.0
model initialize at round 435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([9.9718129, 8.1430406, 0.       ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 3.1471092296245224}
done in step count: 2
reward sum = 0.9698247101274654
running average episode reward sum: 0.4555336304048446
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.97779088,  8.5061764 ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.0954162451282454}
episode index:436
target Thresh 5.249454947501831
target distance 1.0
model initialize at round 436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.        , 10.51463413,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.111566473696191}
done in step count: 0
reward sum = 0.9968013635841839
running average episode reward sum: 0.4567722293366051
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.        , 10.51463413,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.111566473696191}
episode index:437
target Thresh 5.256328501496381
target distance 2.0
model initialize at round 437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.36062178, 11.89946531,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.9333950772443367}
done in step count: 7
reward sum = 0.9136944469304876
running average episode reward sum: 0.45781543074663683
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.55260695, 10.6554016 ,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.7935438242148034}
episode index:438
target Thresh 5.263198619572987
target distance 2.0
model initialize at round 438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.        , 9.65271425, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.6527142524719487}
done in step count: 0
reward sum = 0.996909544330568
running average episode reward sum: 0.4590434355611788
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.        , 9.65271425, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.6527142524719487}
episode index:439
target Thresh 5.270065303449174
target distance 1.0
model initialize at round 439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.72915912,  2.22703111,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7636858930344579}
done in step count: 0
reward sum = 0.9991531344922762
running average episode reward sum: 0.46027095760420406
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.72915912,  2.22703111,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7636858930344579}
episode index:440
target Thresh 5.276928554841617
target distance 3.0
model initialize at round 440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 5.        , 10.17130899,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.5401184240544799}
done in step count: 3
reward sum = 0.9633793206399813
running average episode reward sum: 0.4614117928945346
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.82131588, 9.05579376, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8232087986539615}
episode index:441
target Thresh 5.28378837546613
target distance 5.0
model initialize at round 441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([6.5181061 , 8.60121632, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 5.115412585413654}
done in step count: 14
reward sum = 0.8477924203146192
running average episode reward sum: 0.4622859572099647
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.14273761, 10.87593787,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8661929489638875}
episode index:442
target Thresh 5.290644767037661
target distance 1.0
model initialize at round 442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 2.115971091728497}
done in step count: 99
reward sum = -0.16329266842780615
running average episode reward sum: 0.46087381584283654
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.69646677, 11.88956001,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 8.984624427744569}
episode index:443
target Thresh 5.2974977312703135
target distance 4.0
model initialize at round 443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([6.        , 9.24499059, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 2.137765004242677}
done in step count: 19
reward sum = 0.75554998767652
running average episode reward sum: 0.461537500914534
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.1391495 , 9.22211852, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.790229200059494}
episode index:444
target Thresh 5.304347269877328
target distance 4.0
model initialize at round 444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.88128221, 2.64798628, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.4659276257604197}
done in step count: 2
reward sum = 0.9707484090192788
running average episode reward sum: 0.46268179509005025
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.95174372, 6.63476695, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1440040167233338}
episode index:445
target Thresh 5.311193384571087
target distance 1.0
model initialize at round 445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 6.05427558, 11.86770437,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 3.055360104067432}
done in step count: 99
reward sum = -0.17368854213859367
running average episode reward sum: 0.46125495576890985
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([6.04433917, 7.92876382, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.4960585747332706}
episode index:446
target Thresh 5.318036077063125
target distance 1.0
model initialize at round 446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.24386227,  9.54836214,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.9861064054664521}
done in step count: 18
reward sum = 0.7942872463033449
running average episode reward sum: 0.46199999445019496
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.78997841,  7.69699911,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8460942191180593}
episode index:447
target Thresh 5.324875349064107
target distance 2.0
model initialize at round 447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.60930379, 1.11336839, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.9825816329894024}
done in step count: 6
reward sum = 0.9318196254561232
running average episode reward sum: 0.4630486989836903
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.93750679, 3.31171195, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9879692888454528}
episode index:448
target Thresh 5.331711202283857
target distance 2.0
model initialize at round 448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 3.71101093, 10.        ,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 4.062700646950861}
done in step count: 79
reward sum = 0.27861190623726817
running average episode reward sum: 0.4626379266167718
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.67383755, 5.02892552, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0243864433938055}
episode index:449
target Thresh 5.338543638431335
target distance 3.0
model initialize at round 449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([12.89618516, 10.63286638,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.1356085726000424}
done in step count: 2
reward sum = 0.976770650086323
running average episode reward sum: 0.46378044378003747
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.14300168, 11.81452942,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8269870923271675}
episode index:450
target Thresh 5.345372659214651
target distance 1.0
model initialize at round 450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.865477  ,  9.95164609,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.2863439344460446}
done in step count: 0
reward sum = 0.9948570389424789
running average episode reward sum: 0.4649579972061183
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.865477  ,  9.95164609,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.2863439344460446}
episode index:451
target Thresh 5.3521982663410625
target distance 2.0
model initialize at round 451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([5., 9., 0.]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9999999999999813}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.46612844411540866
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([5., 9., 0.]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9999999999999813}
episode index:452
target Thresh 5.3590204615169705
target distance 1.0
model initialize at round 452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.92381954, 8.        , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.56385768934676}
done in step count: 99
reward sum = -0.19821978622249992
running average episode reward sum: 0.4646618917305568
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 1.18348629, 11.86577958,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 6.914161115920926}
episode index:453
target Thresh 5.36583924644792
target distance 4.0
model initialize at round 453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 9.98035419, 11.86217886,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 3.503003999368458}
done in step count: 45
reward sum = 0.5408841417936874
running average episode reward sum: 0.46482978214919807
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.69019445,  9.6118747 ,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.922365990536191}
episode index:454
target Thresh 5.372654622838612
target distance 2.0
model initialize at round 454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.        ,  2.02971959,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9702804088592454}
done in step count: 0
reward sum = 0.9940884964479053
running average episode reward sum: 0.46599298811468975
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.        ,  2.02971959,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9702804088592454}
episode index:455
target Thresh 5.379466592392889
target distance 4.0
model initialize at round 455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([14.13221371,  7.39308441,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 3.520360842229597}
done in step count: 3
reward sum = 0.957310605898443
running average episode reward sum: 0.4670704390308822
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.09955097,  8.15000512,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8558047021657597}
episode index:456
target Thresh 5.38627515681374
target distance 3.0
model initialize at round 456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([13.09227849,  4.4159853 ,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 4.712352182871395}
done in step count: 41
reward sum = 0.5197338762716311
running average episode reward sum: 0.46718567631149654
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.84975836,  9.07018484,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.8526518494821524}
episode index:457
target Thresh 5.393080317803312
target distance 5.0
model initialize at round 457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 5.       , 10.8531661,  0.       ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.526219591383108}
done in step count: 9
reward sum = 0.891162021767282
running average episode reward sum: 0.4681113888561598
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.27139424, 9.52604059, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.5919235912008288}
episode index:458
target Thresh 5.399882077062895
target distance 2.0
model initialize at round 458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([3.46663015, 1.08988796, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 2.4082217463505877}
done in step count: 90
reward sum = 0.21585817708419258
running average episode reward sum: 0.46756181758868276
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.32534427, 2.02546123, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0274116574871284}
episode index:459
target Thresh 5.406680436292921
target distance 2.0
model initialize at round 459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.       , 9.6674149, 0.       ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.666068137573828}
done in step count: 2
reward sum = 0.9745983907543564
running average episode reward sum: 0.46866407100860813
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.35346902, 11.75986045,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8380502673760759}
episode index:460
target Thresh 5.413475397192986
target distance 1.0
model initialize at round 460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.87557199, 3.82869957, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.2055576614797585}
done in step count: 0
reward sum = 0.9944221185474265
running average episode reward sum: 0.46980454399676175
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.87557199, 3.82869957, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.2055576614797585}
episode index:461
target Thresh 5.420266961461833
target distance 4.0
model initialize at round 461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.91287247,  7.49465068,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 3.9933137836384063}
done in step count: 99
reward sum = -0.23031105378961106
running average episode reward sum: 0.4682891422699514
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.826338  ,  1.12572609,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.041752618266703}
episode index:462
target Thresh 5.427055130797349
target distance 3.0
model initialize at round 462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.86490633, 2.01883714, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 5.318821206177425}
done in step count: 68
reward sum = 0.32112212997622913
running average episode reward sum: 0.4679712869518224
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.24314962, 7.36957111, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8422620152749304}
episode index:463
target Thresh 5.433839906896576
target distance 3.0
model initialize at round 463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 6.18768048, 10.78092861,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 2.540907072751659}
done in step count: 4
reward sum = 0.9535337928416087
running average episode reward sum: 0.4690177578696883
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.94505668, 8.75019515, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.25577574950629833}
episode index:464
target Thresh 5.4406212914557095
target distance 2.0
model initialize at round 464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.68494391,  3.33604956,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.3726939772248663}
done in step count: 1
reward sum = 0.9893588026672518
running average episode reward sum: 0.47013677086925293
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.25123811,  2.9758729 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.2300293885881348}
episode index:465
target Thresh 5.447399286170094
target distance 1.0
model initialize at round 465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([11.        ,  8.96512675,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.439083957821629}
done in step count: 4
reward sum = 0.952437561163885
running average episode reward sum: 0.47117175110593673
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.883945  , 9.13775525, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.8700199795269052}
episode index:466
target Thresh 5.454173892734229
target distance 2.0
model initialize at round 466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.65010202, 7.79665899, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.2309505129079565}
done in step count: 99
reward sum = -0.26942211051567544
running average episode reward sum: 0.46958589701252856
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.12992873,  1.12992873,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 13.464531941268637}
episode index:467
target Thresh 5.4609451128417685
target distance 3.0
model initialize at round 467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([13.37731636,  8.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 3.9847797301797367}
done in step count: 38
reward sum = 0.6076867341915246
running average episode reward sum: 0.469880984271458
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.85846519, 10.04781156,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.2820395072879873}
episode index:468
target Thresh 5.467712948185516
target distance 4.0
model initialize at round 468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.        , 5.04974592, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.950254082679721}
done in step count: 99
reward sum = -0.18964523190360352
running average episode reward sum: 0.46847474500456027
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.40064206, 1.1042466 , 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.913887498245598}
episode index:469
target Thresh 5.474477400457428
target distance 2.0
model initialize at round 469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.12527883,  3.        ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.294831681504138}
done in step count: 26
reward sum = 0.6946150520244739
running average episode reward sum: 0.4689558945939643
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.7778065 ,  5.90970952,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1968936333551496}
episode index:470
target Thresh 5.481238471348622
target distance 2.0
model initialize at round 470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([16.21300018,  5.60084176,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 3.41493010956073}
done in step count: 28
reward sum = 0.6791525770765234
running average episode reward sum: 0.46940217205146445
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.13937535,  2.07728585,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.2617750207888294}
episode index:471
target Thresh 5.4879961625493605
target distance 4.0
model initialize at round 471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 3.41756701, 10.47247458,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.6357623560502765}
done in step count: 1
reward sum = 0.9809267421878792
running average episode reward sum: 0.47048591054751615
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.43972381, 11.90575564,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.065036476362304}
episode index:472
target Thresh 5.494750475749074
target distance 5.0
model initialize at round 472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([16.        ,  7.94996691,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 6.27710970115704}
done in step count: 6
reward sum = 0.9206537807183042
running average episode reward sum: 0.471437639659928
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.23013181,  1.11903482,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.9105274856386432}
episode index:473
target Thresh 5.50150141263633
target distance 3.0
model initialize at round 473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.0640167,  3.       ,  0.       ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.4601820220212622}
done in step count: 1
reward sum = 0.9837246352931329
running average episode reward sum: 0.47251841391231875
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.43540328,  1.09095486,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0079380342452413}
episode index:474
target Thresh 5.508248974898873
target distance 2.0
model initialize at round 474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.,  5.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 4.1231056256176375}
done in step count: 20
reward sum = 0.757756958362113
running average episode reward sum: 0.4731189161111604
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.39772684,  8.67832568,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5115281060354616}
episode index:475
target Thresh 5.51499316422359
target distance 3.0
model initialize at round 475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 5.        , 10.90947914,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 4.035347216257204}
done in step count: 99
reward sum = -0.18719953625188682
running average episode reward sum: 0.47173169247174224
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([1.12880153, 1.14000285, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 6.525591715251731}
episode index:476
target Thresh 5.521733982296526
target distance 3.0
model initialize at round 476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([13.93332839,  5.39407003,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 2.492902482961487}
done in step count: 99
reward sum = -0.21575689270780846
running average episode reward sum: 0.470290416611827
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.3563085 ,  1.10912827,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 2.961668127331787}
episode index:477
target Thresh 5.528471430802886
target distance 2.0
model initialize at round 477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.        , 9.48848474, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 2.0587902616498135}
done in step count: 7
reward sum = 0.918920009606647
running average episode reward sum: 0.47122897224570737
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.96851647, 8.46384692, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.5370766573383697}
episode index:478
target Thresh 5.535205511427035
target distance 4.0
model initialize at round 478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([5.        , 9.13250995, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.736333148341938}
done in step count: 3
reward sum = 0.9608124864799231
running average episode reward sum: 0.47225106726498545
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.54607862, 10.41450214,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7408457036661248}
episode index:479
target Thresh 5.541936225852492
target distance 4.0
model initialize at round 479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([3.00901866, 7.92697978, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 3.639145807793672}
done in step count: 20
reward sum = 0.7865136007002925
running average episode reward sum: 0.472905780876309
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.53255821, 10.1621261 ,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.5566894239588063}
episode index:480
target Thresh 5.548663575761935
target distance 1.0
model initialize at round 480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([4., 5., 0.]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.414213562373068}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.4739891368416501
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([4., 5., 0.]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.414213562373068}
episode index:481
target Thresh 5.555387562837203
target distance 4.0
model initialize at round 481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([5.23478377, 8.        , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.9990429294424743}
done in step count: 46
reward sum = 0.5207272557597669
running average episode reward sum: 0.47408610389334743
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.45318019, 6.43581778, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.6287363665073724}
episode index:482
target Thresh 5.56210818875929
target distance 4.0
model initialize at round 482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.36099711, 7.39119766, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 2.633660677642084}
done in step count: 49
reward sum = 0.4913726053174639
running average episode reward sum: 0.4741218937513684
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.30505625, 10.59270891,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.6666057021499859}
episode index:483
target Thresh 5.568825455208358
target distance 4.0
model initialize at round 483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.18821311,  9.        ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.008836522657996}
done in step count: 1
reward sum = 0.9845393708574967
running average episode reward sum: 0.4751764753156372
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.86311245, 10.59456396,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.42792123473655547}
episode index:484
target Thresh 5.575539363863714
target distance 3.0
model initialize at round 484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.34302545, 10.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.9353461377254417}
done in step count: 1
reward sum = 0.9801759321726857
running average episode reward sum: 0.47621771130915697
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.8878949 , 11.87102483,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8782094373074896}
episode index:485
target Thresh 5.582249916403845
target distance 1.0
model initialize at round 485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.03073028,  3.23636198,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.2367438282102192}
done in step count: 23
reward sum = 0.7452194209899552
running average episode reward sum: 0.47677121276940554
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.60408335,  2.44374488,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.5946927864599774}
episode index:486
target Thresh 5.588957114506384
target distance 5.0
model initialize at round 486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([8.85907185, 9.        , 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.7236312436361496}
done in step count: 99
reward sum = -0.2950985121225928
running average episode reward sum: 0.47518626466901126
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.12938315,  1.13456874,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 9.929866078821671}
episode index:487
target Thresh 5.595660959848133
target distance 3.0
model initialize at round 487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([11.        , 10.44012117,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.092568829721377}
done in step count: 8
reward sum = 0.905456851999023
running average episode reward sum: 0.4760679666922285
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.05866337,  9.47075514,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.5324861628003698}
episode index:488
target Thresh 5.6023614541050515
target distance 2.0
model initialize at round 488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.96745958, 8.14150659, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.8591098923911958}
done in step count: 0
reward sum = 0.995874221463982
running average episode reward sum: 0.4771309651682444
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.96745958, 8.14150659, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.8591098923911958}
episode index:489
target Thresh 5.609058598952262
target distance 5.0
model initialize at round 489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.19351739, 5.33032703, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 3.3359447136315614}
done in step count: 6
reward sum = 0.9240340866273393
running average episode reward sum: 0.4780430123548956
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.43704345, 1.10166839, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.0601508186573867}
episode index:490
target Thresh 5.615752396064055
target distance 5.0
model initialize at round 490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 3.605551275463995}
done in step count: 99
reward sum = -0.27148732810387355
running average episode reward sum: 0.4765164739832891
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([16.00783329,  1.14297853,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 9.317251776967606}
episode index:491
target Thresh 5.6224428471138745
target distance 3.0
model initialize at round 491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([5.        , 7.77200198, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.6720761634492063}
done in step count: 99
reward sum = -0.1786870234731542
running average episode reward sum: 0.4751847595575646
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([1.94808768, 2.30847108, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.838476949892202}
episode index:492
target Thresh 5.629129953774339
target distance 5.0
model initialize at round 492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([13.12291009,  4.80321397,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 4.7031649118819985}
done in step count: 3
reward sum = 0.9533487249324201
running average episode reward sum: 0.4761546661810431
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.65478964,  9.01995701,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.3457867471237528}
episode index:493
target Thresh 5.6358137177172205
target distance 3.0
model initialize at round 493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([1.75902414, 6.1854465 , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.8835008569815703}
done in step count: 99
reward sum = -0.173440495078055
running average episode reward sum: 0.4748396962189801
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([2.10499068, 1.13297947, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 7.123695057805513}
episode index:494
target Thresh 5.6424941406134606
target distance 1.0
model initialize at round 494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.86190932,  8.00089729,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.0022801493115012}
done in step count: 99
reward sum = -0.2964055851905285
running average episode reward sum: 0.4732816249434053
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.12981998,  1.13081725,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 9.933642903896791}
episode index:495
target Thresh 5.64917122413317
target distance 2.0
model initialize at round 495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([1.79900599, 2.93683946, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.0167873742211833}
done in step count: 99
reward sum = -0.2142984252235538
running average episode reward sum: 0.47189537484226224
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.6400794 , 1.10205536, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.128360020676775}
episode index:496
target Thresh 5.655844969945614
target distance 2.0
model initialize at round 496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.8130455,  3.       ,  0.       ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 4.081794089349574}
done in step count: 99
reward sum = -0.1606398186532413
running average episode reward sum: 0.47062266821551074
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.57446485,  1.09443587,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 6.075182149594304}
episode index:497
target Thresh 5.662515379719231
target distance 3.0
model initialize at round 497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.13820088,  1.12899782,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 4.946651386288322}
done in step count: 71
reward sum = 0.27707781150875577
running average episode reward sum: 0.4702340239249349
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.90848965,  5.87790452,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.1525825982989603}
episode index:498
target Thresh 5.6691824551216214
target distance 2.0
model initialize at round 498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86939625,  7.90319569,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.2698982731058153}
done in step count: 59
reward sum = 0.41183858302057513
running average episode reward sum: 0.4701169989932628
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.74266135,  9.11950031,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9173346644675805}
episode index:499
target Thresh 5.67584619781956
target distance 4.0
model initialize at round 499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.63819861,  5.60953736,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.765492211098591}
done in step count: 99
reward sum = -0.23986276369912982
running average episode reward sum: 0.46869703946787805
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.87007127, 1.12992873, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 12.053780336369307}
episode index:500
target Thresh 5.682506609478977
target distance 2.0
model initialize at round 500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.78452742, 9.        , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.21547257900239636}
done in step count: 0
reward sum = 0.9969651788803957
running average episode reward sum: 0.4697514668918551
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.78452742, 9.        , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.21547257900239636}
episode index:501
target Thresh 5.6891636917649775
target distance 2.0
model initialize at round 501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.58126688, 8.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5812668800353937}
done in step count: 0
reward sum = 0.9951246963474814
running average episode reward sum: 0.4707980271098942
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.58126688, 8.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5812668800353937}
episode index:502
target Thresh 5.695817446341831
target distance 4.0
model initialize at round 502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([5.18457605, 7.24132769, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 4.3474118790113785}
done in step count: 44
reward sum = 0.51476787202016
running average episode reward sum: 0.47088544230852303
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.24501648, 10.07208201,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.1962574584443464}
episode index:503
target Thresh 5.702467874872975
target distance 5.0
model initialize at round 503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 1.23059849, 10.30998154,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 6.356716586199958}
done in step count: 26
reward sum = 0.7117196351214553
running average episode reward sum: 0.4713632879291836
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.96750778, 3.12697772, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8736267227932106}
episode index:504
target Thresh 5.709114979021022
target distance 1.0
model initialize at round 504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.14180366, 5.03258944, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.2932069228425933}
done in step count: 0
reward sum = 0.9959093779810246
running average episode reward sum: 0.4724019930579991
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.14180366, 5.03258944, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.2932069228425933}
episode index:505
target Thresh 5.7157587604477404
target distance 3.0
model initialize at round 505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.58541536,  9.        ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.082534258570092}
done in step count: 1
reward sum = 0.9809767829068761
running average episode reward sum: 0.47340708157548705
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.83966613,  7.        ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.3057714986845617}
episode index:506
target Thresh 5.722399220814084
target distance 1.0
model initialize at round 506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.56813061, 9.        , 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.0892709341431006}
done in step count: 0
reward sum = 0.9954623124117787
running average episode reward sum: 0.47443677631086434
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.56813061, 9.        , 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.0892709341431006}
episode index:507
target Thresh 5.729036361780161
target distance 4.0
model initialize at round 507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.62374723,  3.        ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 6.01178560389935}
done in step count: 15
reward sum = 0.8137497988488424
running average episode reward sum: 0.4751047153316084
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.34883748,  8.16781286,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9023430706690263}
episode index:508
target Thresh 5.735670185005259
target distance 4.0
model initialize at round 508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.42216086, 9.        , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 2.0440694202959144}
done in step count: 2
reward sum = 0.9702179886878889
running average episode reward sum: 0.4760774329609921
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.31210225, 11.89086874,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9439570528964305}
episode index:509
target Thresh 5.742300692147836
target distance 4.0
model initialize at round 509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14., 11.,  0.]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 2.0000000000000178}
done in step count: 1
reward sum = 0.9809989690894452
running average episode reward sum: 0.4770674751886949
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.        , 11.20443344,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.20443344116208095}
episode index:510
target Thresh 5.748927884865516
target distance 4.0
model initialize at round 510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([16.        ,  8.57754278,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.714675269229227}
done in step count: 3
reward sum = 0.9558406353683815
running average episode reward sum: 0.4780044089659546
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.34573781,  4.69587505,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.7214921987721734}
episode index:511
target Thresh 5.755551764815097
target distance 5.0
model initialize at round 511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([6.        , 8.89940059, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 3.0016862329231166}
done in step count: 99
reward sum = -0.22524572351944186
running average episode reward sum: 0.4766308735509441
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([1.16267837, 1.12669452, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 8.084843225775906}
episode index:512
target Thresh 5.762172333652551
target distance 5.0
model initialize at round 512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([13.05225855,  7.0000866 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 4.205912418034636}
done in step count: 2
reward sum = 0.9661508759103963
running average episode reward sum: 0.4775851035750366
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.35928228,  3.00011083,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0624792310698719}
episode index:513
target Thresh 5.768789593033022
target distance 1.0
model initialize at round 513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([2.73176152, 9.        , 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.01790779887477}
done in step count: 19
reward sum = 0.7664202696831272
running average episode reward sum: 0.4781470396958694
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.53688374, 11.54026904,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7115949067899856}
episode index:514
target Thresh 5.77540354461082
target distance 2.0
model initialize at round 514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.97380116, 11.86775028,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8681456786036915}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.4791486959298685
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.97380116, 11.86775028,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8681456786036915}
episode index:515
target Thresh 5.782014190039436
target distance 1.0
model initialize at round 515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.3200483200778608}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.4801464697753636
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.3200483200778608}
episode index:516
target Thresh 5.788621530971529
target distance 4.0
model initialize at round 516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.7140047,  8.       ,  0.       ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 6.042334209978386}
done in step count: 11
reward sum = 0.8689510253714876
running average episode reward sum: 0.4808985095347372
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.94970475,  2.37268553,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.02021253688389}
episode index:517
target Thresh 5.79522556905894
target distance 4.0
model initialize at round 517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.14240644,  5.03120929,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.341637639135095}
done in step count: 99
reward sum = -0.1962099514167016
running average episode reward sum: 0.47959135034371125
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.13619248,  1.13196921,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 10.09660090792305}
episode index:518
target Thresh 5.8018263059526705
target distance 4.0
model initialize at round 518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([14.39198744,  4.10854864,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 6.573876337037968}
done in step count: 99
reward sum = -0.2572894255085964
running average episode reward sum: 0.47817154152704017
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([13.73678697,  1.11599203,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 8.724744036377944}
episode index:519
target Thresh 5.808423743302909
target distance 1.0
model initialize at round 519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([16.,  3.,  0.]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.236067977499765}
done in step count: 99
reward sum = -0.20530834095052922
running average episode reward sum: 0.4768571571376602
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.30709573,  1.11364965,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.8984646781968455}
episode index:520
target Thresh 5.815017882759017
target distance 4.0
model initialize at round 520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.86740305, 5.00494245, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 2.1845325096861363}
done in step count: 1
reward sum = 0.9803956225728866
running average episode reward sum: 0.47782364171623076
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.91303186, 3.00480781, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.08710093440136299}
episode index:521
target Thresh 5.821608725969524
target distance 5.0
model initialize at round 521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([8.24566543, 9.        , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.812393485742926}
done in step count: 99
reward sum = -0.21369372799059425
running average episode reward sum: 0.4764988957972522
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.87301698, 1.15941566, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.841403605403357}
episode index:522
target Thresh 5.828196274582147
target distance 1.0
model initialize at round 522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.07639211, 8.59591722, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.4061593773408652}
done in step count: 18
reward sum = 0.7778161275223319
running average episode reward sum: 0.4770750281714875
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.28871094, 9.62070226, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.8061010506940823}
episode index:523
target Thresh 5.834780530243769
target distance 3.0
model initialize at round 523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.        , 10.02625251,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 4.026252508163454}
done in step count: 99
reward sum = -0.1518461785916455
running average episode reward sum: 0.4758747968608708
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.80508991,  1.14881702,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 5.176130395947312}
episode index:524
target Thresh 5.841361494600452
target distance 5.0
model initialize at round 524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([5.36805108, 7.45388722, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 4.1877204855025925}
done in step count: 99
reward sum = -0.20388240530018942
running average episode reward sum: 0.4745800212377069
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.12992873, 1.12992873, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 3.4255620992904388}
episode index:525
target Thresh 5.847939169297445
target distance 1.0
model initialize at round 525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.96075672,  8.16499031,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.8354292720052412}
done in step count: 99
reward sum = -0.16584007049052396
running average episode reward sum: 0.4733624925462083
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.82697762,  1.12578272,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 8.912666506024918}
episode index:526
target Thresh 5.8545135559791595
target distance 3.0
model initialize at round 526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([6.93231989, 8.15670517, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 2.2330354296448194}
done in step count: 9
reward sum = 0.8939684896487888
running average episode reward sum: 0.4741606063927028
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.47498899, 8.87492916, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.4911794554006622}
episode index:527
target Thresh 5.861084656289194
target distance 4.0
model initialize at round 527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([10.16027701,  8.36922416,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 4.468482368562927}
done in step count: 99
reward sum = -0.2769650224534606
running average episode reward sum: 0.47273801997443354
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([13.12992873,  1.12992873,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 11.380423895741725}
episode index:528
target Thresh 5.867652471870325
target distance 1.0
model initialize at round 528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.236067977499766}
done in step count: 99
reward sum = -0.26531848915414824
running average episode reward sum: 0.4713428280857217
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4.87007127, 1.12992873, 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 12.729248326088179}
episode index:529
target Thresh 5.874217004364503
target distance 3.0
model initialize at round 529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.79071075, 5.        , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0216662817770084}
done in step count: 1
reward sum = 0.9838568060319515
running average episode reward sum: 0.47230983559128065
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.22546804, 3.        , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0251028420805375}
episode index:530
target Thresh 5.880778255412867
target distance 2.0
model initialize at round 530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.86740357, 6.00494981, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 2.9979839027385218}
done in step count: 99
reward sum = -0.29192716827077153
running average episode reward sum: 0.4708705945293935
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.87124331, 1.14044142, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 7.860613167766014}
episode index:531
target Thresh 5.887336226655725
target distance 2.0
model initialize at round 531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.8593564 , 4.86112618, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.834076116385946}
done in step count: 7
reward sum = 0.9149967928920832
running average episode reward sum: 0.4717054182105264
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.09237159, 7.5213493 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.0467064685060146}
episode index:532
target Thresh 5.893890919732571
target distance 4.0
model initialize at round 532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([14.34955549, 10.85763419,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 4.195868037892127}
done in step count: 99
reward sum = -0.17354157674548276
running average episode reward sum: 0.4704948234732731
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.26941552,  1.13862416,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 5.867564357353437}
episode index:533
target Thresh 5.900442336282081
target distance 2.0
model initialize at round 533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.02726555, 9.64796484, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.6655964096020732}
done in step count: 99
reward sum = -0.2221419593679591
running average episode reward sum: 0.4691977508462296
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([2.34606939, 5.1959487 , 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 6.0351054614068405}
episode index:534
target Thresh 5.906990477942106
target distance 5.0
model initialize at round 534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([4.67749567, 5.4887433 , 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 5.602720055868708}
done in step count: 52
reward sum = 0.4575672433536685
running average episode reward sum: 0.46917601157988836
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.72103801, 9.48215063, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.5882072434745537}
episode index:535
target Thresh 5.91353534634968
target distance 2.0
model initialize at round 535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.95553041, 11.2549963 ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.25884485267914836}
done in step count: 0
reward sum = 0.995950663754498
running average episode reward sum: 0.4701588001100649
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.95553041, 11.2549963 ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.25884485267914836}
episode index:536
target Thresh 5.9200769431410265
target distance 5.0
model initialize at round 536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.        , 3.33431792, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.665682077407845}
done in step count: 99
reward sum = -0.1546283626885498
running average episode reward sum: 0.4689953230843691
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.71478415, 2.73477116, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 5.3135253033910885}
episode index:537
target Thresh 5.926615269951538
target distance 5.0
model initialize at round 537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.7495265 , 5.76850629, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.297363029286435}
done in step count: 99
reward sum = -0.17843636290814768
running average episode reward sum: 0.46779191846356516
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.62788842, 1.92813985, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 8.080432740799765}
episode index:538
target Thresh 5.933150328415797
target distance 3.0
model initialize at round 538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.09705618, 1.41796412, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 4.670156353454911}
done in step count: 23
reward sum = 0.7288275417804599
running average episode reward sum: 0.46827621461072083
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.55342022, 5.14906356, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.961002771107694}
episode index:539
target Thresh 5.939682120167571
target distance 5.0
model initialize at round 539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([11.27719533,  9.17676735,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 6.376377871969853}
done in step count: 99
reward sum = -0.18509565203948833
running average episode reward sum: 0.46706626670951673
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([16.26526011,  2.10469882,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 2.278826387109729}
episode index:540
target Thresh 5.946210646839812
target distance 3.0
model initialize at round 540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 5.2774169 , 11.88612501,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.090692060775535}
done in step count: 9
reward sum = 0.8885551055107014
running average episode reward sum: 0.467845358832994
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.9887693 , 8.15065266, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.0001804590333363}
episode index:541
target Thresh 5.952735910064641
target distance 5.0
model initialize at round 541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([8.25355446, 8.1168399 , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.919552042454345}
done in step count: 99
reward sum = -0.2431878180630819
running average episode reward sum: 0.4665334895029274
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([1.60399155, 1.09882486, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 3.7626684313012575}
episode index:542
target Thresh 5.959257911473382
target distance 3.0
model initialize at round 542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([1.13325586, 2.0017149 , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 5.335502563923527}
done in step count: 99
reward sum = -0.21507275257382866
running average episode reward sum: 0.4652782293886056
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.88188656, 1.70284658, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 5.621506150784196}
episode index:543
target Thresh 5.965776652696532
target distance 5.0
model initialize at round 543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([13.        ,  9.62191713,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 3.9842752702407145}
done in step count: 77
reward sum = 0.28191737490102764
running average episode reward sum: 0.46494116899432697
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.65423015,  6.47476655,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8389798944609692}
episode index:544
target Thresh 5.972292135363777
target distance 5.0
model initialize at round 544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.72586179, 4.89884377, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 5.152540176090002}
done in step count: 99
reward sum = -0.1620604215258774
running average episode reward sum: 0.46379070736034494
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.87007127, 1.12992873, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.065061003366859}
episode index:545
target Thresh 5.97880436110399
target distance 4.0
model initialize at round 545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([15.97281492,  7.        ,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 4.978278655240382}
done in step count: 99
reward sum = -0.23627499578727093
running average episode reward sum: 0.46250853574285844
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([15.35969034,  1.62040761,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 9.028016826674355}
episode index:546
target Thresh 5.985313331545224
target distance 1.0
model initialize at round 546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.19263762, 9.73256993, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.3295877642087919}
done in step count: 0
reward sum = 0.9995696740358775
running average episode reward sum: 0.4634903659773978
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.19263762, 9.73256993, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.3295877642087919}
episode index:547
target Thresh 5.991819048314724
target distance 5.0
model initialize at round 547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 8.33333956, 11.87401351,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 4.420603057419338}
done in step count: 99
reward sum = -0.20675768835976382
running average episode reward sum: 0.46226728558627156
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([15.37244917,  5.50693221,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 12.629584075624425}
episode index:548
target Thresh 5.998321513038917
target distance 4.0
model initialize at round 548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([16.53197432,  8.72849715,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.030959722230782}
done in step count: 66
reward sum = 0.4058196865796007
running average episode reward sum: 0.46216446664454724
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.19652528,  5.05885975,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.20515032521409637}
episode index:549
target Thresh 6.0048207273434215
target distance 6.0
model initialize at round 549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.91076461, 5.55823731, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 4.534175552037228}
done in step count: 99
reward sum = -0.2507233754917355
running average episode reward sum: 0.46086830693157216
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([16.70892348,  1.11193423,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 15.508528272257058}
episode index:550
target Thresh 6.01131669285304
target distance 5.0
model initialize at round 550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([16.85637761,  3.02463595,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 7.970404751192155}
done in step count: 99
reward sum = -0.19992386920849362
running average episode reward sum: 0.45966904708376805
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([16.85420375,  1.1282284 ,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 9.67280816633618}
episode index:551
target Thresh 6.017809411191763
target distance 2.0
model initialize at round 551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.,  8.,  0.]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9999999999999822}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.4606370379408724
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.,  8.,  0.]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9999999999999822}
episode index:552
target Thresh 6.024298883982773
target distance 1.0
model initialize at round 552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.86740305,  8.00494245,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 3.118133697419593}
done in step count: 99
reward sum = -0.264118885535187
running average episode reward sum: 0.45932644856749794
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.87007127,  1.12992873,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 9.908346531825927}
episode index:553
target Thresh 6.030785112848433
target distance 4.0
model initialize at round 553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.14069226,  4.00021643,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.176987726442429}
done in step count: 1
reward sum = 0.9813469908029198
running average episode reward sum: 0.4602687239144933
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.16766784,  2.00827879,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.832373332384026}
episode index:554
target Thresh 6.0372680994103085
target distance 4.0
model initialize at round 554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2., 4., 0.]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 2.0000000000000195}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.46120168116943777
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13224972, 2.02619884, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8681456786036912}
episode index:555
target Thresh 6.043747845289138
target distance 2.0
model initialize at round 555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.14703771, 6.0049377 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8529765791503026}
done in step count: 0
reward sum = 0.9963266311477954
running average episode reward sum: 0.46216413611544205
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.14703771, 6.0049377 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8529765791503026}
episode index:556
target Thresh 6.050224352104862
target distance 1.0
model initialize at round 556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([5.34812218, 8.        , 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.0700072789570045}
done in step count: 99
reward sum = -0.23037678521593874
running average episode reward sum: 0.4609207951435725
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.87007127, 1.12992873, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.934538031292462}
episode index:557
target Thresh 6.056697621476606
target distance 4.0
model initialize at round 557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([4.86740305, 4.00494245, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 5.431258570677318}
done in step count: 99
reward sum = -0.3040760511127432
running average episode reward sum: 0.45954983305350733
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([3.70322618, 1.11171372, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 8.54948993363943}
episode index:558
target Thresh 6.06316765502269
target distance 5.0
model initialize at round 558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4., 9., 0.]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.0000000000000195}
done in step count: 99
reward sum = -0.2544841701236365
running average episode reward sum: 0.4582724913662495
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.90579454, 1.54607724, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 4.545095342372166}
episode index:559
target Thresh 6.069634454360618
target distance 3.0
model initialize at round 559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.59473944, 10.27351272,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.327100920170359}
done in step count: 7
reward sum = 0.9210552245176269
running average episode reward sum: 0.45909888910401975
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.14179593,  6.98891164,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.1422288251763705}
episode index:560
target Thresh 6.076098021107095
target distance 1.0
model initialize at round 560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.41617888, 2.79764736, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.2723430119604708}
done in step count: 99
reward sum = -0.21309768559552195
running average episode reward sum: 0.45790067774091897
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([4.83597274, 1.12657562, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 4.0372402730806725}
episode index:561
target Thresh 6.082558356878008
target distance 5.0
model initialize at round 561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([15.95396733,  9.        ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 4.953967332839975}
done in step count: 99
reward sum = -0.1917998340992671
running average episode reward sum: 0.45674462700810725
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([13.7239977,  1.1143857,  0.       ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 8.342845816301113}
episode index:562
target Thresh 6.089015463288443
target distance 6.0
model initialize at round 562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.19688315, 1.12481671, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 8.078969134378452}
done in step count: 99
reward sum = -0.23001413403690746
running average episode reward sum: 0.45552480682863117
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.91301011, 2.50442088, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 6.585901241303694}
episode index:563
target Thresh 6.095469341952676
target distance 3.0
model initialize at round 563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([5.        , 9.90023303, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.4864344545255672}
done in step count: 1
reward sum = 0.9810901286922642
running average episode reward sum: 0.45645666023619075
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.99999999, 11.67246749,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.2050778005558365}
episode index:564
target Thresh 6.101919994484178
target distance 6.0
model initialize at round 564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.53777778,  9.18750966,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 7.350173845116508}
done in step count: 19
reward sum = 0.7818637365928058
running average episode reward sum: 0.4570326019642556
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.51118622,  2.09707773,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5203224354643282}
episode index:565
target Thresh 6.108367422495611
target distance 1.0
model initialize at round 565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.21735045, 7.        , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.0078632315115863}
done in step count: 26
reward sum = 0.6867498016722291
running average episode reward sum: 0.4574384627411248
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.46892531, 9.76243626, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5256685946571827}
episode index:566
target Thresh 6.114811627598833
target distance 4.0
model initialize at round 566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([14.26410973,  5.42777431,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 4.229301272342621}
done in step count: 71
reward sum = 0.31680833584363943
running average episode reward sum: 0.4571904378259617
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.39003944,  8.71036116,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.6752351737727964}
episode index:567
target Thresh 6.121252611404897
target distance 4.0
model initialize at round 567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.0000000000000195}
done in step count: 1
reward sum = 0.9795334927811942
running average episode reward sum: 0.45811005588046033
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.59719499, 11.90120264,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0811142711413557}
episode index:568
target Thresh 6.127690375524044
target distance 5.0
model initialize at round 568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([12.03619596, 11.8674034 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.057601649715479}
done in step count: 2
reward sum = 0.9654902186710821
running average episode reward sum: 0.45900176091172684
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.95976473, 11.86310832,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8640456298338055}
episode index:569
target Thresh 6.13412492156572
target distance 5.0
model initialize at round 569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.8728443, 3.8534889, 0.       ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.237382616490512}
done in step count: 99
reward sum = -0.2162488597958265
running average episode reward sum: 0.4578171106999592
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.87007127, 1.12992873, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 6.924947894780221}
episode index:570
target Thresh 6.140556251138555
target distance 6.0
model initialize at round 570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([16.,  8.,  0.]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 4.123105625617674}
done in step count: 90
reward sum = 0.21808632661645172
running average episode reward sum: 0.4573972669449968
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.44264006,  3.0171622 ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.1298762063437853}
episode index:571
target Thresh 6.1469843658503915
target distance 3.0
model initialize at round 571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.48921177,  8.8086735 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.2452705803574298}
done in step count: 1
reward sum = 0.9863168662531092
running average episode reward sum: 0.4583219515591719
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.24807751, 10.8086735 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.31328626058061415}
episode index:572
target Thresh 6.153409267308248
target distance 4.0
model initialize at round 572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.50127387,  8.9405067 ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 6.127270440228677}
done in step count: 64
reward sum = 0.37815724055881084
running average episode reward sum: 0.4581820480495727
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.19233698,  2.97497947,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.19395757200174205}
episode index:573
target Thresh 6.159830957118357
target distance 2.0
model initialize at round 573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.        ,  6.78245926,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.7824592590332324}
done in step count: 99
reward sum = -0.19552192256750187
running average episode reward sum: 0.45704319095790535
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.41994557,  1.09335329,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.929152940669873}
episode index:574
target Thresh 6.166249436886138
target distance 6.0
model initialize at round 574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2., 7., 0.]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 4.0000000000000195}
done in step count: 2
reward sum = 0.9648718185558298
running average episode reward sum: 0.4579263711798148
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.1318198 , 3.04328089, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8692583550642108}
episode index:575
target Thresh 6.172664708216209
target distance 4.0
model initialize at round 575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([13.13224972,  3.02619884,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 6.342932224656885}
done in step count: 94
reward sum = 0.11616822289349132
running average episode reward sum: 0.45733304106126205
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.31007353,  8.2746814 ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.0010422605200646}
episode index:576
target Thresh 6.179076772712394
target distance 1.0
model initialize at round 576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.15865868, 7.83640254, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 2.3214239595220416}
done in step count: 99
reward sum = -0.22498290359255027
running average episode reward sum: 0.45615051776030224
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.87007127, 1.12992873, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 8.871022821253268}
episode index:577
target Thresh 6.185485631977702
target distance 4.0
model initialize at round 577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([7.50067291, 8.08699824, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.66736209979073}
done in step count: 99
reward sum = -0.26491478396843815
running average episode reward sum: 0.45490299993724215
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.87054012, 1.13963446, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.957304920418832}
episode index:578
target Thresh 6.191891287614355
target distance 4.0
model initialize at round 578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.        ,  7.93036461,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.8780880203103263}
done in step count: 99
reward sum = -0.2541471183345636
running average episode reward sum: 0.4536783883340093
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.8563246 ,  1.12843377,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 8.912798619266635}
episode index:579
target Thresh 6.198293741223759
target distance 5.0
model initialize at round 579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.,  7.,  0.]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 3.0000000000000195}
done in step count: 2
reward sum = 0.9673863439139022
running average episode reward sum: 0.4545640917056988
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.54634529,  3.        ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0980904301148142}
episode index:580
target Thresh 6.204692994406534
target distance 6.0
model initialize at round 580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.8700727 , 1.12993015, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 7.950769466037958}
done in step count: 19
reward sum = 0.748096933669571
running average episode reward sum: 0.45506931174350235
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.92362109, 8.02399626, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9789877588024276}
episode index:581
target Thresh 6.21108904876249
target distance 2.0
model initialize at round 581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.73643014, 11.88547496,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.954648529859384}
done in step count: 5
reward sum = 0.9285820035932421
running average episode reward sum: 0.4558829074339658
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.89786076,  8.01108154,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8979291451211731}
episode index:582
target Thresh 6.217481905890644
target distance 3.0
model initialize at round 582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.34450841,  5.        ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0576795564897516}
done in step count: 1
reward sum = 0.9841453772620029
running average episode reward sum: 0.4567890180168612
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.64104879,  3.09296346,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.110701958246226}
episode index:583
target Thresh 6.223871567389207
target distance 2.0
model initialize at round 583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([13.90203786,  8.88047194,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.5680765062682323}
done in step count: 99
reward sum = 0.1286731625837528
running average episode reward sum: 0.4562271757986539
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.5602157 ,  9.57186318,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.705083522663105}
episode index:584
target Thresh 6.230258034855592
target distance 6.0
model initialize at round 584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([6.49914885, 9.78458059, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 5.879233841680599}
done in step count: 5
reward sum = 0.9338894378570937
running average episode reward sum: 0.4570436924859333
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.99238168, 6.65598433, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.6560285682498964}
episode index:585
target Thresh 6.236641309886422
target distance 6.0
model initialize at round 585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([13.44520497,  5.6196363 ,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 6.240787885048493}
done in step count: 3
reward sum = 0.95330443453641
running average episode reward sum: 0.4578905538204904
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.55952671, 10.54087389,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.7782125067255771}
episode index:586
target Thresh 6.243021394077512
target distance 2.0
model initialize at round 586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.00494245, 11.86740305,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.8674095874256116}
done in step count: 4
reward sum = 0.9438022907494453
running average episode reward sum: 0.45871834212871687
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.61780338, 10.74410471,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.967146751135198}
episode index:587
target Thresh 6.249398289023884
target distance 2.0
model initialize at round 587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.13106632, 4.80792403, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.194765619725835}
done in step count: 99
reward sum = -0.20668547974636872
running average episode reward sum: 0.4575867029758681
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([4.53908369, 1.08787709, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 7.363721143125439}
episode index:588
target Thresh 6.255771996319764
target distance 2.0
model initialize at round 588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.02619884, 8.13224972, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.8679340110709985}
done in step count: 99
reward sum = -0.21083948878811684
running average episode reward sum: 0.45645185375385794
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([13.12992873,  1.12992873,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 9.784399608361012}
episode index:589
target Thresh 6.262142517558577
target distance 6.0
model initialize at round 589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.68581518, 1.11023545, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 6.923813834313668}
done in step count: 99
reward sum = -0.20609093458269923
running average episode reward sum: 0.45532889987532144
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([4.59261073, 1.09401776, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 7.376599560889139}
episode index:590
target Thresh 6.268509854332949
target distance 6.0
model initialize at round 590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([5.19380736, 9.77986765, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 6.60358050727583}
done in step count: 48
reward sum = 0.5299816928756133
running average episode reward sum: 0.45545521593792765
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.89334496, 3.71373504, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.30548800952993077}
episode index:591
target Thresh 6.27487400823472
target distance 5.0
model initialize at round 591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([10.31195148, 11.88785682,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.358836865066253}
done in step count: 4
reward sum = 0.9448191148541082
running average episode reward sum: 0.4562818441455564
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.89846871,  8.05011159,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8998651005075938}
episode index:592
target Thresh 6.281234980854928
target distance 4.0
model initialize at round 592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4., 3., 0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 6.082762530298199}
done in step count: 99
reward sum = -0.1676913308984281
running average episode reward sum: 0.45522961282170477
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4.87276248, 1.15655759, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 8.063921390379186}
episode index:593
target Thresh 6.287592773783816
target distance 2.0
model initialize at round 593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.4272809 ,  7.04718423,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.4956149628340762}
done in step count: 5
reward sum = 0.9470756254802785
running average episode reward sum: 0.4560576364120391
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.25029969,  4.36289501,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6845090954485422}
episode index:594
target Thresh 6.293947388610828
target distance 2.0
model initialize at round 594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.23470044,  9.        ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.7652995586395512}
done in step count: 0
reward sum = 0.9946627880040724
running average episode reward sum: 0.4569628551542106
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.23470044,  9.        ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.7652995586395512}
episode index:595
target Thresh 6.300298826924623
target distance 3.0
model initialize at round 595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([8.02619884, 8.13224972, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.0458292297909835}
done in step count: 99
reward sum = -0.22833183278926028
running average episode reward sum: 0.455813031852292
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.85781391, 1.13211537, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 10.097727878189502}
episode index:596
target Thresh 6.306647090313058
target distance 6.0
model initialize at round 596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([9.14069951, 8.12687206, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 4.544667281321338}
done in step count: 78
reward sum = 0.21203988251030034
running average episode reward sum: 0.4554047016188883
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.20488792, 10.3396197 ,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.3966366104584925}
episode index:597
target Thresh 6.3129921803632
target distance 1.0
model initialize at round 597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.38965225,  7.67613304,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.7208285312806526}
done in step count: 5
reward sum = 0.9302968271680929
running average episode reward sum: 0.45619883560810104
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.06785589,  6.73516965,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.7382945451651378}
episode index:598
target Thresh 6.3193340986613205
target distance 3.0
model initialize at round 598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([4.35002303, 6.57828927, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.374506053511939}
done in step count: 2
reward sum = 0.9758451888232839
running average episode reward sum: 0.45706635873533835
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.31848609, 8.06993008, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9830887318999566}
episode index:599
target Thresh 6.325672846792898
target distance 3.0
model initialize at round 599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.65307403,  8.83422184,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.7245639604417002}
done in step count: 1
reward sum = 0.9833105979151069
running average episode reward sum: 0.45794343246730457
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.65307403, 10.4070555 ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6869794864463273}
episode index:600
target Thresh 6.332008426342625
target distance 4.0
model initialize at round 600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 5.951181230293016}
done in step count: 23
reward sum = 0.7379725745634982
running average episode reward sum: 0.45840937113967767
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.86641259,  6.9873298 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.313579425623723}
episode index:601
target Thresh 6.338340838894389
target distance 5.0
model initialize at round 601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4.13836849, 9.34394979, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 3.9692089914318327}
done in step count: 2
reward sum = 0.9688584730807795
running average episode reward sum: 0.45925729323592535
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.21197957, 5.34394979, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.6894470313481136}
episode index:602
target Thresh 6.3446700860313
target distance 6.0
model initialize at round 602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.86775028, 3.02619884, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.97489780208337}
done in step count: 99
reward sum = -0.21452310776628838
running average episode reward sum: 0.4581399128030858
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([1.31920293, 1.10726854, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 10.555302117071362}
episode index:603
target Thresh 6.350996169335665
target distance 5.0
model initialize at round 603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([3.31129539, 9.        , 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 5.688704609870921}
done in step count: 99
reward sum = -0.23245139456284206
running average episode reward sum: 0.45699654971142045
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([1.12992873, 1.12992873, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 11.129961533789027}
episode index:604
target Thresh 6.357319090389004
target distance 4.0
model initialize at round 604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([1.42846263, 4.80119824, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 5.4311388685256725}
done in step count: 99
reward sum = -0.19484074919849848
running average episode reward sum: 0.4559191326884289
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.39896376, 5.85653173, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.373262961819899}
episode index:605
target Thresh 6.363638850772054
target distance 4.0
model initialize at round 605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([10.        ,  9.72766149,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 2.018456901642752}
done in step count: 1
reward sum = 0.9836214212416801
running average episode reward sum: 0.45678992854412737
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.       , 9.8518976, 0.       ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.14810240268718822}
episode index:606
target Thresh 6.3699554520647474
target distance 6.0
model initialize at round 606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.90527689, 3.22180808, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.778853754623358}
done in step count: 99
reward sum = -0.19952892854647558
running average episode reward sum: 0.4557086783677013
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.84173517, 1.12785801, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.061285330881185}
episode index:607
target Thresh 6.37626889584624
target distance 5.0
model initialize at round 607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([9.7116408 , 8.33459545, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 4.339676001769729}
done in step count: 99
reward sum = -0.1699576243295837
running average episode reward sum: 0.45467962194879125
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.37016037,  1.10917637,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 8.008897367185089}
episode index:608
target Thresh 6.382579183694892
target distance 4.0
model initialize at round 608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.1102274 ,  4.30504283,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 4.696250944672872}
done in step count: 27
reward sum = 0.6810062762797545
running average episode reward sum: 0.4550512584912066
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.06451559,  8.1265129 ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.8758664110467145}
episode index:609
target Thresh 6.3888863171882715
target distance 4.0
model initialize at round 609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.19573641, 9.99611402, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 5.137211385045639}
done in step count: 27
reward sum = 0.6934831434644322
running average episode reward sum: 0.4554421304337857
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.09166244, 5.72639336, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.1630668288207064}
episode index:610
target Thresh 6.395190297903164
target distance 4.0
model initialize at round 610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.02313542,  6.86767018,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.34544129505201}
done in step count: 2
reward sum = 0.9716644004068534
running average episode reward sum: 0.45628701139937167
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.51556492,  8.22090789,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9174213078919904}
episode index:611
target Thresh 6.401491127415568
target distance 5.0
model initialize at round 611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.86752314,  3.97615675,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 4.436097063407484}
done in step count: 29
reward sum = 0.6442692780323248
running average episode reward sum: 0.4565941719657654
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.2584947 ,  7.22482395,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8171397787583331}
episode index:612
target Thresh 6.407788807300689
target distance 2.0
model initialize at round 612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 4.09086735, 11.69505588,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.4110192688232095}
done in step count: 47
reward sum = 0.543284283432479
running average episode reward sum: 0.45673559139719555
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.24512719, 8.21718281, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.820298779080342}
episode index:613
target Thresh 6.414083339132943
target distance 2.0
model initialize at round 613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([16.        ,  2.82935691,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.7487301332298584}
done in step count: 99
reward sum = -0.22697268025199366
running average episode reward sum: 0.4556220600101447
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.43676322,  1.08931238,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 4.930072514910372}
episode index:614
target Thresh 6.420374724485967
target distance 4.0
model initialize at round 614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 2.2360679774997982}
done in step count: 2
reward sum = 0.9631846492106022
running average episode reward sum: 0.45644736503323485
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.34425663,  1.10306161,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.1110795878122086}
episode index:615
target Thresh 6.426662964932608
target distance 4.0
model initialize at round 615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.24953604, 7.        , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 2.0155069421820104}
done in step count: 1
reward sum = 0.981728324577157
running average episode reward sum: 0.45730009386366327
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.31900269, 5.        , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.3190026879310728}
episode index:616
target Thresh 6.432948062044924
target distance 4.0
model initialize at round 616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.63259399, 6.        , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 2.033466787691763}
done in step count: 64
reward sum = 0.3715287300494828
running average episode reward sum: 0.45716108030804875
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.54467371, 4.50322993, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7415590445277208}
episode index:617
target Thresh 6.439230017394191
target distance 4.0
model initialize at round 617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([5.26941489, 7.26794429, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.0125641153766365}
done in step count: 99
reward sum = -0.2971540933877062
running average episode reward sum: 0.4559405055933307
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.8577461 , 1.12857412, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 8.912795605524975}
episode index:618
target Thresh 6.445508832550897
target distance 6.0
model initialize at round 618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([13.045681  ,  7.05521678,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 5.017248634174401}
done in step count: 89
reward sum = 0.20665275207263473
running average episode reward sum: 0.4555377790125218
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.88046731,  2.29635805,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.1270912479793678}
episode index:619
target Thresh 6.451784509084743
target distance 2.0
model initialize at round 619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.        , 9.46385753, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.46385753154746645}
done in step count: 0
reward sum = 0.9968386275426231
running average episode reward sum: 0.45641084489724776
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.        , 9.46385753, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.46385753154746645}
episode index:620
target Thresh 6.458057048564655
target distance 6.0
model initialize at round 620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.80047894,  7.11672115,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.174500191047338}
done in step count: 99
reward sum = -0.2655387228702045
running average episode reward sum: 0.455248285206801
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.12836067,  1.1444249 ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.685304042574039}
episode index:621
target Thresh 6.464326452558759
target distance 2.0
model initialize at round 621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 5.        , 10.82102382,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.8210238218307868}
done in step count: 87
reward sum = 0.17490741811061258
running average episode reward sum: 0.45479757641725727
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.54866636, 9.99364048, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.1350577913737159}
episode index:622
target Thresh 6.470592722634414
target distance 1.0
model initialize at round 622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([4.87142755, 5.87479791, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.8756110085607136}
done in step count: 1
reward sum = 0.9855189121831833
running average episode reward sum: 0.4556494565709746
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.74455052, 5.86418309, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.75683664172128}
episode index:623
target Thresh 6.476855860358184
target distance 5.0
model initialize at round 623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.80389607,  7.63309896,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.637247321847864}
done in step count: 63
reward sum = 0.4178596480531598
running average episode reward sum: 0.45558889598040114
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.12495005,  3.71093528,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.1274490648101456}
episode index:624
target Thresh 6.4831158672958535
target distance 1.0
model initialize at round 624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.33729884,  7.67925799,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 2.41350705349114}
done in step count: 78
reward sum = 0.27955714887231
running average episode reward sum: 0.4553072451850282
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.24574353, 10.92355596,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.1924170542869175}
episode index:625
target Thresh 6.489372745012425
target distance 3.0
model initialize at round 625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([13.        ,  9.55909014,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.0928867767824677}
done in step count: 1
reward sum = 0.9858018974081206
running average episode reward sum: 0.45615468073171045
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.99410594,  9.02425921,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.9757585920643188}
episode index:626
target Thresh 6.495626495072117
target distance 4.0
model initialize at round 626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([5.62068126, 8.01293128, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 3.993209877144651}
done in step count: 87
reward sum = 0.1973995185723655
running average episode reward sum: 0.45574199307276414
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.50208246, 4.89518785, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5129058262489986}
episode index:627
target Thresh 6.501877119038368
target distance 4.0
model initialize at round 627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.09194528, 5.42816928, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 3.5463936619246343}
done in step count: 2
reward sum = 0.9709907098385013
running average episode reward sum: 0.4565624528128369
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.09835732, 2.5190746 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.0403835633980933}
episode index:628
target Thresh 6.508124618473833
target distance 1.0
model initialize at round 628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.27250102,  1.15600323,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.1142644654487937}
done in step count: 0
reward sum = 0.9970477066323086
running average episode reward sum: 0.45742172984593626
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.27250102,  1.15600323,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.1142644654487937}
episode index:629
target Thresh 6.514368994940389
target distance 4.0
model initialize at round 629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([10.54262495,  9.        ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.9941760393771792}
done in step count: 99
reward sum = -0.1810468896523746
running average episode reward sum: 0.45640828759276436
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.55590941,  1.09039263,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 5.926269966915448}
episode index:630
target Thresh 6.520610249999128
target distance 1.0
model initialize at round 630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.34946084,  3.        ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.020285231373162}
done in step count: 50
reward sum = 0.48253416631396834
running average episode reward sum: 0.4564496915210072
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.37973967,  6.53657789,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8201455371243455}
episode index:631
target Thresh 6.526848385210365
target distance 3.0
model initialize at round 631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.29804084,  5.62951314,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 4.559173759444478}
done in step count: 99
reward sum = -0.296486744644234
running average episode reward sum: 0.45525833640049257
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.86420601,  1.12924732,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 9.064519682837878}
episode index:632
target Thresh 6.533083402133634
target distance 5.0
model initialize at round 632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([12.67980307,  7.37705553,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.083393274095774}
done in step count: 99
reward sum = -0.2586216515901055
running average episode reward sum: 0.4541305639076164
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.85084182,  1.12791111,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.9644676996424977}
episode index:633
target Thresh 6.539315302327687
target distance 3.0
model initialize at round 633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([7.45734262, 9.06713581, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 2.7083836736591755}
done in step count: 62
reward sum = 0.3785108464641107
running average episode reward sum: 0.4540112899053396
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.20937511,  9.49529114,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.5464146525467437}
episode index:634
target Thresh 6.545544087350501
target distance 2.0
model initialize at round 634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.        , 11.35376263,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.35376262664799185}
done in step count: 0
reward sum = 0.9969061393723432
running average episode reward sum: 0.4548662424241853
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.        , 11.35376263,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.35376262664799185}
episode index:635
target Thresh 6.551769758759271
target distance 5.0
model initialize at round 635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([13.12419156,  4.23665059,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 5.215523689845692}
done in step count: 3
reward sum = 0.9602944396931157
running average episode reward sum: 0.45566094084756414
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.44404041,  8.11315184,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.9918021683136944}
episode index:636
target Thresh 6.557992318110417
target distance 2.0
model initialize at round 636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.05158517, 8.13234798, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 2.0946634015467245}
done in step count: 43
reward sum = 0.5097098340012532
running average episode reward sum: 0.4557457899733941
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.28495531, 10.28297276,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.4015882362202201}
episode index:637
target Thresh 6.56421176695958
target distance 2.0
model initialize at round 637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([16.        ,  5.65392363,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.6768785262949522}
done in step count: 1
reward sum = 0.9829882051822733
running average episode reward sum: 0.45657218874331396
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.01452792,  7.65392363,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6540849907896817}
episode index:638
target Thresh 6.570428106861616
target distance 4.0
model initialize at round 638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.89674079, 7.71528113, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.7162138558451865}
done in step count: 12
reward sum = 0.854057186662578
running average episode reward sum: 0.45719423099357887
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.49466184, 1.75875323, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.5503547389249694}
episode index:639
target Thresh 6.5766413393706165
target distance 4.0
model initialize at round 639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([7.77558124, 9.        , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.421089186716686}
done in step count: 99
reward sum = -0.23112761803208243
running average episode reward sum: 0.4561187281044763
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.84131475, 1.13225455, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.869021297121291}
episode index:640
target Thresh 6.582851466039886
target distance 6.0
model initialize at round 640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([10.,  9.,  0.]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 4.123105625617675}
done in step count: 99
reward sum = -0.26532589520934935
running average episode reward sum: 0.4549932294721614
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([4.87031002, 1.13190325, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 8.93976171116014}
episode index:641
target Thresh 6.589058488421955
target distance 6.0
model initialize at round 641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([4.88213432, 2.24555883, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 7.0786884757242685}
done in step count: 99
reward sum = -0.26514481746654
running average episode reward sum: 0.45387151911867435
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([4.87007127, 1.12992873, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 8.153196811427431}
episode index:642
target Thresh 6.595262408068587
target distance 2.0
model initialize at round 642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.542575, 5.      , 0.      ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.5425750017166071}
done in step count: 0
reward sum = 0.9968430718080568
running average episode reward sum: 0.45471595388179936
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.542575, 5.      , 0.      ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.5425750017166071}
episode index:643
target Thresh 6.601463226530752
target distance 6.0
model initialize at round 643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.90362436, 6.37539884, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 4.467734544504711}
done in step count: 4
reward sum = 0.9388140924873
running average episode reward sum: 0.45546765906596937
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.90750589, 2.4120182 , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9966573815024236}
episode index:644
target Thresh 6.607660945358661
target distance 3.0
model initialize at round 644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([12.14612992,  7.82335934,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 5.278933263011125}
done in step count: 99
reward sum = -0.256227196803302
running average episode reward sum: 0.45436425618865267
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([13.13281472,  3.97762619,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 7.926894416563293}
episode index:645
target Thresh 6.613855566101741
target distance 2.0
model initialize at round 645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.        , 9.42478108, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.424781084060598}
done in step count: 27
reward sum = 0.6861922764897139
running average episode reward sum: 0.45472312309314344
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.64696334, 6.56560314, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5597638033848469}
episode index:646
target Thresh 6.620047090308649
target distance 6.0
model initialize at round 646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([6., 9., 0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 4.4721359549996045}
done in step count: 18
reward sum = 0.7701819144137347
running average episode reward sum: 0.45521069464077957
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.17460127, 6.52565719, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5054569268495651}
episode index:647
target Thresh 6.626235519527264
target distance 5.0
model initialize at round 647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([15.87342238,  4.        ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.536907042863046}
done in step count: 2
reward sum = 0.9734445742795713
running average episode reward sum: 0.4560104382821975
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.90898621,  6.60712099,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9902574651944236}
episode index:648
target Thresh 6.632420855304696
target distance 2.0
model initialize at round 648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.        , 2.99865448, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.001345515251155227}
done in step count: 0
reward sum = 0.9962520172117418
running average episode reward sum: 0.4568428598213802
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.        , 2.99865448, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.001345515251155227}
episode index:649
target Thresh 6.638603099187274
target distance 4.0
model initialize at round 649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.83771986, 1.32550042, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 4.6773155941448366}
done in step count: 99
reward sum = -0.2752489439278843
running average episode reward sum: 0.45571656473868905
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.87007127, 1.12992873, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 4.947182859554558}
episode index:650
target Thresh 6.644782252720565
target distance 4.0
model initialize at round 650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.08901215,  5.        ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 2.0019798106212012}
done in step count: 1
reward sum = 0.9832088627189924
running average episode reward sum: 0.45652684476630856
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.60633886,  3.        ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.6063388586044276}
episode index:651
target Thresh 6.650958317449355
target distance 4.0
model initialize at round 651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([15.79656887,  6.        ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 2.6884307144892414}
done in step count: 1
reward sum = 0.982136019774681
running average episode reward sum: 0.4573329938077324
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.21258259,  4.        ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.21258258819572085}
episode index:652
target Thresh 6.657131294917658
target distance 3.0
model initialize at round 652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([14.45094824,  4.        ,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 5.5684061693794}
done in step count: 99
reward sum = -0.17338315492498657
running average episode reward sum: 0.45636711915423667
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([16.04621428,  1.14548447,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 8.835454950948778}
episode index:653
target Thresh 6.663301186668722
target distance 6.0
model initialize at round 653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([7.36269069, 8.68164802, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.7641579439789234}
done in step count: 4
reward sum = 0.9455861890345103
running average episode reward sum: 0.4571151605454909
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.62923151, 4.00065702, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6292318494487282}
episode index:654
target Thresh 6.66946799424502
target distance 3.0
model initialize at round 654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4., 6., 0.]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0000000000000187}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.4579104961788698
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.86775028, 4.02619884, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.304330955847632}
episode index:655
target Thresh 6.675631719188251
target distance 4.0
model initialize at round 655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([16.,  3.,  0.]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 7.211102550927953}
done in step count: 99
reward sum = -0.20470433860485412
running average episode reward sum: 0.4569004125892604
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([15.48160421,  1.087387  ,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 8.644710076736306}
episode index:656
target Thresh 6.681792363039347
target distance 6.0
model initialize at round 656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.23128407,  7.00000011,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.073195924145002}
done in step count: 2
reward sum = 0.9712952557357063
running average episode reward sum: 0.45768335755599776
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.62864425,  3.93007853,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.1226039666048788}
episode index:657
target Thresh 6.68794992733847
target distance 6.0
model initialize at round 657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([10.15666318,  8.6454612 ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.02918352359405}
done in step count: 4
reward sum = 0.9511792368376554
running average episode reward sum: 0.45843335129350793
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.67611173,  9.95445561,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.32707476933164525}
episode index:658
target Thresh 6.694104413625011
target distance 3.0
model initialize at round 658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.72321701,  3.40068805,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.7552326201167332}
done in step count: 2
reward sum = 0.9743666185661813
running average episode reward sum: 0.45921625458223736
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.40396964,  4.54674447,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6071507651589731}
episode index:659
target Thresh 6.7002558234375895
target distance 4.0
model initialize at round 659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([9.84859669, 9.40703416, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.5311305527639814}
done in step count: 99
reward sum = -0.251596556946136
running average episode reward sum: 0.458139265473861
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.12882508,  1.13977359,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.861067936423074}
episode index:660
target Thresh 6.706404158314063
target distance 5.0
model initialize at round 660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([5.        , 8.30728817, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.031215325453589}
done in step count: 6
reward sum = 0.9246102599345144
running average episode reward sum: 0.45884497045791645
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.76162205, 10.20148583,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8333360240662778}
episode index:661
target Thresh 6.712549419791511
target distance 3.0
model initialize at round 661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.42828763, 7.91673493, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 3.924678873407172}
done in step count: 99
reward sum = -0.27316013244777543
running average episode reward sum: 0.4577392225683308
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.83446676, 1.12923396, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 10.269674976753318}
episode index:662
target Thresh 6.718691609406247
target distance 5.0
model initialize at round 662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.30252659,  4.53144622,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 6.866135502355846}
done in step count: 20
reward sum = 0.763071853436331
running average episode reward sum: 0.45819975443992655
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.77120601, 10.44101297,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9524837024889313}
episode index:663
target Thresh 6.724830728693823
target distance 6.0
model initialize at round 663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 8.72671226, 11.87362522,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.5316842525613215}
done in step count: 7
reward sum = 0.9188889230867355
running average episode reward sum: 0.45889356342885246
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.80789251, 9.06562718, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.20300791437396587}
episode index:664
target Thresh 6.7309667791890195
target distance 3.0
model initialize at round 664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5.       , 9.5335145, 0.       ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.1034530900874577}
done in step count: 14
reward sum = 0.8433269280633979
running average episode reward sum: 0.4594716587140172
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.84323456, 10.80648419,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.8215790631064315}
episode index:665
target Thresh 6.737099762425846
target distance 2.0
model initialize at round 665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([10.10050532,  8.12995531,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.402886213408287}
done in step count: 1
reward sum = 0.9821222203355617
running average episode reward sum: 0.4602564193170525
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.81265062, 8.12572213, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.8941261525590399}
episode index:666
target Thresh 6.7432296799375475
target distance 5.0
model initialize at round 666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.8700727 , 1.12993015, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 7.12004435494473}
done in step count: 57
reward sum = 0.3718834872275376
running average episode reward sum: 0.460123926165494
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.53640435, 7.0097536 , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.126196057879673}
episode index:667
target Thresh 6.749356533256608
target distance 2.0
model initialize at round 667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.24753481,  7.63405406,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.645659520598296}
done in step count: 4
reward sum = 0.9529018487428399
running average episode reward sum: 0.46086161766635836
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.25433207,  4.22896985,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0726174315863954}
episode index:668
target Thresh 6.7554803239147345
target distance 6.0
model initialize at round 668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([8.02848341, 8.14298738, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.118634386826507}
done in step count: 49
reward sum = 0.4505168497727056
running average episode reward sum: 0.4608461546351272
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.83401005, 8.31872859, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7012013989713648}
episode index:669
target Thresh 6.76160105344288
target distance 3.0
model initialize at round 669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([9.        , 9.81123948, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.5534321908596793}
done in step count: 57
reward sum = 0.403624575178533
running average episode reward sum: 0.46076074929265465
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.75205523, 11.40847629,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.4778383530021488}
episode index:670
target Thresh 6.767718723371225
target distance 4.0
model initialize at round 670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.        , 10.12231588,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.003736802923435}
done in step count: 1
reward sum = 0.9836063817437223
running average episode reward sum: 0.461539952917768
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.        , 10.86757958,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8675795793532632}
episode index:671
target Thresh 6.773833335229187
target distance 5.0
model initialize at round 671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([14.75942361,  9.28929555,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 3.7705381345056965}
done in step count: 27
reward sum = 0.6893875871298227
running average episode reward sum: 0.4618790118972502
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.33391918,  9.33967042,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.747689544970879}
episode index:672
target Thresh 6.77994489054542
target distance 6.0
model initialize at round 672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.79146248, 7.09775496, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.102018569255002}
done in step count: 6
reward sum = 0.93106765507284
running average episode reward sum: 0.46257617184253347
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.33792529, 2.09869849, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.35204387603908865}
episode index:673
target Thresh 6.786053390847812
target distance 5.0
model initialize at round 673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.12305465,  6.26836181,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 4.357516002776082}
done in step count: 91
reward sum = 0.25595860576429286
running average episode reward sum: 0.46226961759019186
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.12516726,  2.18535425,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8942530567204526}
episode index:674
target Thresh 6.792158837663486
target distance 2.0
model initialize at round 674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.43017828,  9.62031126,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8423080474802762}
done in step count: 0
reward sum = 0.9984335542533123
running average episode reward sum: 0.4630639345333965
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.43017828,  9.62031126,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8423080474802762}
episode index:675
target Thresh 6.798261232518808
target distance 1.0
model initialize at round 675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.16045196,  1.1370052 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.0434261758519265}
done in step count: 12
reward sum = 0.8666360381797467
running average episode reward sum: 0.46366093468671954
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.09332236,  2.88246023,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.15008218188447606}
episode index:676
target Thresh 6.804360576939373
target distance 4.0
model initialize at round 676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 2.        , 11.38512361,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 5.385123610496487}
done in step count: 99
reward sum = -0.1844704562662089
running average episode reward sum: 0.4627035766498615
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.71580779, 9.67206251, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 3.741179478914238}
episode index:677
target Thresh 6.81045687245002
target distance 3.0
model initialize at round 677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.69167896, 3.28412366, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.8500409157107032}
done in step count: 1
reward sum = 0.9850422259491747
running average episode reward sum: 0.46347398763702863
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.10110789, 4.61530772, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.3977574074663477}
episode index:678
target Thresh 6.81655012057482
target distance 3.0
model initialize at round 678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.        , 8.04881144, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 4.1704764765972255}
done in step count: 51
reward sum = 0.49711226728180974
running average episode reward sum: 0.463523528549613
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.08085703, 3.73057896, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.2812926513195062}
episode index:679
target Thresh 6.822640322837088
target distance 2.0
model initialize at round 679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.        ,  8.85631526,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.1436847448348892}
done in step count: 25
reward sum = 0.7205799550128991
running average episode reward sum: 0.46390155270617667
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.16943182, 11.8503059 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.1886394019291262}
episode index:680
target Thresh 6.828727480759374
target distance 5.0
model initialize at round 680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([2.54932404, 7.50301719, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 5.103277349656487}
done in step count: 20
reward sum = 0.7548394056313262
running average episode reward sum: 0.4643287742229537
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.35294391, 10.76537512,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.0022378227206308}
episode index:681
target Thresh 6.834811595863467
target distance 6.0
model initialize at round 681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([5.        , 9.50344288, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.579876088410439}
done in step count: 99
reward sum = -0.20536513386928654
running average episode reward sum: 0.4633468183459856
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 4.05923553, 11.86356759,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 8.863765519473244}
episode index:682
target Thresh 6.840892669670395
target distance 6.0
model initialize at round 682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.52189582, 11.91295501,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 7.93014704573568}
done in step count: 54
reward sum = 0.4571304749532278
running average episode reward sum: 0.46333771681832414
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.85994925,  3.79003703,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8852102408180496}
episode index:683
target Thresh 6.846970703700425
target distance 1.0
model initialize at round 683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.59765839,  1.583565  ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.5373625703249052}
done in step count: 9
reward sum = 0.8798110438814505
running average episode reward sum: 0.4639465959514573
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.86884051,  3.09825696,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8743787885345542}
episode index:684
target Thresh 6.853045699473069
target distance 5.0
model initialize at round 684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 5.98656382, 11.75220268,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 6.481311533622326}
done in step count: 8
reward sum = 0.9025446297134951
running average episode reward sum: 0.46458688505183987
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.55522809, 5.32862002, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8053403835275602}
episode index:685
target Thresh 6.859117658507074
target distance 5.0
model initialize at round 685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([12.11063012,  7.85455322,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 3.5988053817105095}
done in step count: 7
reward sum = 0.9028189306250173
running average episode reward sum: 0.46522570727570745
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.51929174,  9.8825942 ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5323983788916405}
episode index:686
target Thresh 6.865186582320431
target distance 3.0
model initialize at round 686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 9.09573148, 11.86936045,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 4.082646813808891}
done in step count: 94
reward sum = 0.14477210032246535
running average episode reward sum: 0.46475925369935633
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.08812596,  9.84273631,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.2416596701914413}
episode index:687
target Thresh 6.871252472430373
target distance 5.0
model initialize at round 687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 9.22543764, 10.55437481,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 3.2727327130918886}
done in step count: 2
reward sum = 0.9716524261791327
running average episode reward sum: 0.465496017031449
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.22543764, 10.78160119,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.1003850548115932}
episode index:688
target Thresh 6.8773153303533645
target distance 6.0
model initialize at round 688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.        ,  2.42554712,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.574452877044682}
done in step count: 75
reward sum = 0.3325223917309551
running average episode reward sum: 0.4653030219294163
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.33214137,  7.20540162,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8612226673476094}
episode index:689
target Thresh 6.88337515760513
target distance 3.0
model initialize at round 689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.4142135623730956}
done in step count: 1
reward sum = 0.9789183777770409
running average episode reward sum: 0.46604739201035494
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.32945418, 11.89469515,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.1180836784889556}
episode index:690
target Thresh 6.88943195570062
target distance 3.0
model initialize at round 690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([ 5.6218958 , 11.90216929,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 3.2127492577797283}
done in step count: 99
reward sum = -0.1806662079695567
running average episode reward sum: 0.4651114823142914
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([11.48661958, 11.91280666,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 5.34922404307538}
episode index:691
target Thresh 6.895485726154035
target distance 1.0
model initialize at round 691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([10.97380116, 11.86775028,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 2.717421961331027}
done in step count: 49
reward sum = 0.47878171759429206
running average episode reward sum: 0.4651312369895515
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.85053562, 9.93197758, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.164215259388649}
episode index:692
target Thresh 6.901536470478821
target distance 6.0
model initialize at round 692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([14.,  3.,  0.]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 8.062257748298542}
done in step count: 14
reward sum = 0.80012705729562
running average episode reward sum: 0.46561463644165263
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.89749095, 10.80363184,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.8101433466733262}
episode index:693
target Thresh 6.90758419018766
target distance 6.0
model initialize at round 693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([10.00719136, 11.853507  ,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 4.9193581714659445}
done in step count: 53
reward sum = 0.45189925048974433
running average episode reward sum: 0.46559487363768737
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.63634148, 8.95579008, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.6378753764590407}
episode index:694
target Thresh 6.913628886792484
target distance 4.0
model initialize at round 694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.90572568,  6.43264176,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.7224377596133285}
done in step count: 2
reward sum = 0.9666234396964601
running average episode reward sum: 0.4663157780492827
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.90604534,  9.45406388,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0134555587585035}
episode index:695
target Thresh 6.919670561804465
target distance 1.0
model initialize at round 695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.0103711 , 11.85413265,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.307252056893654}
done in step count: 0
reward sum = 0.9962482867792113
running average episode reward sum: 0.46707717533194065
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.0103711 , 11.85413265,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.307252056893654}
episode index:696
target Thresh 6.925709216734026
target distance 4.0
model initialize at round 696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 2.2360679774998142}
done in step count: 8
reward sum = 0.895737238642683
running average episode reward sum: 0.4676921825963751
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.55441039, 11.90659421,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0101797654978542}
episode index:697
target Thresh 6.931744853090826
target distance 6.0
model initialize at round 697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.12844018,  5.87032435,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.584648102119205}
done in step count: 10
reward sum = 0.8555768376548893
running average episode reward sum: 0.468247891271244
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.42513544, 10.67526516,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5349699615647092}
episode index:698
target Thresh 6.93777747238378
target distance 2.0
model initialize at round 698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.85057389,  7.99854968,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8505751287625084}
done in step count: 0
reward sum = 0.9964023961291957
running average episode reward sum: 0.46900347711510376
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.85057389,  7.99854968,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8505751287625084}
episode index:699
target Thresh 6.943807076121034
target distance 3.0
model initialize at round 699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([11.65418575, 11.89422444,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 3.463251411008839}
done in step count: 6
reward sum = 0.9166707787309544
running average episode reward sum: 0.4696430018316978
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.07903306, 11.86819536,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8717851804386902}
episode index:700
target Thresh 6.949833665810001
target distance 2.0
model initialize at round 700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.,  6.,  0.]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 4.123105625617638}
done in step count: 99
reward sum = -0.22199475681427294
running average episode reward sum: 0.4686563573828448
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([11.61812714, 11.88973231,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.45197919614578}
episode index:701
target Thresh 6.955857242957316
target distance 5.0
model initialize at round 701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 3.63064698, 11.79380515,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.440875183231884}
done in step count: 25
reward sum = 0.6959865946593767
running average episode reward sum: 0.4689801896296774
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.35292337, 11.86837876,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0829542143755317}
episode index:702
target Thresh 6.961877809068881
target distance 3.0
model initialize at round 702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([16.16701913,  9.25946331,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.9140864803162194}
done in step count: 99
reward sum = -0.190365767770991
running average episode reward sum: 0.4680422864185812
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 4.28356817, 11.87881226,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.3564730936348}
episode index:703
target Thresh 6.967895365649835
target distance 4.0
model initialize at round 703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([13.        , 10.53288507,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 2.5198683798918857}
done in step count: 99
reward sum = -0.22554476970531212
running average episode reward sum: 0.46705707753204156
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.33643299, 11.87939999,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 2.9548714807544934}
episode index:704
target Thresh 6.97390991420457
target distance 4.0
model initialize at round 704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.86278439, 7.51811683, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 3.5871865428651195}
done in step count: 2
reward sum = 0.9730236980434189
running average episode reward sum: 0.46777476068170304
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.91392738, 10.42285645,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.080906067292487}
episode index:705
target Thresh 6.979921456236719
target distance 2.0
model initialize at round 705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 7.22395275, 11.87973186,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 2.912106881115272}
done in step count: 10
reward sum = 0.8564965317975254
running average episode reward sum: 0.46832535809121556
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.03076689, 11.86731257,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.3006321189728827}
episode index:706
target Thresh 6.98592999324917
target distance 2.0
model initialize at round 706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.87471476, 5.83884419, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 3.9372389629606026}
done in step count: 99
reward sum = -0.20254425587082778
running average episode reward sum: 0.4673764618904206
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([10.66274314, 11.59893633,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 11.68467906618741}
episode index:707
target Thresh 6.991935526744057
target distance 6.0
model initialize at round 707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.        ,  9.26012659,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 6.2601265907287935}
done in step count: 99
reward sum = -0.19022038257491347
running average episode reward sum: 0.46644765278806843
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.23061052, 11.84833152,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 8.881718915425983}
episode index:708
target Thresh 6.997938058222762
target distance 2.0
model initialize at round 708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.        , 10.61282885,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.38717114925382745}
done in step count: 0
reward sum = 0.9968875742064671
running average episode reward sum: 0.46719580500445546
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.        , 10.61282885,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.38717114925382745}
episode index:709
target Thresh 7.003937589185919
target distance 1.0
model initialize at round 709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([12.        ,  9.95689368,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.3840684663592007}
done in step count: 0
reward sum = 0.9963132661679536
running average episode reward sum: 0.4679410408652491
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([12.        ,  9.95689368,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.3840684663592007}
episode index:710
target Thresh 7.009934121133411
target distance 5.0
model initialize at round 710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 5.03235568, 11.8580069 ,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 4.166939450340967}
done in step count: 29
reward sum = 0.6787944482325832
running average episode reward sum: 0.468237599806694
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.28385164, 8.23056995, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8201185039511466}
episode index:711
target Thresh 7.015927655564373
target distance 7.0
model initialize at round 711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 8.85089636, 11.57807765,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 8.997691720249955}
done in step count: 99
reward sum = -0.19697287189118406
running average episode reward sum: 0.46730331543633175
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 2.23519278, 11.83684089,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 8.033095273767264}
episode index:712
target Thresh 7.021918193977184
target distance 6.0
model initialize at round 712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([10.98123265, 11.86484517,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 4.935365113758179}
done in step count: 99
reward sum = -0.22865321499870167
running average episode reward sum: 0.46632721932071464
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([12.91768563, 10.34602089,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.4794768371611324}
episode index:713
target Thresh 7.027905737869482
target distance 6.0
model initialize at round 713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([11.        , 10.60426927,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 4.309719234270923}
done in step count: 16
reward sum = 0.7985154879765745
running average episode reward sum: 0.46679246899670324
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.32779554, 9.10822436, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.3451991103858082}
episode index:714
target Thresh 7.033890288738151
target distance 6.0
model initialize at round 714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.99554889, 11.85282048,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 6.9260429231892084}
done in step count: 99
reward sum = -0.18982253193574225
running average episode reward sum: 0.46587412633805647
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([12.46961121, 11.91250676,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 7.361088036898891}
episode index:715
target Thresh 7.039871848079331
target distance 6.0
model initialize at round 715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 2.27947797, 11.8863133 ,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 8.071811039590317}
done in step count: 99
reward sum = -0.2587166691732031
running average episode reward sum: 0.46486212802030324
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 1.12992873, 11.87007127,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 8.377071743440359}
episode index:716
target Thresh 7.04585041738841
target distance 5.0
model initialize at round 716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 1.12376922, 10.82114823,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 7.113306589818675}
done in step count: 29
reward sum = 0.6715379916569741
running average episode reward sum: 0.4651503788761424
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.21632655, 8.13027425, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.1707121545156955}
episode index:717
target Thresh 7.051825998160032
target distance 1.0
model initialize at round 717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.        , 4.55214095, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 2.0748155652838225}
done in step count: 1
reward sum = 0.9842398295219944
running average episode reward sum: 0.4658733446848414
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.85405385, 4.27414429, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.3105726485483181}
episode index:718
target Thresh 7.057798591888093
target distance 6.0
model initialize at round 718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([14.06495881,  5.4046452 ,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 4.71714139758751}
done in step count: 55
reward sum = 0.424802971945508
running average episode reward sum: 0.46581622316503696
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.44117776,  9.46460847,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.7739033421198217}
episode index:719
target Thresh 7.063768200065736
target distance 7.0
model initialize at round 719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([9.99505755, 8.13259695, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 5.13149342950278}
done in step count: 29
reward sum = 0.6767851090719741
running average episode reward sum: 0.46610923550657446
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.7708926 ,  7.30810407,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8301828243002106}
episode index:720
target Thresh 7.069734824185368
target distance 2.0
model initialize at round 720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.27919245,  5.08034205,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.9511405303431981}
done in step count: 69
reward sum = 0.3379398245626348
running average episode reward sum: 0.46593146933328183
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.74698662,  6.93672796,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.1981019445097736}
episode index:721
target Thresh 7.075698465738643
target distance 3.0
model initialize at round 721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.90934467, 6.42786548, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 2.573731608554455}
done in step count: 82
reward sum = 0.2842071342476725
running average episode reward sum: 0.4656797735783156
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.29749621, 9.88075096, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.9296377023208662}
episode index:722
target Thresh 7.0816591262164765
target distance 2.0
model initialize at round 722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.59253546, 11.90655032,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.9512319173710457}
done in step count: 99
reward sum = -0.2212919712930409
running average episode reward sum: 0.46472960518983514
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.55995809, 11.90610914,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 4.163100924626892}
episode index:723
target Thresh 7.0876168071090255
target distance 1.0
model initialize at round 723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.55546296,  2.50749874,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7423587986501016}
done in step count: 0
reward sum = 0.9995754294464477
running average episode reward sum: 0.46546834251615643
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.55546296,  2.50749874,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7423587986501016}
episode index:724
target Thresh 7.093571509905715
target distance 3.0
model initialize at round 724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.21464717,  6.04468226,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.9670640194688374}
done in step count: 1
reward sum = 0.988042636466961
running average episode reward sum: 0.46618913464574374
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.90587866,  7.22447801,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7812126398629662}
episode index:725
target Thresh 7.099523236095222
target distance 2.0
model initialize at round 725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.        ,  2.97465694,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0253430604934484}
done in step count: 1
reward sum = 0.9868328716905007
running average episode reward sum: 0.46690627477941427
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.71799457,  3.35533166,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7036507170574531}
episode index:726
target Thresh 7.105471987165473
target distance 3.0
model initialize at round 726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.10682379,  7.00000485,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.079154675798905}
done in step count: 99
reward sum = -0.18859347877309493
running average episode reward sum: 0.46600462449942454
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([11.69993896,  8.57663258,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 6.967235952549074}
episode index:727
target Thresh 7.111417764603662
target distance 4.0
model initialize at round 727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([12.       ,  9.7521261,  0.       ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.6589369797285927}
done in step count: 99
reward sum = -0.14293605187128117
running average episode reward sum: 0.46516816752638784
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([11.77453248, 11.87912334,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.472169894721015}
episode index:728
target Thresh 7.117360569896228
target distance 7.0
model initialize at round 728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([4.67865016, 3.9999999 , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 6.0026132447671365}
done in step count: 42
reward sum = 0.5712717948176869
running average episode reward sum: 0.4653137143402305
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.59756312, 8.19166925, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.0052264863028642}
episode index:729
target Thresh 7.123300404528875
target distance 7.0
model initialize at round 729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([13.50366938,  3.84664714,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 7.080921586249989}
done in step count: 99
reward sum = -0.21006937527486155
running average episode reward sum: 0.46438853202568925
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 4.24118675, 11.87465565,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 6.05625824475103}
episode index:730
target Thresh 7.12923726998656
target distance 1.0
model initialize at round 730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.        ,  1.80566204,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.5577044560474176}
done in step count: 13
reward sum = 0.8471528890817346
running average episode reward sum: 0.4649121494772023
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.28457308,  3.65000842,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.966616072987113}
episode index:731
target Thresh 7.135171167753502
target distance 5.0
model initialize at round 731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 7.4921021293843415}
done in step count: 67
reward sum = 0.3518954252356954
running average episode reward sum: 0.46475775504517836
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.80593746,  4.66745221,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.3850302643373534}
episode index:732
target Thresh 7.141102099313175
target distance 1.0
model initialize at round 732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.82703424, 8.        , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.00746535608009}
done in step count: 99
reward sum = -0.20924430114113982
running average episode reward sum: 0.4638382433723457
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 2.12344371, 11.87196226,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.650604872613928}
episode index:733
target Thresh 7.14703006614831
target distance 3.0
model initialize at round 733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([12.82335934,  7.14612992,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.278933263011125}
done in step count: 99
reward sum = -0.2035545847917823
running average episode reward sum: 0.4629289888380621
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([11.5380649 , 11.91041504,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.21163310997187}
episode index:734
target Thresh 7.152955069740901
target distance 1.0
model initialize at round 734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.        ,  9.45660782,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.7668351188543072}
done in step count: 99
reward sum = -0.17924728118699526
running average episode reward sum: 0.4620552796271437
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.34394555, 11.89506358,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.9499275635946542}
episode index:735
target Thresh 7.158877111572197
target distance 6.0
model initialize at round 735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 6.50616217, 11.        ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.615787849506958}
done in step count: 99
reward sum = -0.2625846424286936
running average episode reward sum: 0.4610707145156548
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.16066963, 11.87426106,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.0536138879509456}
episode index:736
target Thresh 7.164796193122711
target distance 4.0
model initialize at round 736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([3.40582323, 6.46831393, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 3.6248017124989302}
done in step count: 2
reward sum = 0.9683166107009172
running average episode reward sum: 0.46175897217669315
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.82673942, 8.98437313, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.8268870994733329}
episode index:737
target Thresh 7.170712315872209
target distance 2.0
model initialize at round 737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.66292465, 8.        , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.33707535266875643}
done in step count: 0
reward sum = 0.9969147849815172
running average episode reward sum: 0.4624841155544775
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.66292465, 8.        , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.33707535266875643}
episode index:738
target Thresh 7.176625481299724
target distance 5.0
model initialize at round 738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.06000984,  8.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 3.57261274587}
done in step count: 8
reward sum = 0.8923401941767609
running average episode reward sum: 0.46306578819131416
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.42649674, 11.90300868,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9986611717815446}
episode index:739
target Thresh 7.18253569088355
target distance 4.0
model initialize at round 739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.17030585, 9.        , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 2.007237923591647}
done in step count: 1
reward sum = 0.9822607068245559
running average episode reward sum: 0.46376740294622393
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.1371425 , 10.96424527,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.863597981826038}
episode index:740
target Thresh 7.188442946101235
target distance 2.0
model initialize at round 740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.00494245, 11.86740305,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.8674095874256116}
done in step count: 99
reward sum = -0.19669055297446136
running average episode reward sum: 0.4628760966629301
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.25273987, 11.88226692,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.0251731961812656}
episode index:741
target Thresh 7.1943472484295965
target distance 3.0
model initialize at round 741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.18196601,  6.65821159,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 3.748558608714738}
done in step count: 99
reward sum = -0.17555132409163096
running average episode reward sum: 0.4620156823492447
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 6.9794024 , 11.85083668,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.944341577428908}
episode index:742
target Thresh 7.200248599344707
target distance 5.0
model initialize at round 742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.19507349,  5.68140697,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.392966242906759}
done in step count: 2
reward sum = 0.9716541397394605
running average episode reward sum: 0.46270160221114276
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.9347353 ,  9.55047321,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.45423981921577383}
episode index:743
target Thresh 7.2061470003219075
target distance 2.0
model initialize at round 743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.40439453, 7.        , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5956054726229332}
done in step count: 0
reward sum = 0.9950905324950827
running average episode reward sum: 0.46341717873034155
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.40439453, 7.        , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5956054726229332}
episode index:744
target Thresh 7.212042452835798
target distance 3.0
model initialize at round 744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 4.06031787, 11.        ,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.8713950863191053}
done in step count: 99
reward sum = -0.24202326575242297
running average episode reward sum: 0.4624702788048613
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 1.09684273, 11.55705655,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.711868592392643}
episode index:745
target Thresh 7.217934958360236
target distance 6.0
model initialize at round 745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 1.29507818, 10.68719857,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 6.90111462795234}
done in step count: 81
reward sum = 0.21893217240473065
running average episode reward sum: 0.4621438202171936
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.51088832, 4.56252203, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7454269056834907}
episode index:746
target Thresh 7.223824518368357
target distance 1.0
model initialize at round 746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.10380078,  1.34875398,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.1078332087199054}
done in step count: 0
reward sum = 0.9966662023903912
running average episode reward sum: 0.46285937896173596
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.10380078,  1.34875398,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.1078332087199054}
episode index:747
target Thresh 7.229711134332542
target distance 4.0
model initialize at round 747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.        , 2.22290611, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 4.880637869004108}
done in step count: 6
reward sum = 0.9311336539984927
running average episode reward sum: 0.4634854140887904
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.11226147, 6.79376745, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.23480737281872346}
episode index:748
target Thresh 7.235594807724451
target distance 3.0
model initialize at round 748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([14.        , 10.04997039,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.4499785572701178}
done in step count: 99
reward sum = -0.2050464841367465
running average episode reward sum: 0.4625928481365534
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([15.02901351, 11.86396008,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 3.5098665462870615}
episode index:749
target Thresh 7.241475540015003
target distance 3.0
model initialize at round 749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.,  8.,  0.]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 4.999999999999981}
done in step count: 99
reward sum = -0.20293353677168274
running average episode reward sum: 0.4617054796233424
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 8.91772794, 11.8089024 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.302890739166228}
episode index:750
target Thresh 7.247353332674375
target distance 2.0
model initialize at round 750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.02429545, 7.51508665, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.48552160100250724}
done in step count: 0
reward sum = 0.9982779416208586
running average episode reward sum: 0.4624199569362552
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.02429545, 7.51508665, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.48552160100250724}
episode index:751
target Thresh 7.253228187172021
target distance 1.0
model initialize at round 751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.62603215, 11.90008434,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.000559106211791}
done in step count: 99
reward sum = -0.2269396464976436
running average episode reward sum: 0.46150325533594416
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([10.13228269, 11.86527221,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 4.294004839253591}
episode index:752
target Thresh 7.259100104976653
target distance 7.0
model initialize at round 752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([11.00057852, 11.86574979,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 10.178198984084522}
done in step count: 99
reward sum = -0.2099253689364038
running average episode reward sum: 0.4606115838561668
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([12.5129936 , 11.91291274,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 9.570748513626132}
episode index:753
target Thresh 7.26496908755625
target distance 2.0
model initialize at round 753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([4.61244234, 4.        , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 2.797294227077612}
done in step count: 78
reward sum = 0.21313301597952486
running average episode reward sum: 0.4602833629438635
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.64832584, 3.54259893, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.6465974884651022}
episode index:754
target Thresh 7.2708351363780555
target distance 4.0
model initialize at round 754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 2., 10.,  0.]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 6.082762530298197}
done in step count: 99
reward sum = -0.2818349259134669
running average episode reward sum: 0.45930042481292666
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 3.25020587, 11.86708305,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 7.871060837863258}
episode index:755
target Thresh 7.276698252908585
target distance 3.0
model initialize at round 755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([6.        , 8.42388308, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.866586334459157}
done in step count: 99
reward sum = -0.21902514658590913
running average episode reward sum: 0.45840316876610276
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([10.60692488, 11.90040434,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 5.920231688496703}
episode index:756
target Thresh 7.282558438613616
target distance 1.0
model initialize at round 756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([16.,  7.,  0.]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.2360679774997645}
done in step count: 48
reward sum = 0.49363590420232467
running average episode reward sum: 0.4584497113492418
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.6898359 ,  5.38365648,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.4933498383296753}
episode index:757
target Thresh 7.2884156949582
target distance 4.0
model initialize at round 757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.86775028, 3.02619884, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 6.080155405664256}
done in step count: 12
reward sum = 0.8340318218561652
running average episode reward sum: 0.4589452022602008
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.95336297, 9.05376414, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9548777645415635}
episode index:758
target Thresh 7.294270023406643
target distance 3.0
model initialize at round 758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.78368878, 8.71345341, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.19926073939716}
done in step count: 6
reward sum = 0.9314337562249314
running average episode reward sum: 0.4595677168240542
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.02210437, 10.77893313,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7792466989818048}
episode index:759
target Thresh 7.300121425422531
target distance 6.0
model initialize at round 759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([4.       , 4.9816215, 0.       ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 5.66986470195562}
done in step count: 11
reward sum = 0.846539437874276
running average episode reward sum: 0.46007689014122555
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.39303069, 9.06945206, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.6109298874909502}
episode index:760
target Thresh 7.305969902468714
target distance 4.0
model initialize at round 760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.28805256, 7.14888239, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.156933597899927}
done in step count: 51
reward sum = 0.4440109331529788
running average episode reward sum: 0.46005577850260765
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.23256729, 2.50867714, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.5593209930459542}
episode index:761
target Thresh 7.311815456007313
target distance 1.0
model initialize at round 761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.7468189,  9.       ,  0.       ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.134886055404151}
done in step count: 99
reward sum = -0.20449025882260213
running average episode reward sum: 0.45918367084207584
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.87007121,  1.12992866,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.045669442964872}
episode index:762
target Thresh 7.317658087499715
target distance 4.0
model initialize at round 762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([13.13259695,  3.00494245,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 6.363071958997278}
done in step count: 81
reward sum = 0.18291272103825595
running average episode reward sum: 0.45882158571782444
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.93035895,  8.79419921,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.9528492772728073}
episode index:763
target Thresh 7.323497798406578
target distance 2.0
model initialize at round 763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.        ,  8.62132299,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.3786770105362365}
done in step count: 0
reward sum = 0.9968924527470343
running average episode reward sum: 0.45952586695739145
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.        ,  8.62132299,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.3786770105362365}
episode index:764
target Thresh 7.3293345901878295
target distance 5.0
model initialize at round 764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.85760294, 3.00075102, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 6.060236882049476}
done in step count: 9
reward sum = 0.8861058380602735
running average episode reward sum: 0.4600834878346501
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.57296638, 8.73900323, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5004767958992085}
episode index:765
target Thresh 7.335168464302668
target distance 3.0
model initialize at round 765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([13.        ,  9.75958657,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.2557753626129904}
done in step count: 1
reward sum = 0.9832010427609388
running average episode reward sum: 0.46076640892463216
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.        ,  8.73096728,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.0355571460244979}
episode index:766
target Thresh 7.340999422209562
target distance 6.0
model initialize at round 766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.        , 8.21943331, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 4.336313807565946}
done in step count: 99
reward sum = -0.17026504048853588
running average episode reward sum: 0.4599436821326985
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.13733922, 2.81332834, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 2.2085503903063897}
episode index:767
target Thresh 7.34682746536625
target distance 3.0
model initialize at round 767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([11.74327374,  8.94814301,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 4.39292762801279}
done in step count: 6
reward sum = 0.9296265100075045
running average episode reward sum: 0.46055524831482714
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.93709886,  5.31376493,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.1614959519571246}
episode index:768
target Thresh 7.352652595229744
target distance 6.0
model initialize at round 768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4.8647604 , 9.50613141, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 4.164626362879896}
done in step count: 99
reward sum = -0.24152881850466656
running average episode reward sum: 0.4596422651330072
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([13.12992873,  1.12992873,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 9.784399608361012}
episode index:769
target Thresh 7.3584748132563265
target distance 3.0
model initialize at round 769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.10138857, 5.35503361, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.731090286381443}
done in step count: 99
reward sum = -0.18940152827015966
running average episode reward sum: 0.45879935111560055
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.14284728, 1.12851525, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 8.912797118959855}
episode index:770
target Thresh 7.364294120901549
target distance 3.0
model initialize at round 770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4., 8., 0.]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0000000000000178}
done in step count: 21
reward sum = 0.7539544814378238
running average episode reward sum: 0.4591821722963038
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.5138682 , 7.15867069, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5113712090641807}
episode index:771
target Thresh 7.370110519620246
target distance 3.0
model initialize at round 771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([7.36005634, 8.17009801, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.5932641226728417}
done in step count: 1
reward sum = 0.9870594391696484
running average episode reward sum: 0.459865951139404
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.63510007, 8.17248542, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.0431358874970116}
episode index:772
target Thresh 7.375924010866511
target distance 7.0
model initialize at round 772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.9718367 , 6.64630869, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 6.898260518099847}
done in step count: 99
reward sum = -0.252574189304913
running average episode reward sum: 0.45894429507155876
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.87007127, 1.12992873, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 10.78212365893401}
episode index:773
target Thresh 7.381734596093717
target distance 2.0
model initialize at round 773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([11.81835639,  9.        ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.1816436052321677}
done in step count: 96
reward sum = 0.14312726381239266
running average episode reward sum: 0.45853626273143067
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.39743021,  8.80941366,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.440765156170245}
episode index:774
target Thresh 7.387542276754512
target distance 4.0
model initialize at round 774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([5.20305407, 9.        , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.2030540704726826}
done in step count: 99
reward sum = -0.1948748630818357
running average episode reward sum: 0.457693151601349
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.39437992, 1.10116362, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 8.020966991790775}
episode index:775
target Thresh 7.393347054300814
target distance 7.0
model initialize at round 775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([9.97380116, 8.13224972, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 7.183499286638246}
done in step count: 99
reward sum = -0.2061590984099136
running average episode reward sum: 0.4568376718977262
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.87007127,  1.12992873,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 2.6446801595504525}
episode index:776
target Thresh 7.399148930183821
target distance 7.0
model initialize at round 776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4., 2., 0.]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.295630140986992}
done in step count: 99
reward sum = -0.2802832588081789
running average episode reward sum: 0.45588899631123214
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.26001192, 1.11698088, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.960910264586976}
episode index:777
target Thresh 7.404947905853998
target distance 5.0
model initialize at round 777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([5.96833286, 8.00005299, 0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 6.113992351336867}
done in step count: 99
reward sum = -0.22887488068892323
running average episode reward sum: 0.4550088370862962
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([1.14735809, 1.12807987, 0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 13.406974413500686}
episode index:778
target Thresh 7.410743982761094
target distance 4.0
model initialize at round 778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.13624287,  5.01817999,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.1952509779016554}
done in step count: 1
reward sum = 0.9818987157718132
running average episode reward sum: 0.4556852040679208
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.55275566,  3.01787107,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.44760124119242256}
episode index:779
target Thresh 7.416537162354125
target distance 3.0
model initialize at round 779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.13259695, 2.00494245, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 5.069811430677354}
done in step count: 99
reward sum = -0.2045728346902343
running average episode reward sum: 0.4548387194028462
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.48470358, 1.08726716, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 6.096290217345165}
episode index:780
target Thresh 7.4223274460813835
target distance 6.0
model initialize at round 780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.17339015, 6.        , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 4.003756254336859}
done in step count: 2
reward sum = 0.9689789333466567
running average episode reward sum: 0.45549702953593685
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.78599685, 2.        , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.21400314569471446}
episode index:781
target Thresh 7.428114835390444
target distance 5.0
model initialize at round 781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([4., 8., 0.]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 3.605551275463995}
done in step count: 77
reward sum = 0.26709034669365717
running average episode reward sum: 0.4552561002739902
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.74720433, 10.21842385,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.3340877545803058}
episode index:782
target Thresh 7.433899331728155
target distance 6.0
model initialize at round 782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.81529934, 4.1851115 , 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 5.817821117938}
done in step count: 99
reward sum = -0.24336190458543264
running average episode reward sum: 0.4543638678284482
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.86205261, 1.13092985, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 8.870142884232411}
episode index:783
target Thresh 7.439680936540637
target distance 2.0
model initialize at round 783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.89207745,  2.        ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.8920774459837553}
done in step count: 1
reward sum = 0.9834427829554777
running average episode reward sum: 0.45503871338345714
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.81697359,  1.12729003,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.1954365449966295}
episode index:784
target Thresh 7.4454596512732945
target distance 7.0
model initialize at round 784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([9.99505755, 8.13259695, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 5.440350974279132}
done in step count: 2
reward sum = 0.9634852762888503
running average episode reward sum: 0.4556864160113621
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.15148065,  6.3452541 ,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9160706760819739}
episode index:785
target Thresh 7.451235477370803
target distance 2.0
model initialize at round 785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.78018057, 9.32351458, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.3911294724584403}
done in step count: 0
reward sum = 0.9974355362722283
running average episode reward sum: 0.4563756642559688
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.78018057, 9.32351458, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.3911294724584403}
episode index:786
target Thresh 7.457008416277121
target distance 1.0
model initialize at round 786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.2352784 ,  7.74850187,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 2.3778231580291647}
done in step count: 99
reward sum = -0.18925729806948055
running average episode reward sum: 0.45555529200396694
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.2459372 ,  1.13504786,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 8.868362958392389}
episode index:787
target Thresh 7.4627784694354835
target distance 6.0
model initialize at round 787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([7.00024661, 8.16516178, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 5.101002062413132}
done in step count: 3
reward sum = 0.9573977526662913
running average episode reward sum: 0.45619214791851304
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.39551597, 4.54816431, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.754689625945912}
episode index:788
target Thresh 7.468545638288403
target distance 4.0
model initialize at round 788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.83470106, 7.        , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 2.006819309044131}
done in step count: 1
reward sum = 0.9822277037394892
running average episode reward sum: 0.4568588596495916
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.14501025, 5.02518854, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8553607011202279}
episode index:789
target Thresh 7.474309924277672
target distance 6.0
model initialize at round 789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([12.34387763,  7.66010415,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.897416406969633}
done in step count: 5
reward sum = 0.9390847909127722
running average episode reward sum: 0.45746927222081085
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.34805014,  2.09709558,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.36133980266084975}
episode index:790
target Thresh 7.480071328844362
target distance 6.0
model initialize at round 790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([12.14612992,  7.82335934,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 5.604406279193081}
done in step count: 42
reward sum = 0.5317614081684501
running average episode reward sum: 0.4575631940108837
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.49428173,  3.95666683,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.082110154191342}
episode index:791
target Thresh 7.485829853428826
target distance 7.0
model initialize at round 791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([10.        ,  9.27537405,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 6.620390579052291}
done in step count: 3
reward sum = 0.9529039213583751
running average episode reward sum: 0.45818862422218104
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.42492141,  4.21627206,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6144013264168434}
episode index:792
target Thresh 7.491585499470692
target distance 2.0
model initialize at round 792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.01823739, 1.13587746, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.864211749887471}
done in step count: 99
reward sum = -0.2534211178793665
running average episode reward sum: 0.4572912601085599
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.14179772, 1.12861967, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 2.058780096045313}
episode index:793
target Thresh 7.497338268408872
target distance 7.0
model initialize at round 793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.        ,  7.65622807,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 5.743946024193268}
done in step count: 6
reward sum = 0.9293895880925912
running average episode reward sum: 0.45788584238561786
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.04471782,  1.94335353,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.956960223015005}
episode index:794
target Thresh 7.5030881616815615
target distance 3.0
model initialize at round 794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([11.05900508,  8.14015437,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.0496243663233775}
done in step count: 99
reward sum = -0.2249792088360816
running average episode reward sum: 0.45702689263565344
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.12992873,  1.12992873,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 10.35445608400778}
episode index:795
target Thresh 7.508835180726228
target distance 5.0
model initialize at round 795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 4.9999999999999964}
done in step count: 33
reward sum = 0.6171650754277451
running average episode reward sum: 0.45722807125725157
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.8075605 ,  8.85479548,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.24107532940459955}
episode index:796
target Thresh 7.514579326979633
target distance 2.0
model initialize at round 796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.        ,  8.90249145,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.3236914846905985}
done in step count: 99
reward sum = -0.1686274625779769
running average episode reward sum: 0.4564428070993654
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.70029807,  1.11127884,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.913486954239552}
episode index:797
target Thresh 7.520320601877808
target distance 5.0
model initialize at round 797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.86687455,  4.00221713,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 7.051271985560038}
done in step count: 99
reward sum = -0.1809795144603461
running average episode reward sum: 0.4556440322603182
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.89324665,  1.35680512,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 9.684477116362823}
episode index:798
target Thresh 7.526059006856075
target distance 1.0
model initialize at round 798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.51371467, 5.49624801, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6947916992472472}
done in step count: 0
reward sum = 0.9981585204721738
running average episode reward sum: 0.4563230241103956
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.51371467, 5.49624801, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6947916992472472}
episode index:799
target Thresh 7.531794543349034
target distance 4.0
model initialize at round 799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([2.96346474, 8.17991471, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 3.145327690706412}
done in step count: 99
reward sum = -0.21433208460556177
running average episode reward sum: 0.4554847052245006
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([15.70792256,  1.36851998,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 12.348410735179021}
episode index:800
target Thresh 7.537527212790568
target distance 3.0
model initialize at round 800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([4.90362436, 6.37539884, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 4.187185740484755}
done in step count: 99
reward sum = -0.2884401112612869
running average episode reward sum: 0.4545559601352549
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([4.89471443, 2.34139597, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 7.942697466199378}
episode index:801
target Thresh 7.543257016613847
target distance 2.0
model initialize at round 801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([16.2022413 ,  3.05644739,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.4425289826626937}
done in step count: 8
reward sum = 0.9000078375633693
running average episode reward sum: 0.4551113864163373
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.56116896,  1.09683698,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0633033607577569}
episode index:802
target Thresh 7.548983956251318
target distance 2.0
model initialize at round 802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.52270992,  7.56352163,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.4760047804877696}
done in step count: 10
reward sum = 0.8771588560905088
running average episode reward sum: 0.4556369747970025
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.98262036, 10.4246965 ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.5755659567792072}
episode index:803
target Thresh 7.5547080331347205
target distance 2.0
model initialize at round 803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.24228573, 6.35444629, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.6895229976034465}
done in step count: 0
reward sum = 0.9981935082482694
running average episode reward sum: 0.4563117963560215
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.24228573, 6.35444629, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.6895229976034465}
episode index:804
target Thresh 7.560429248695071
target distance 6.0
model initialize at round 804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([4.90952046, 3.44910243, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 5.931489568586949}
done in step count: 99
reward sum = -0.24851402791645477
running average episode reward sum: 0.4554362363258694
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([4.84476008, 1.74234806, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 7.570902901012674}
episode index:805
target Thresh 7.566147604362673
target distance 1.0
model initialize at round 805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.4168011 ,  4.51061976,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.6192878128222457}
done in step count: 14
reward sum = 0.8431463477754235
running average episode reward sum: 0.4559172662408192
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.72160947,  2.97250807,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.7221329794408385}
episode index:806
target Thresh 7.57186310156712
target distance 6.0
model initialize at round 806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([10.99505776,  8.1325968 ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 4.162011277052037}
done in step count: 99
reward sum = -0.23896145220715329
running average episode reward sum: 0.4550562021535231
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.14743177,  1.12807291,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 6.157234509436031}
episode index:807
target Thresh 7.577575741737279
target distance 1.0
model initialize at round 807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([2.        , 7.33283997, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.333728040486353}
done in step count: 99
reward sum = -0.15332844984101968
running average episode reward sum: 0.45430325085154966
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.87146965, 1.24694385, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 8.796331698947641}
episode index:808
target Thresh 7.5832855263013155
target distance 4.0
model initialize at round 808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([6.        , 8.82909286, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.464645207197777}
done in step count: 2
reward sum = 0.9700930380946043
running average episode reward sum: 0.4549408154834941
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.00132799, 5.42857146, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.5714300804829606}
episode index:809
target Thresh 7.5889924566866735
target distance 4.0
model initialize at round 809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([12.        ,  9.53261209,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.227092185671534}
done in step count: 1
reward sum = 0.9794450792698024
running average episode reward sum: 0.4555883516116253
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.        ,  7.53261209,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.5326120853423841}
episode index:810
target Thresh 7.594696534320088
target distance 1.0
model initialize at round 810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4., 9., 0.]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.2360679774997645}
done in step count: 26
reward sum = 0.6632797049322077
running average episode reward sum: 0.45584444452570744
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.25086262, 9.14740845, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.1349533718637084}
episode index:811
target Thresh 7.600397760627574
target distance 4.0
model initialize at round 811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.61624777,  6.99881589,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 5.036657786798067}
done in step count: 99
reward sum = -0.1627004137837924
running average episode reward sum: 0.4550826897740948
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.36338224,  7.66444003,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 5.676083819139829}
episode index:812
target Thresh 7.606096137034443
target distance 2.0
model initialize at round 812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 2.701289205785704e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.4557455646946744
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 2.701289205785704e-14}
episode index:813
target Thresh 7.611791664965287
target distance 2.0
model initialize at round 813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16., 11.,  0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.9999999999999822}
done in step count: 48
reward sum = 0.5279576778946267
running average episode reward sum: 0.4558342773644532
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.86957674,  7.81489761,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8890593952669783}
episode index:814
target Thresh 7.6174843458439865
target distance 4.0
model initialize at round 814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([15.91137496, 11.86952633,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 5.255162478755217}
done in step count: 41
reward sum = 0.5686601312154463
running average episode reward sum: 0.4559727139949452
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.65541112, 10.16412914,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.38168032174074157}
episode index:815
target Thresh 7.623174181093715
target distance 2.0
model initialize at round 815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.       , 4.6409775, 0.       ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.3590224981308383}
done in step count: 0
reward sum = 0.9969033269992866
running average episode reward sum: 0.45663561915794076
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.       , 4.6409775, 0.       ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.3590224981308383}
episode index:816
target Thresh 7.6288611721369275
target distance 7.0
model initialize at round 816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.87771619, 2.22356393, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 6.83304263016328}
done in step count: 41
reward sum = 0.5783001605233979
running average episode reward sum: 0.4567845353652424
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.95890861, 8.38581305, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6155599977448889}
episode index:817
target Thresh 7.6345453203953735
target distance 6.0
model initialize at round 817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.000000000000018}
done in step count: 3
reward sum = 0.9533899688134424
running average episode reward sum: 0.45739163247214726
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.79963006,  3.37146032,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0170891631711292}
episode index:818
target Thresh 7.640226627290092
target distance 1.0
model initialize at round 818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([15.48176575,  4.48699129,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.559740440618677}
done in step count: 9
reward sum = 0.9028729045764671
running average episode reward sum: 0.4579355656493198
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.94201717,  3.63681281,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.36778654340132144}
episode index:819
target Thresh 7.645905094241409
target distance 5.0
model initialize at round 819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([10.04876781,  9.38016331,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.185341876799851}
done in step count: 2
reward sum = 0.970811298566442
running average episode reward sum: 0.4585610238601943
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.75200355,  8.29977012,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.38905573107204305}
episode index:820
target Thresh 7.651580722668939
target distance 4.0
model initialize at round 820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.,  4.,  0.]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 2.236067977499799}
done in step count: 1
reward sum = 0.9810262226907297
running average episode reward sum: 0.4591974004726554
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.92867446,  2.        ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.9286744594574969}
episode index:821
target Thresh 7.657253513991591
target distance 6.0
model initialize at round 821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([15.77600169,  9.00928485,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 5.776009154459368}
done in step count: 7
reward sum = 0.9178111949712197
running average episode reward sum: 0.4597553247968629
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.14068902,  9.71576757,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.7294632344679715}
episode index:822
target Thresh 7.662923469627561
target distance 3.0
model initialize at round 822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2., 9., 0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.4142135623730954}
done in step count: 2
reward sum = 0.9709898520630027
running average episode reward sum: 0.46037650891261755
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.51185626, 7.50768661, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.693294152576005}
episode index:823
target Thresh 7.668590590994341
target distance 1.0
model initialize at round 823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.2091341 , 7.95704418, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.2415322949302747}
done in step count: 0
reward sum = 0.9992802470476354
running average episode reward sum: 0.4610305183035581
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.2091341 , 7.95704418, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.2415322949302747}
episode index:824
target Thresh 7.674254879508708
target distance 6.0
model initialize at round 824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.        , 8.81278014, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.898170307583461}
done in step count: 66
reward sum = 0.4069753461304465
running average episode reward sum: 0.46096499688274223
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.31223625, 2.99611549, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.31226041322492815}
episode index:825
target Thresh 7.679916336586739
target distance 7.0
model initialize at round 825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([11.        ,  8.84686017,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 5.330186909375526}
done in step count: 5
reward sum = 0.930071143439668
running average episode reward sum: 0.4615329219996392
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.87313915,  7.14440979,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8850006565801368}
episode index:826
target Thresh 7.685574963643794
target distance 7.0
model initialize at round 826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 9.        , 10.14106917,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.439133865520496}
done in step count: 5
reward sum = 0.9346375029122026
running average episode reward sum: 0.4621049952534633
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.33664847,  7.58297477,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7835466173458395}
episode index:827
target Thresh 7.69123076209453
target distance 2.0
model initialize at round 827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.39920201,  1.11617425,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.9773107218086}
done in step count: 17
reward sum = 0.8030061787199121
running average episode reward sum: 0.462516711658616
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.86805344,  2.8837998 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8757963583393542}
episode index:828
target Thresh 7.696883733352895
target distance 2.0
model initialize at round 828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.90137763, 2.60749083, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.556673966823298}
done in step count: 2
reward sum = 0.9726322399931034
running average episode reward sum: 0.46313205005226443
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.72623671, 5.63596133, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6923822321952343}
episode index:829
target Thresh 7.702533878832139
target distance 5.0
model initialize at round 829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([4.86775037, 5.97380131, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 4.355326188690749}
done in step count: 2
reward sum = 0.963199625768036
running average episode reward sum: 0.46373454110734363
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.88111674, 9.92235765, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.9299875605935151}
episode index:830
target Thresh 7.708181199944794
target distance 4.0
model initialize at round 830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.89407748,  5.        ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.0028029309638113}
done in step count: 1
reward sum = 0.9837970188695925
running average episode reward sum: 0.46436036839706957
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.34412563,  7.        ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6558743715286308}
episode index:831
target Thresh 7.713825698102687
target distance 4.0
model initialize at round 831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([12.        , 10.35350752,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 2.4149498138392524}
done in step count: 1
reward sum = 0.98240104984195
running average episode reward sum: 0.46498301344688314
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.        ,  9.76751763,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.7675176262855192}
episode index:832
target Thresh 7.719467374716946
target distance 3.0
model initialize at round 832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([4.51354837, 8.609308  , 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 2.0355742832425077}
done in step count: 4
reward sum = 0.9535106169143001
running average episode reward sum: 0.4655694811581285
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.21343383, 9.3679597 , 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.6671048900168476}
episode index:833
target Thresh 7.725106231197992
target distance 4.0
model initialize at round 833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.        ,  8.81214714,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.4055560967912153}
done in step count: 3
reward sum = 0.957606702715353
running average episode reward sum: 0.466159453845847
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.86608307, 11.47323124,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.4918145520912634}
episode index:834
target Thresh 7.730742268955536
target distance 3.0
model initialize at round 834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.45948553, 9.6490615 , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.202158452277403}
done in step count: 1
reward sum = 0.9838221295663011
running average episode reward sum: 0.46677940914611105
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.45948553, 8.30786568, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6220427379414556}
episode index:835
target Thresh 7.736375489398588
target distance 4.0
model initialize at round 835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.38003999, 8.        , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.0938840498135094}
done in step count: 1
reward sum = 0.9845953790290228
running average episode reward sum: 0.46739880623927244
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.93300998, 6.40483904, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.4103441359068563}
episode index:836
target Thresh 7.742005893935455
target distance 6.0
model initialize at round 836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.53833151, 7.        , 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 4.6967873872863475}
done in step count: 2
reward sum = 0.9659075635355293
running average episode reward sum: 0.4679943961524101
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.59054303, 11.        ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.590543031692528}
episode index:837
target Thresh 7.747633483973736
target distance 2.0
model initialize at round 837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.13259695, 7.99505755, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8674171277527281}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.46862208780402465
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.13259695, 7.99505755, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8674171277527281}
episode index:838
target Thresh 7.753258260920329
target distance 7.0
model initialize at round 838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 9.02619884, 11.86775028,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.786766489554631}
done in step count: 4
reward sum = 0.9438396204506517
running average episode reward sum: 0.4691884972589074
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.25056876, 9.04765822, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7509450656214561}
episode index:839
target Thresh 7.758880226181429
target distance 2.0
model initialize at round 839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.29771838,  4.5322597 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.5544521774652411}
done in step count: 0
reward sum = 0.9981726580086876
running average episode reward sum: 0.46981824030741903
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.29771838,  4.5322597 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.5544521774652411}
episode index:840
target Thresh 7.76449938116253
target distance 7.0
model initialize at round 840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([10.56938438,  8.09044032,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 6.748550763643492}
done in step count: 3
reward sum = 0.9507363578790007
running average episode reward sum: 0.4703900811130927
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.32364482,  3.47515168,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.5749044202341469}
episode index:841
target Thresh 7.770115727268415
target distance 6.0
model initialize at round 841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([4.86971564, 5.91613676, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 4.6060884089901135}
done in step count: 9
reward sum = 0.8825683931417797
running average episode reward sum: 0.47087960404899376
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.14152247, 10.29720245,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.9084673705760749}
episode index:842
target Thresh 7.7757292659031725
target distance 5.0
model initialize at round 842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([7.        , 8.48685944, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 3.9135502386296794}
done in step count: 3
reward sum = 0.9561421833280223
running average episode reward sum: 0.4714552417468337
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.44764137, 10.69816024,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5398981749777174}
episode index:843
target Thresh 7.781339998470187
target distance 3.0
model initialize at round 843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.8870475 , 4.31078017, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.692991999281126}
done in step count: 1
reward sum = 0.9857082111224942
running average episode reward sum: 0.47206454621291855
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.06537364, 6.31078017, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.317581526416587}
episode index:844
target Thresh 7.786947926372146
target distance 3.0
model initialize at round 844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.94478322, 8.045681  , 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.955915083918433}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.4726822213064007
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.94478322, 8.045681  , 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.955915083918433}
episode index:845
target Thresh 7.792553051011026
target distance 2.0
model initialize at round 845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.86869028, 3.08942049, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.2584823576357076}
done in step count: 0
reward sum = 0.9961143184787351
running average episode reward sum: 0.4733009353692522
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.86869028, 3.08942049, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.2584823576357076}
episode index:846
target Thresh 7.798155373788113
target distance 1.0
model initialize at round 846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.10522479, 4.        , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0055208879229547}
done in step count: 0
reward sum = 0.9963995327348384
running average episode reward sum: 0.473918525212659
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.10522479, 4.        , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0055208879229547}
episode index:847
target Thresh 7.803754896103982
target distance 2.0
model initialize at round 847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([15.62071836,  7.        ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.6207183599472206}
done in step count: 3
reward sum = 0.9574682920441698
running average episode reward sum: 0.47448874899429994
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.71597934,  7.93195   ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.17522645474124}
episode index:848
target Thresh 7.809351619358519
target distance 3.0
model initialize at round 848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.23270869, 8.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.232708692550668}
done in step count: 28
reward sum = 0.6786927083800542
running average episode reward sum: 0.47472927191466
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.23377603, 8.96654957, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.2334169006649676}
episode index:849
target Thresh 7.814945544950901
target distance 5.0
model initialize at round 849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.13337648, 3.03109341, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 4.0624199455271315}
done in step count: 4
reward sum = 0.9520847276488374
running average episode reward sum: 0.4752908665684649
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.94412621, 6.5333547 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.4699784169270734}
episode index:850
target Thresh 7.820536674279611
target distance 1.0
model initialize at round 850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.95064914, 11.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.3797585990635954}
done in step count: 0
reward sum = 0.9963221999696298
running average episode reward sum: 0.4759031243045415
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.95064914, 11.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.3797585990635954}
episode index:851
target Thresh 7.826125008742432
target distance 3.0
model initialize at round 851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 4.12475705, 10.62987566,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.800141540978593}
done in step count: 27
reward sum = 0.7262038885949571
running average episode reward sum: 0.476196904544319
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.41351499, 6.82231959, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.4500721910775765}
episode index:852
target Thresh 7.831710549736447
target distance 3.0
model initialize at round 852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.99827719,  8.6611197 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.661120592876251}
done in step count: 1
reward sum = 0.9851679191910162
running average episode reward sum: 0.47679358803159533
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.08815494,  7.85141516,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8559667422710632}
episode index:853
target Thresh 7.8372932986580395
target distance 5.0
model initialize at round 853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([5.        , 8.86476243, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 4.148115687166091}
done in step count: 2
reward sum = 0.9679836758547624
running average episode reward sum: 0.47736875206885904
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13259647, 6.01399467, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8675164133984384}
episode index:854
target Thresh 7.842873256902902
target distance 4.0
model initialize at round 854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.        ,  8.57891834,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.099836151457514}
done in step count: 56
reward sum = 0.4851068307224714
running average episode reward sum: 0.47737780245324923
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.36360288,  5.74145617,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8258112971195911}
episode index:855
target Thresh 7.848450425866019
target distance 6.0
model initialize at round 855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.        , 8.93845534, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.499996263530597}
done in step count: 2
reward sum = 0.9647745991288478
running average episode reward sum: 0.4779471912344123
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.98751548, 11.86117532,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8612658131423157}
episode index:856
target Thresh 7.854024806941684
target distance 1.0
model initialize at round 856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([14.        , 10.77354199,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 2.673097674879697}
done in step count: 4
reward sum = 0.947872279696891
running average episode reward sum: 0.4784955285605062
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.08041155,  9.1017466 ,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.9252001386578758}
episode index:857
target Thresh 7.859596401523492
target distance 5.0
model initialize at round 857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([1.13259695, 7.00494245, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 4.153514100064503}
done in step count: 5
reward sum = 0.9341107009699613
running average episode reward sum: 0.4790265485749694
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.29997391, 4.91283516, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9608602325738501}
episode index:858
target Thresh 7.865165211004344
target distance 5.0
model initialize at round 858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([15.8455404,  9.       ,  0.       ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 4.947652151262702}
done in step count: 17
reward sum = 0.7986433892177545
running average episode reward sum: 0.47939862871541505
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.65897138, 10.88215837,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9457821639130648}
episode index:859
target Thresh 7.870731236776439
target distance 2.0
model initialize at round 859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13329913, 2.00156452, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8667022773949296}
done in step count: 0
reward sum = 0.9941959467023969
running average episode reward sum: 0.4799972302479581
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13329913, 2.00156452, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8667022773949296}
episode index:860
target Thresh 7.876294480231286
target distance 4.0
model initialize at round 860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([3., 8., 0.]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.236067977499814}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.48057570036428876
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13259695, 6.00494245, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.867417127752728}
episode index:861
target Thresh 7.881854942759695
target distance 2.0
model initialize at round 861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([12.        ,  8.43597519,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.04696711496874}
done in step count: 12
reward sum = 0.8665584676739213
running average episode reward sum: 0.48102347619643454
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.30378426,  8.25878273,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.74275490666718}
episode index:862
target Thresh 7.887412625751783
target distance 1.0
model initialize at round 862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.56598288,  9.48613858,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6516913250464598}
done in step count: 0
reward sum = 0.9996606817017392
running average episode reward sum: 0.48162444630710116
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.56598288,  9.48613858,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6516913250464598}
episode index:863
target Thresh 7.892967530596969
target distance 7.0
model initialize at round 863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([6., 9., 0.]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.8309518948453265}
done in step count: 3
reward sum = 0.9511203024558099
running average episode reward sum: 0.4821678442887548
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.84465611, 3.91509569, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.17703238856276576}
episode index:864
target Thresh 7.898519658683978
target distance 2.0
model initialize at round 864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.73622203, 10.23762763,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.4759222195342498}
done in step count: 1
reward sum = 0.9892661342385808
running average episode reward sum: 0.4827540850863846
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.35094976, 10.16051686,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0611306017911404}
episode index:865
target Thresh 7.904069011400847
target distance 4.0
model initialize at round 865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2., 8., 0.]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.0000000000000178}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.48332603187082146
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13224972, 6.02619884, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8681456786036912}
episode index:866
target Thresh 7.909615590134909
target distance 1.0
model initialize at round 866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([14.79555321,  6.        ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.5654686442757073}
done in step count: 1
reward sum = 0.9832345795790465
running average episode reward sum: 0.48390262765825887
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.28262055,  7.18091428,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.3355657201679048}
episode index:867
target Thresh 7.91515939627281
target distance 5.0
model initialize at round 867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.43718743,  9.        ,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.052336479698783}
done in step count: 4
reward sum = 0.9484081732791485
running average episode reward sum: 0.484437772296071
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.67755372,  5.34182245,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9446040071049152}
episode index:868
target Thresh 7.920700431200504
target distance 6.0
model initialize at round 868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([14.73390651,  4.99659503,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 6.199767916223133}
done in step count: 56
reward sum = 0.46645234682417397
running average episode reward sum: 0.4844170756039284
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.31656893,  9.18698054,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.3676650748077206}
episode index:869
target Thresh 7.926238696303248
target distance 5.0
model initialize at round 869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([12.00000131, 11.73280905,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 3.464481931302268}
done in step count: 3
reward sum = 0.9592807242037487
running average episode reward sum: 0.4849628958896753
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.10596419, 9.32881263, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.1179411990403385}
episode index:870
target Thresh 7.931774192965608
target distance 1.0
model initialize at round 870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.48457852, 1.08711927, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.048337030328119}
done in step count: 0
reward sum = 0.9968005718036501
running average episode reward sum: 0.4855505396048463
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.48457852, 1.08711927, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.048337030328119}
episode index:871
target Thresh 7.937306922571459
target distance 1.0
model initialize at round 871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([12.07779741,  9.77939725,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.9348199811897446}
done in step count: 3
reward sum = 0.9629548882892612
running average episode reward sum: 0.48609802165608995
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.81812143, 10.49870092,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.9581363597841772}
episode index:872
target Thresh 7.942836886503983
target distance 3.0
model initialize at round 872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.30446386, 3.        , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0453220767303857}
done in step count: 3
reward sum = 0.9584455204307816
running average episode reward sum: 0.48663908408309414
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.82425573, 1.15178415, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8662310191646494}
episode index:873
target Thresh 7.94836408614567
target distance 7.0
model initialize at round 873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4.        , 8.93640733, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 5.11187141589778}
done in step count: 3
reward sum = 0.9574347400843377
running average episode reward sum: 0.4871777518817226
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([10.       , 10.9297086,  0.       ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.3654149849023287}
episode index:874
target Thresh 7.953888522878321
target distance 7.0
model initialize at round 874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([8.99505755, 8.13259695, 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 5.90445701395561}
done in step count: 10
reward sum = 0.8771299387790452
running average episode reward sum: 0.48762341152389094
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.69001935,  5.82168351,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.072982056616452}
episode index:875
target Thresh 7.959410198083046
target distance 4.0
model initialize at round 875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4.29243565, 6.95538223, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 2.4188533922492903}
done in step count: 1
reward sum = 0.9838497951447657
running average episode reward sum: 0.48818987999834396
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.28161311, 8.26379287, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.788230219702947}
episode index:876
target Thresh 7.964929113140262
target distance 4.0
model initialize at round 876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([11.96255553, 11.17379165,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.9793539739581574}
done in step count: 1
reward sum = 0.9813617568903638
running average episode reward sum: 0.4887522196527248
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.94155204,  9.61323512,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.616014180992397}
episode index:877
target Thresh 7.9704452694297
target distance 7.0
model initialize at round 877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([5.22601899, 7.22012512, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.362166430281847}
done in step count: 3
reward sum = 0.9549598120554496
running average episode reward sum: 0.4892832077989694
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.88398194, 1.27125204, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.14564290788454}
episode index:878
target Thresh 7.9759586683303985
target distance 3.0
model initialize at round 878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.34427083, 3.01025653, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 2.0193071787567534}
done in step count: 1
reward sum = 0.9858398662366358
running average episode reward sum: 0.48984811867318745
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.71524525, 5.01025653, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.7153187817881232}
episode index:879
target Thresh 7.981469311220703
target distance 2.0
model initialize at round 879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.29964709, 9.3764416 , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.3952584513258786}
done in step count: 2
reward sum = 0.9751481373439354
running average episode reward sum: 0.49039959596713145
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.16838449, 7.77805042, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.7960626844002169}
episode index:880
target Thresh 7.986977199478282
target distance 3.0
model initialize at round 880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.6324867,  6.155918 ,  0.       ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.3176440545919677}
done in step count: 1
reward sum = 0.9853509107288039
running average episode reward sum: 0.49096140222679285
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.59350771,  5.79128039,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9891289360644056}
episode index:881
target Thresh 7.992482334480102
target distance 3.0
model initialize at round 881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 8.97404063, 11.86768932,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 2.130928437293872}
done in step count: 6
reward sum = 0.9209881797466579
running average episode reward sum: 0.49144896093146384
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.98009136, 9.37436562, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.625951059796723}
episode index:882
target Thresh 7.997984717602448
target distance 3.0
model initialize at round 882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.        ,  7.91145122,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.0885487794875983}
done in step count: 4
reward sum = 0.9562996981056932
running average episode reward sum: 0.4919754057074256
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.64182661,  9.12012315,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9499849728591068}
episode index:883
target Thresh 8.003484350220917
target distance 7.0
model initialize at round 883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.11472407, 9.26197892, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.589515859378542}
done in step count: 6
reward sum = 0.9258248467922997
running average episode reward sum: 0.4924661856181551
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.86033675, 4.77311936, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7856330954011317}
episode index:884
target Thresh 8.008981233710415
target distance 2.0
model initialize at round 884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.59229949,  9.37300104,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.4322540131119295}
done in step count: 5
reward sum = 0.9493466946815821
running average episode reward sum: 0.49298243478093867
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.70401773,  8.7169119 ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7756082601116784}
episode index:885
target Thresh 8.014475369445165
target distance 5.0
model initialize at round 885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.33148503,  5.23683894,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.822079728184281}
done in step count: 5
reward sum = 0.9404693376841363
running average episode reward sum: 0.49348749900543437
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.20108539,  8.36976123,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0175782354376925}
episode index:886
target Thresh 8.019966758798702
target distance 2.0
model initialize at round 886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.        , 10.71688795,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.7168879508972683}
done in step count: 2
reward sum = 0.9707366433694822
running average episode reward sum: 0.49402554764620554
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.81211877,  9.26517576,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.8543155611617692}
episode index:887
target Thresh 8.025455403143868
target distance 6.0
model initialize at round 887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([13.045681  ,  7.05521678,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 5.017248634174401}
done in step count: 2
reward sum = 0.9641109093008463
running average episode reward sum: 0.4945549230534743
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.5184183 ,  3.19257613,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5530308337169468}
episode index:888
target Thresh 8.030941303852831
target distance 4.0
model initialize at round 888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.17664066, 7.14612992, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.8622664145406234}
done in step count: 1
reward sum = 0.9824112298103123
running average episode reward sum: 0.49510369280235716
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.80080189, 8.56984113, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.4740427647896215}
episode index:889
target Thresh 8.03642446229706
target distance 1.0
model initialize at round 889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.80981839,  3.1084736 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9115856348516196}
done in step count: 0
reward sum = 0.9989120311239091
running average episode reward sum: 0.4956697695869881
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.80981839,  3.1084736 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9115856348516196}
episode index:890
target Thresh 8.041904879847351
target distance 1.0
model initialize at round 890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.74174213, 6.81047714, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.217235295888292}
done in step count: 4
reward sum = 0.9511132086307589
running average episode reward sum: 0.4961809294512348
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.93848932, 8.3554734 , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.36075601384471506}
episode index:891
target Thresh 8.0473825578738
target distance 1.0
model initialize at round 891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.17086565, 4.41225004, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.2413204578302928}
done in step count: 1
reward sum = 0.9875040448558525
running average episode reward sum: 0.4967317401187287
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.96136725, 4.00610214, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.03911170646008463}
episode index:892
target Thresh 8.052857497745833
target distance 5.0
model initialize at round 892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.96668875,  5.93516231,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.213676591565221}
done in step count: 5
reward sum = 0.9398215998546664
running average episode reward sum: 0.49722792137263233
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.91810727,  9.29167187,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9633241626874338}
episode index:893
target Thresh 8.058329700832186
target distance 8.0
model initialize at round 893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 6.288603024860095}
done in step count: 20
reward sum = 0.784587410587607
running average episode reward sum: 0.49754935256862226
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.47558692, 10.0020623 ,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.4755913869669521}
episode index:894
target Thresh 8.063799168500905
target distance 7.0
model initialize at round 894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 8.38148606, 11.64199563,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 7.106934336253714}
done in step count: 4
reward sum = 0.9426678450503021
running average episode reward sum: 0.49804669166636717
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.34331755, 6.13830056, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0834010196931072}
episode index:895
target Thresh 8.069265902119358
target distance 8.0
model initialize at round 895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([5.        , 9.22441411, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 6.049920121165438}
done in step count: 16
reward sum = 0.8026366060414047
running average episode reward sum: 0.4983866357672322
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.71930039, 10.55394636,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.6210063157186997}
episode index:896
target Thresh 8.074729903054232
target distance 5.0
model initialize at round 896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.42626357,  6.        ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 3.030132114575023}
done in step count: 2
reward sum = 0.9692535433689251
running average episode reward sum: 0.4989115710042464
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.02646768,  2.        ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0003502077441446}
episode index:897
target Thresh 8.080191172671524
target distance 3.0
model initialize at round 897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.5563972, 6.       , 0.       ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0939759798530657}
done in step count: 1
reward sum = 0.9845456574766037
running average episode reward sum: 0.49945236620076344
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.88255969, 6.28390528, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.1365312314596514}
episode index:898
target Thresh 8.085649712336552
target distance 7.0
model initialize at round 898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([13.1091805 ,  4.47246161,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 7.527111401659667}
done in step count: 26
reward sum = 0.6981261786210533
running average episode reward sum: 0.49967336043037447
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.96180106, 10.99334917,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.3826799480903935}
episode index:899
target Thresh 8.091105523413953
target distance 7.0
model initialize at round 899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 9.97380116, 11.86775028,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.100555391695825}
done in step count: 3
reward sum = 0.9494214355326179
running average episode reward sum: 0.5001730805138214
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.61683295, 11.90099552,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.979086269431571}
episode index:900
target Thresh 8.096558607267676
target distance 3.0
model initialize at round 900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2., 10.,  0.]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.0000000000000195}
done in step count: 1
reward sum = 0.9810293210956054
running average episode reward sum: 0.5007067722347779
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.96790801, 11.86749572,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8680891196937246}
episode index:901
target Thresh 8.102008965260996
target distance 8.0
model initialize at round 901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([10.7871325, 10.3880707,  0.       ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 8.982329746412484}
done in step count: 9
reward sum = 0.893397772585646
running average episode reward sum: 0.5011421281109983
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.78486369,  1.68719233,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8449021591870095}
episode index:902
target Thresh 8.1074565987565
target distance 2.0
model initialize at round 902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.        ,  5.66643465,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.3335653543472619}
done in step count: 4
reward sum = 0.951350034030844
running average episode reward sum: 0.5016406972205442
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.46880368,  6.88931684,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.4816924894006833}
episode index:903
target Thresh 8.112901509116096
target distance 4.0
model initialize at round 903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.10322738, 8.05824661, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 3.0754047099616186}
done in step count: 2
reward sum = 0.9751894019362437
running average episode reward sum: 0.5021645342832828
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.53527355, 10.6435082 ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.585710745959463}
episode index:904
target Thresh 8.118343697701011
target distance 2.0
model initialize at round 904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9999999999999813}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.5027079988865116
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9999999999999813}
episode index:905
target Thresh 8.123783165871796
target distance 3.0
model initialize at round 905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2., 8., 0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0000000000000187}
done in step count: 1
reward sum = 0.978668286204145
running average episode reward sum: 0.5032333413669947
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.13461809, 9.9804895 , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.3077635510136292}
episode index:906
target Thresh 8.129219914988315
target distance 2.0
model initialize at round 906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 7.00494245, 11.86740305,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.8674095874256116}
done in step count: 23
reward sum = 0.7534388818003928
running average episode reward sum: 0.5035092019407912
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.39036634, 9.82351809, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.6346645296098797}
episode index:907
target Thresh 8.134653946409758
target distance 2.0
model initialize at round 907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.86740305,  8.99505755,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.867417127752727}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.5040493900446068
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.86740305,  8.99505755,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.867417127752727}
episode index:908
target Thresh 8.140085261494628
target distance 8.0
model initialize at round 908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.306884440612536}
done in step count: 6
reward sum = 0.9152776519030438
running average episode reward sum: 0.5045017863722838
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.73125555, 11.87229099,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.138255788172148}
episode index:909
target Thresh 8.14551386160076
target distance 6.0
model initialize at round 909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.82463682,  7.89376998,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 4.896910950588032}
done in step count: 8
reward sum = 0.9114138678739394
running average episode reward sum: 0.5049489425058021
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.87993105,  3.36433748,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.38361224600093946}
episode index:910
target Thresh 8.150939748085298
target distance 5.0
model initialize at round 910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.91849959,  9.        ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.188987792241629}
done in step count: 4
reward sum = 0.952103340156903
running average episode reward sum: 0.5054397815811601
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.88998692,  6.37151135,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.964415575930716}
episode index:911
target Thresh 8.156362922304718
target distance 3.0
model initialize at round 911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 5.25135946, 10.37445509,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.282287459575421}
done in step count: 1
reward sum = 0.9856692008213276
running average episode reward sum: 0.5059663489268182
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.61445153, 9.55048537, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7613238995735598}
episode index:912
target Thresh 8.161783385614813
target distance 1.0
model initialize at round 912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.34534347,  4.81534004,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.2339697437479737}
done in step count: 6
reward sum = 0.9367576232986835
running average episode reward sum: 0.5064381904102485
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.70921565,  5.20778956,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.0632893364440472}
episode index:913
target Thresh 8.167201139370695
target distance 1.0
model initialize at round 913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.54630232,  5.00516307,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.8386819644000452}
done in step count: 17
reward sum = 0.8176032273239094
running average episode reward sum: 0.5067786335578565
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.91258948,  5.61098908,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9920428672357708}
episode index:914
target Thresh 8.172616184926808
target distance 7.0
model initialize at round 914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([14.96816754,  4.05142641,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 6.265717135855372}
done in step count: 15
reward sum = 0.8359412157026251
running average episode reward sum: 0.5071383740847906
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.97949677,  9.35870966,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.1707549789662421}
episode index:915
target Thresh 8.178028523636911
target distance 3.0
model initialize at round 915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([5.20057011, 9.20312357, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.8409987152719567}
done in step count: 11
reward sum = 0.8738023342369082
running average episode reward sum: 0.5075386622508955
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.31900723, 10.6540628 ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7638217712659618}
episode index:916
target Thresh 8.183438156854086
target distance 3.0
model initialize at round 916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.        ,  8.48449469,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 2.6781922719236246}
done in step count: 22
reward sum = 0.752148324093591
running average episode reward sum: 0.5078054121547588
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.09175182,  5.98852485,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.09246661409441108}
episode index:917
target Thresh 8.188845085930746
target distance 8.0
model initialize at round 917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([10.        , 10.19808531,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 6.389959234735029}
done in step count: 3
reward sum = 0.9533194330173617
running average episode reward sum: 0.5082907215456768
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.        , 7.27897072, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.7210292816162065}
episode index:918
target Thresh 8.19424931221862
target distance 5.0
model initialize at round 918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2., 8., 0.]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.0000000000000178}
done in step count: 2
reward sum = 0.9704043988565721
running average episode reward sum: 0.5087935655906288
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.20906137, 5.6070233 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9970261772007631}
episode index:919
target Thresh 8.199650837068766
target distance 3.0
model initialize at round 919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.        , 3.15564811, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.1556481122970705}
done in step count: 1
reward sum = 0.9839406058269872
running average episode reward sum: 0.5093100297647988
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.32476236, 2.67767852, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9566577505171042}
episode index:920
target Thresh 8.205049661831566
target distance 8.0
model initialize at round 920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([10.02619884, 11.86775028,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 6.088354702269997}
done in step count: 3
reward sum = 0.9529129629051103
running average episode reward sum: 0.5097916833295548
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.09186938, 11.83721508,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.842240502909182}
episode index:921
target Thresh 8.210445787856724
target distance 7.0
model initialize at round 921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([2.47253609, 5.53368187, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.675718448722155}
done in step count: 8
reward sum = 0.9099327634276773
running average episode reward sum: 0.5102256758242383
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.58887428, 10.57675856,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7251939269835154}
episode index:922
target Thresh 8.215839216493272
target distance 5.0
model initialize at round 922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([10.31832504,  9.37179911,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 4.723633851396572}
done in step count: 6
reward sum = 0.9326820139667122
running average episode reward sum: 0.5106833749988239
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.91724957,  9.62552972,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.3835043954039805}
episode index:923
target Thresh 8.221229949089572
target distance 6.0
model initialize at round 923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([8.88044834, 9.89425105, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 5.674104787972899}
done in step count: 9
reward sum = 0.8973354096260443
running average episode reward sum: 0.5111018295817538
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.01536728, 6.81338294, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0021614217912675}
episode index:924
target Thresh 8.2266179869933
target distance 3.0
model initialize at round 924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.04091167,  5.3586688 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.9010046056021155}
done in step count: 1
reward sum = 0.9858196227518455
running average episode reward sum: 0.5116150380068025
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.40801871,  6.95124578,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5939855413311733}
episode index:925
target Thresh 8.232003331551468
target distance 4.0
model initialize at round 925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([4.        , 7.93751168, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.5537269007177485}
done in step count: 12
reward sum = 0.8430632739983481
running average episode reward sum: 0.5119729734668366
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.15844178, 4.76711317, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.28167370101055705}
episode index:926
target Thresh 8.237385984110412
target distance 4.0
model initialize at round 926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 7.        , 10.85468471,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 2.174968035251759}
done in step count: 1
reward sum = 0.981297518477565
running average episode reward sum: 0.5124792566869129
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.        , 10.40135458,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.40135458111760336}
episode index:927
target Thresh 8.242765946015801
target distance 5.0
model initialize at round 927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.        ,  9.50170279,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.3533408032889604}
done in step count: 2
reward sum = 0.9703504410375992
running average episode reward sum: 0.5129726523597046
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.        , 10.76842272,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0264638499760335}
episode index:928
target Thresh 8.248143218612615
target distance 7.0
model initialize at round 928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([3.99389136, 5.        , 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 8.06756210377367}
done in step count: 78
reward sum = 0.3564217518074328
running average episode reward sum: 0.5128041368585718
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.73574796,  8.53229522,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.5371935462867281}
episode index:929
target Thresh 8.253517803245183
target distance 5.0
model initialize at round 929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.0001297 ,  6.99018427,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.13266034878785}
done in step count: 2
reward sum = 0.9683692095105417
running average episode reward sum: 0.5132939917754019
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.8253451 , 10.35206272,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.049293696468206}
episode index:930
target Thresh 8.258889701257143
target distance 3.0
model initialize at round 930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.04919678, 6.11745679, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.117844958340995}
done in step count: 9
reward sum = 0.900391557880866
running average episode reward sum: 0.5137097786348063
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.68191159, 3.01789947, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6821464740625106}
episode index:931
target Thresh 8.264258913991473
target distance 6.0
model initialize at round 931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 4.        , 10.24612784,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.070420522232087}
done in step count: 2
reward sum = 0.9652059522027686
running average episode reward sum: 0.5141942165892783
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.93005091, 11.86471283,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8675374080440518}
episode index:932
target Thresh 8.269625442790474
target distance 2.0
model initialize at round 932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([16.        ,  7.87911844,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.5056130713681504}
done in step count: 4
reward sum = 0.9506758929242406
running average episode reward sum: 0.51466204260893
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.97972918,  5.89193314,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.3249203709236335}
episode index:933
target Thresh 8.27498928899578
target distance 2.0
model initialize at round 933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.        , 10.38440859,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.3844085931777244}
done in step count: 4
reward sum = 0.9511554933966325
running average episode reward sum: 0.5151293803506727
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.53019887,  9.04588747,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.5321808939544943}
episode index:934
target Thresh 8.280350453948353
target distance 4.0
model initialize at round 934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 8.4824959 , 10.85271931,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 3.585374169817914}
done in step count: 3
reward sum = 0.9635972552136404
running average episode reward sum: 0.5156090251366223
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.83952779, 9.8386572 , 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.8548908714546882}
episode index:935
target Thresh 8.285708938988485
target distance 8.0
model initialize at round 935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([16.        ,  3.26034939,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.300734396432862}
done in step count: 5
reward sum = 0.9346423307085061
running average episode reward sum: 0.5160567102921478
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.68877989, 10.96815338,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.6895157278630621}
episode index:936
target Thresh 8.291064745455794
target distance 3.0
model initialize at round 936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.86924348, 5.89648124, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 2.0861987786851692}
done in step count: 2
reward sum = 0.974533155185492
running average episode reward sum: 0.5165460127947021
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.52427294, 4.17391322, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.5523657540772394}
episode index:937
target Thresh 8.296417874689233
target distance 3.0
model initialize at round 937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.        ,  9.88052827,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.049170890778715}
done in step count: 18
reward sum = 0.7882203217243681
running average episode reward sum: 0.5168356442541154
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.89603424,  7.61267573,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.0854717432091843}
episode index:938
target Thresh 8.301768328027087
target distance 7.0
model initialize at round 938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([9.04602778, 9.17983913, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 6.831239187068218}
done in step count: 43
reward sum = 0.5953379537148269
running average episode reward sum: 0.5169192462876199
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.07561702, 5.47023304, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.5351364013091153}
episode index:939
target Thresh 8.307116106806966
target distance 7.0
model initialize at round 939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 7.        , 10.16124988,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 5.002599476523924}
done in step count: 3
reward sum = 0.9588759932637388
running average episode reward sum: 0.5173894130397221
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.85305834, 10.88070351,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.892877663787534}
episode index:940
target Thresh 8.312461212365816
target distance 2.0
model initialize at round 940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.17358017, 8.99935782, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.0068786126544196}
done in step count: 4
reward sum = 0.9501244639368309
running average episode reward sum: 0.5178492802564034
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.56827575, 7.85696203, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0282612704240088}
episode index:941
target Thresh 8.317803646039915
target distance 6.0
model initialize at round 941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.15849924, 4.02280152, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 4.979721551874035}
done in step count: 9
reward sum = 0.8995318024882053
running average episode reward sum: 0.5182544634010232
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.12374614, 8.80613639, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8974429912272364}
episode index:942
target Thresh 8.323143409164867
target distance 3.0
model initialize at round 942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([2.55837679, 8.5715605 , 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 3.4681883929878063}
done in step count: 8
reward sum = 0.9103559801212033
running average episode reward sum: 0.5186702656456894
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.76176963, 9.07439686, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.24957684057225174}
episode index:943
target Thresh 8.328480503075617
target distance 8.0
model initialize at round 943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([9.        , 9.21945536, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 6.122668648421646}
done in step count: 80
reward sum = 0.3056646115865439
running average episode reward sum: 0.5184446240629996
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.0606608 ,  8.43714632,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.4413350595168008}
episode index:944
target Thresh 8.333814929106438
target distance 6.0
model initialize at round 944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 3.       , 10.3949995,  0.       ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 5.394999504089343}
done in step count: 3
reward sum = 0.9579372283977571
running average episode reward sum: 0.5189096956019782
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.23694116, 4.77644777, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.795131683045409}
episode index:945
target Thresh 8.339146688590935
target distance 6.0
model initialize at round 945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([7.67912705, 8.17578923, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 4.824594315119677}
done in step count: 11
reward sum = 0.8603349243099883
running average episode reward sum: 0.5192706102200627
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.34902826, 6.67942671, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.7256248571696204}
episode index:946
target Thresh 8.34447578286205
target distance 2.0
model initialize at round 946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([1.1053849 , 8.61493435, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.2291559296277152}
done in step count: 11
reward sum = 0.8801657056000733
running average episode reward sum: 0.5196517032458071
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.30914219, 5.09315763, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9580876689192521}
episode index:947
target Thresh 8.349802213252053
target distance 6.0
model initialize at round 947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([7.01819186, 8.13605185, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.7665232820832655}
done in step count: 2
reward sum = 0.9632156055084472
running average episode reward sum: 0.5201195976574765
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.80892953, 4.90933013, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9291873955645379}
episode index:948
target Thresh 8.355125981092556
target distance 6.0
model initialize at round 948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([12.02518179,  8.00006106,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.639119459823589}
done in step count: 16
reward sum = 0.8245568919725329
running average episode reward sum: 0.5204403956493786
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.76379608,  3.2777452 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.051207141701764}
episode index:949
target Thresh 8.3604470877145
target distance 1.0
model initialize at round 949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.30653787, 4.        , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 2.02335500188547}
done in step count: 2
reward sum = 0.9723702723757006
running average episode reward sum: 0.5209161113090905
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.2760061 , 1.61995578, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8176801172282736}
episode index:950
target Thresh 8.365765534448157
target distance 5.0
model initialize at round 950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([1.25599809, 8.00000044, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 3.47009297711856}
done in step count: 5
reward sum = 0.9382181052058807
running average episode reward sum: 0.5213549146675519
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.60708143, 4.07293556, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0068929810882368}
episode index:951
target Thresh 8.371081322623143
target distance 6.0
model initialize at round 951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 8.        , 10.76257944,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.00703987025367}
done in step count: 3
reward sum = 0.9573544590124188
running average episode reward sum: 0.5218128973821998
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.54009793, 11.90883394,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0572061751168351}
episode index:952
target Thresh 8.376394453568404
target distance 1.0
model initialize at round 952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.87006985, 11.8700727 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.230466596525717}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.5223083717818043
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.87006985, 11.8700727 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.230466596525717}
episode index:953
target Thresh 8.381704928612224
target distance 4.0
model initialize at round 953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 6.04680654, 11.86799113,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.2232466394662724}
done in step count: 2
reward sum = 0.9730108573954784
running average episode reward sum: 0.5227808062530975
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.89818235, 11.85165455,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.2377588651546898}
episode index:954
target Thresh 8.38701274908222
target distance 7.0
model initialize at round 954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([10.38300914, 11.90146828,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 7.011513666657316}
done in step count: 25
reward sum = 0.7203161971442047
running average episode reward sum: 0.5229876495943447
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.98502661, 9.62212025, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6223004121932744}
episode index:955
target Thresh 8.392317916305348
target distance 2.0
model initialize at round 955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.31237888, 4.04823208, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0017198925185145}
done in step count: 0
reward sum = 0.9991027215950792
running average episode reward sum: 0.5234856779123371
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.31237888, 4.04823208, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0017198925185145}
episode index:956
target Thresh 8.397620431607903
target distance 5.0
model initialize at round 956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([6.06217396, 9.3090502 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 4.67257639203819}
done in step count: 5
reward sum = 0.9389461675327295
running average episode reward sum: 0.5239198059056709
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.11728025, 6.62797552, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9579125079820544}
episode index:957
target Thresh 8.402920296315509
target distance 1.0
model initialize at round 957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.37700772,  5.36873662,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8869119608819314}
done in step count: 0
reward sum = 0.9997914238360718
running average episode reward sum: 0.5244165403711515
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.37700772,  5.36873662,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8869119608819314}
episode index:958
target Thresh 8.408217511753133
target distance 1.0
model initialize at round 958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.2360679774997636}
done in step count: 55
reward sum = 0.48791961462650923
running average episode reward sum: 0.5243784830971737
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.82475021,  9.31482319,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.36031392948140734}
episode index:959
target Thresh 8.413512079245079
target distance 8.0
model initialize at round 959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([10.69736311, 11.88662223,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 6.7557954077861115}
done in step count: 53
reward sum = 0.4738403438307282
running average episode reward sum: 0.5243258392021045
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.14265575, 10.61775821,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.938694806837657}
episode index:960
target Thresh 8.41880400011499
target distance 4.0
model initialize at round 960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([12.49958563,  8.03241146,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.50056442229307}
done in step count: 32
reward sum = 0.638392698351999
running average episode reward sum: 0.5244445352053823
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.8932276 ,  8.39757682,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9777130798110631}
episode index:961
target Thresh 8.424093275685848
target distance 1.0
model initialize at round 961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.49525261,  7.86686087,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.2366403853728822}
done in step count: 18
reward sum = 0.80480242009518
running average episode reward sum: 0.5247359675181575
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.51384613,  8.38299647,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.7855182644368237}
episode index:962
target Thresh 8.42937990727997
target distance 6.0
model initialize at round 962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.65060687, 9.        , 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.787193394216095}
done in step count: 8
reward sum = 0.9003127900843304
running average episode reward sum: 0.5251259746028576
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.24155809, 11.87882359,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9114171443028455}
episode index:963
target Thresh 8.43466389621901
target distance 7.0
model initialize at round 963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.42493057,  3.42188835,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 5.757235161105452}
done in step count: 3
reward sum = 0.958555534594024
running average episode reward sum: 0.5255755903289896
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.51687708,  8.36597931,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8180123120792209}
episode index:964
target Thresh 8.43994524382397
target distance 7.0
model initialize at round 964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([10.83361566,  8.1544523 ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 5.597605903285582}
done in step count: 4
reward sum = 0.9455484197828118
running average episode reward sum: 0.526010795333605
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.89509159,  6.35875519,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9643102366236267}
episode index:965
target Thresh 8.445223951415189
target distance 5.0
model initialize at round 965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([4.88565527, 6.43757819, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 4.4882012164762575}
done in step count: 3
reward sum = 0.9615543690880535
running average episode reward sum: 0.5264616685983611
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.44795126, 3.09801694, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.5606827360929867}
episode index:966
target Thresh 8.45050002031234
target distance 3.0
model initialize at round 966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.        ,  3.23811042,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.591514190875862}
done in step count: 6
reward sum = 0.9272711124469193
running average episode reward sum: 0.5268761561307794
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.02997462,  1.14396573,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.2937325503351655}
episode index:967
target Thresh 8.455773451834439
target distance 7.0
model initialize at round 967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 5.099019513592799}
done in step count: 46
reward sum = 0.546939790783652
running average episode reward sum: 0.526896883026082
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.32744484, 9.33628804, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7400903197910602}
episode index:968
target Thresh 8.461044247299847
target distance 2.0
model initialize at round 968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.37424856,  9.71526599,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9503527212599757}
done in step count: 0
reward sum = 0.9984684204013858
running average episode reward sum: 0.5273835409593898
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.37424856,  9.71526599,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9503527212599757}
episode index:969
target Thresh 8.466312408026262
target distance 8.0
model initialize at round 969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.66973472, 3.40215695, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.745193905979988}
done in step count: 30
reward sum = 0.6419849398723172
running average episode reward sum: 0.5275016867314649
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.6655376 , 11.89017842,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9509378120685178}
episode index:970
target Thresh 8.471577935330725
target distance 8.0
model initialize at round 970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.        ,  8.71777225,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 7.009169989461157}
done in step count: 12
reward sum = 0.8674425622064692
running average episode reward sum: 0.5278517803210375
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.46823132,  2.38076274,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.6540322563153439}
episode index:971
target Thresh 8.476840830529614
target distance 2.0
model initialize at round 971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13259695, 6.00494245, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8674171277527281}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.5283313566789433
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13259695, 6.00494245, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8674171277527281}
episode index:972
target Thresh 8.48210109493866
target distance 1.0
model initialize at round 972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.11511797,  2.71096142,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.454121805361264}
done in step count: 11
reward sum = 0.8754191653011228
running average episode reward sum: 0.5286880759067152
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.15131672,  4.14698337,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.2032874467493846}
episode index:973
target Thresh 8.487358729872923
target distance 2.0
model initialize at round 973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.24675775, 11.46411979,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8847491593159544}
done in step count: 0
reward sum = 0.997226461502106
running average episode reward sum: 0.5291691214771417
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.24675775, 11.46411979,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8847491593159544}
episode index:974
target Thresh 8.492613736646813
target distance 6.0
model initialize at round 974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([10.58073804, 11.8973108 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.652415745950477}
done in step count: 15
reward sum = 0.8247188117250804
running average episode reward sum: 0.5294722493645755
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.89776945, 11.51839868,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.036690489653243}
episode index:975
target Thresh 8.497866116574084
target distance 6.0
model initialize at round 975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4., 5., 0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 4.123105625617674}
done in step count: 2
reward sum = 0.9680868233346203
running average episode reward sum: 0.5299216495428235
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.52715516, 9.        , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.5271551609039258}
episode index:976
target Thresh 8.503115870967827
target distance 2.0
model initialize at round 976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.        ,  4.55906391,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.5590639114379874}
done in step count: 3
reward sum = 0.9570378121448166
running average episode reward sum: 0.5303588206406761
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.88283383,  3.41625894,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9760466580832484}
episode index:977
target Thresh 8.508363001140486
target distance 5.0
model initialize at round 977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([13.13224963,  5.97380131,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 3.7019409713279834}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.5308165928081281
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.91749007,  8.18291006,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.2285861794873858}
episode index:978
target Thresh 8.51360750840384
target distance 5.0
model initialize at round 978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([12.,  9.,  0.]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 3.1622776601684035}
done in step count: 17
reward sum = 0.8079489849541978
running average episode reward sum: 0.5310996698174704
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.58945994, 10.24652242,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.4788699660152491}
episode index:979
target Thresh 8.518849394069013
target distance 7.0
model initialize at round 979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.        ,  8.71154237,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.498821540453527}
done in step count: 3
reward sum = 0.9554949566224071
running average episode reward sum: 0.5315327262325774
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.06759626, 11.67922672,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6825820072835438}
episode index:980
target Thresh 8.524088659446484
target distance 3.0
model initialize at round 980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([5.28440738, 8.78731388, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.230889380571941}
done in step count: 15
reward sum = 0.8145782162944473
running average episode reward sum: 0.5318212537453826
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.08822912, 11.86465853,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.869148291327889}
episode index:981
target Thresh 8.529325305846063
target distance 1.0
model initialize at round 981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.90862489,  8.650316  ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.8839166876295097}
done in step count: 13
reward sum = 0.8643345636087173
running average episode reward sum: 0.5321598620038992
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.93186892,  7.51794137,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.066134581528778}
episode index:982
target Thresh 8.534559334576917
target distance 5.0
model initialize at round 982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.86740305,  7.99505755,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 3.5379193441547394}
done in step count: 2
reward sum = 0.9677960057373114
running average episode reward sum: 0.532603032038216
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.8996926 , 11.51009349,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.5198624238844095}
episode index:983
target Thresh 8.53979074694755
target distance 6.0
model initialize at round 983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 9.51558814, 11.90517767,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 7.3812248600635435}
done in step count: 15
reward sum = 0.8380431333222232
running average episode reward sum: 0.532913438645212
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.10698741, 7.80748795, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8145447144039537}
episode index:984
target Thresh 8.545019544265811
target distance 5.0
model initialize at round 984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([8.        , 9.27138191, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 3.0122496814806916}
done in step count: 26
reward sum = 0.7010284360444087
running average episode reward sum: 0.5330841137694752
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.56612526,  9.89901017,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.9982317258955089}
episode index:985
target Thresh 8.550245727838906
target distance 8.0
model initialize at round 985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.000000000000018}
done in step count: 3
reward sum = 0.9489645380086593
running average episode reward sum: 0.5335058991895961
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.09004037, 11.87004554,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8746922390635339}
episode index:986
target Thresh 8.55546929897338
target distance 2.0
model initialize at round 986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.        ,  5.71451724,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.7145172357559053}
done in step count: 28
reward sum = 0.6970670021859453
running average episode reward sum: 0.5336716145928345
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.09603855,  2.46408069,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0508833456848354}
episode index:987
target Thresh 8.560690258975121
target distance 3.0
model initialize at round 987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.93470371, 5.04358077, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.957508571469199}
done in step count: 8
reward sum = 0.9120474956236859
running average episode reward sum: 0.5340545861323395
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.45477081, 6.55474386, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6364507221626172}
episode index:988
target Thresh 8.565908609149375
target distance 2.0
model initialize at round 988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.27100265, 7.        , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.728997349739068}
done in step count: 0
reward sum = 0.9947579264055111
running average episode reward sum: 0.5345204135744761
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.27100265, 7.        , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.728997349739068}
episode index:989
target Thresh 8.571124350800726
target distance 5.0
model initialize at round 989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 8.87601348, 11.87015703,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 4.3035994040552055}
done in step count: 11
reward sum = 0.8653457158404393
running average episode reward sum: 0.5348545805464621
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.89667257, 10.88397842,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.8899968577524904}
episode index:990
target Thresh 8.576337485233113
target distance 6.0
model initialize at round 990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([10.29220429,  8.11480241,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 6.951585730471541}
done in step count: 3
reward sum = 0.953029963950793
running average episode reward sum: 0.5352765536881415
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.16551809,  3.70442752,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0920522862341464}
episode index:991
target Thresh 8.581548013749813
target distance 5.0
model initialize at round 991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([10.84899735,  9.75390518,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.388446543700861}
done in step count: 3
reward sum = 0.9598781849201502
running average episode reward sum: 0.535704579526077
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.90735714, 11.85460043,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.2464505088257107}
episode index:992
target Thresh 8.586755937653464
target distance 3.0
model initialize at round 992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.10566103, 3.64249591, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.8701965191145817}
done in step count: 1
reward sum = 0.9831390646727522
running average episode reward sum: 0.5361551681314614
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.21158751, 1.803345  , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.812568426914958}
episode index:993
target Thresh 8.591961258246045
target distance 5.0
model initialize at round 993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.13176142,  5.93449973,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 5.190392749772551}
done in step count: 37
reward sum = 0.5983346952837655
running average episode reward sum: 0.5362177229877514
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.61097894, 11.56385834,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.685035483623191}
episode index:994
target Thresh 8.597163976828885
target distance 2.0
model initialize at round 994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.02456212,  3.5681479 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5686785835992288}
done in step count: 0
reward sum = 0.99693336198127
running average episode reward sum: 0.5366807537807097
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.02456212,  3.5681479 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5686785835992288}
episode index:995
target Thresh 8.602364094702667
target distance 3.0
model initialize at round 995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 3.24856663, 10.6422658 ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.4040740638847382}
done in step count: 7
reward sum = 0.9150068471772944
running average episode reward sum: 0.5370605992560075
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.10414299, 9.33427229, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.116133136113112}
episode index:996
target Thresh 8.607561613167418
target distance 5.0
model initialize at round 996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([13.11090382,  1.69060552,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 6.044546838053025}
done in step count: 8
reward sum = 0.9053678770339062
running average episode reward sum: 0.5374300147803585
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.89743597,  6.40676217,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0757892159982188}
episode index:997
target Thresh 8.612756533522516
target distance 7.0
model initialize at round 997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([16.        ,  4.62875962,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 5.73151142300991}
done in step count: 27
reward sum = 0.7119939747387556
running average episode reward sum: 0.5376049285678919
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.85849672, 10.86640739,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8778866338059378}
episode index:998
target Thresh 8.617948857066697
target distance 7.0
model initialize at round 998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 5.099019513592799}
done in step count: 18
reward sum = 0.7853531474682114
running average episode reward sum: 0.5378529247830073
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.36349616, 9.02683042, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.637069075529703}
episode index:999
target Thresh 8.623138585098037
target distance 3.0
model initialize at round 999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.09446243,  3.40039894,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.667667682028628}
done in step count: 1
reward sum = 0.9838370327923136
running average episode reward sum: 0.5382989088910166
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.39228572,  2.14924698,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6257725705796501}
episode index:1000
target Thresh 8.628325718913967
target distance 2.0
model initialize at round 1000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.94957948, 10.68389148,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.32010440259251227}
done in step count: 0
reward sum = 0.9967985741905945
running average episode reward sum: 0.5387569505146925
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.94957948, 10.68389148,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.32010440259251227}
episode index:1001
target Thresh 8.633510259811274
target distance 5.0
model initialize at round 1001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.57149076,  7.        ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 3.3227456471376517}
done in step count: 2
reward sum = 0.969807096935499
running average episode reward sum: 0.5391871402815795
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.53792906,  3.20308733,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9211836728872105}
episode index:1002
target Thresh 8.638692209086093
target distance 1.0
model initialize at round 1002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.35246849, 8.7092275 , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.4440881885692}
done in step count: 26
reward sum = 0.734054221216673
running average episode reward sum: 0.5393814245098298
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.40140012, 9.74943397, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.648926156873271}
episode index:1003
target Thresh 8.64387156803391
target distance 1.0
model initialize at round 1003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.45545864,  9.13651574,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5613927661970926}
done in step count: 0
reward sum = 0.9992184021483753
running average episode reward sum: 0.5398394294676372
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.45545864,  9.13651574,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5613927661970926}
episode index:1004
target Thresh 8.649048337949564
target distance 5.0
model initialize at round 1004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([1.13224972, 9.02619884, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.5561735756854245}
done in step count: 6
reward sum = 0.9165099532639871
running average episode reward sum: 0.5402142260087281
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.33464998, 6.42084113, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7872724505017947}
episode index:1005
target Thresh 8.65422252012725
target distance 2.0
model initialize at round 1005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([14.58438498, 11.67585535,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.0272376423470644}
done in step count: 3
reward sum = 0.9655139034700191
running average episode reward sum: 0.5406369891075962
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.71635654,  9.94693833,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.1873747894677815}
episode index:1006
target Thresh 8.659394115860511
target distance 1.0
model initialize at round 1006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.43791234, 11.        ,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.0916809157644103}
done in step count: 0
reward sum = 0.9954493060335546
running average episode reward sum: 0.541088639869191
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.43791234, 11.        ,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.0916809157644103}
episode index:1007
target Thresh 8.66456312644225
target distance 8.0
model initialize at round 1007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([8.68870509, 9.25721347, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 9.617670761670004}
done in step count: 81
reward sum = 0.28561133033563235
running average episode reward sum: 0.5408351901573522
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.89322905,  2.00110336,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.10677665452296674}
episode index:1008
target Thresh 8.669729553164712
target distance 3.0
model initialize at round 1008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([11.44552946, 10.24088252,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.6327323041471329}
done in step count: 4
reward sum = 0.951316895583558
running average episode reward sum: 0.5412420104798756
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.71889794, 11.0658046 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.2887016008380551}
episode index:1009
target Thresh 8.67489339731951
target distance 8.0
model initialize at round 1009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4., 5., 0.]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.485281374238598}
done in step count: 10
reward sum = 0.8812746003264625
running average episode reward sum: 0.5415786764104168
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.46027639, 10.24861671,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8811533351210833}
episode index:1010
target Thresh 8.680054660197607
target distance 2.0
model initialize at round 1010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.72554299, 7.72939822, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 2.383704063935343}
done in step count: 4
reward sum = 0.9513603510771464
running average episode reward sum: 0.5419839995307598
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.19114008, 10.4019207 ,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.9032135007168345}
episode index:1011
target Thresh 8.68521334308931
target distance 5.0
model initialize at round 1011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 9.76892974, 11.87942491,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.304477752891294}
done in step count: 19
reward sum = 0.7952625984177409
running average episode reward sum: 0.5422342748261026
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.46911314, 11.86428793,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0143147821296306}
episode index:1012
target Thresh 8.6903694472843
target distance 2.0
model initialize at round 1012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.      ,  5.200459,  0.      ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7995409965515528}
done in step count: 0
reward sum = 0.9969698617712061
running average episode reward sum: 0.5426831747144986
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.      ,  5.200459,  0.      ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7995409965515528}
episode index:1013
target Thresh 8.695522974071595
target distance 8.0
model initialize at round 1013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.36831141, 9.24281126, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 6.6747755320066355}
done in step count: 9
reward sum = 0.8981829687768611
running average episode reward sum: 0.5430337662273805
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.14361879,  9.89536868,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.17769093318489668}
episode index:1014
target Thresh 8.700673924739583
target distance 5.0
model initialize at round 1014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([6.28809488, 9.32561395, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 3.7726696361426963}
done in step count: 18
reward sum = 0.7915700656356296
running average episode reward sum: 0.5432786295765512
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.12555811,  9.10816844,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.9006266520731314}
episode index:1015
target Thresh 8.705822300575994
target distance 3.0
model initialize at round 1015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.81291294, 7.17084354, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 2.3180571890731674}
done in step count: 1
reward sum = 0.9852848766065283
running average episode reward sum: 0.5437136750952815
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.29490743, 5.78227282, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0531411579651027}
episode index:1016
target Thresh 8.71096810286793
target distance 5.0
model initialize at round 1016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.68103778, 7.        , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 3.076331006689929}
done in step count: 30
reward sum = 0.6761651969133947
running average episode reward sum: 0.5438439125798618
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.38844316, 9.86502574, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.41122516707047635}
episode index:1017
target Thresh 8.716111332901837
target distance 2.0
model initialize at round 1017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.44943506, 4.31279993, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8211186298665997}
done in step count: 0
reward sum = 0.9985559233056011
running average episode reward sum: 0.5442905844960954
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.44943506, 4.31279993, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8211186298665997}
episode index:1018
target Thresh 8.721251991963523
target distance 3.0
model initialize at round 1018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 3., 11.,  0.]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.4142135623730956}
done in step count: 4
reward sum = 0.9467311207092205
running average episode reward sum: 0.5446855212342829
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.83526204, 9.74442198, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.304070253136672}
episode index:1019
target Thresh 8.726390081338153
target distance 5.0
model initialize at round 1019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.00016787,  7.00007785,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 2.999922154906699}
done in step count: 2
reward sum = 0.9698516553391296
running average episode reward sum: 0.5451023507775229
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.58226936, 10.26993564,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.49735715057276814}
episode index:1020
target Thresh 8.731525602310253
target distance 7.0
model initialize at round 1020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([7.        , 9.71278679, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 6.973946726952655}
done in step count: 21
reward sum = 0.7742929601692651
running average episode reward sum: 0.5453268273782984
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.11744473, 4.30727396, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.32895372626455405}
episode index:1021
target Thresh 8.736658556163697
target distance 3.0
model initialize at round 1021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.        ,  8.46421432,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.7731112738326318}
done in step count: 1
reward sum = 0.9836509037956735
running average episode reward sum: 0.5457557159070826
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.73684382,  7.20518041,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.3336917319980726}
episode index:1022
target Thresh 8.741788944181726
target distance 4.0
model initialize at round 1022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([11.15648744,  8.12715095,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 4.005378642164983}
done in step count: 43
reward sum = 0.583063960552519
running average episode reward sum: 0.5457921853544389
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.57808169,  6.7994581 ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.611878657537651}
episode index:1023
target Thresh 8.74691676764694
target distance 2.0
model initialize at round 1023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.55834597,  2.91545582,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0722917425529295}
done in step count: 0
reward sum = 0.9988840103577874
running average episode reward sum: 0.5462346578397936
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.55834597,  2.91545582,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0722917425529295}
episode index:1024
target Thresh 8.752042027841293
target distance 6.0
model initialize at round 1024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 9.99779086, 11.86687261,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 4.416207796764779}
done in step count: 15
reward sum = 0.8166077612802412
running average episode reward sum: 0.5464984364772965
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.56298929, 10.84301654,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0137227524724324}
episode index:1025
target Thresh 8.757164726046097
target distance 1.0
model initialize at round 1025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3861624 , 1.54768747, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.576708051928273}
done in step count: 6
reward sum = 0.926412474140809
running average episode reward sum: 0.5468687230637131
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.79555796, 2.36439119, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0182882821522536}
episode index:1026
target Thresh 8.762284863542032
target distance 2.0
model initialize at round 1026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.85925393, 5.0303212 , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8597887526937988}
done in step count: 0
reward sum = 0.9957716034496816
running average episode reward sum: 0.5473058242130666
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.85925393, 5.0303212 , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8597887526937988}
episode index:1027
target Thresh 8.767402441609127
target distance 7.0
model initialize at round 1027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([4.14744616, 3.90684366, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 8.448605841103333}
done in step count: 9
reward sum = 0.8867813248739024
running average episode reward sum: 0.5476360532993126
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.9372201 , 10.36563453,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.0060169647219406}
episode index:1028
target Thresh 8.772517461526782
target distance 3.0
model initialize at round 1028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([3.76457167, 7.        , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.02822907162436}
done in step count: 1
reward sum = 0.9799152877772448
running average episode reward sum: 0.5480561497370948
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.76457167, 5.        , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.0273395252359805}
episode index:1029
target Thresh 8.777629924573745
target distance 4.0
model initialize at round 1029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([6.61111065, 8.09802539, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 4.17634178310885}
done in step count: 5
reward sum = 0.931074233395047
running average episode reward sum: 0.5484280119542385
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.05265892, 5.16443762, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.837220078500294}
episode index:1030
target Thresh 8.78273983202814
target distance 4.0
model initialize at round 1030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 5.49639094, 11.17159832,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 3.6943090106652208}
done in step count: 36
reward sum = 0.6384892895456571
running average episode reward sum: 0.5485153652787694
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.85005742, 10.36080553,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.39072165014462085}
episode index:1031
target Thresh 8.787847185167436
target distance 7.0
model initialize at round 1031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 9.16241963, 11.87480919,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 5.492311530641671}
done in step count: 9
reward sum = 0.8909281633490398
running average episode reward sum: 0.5488471606257367
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.72614418, 9.33298665, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.721043567634551}
episode index:1032
target Thresh 8.792951985268479
target distance 4.0
model initialize at round 1032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([4.        , 9.82073087, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.1618508627485324}
done in step count: 60
reward sum = 0.40773153627652314
running average episode reward sum: 0.5487105530513425
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.2755023, 9.5199675, 0.       ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5884451730590808}
episode index:1033
target Thresh 8.798054233607463
target distance 6.0
model initialize at round 1033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([5.17664066, 7.14612992, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.604406279193084}
done in step count: 11
reward sum = 0.8613265496534628
running average episode reward sum: 0.5490128896051163
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.21686786, 10.61051737,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.6478912913445989}
episode index:1034
target Thresh 8.803153931459951
target distance 3.0
model initialize at round 1034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([1.57174826, 4.79846913, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.624241077049462}
done in step count: 3
reward sum = 0.9608270234848183
running average episode reward sum: 0.5494107776571739
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.45241809, 6.07896733, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0715162730101646}
episode index:1035
target Thresh 8.80825108010087
target distance 6.0
model initialize at round 1035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.17767116, 7.        , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.0039439359979445}
done in step count: 9
reward sum = 0.8926880874667352
running average episode reward sum: 0.5497421264118164
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.59904171, 11.13340143,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.6137156607642613}
episode index:1036
target Thresh 8.813345680804508
target distance 1.0
model initialize at round 1036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.47555578,  6.62582865,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8165189808545392}
done in step count: 0
reward sum = 0.998152049109277
running average episode reward sum: 0.5501745371376577
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.47555578,  6.62582865,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8165189808545392}
episode index:1037
target Thresh 8.818437734844508
target distance 8.0
model initialize at round 1037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([10.        , 10.16442885,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 7.303592787489406}
done in step count: 21
reward sum = 0.7615536026382479
running average episode reward sum: 0.5503781778558664
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.21209483, 6.54438192, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9576775203265276}
episode index:1038
target Thresh 8.823527243493892
target distance 6.0
model initialize at round 1038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([10.        ,  9.32077849,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.33817759924906}
done in step count: 2
reward sum = 0.9661740928271766
running average episode reward sum: 0.5507783664169552
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.59857254, 11.90572388,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9906965964297142}
episode index:1039
target Thresh 8.828614208025035
target distance 8.0
model initialize at round 1039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.61330279,  4.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 6.213110810574016}
done in step count: 58
reward sum = 0.4674875555711123
running average episode reward sum: 0.5506982790988343
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.13759647,  9.44161237,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0273931071210205}
episode index:1040
target Thresh 8.833698629709676
target distance 6.0
model initialize at round 1040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 8.        , 11.34089112,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 4.014499565028773}
done in step count: 2
reward sum = 0.9704292532210307
running average episode reward sum: 0.5511014788818527
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.46382966, 11.16970222,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.4938995852674872}
episode index:1041
target Thresh 8.838780509818921
target distance 6.0
model initialize at round 1041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([12.88596842,  7.06572324,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.215586774897025}
done in step count: 5
reward sum = 0.9383827412497027
running average episode reward sum: 0.5514731499589811
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.19266122,  3.7243315 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0846437329020087}
episode index:1042
target Thresh 8.843859849623243
target distance 7.0
model initialize at round 1042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([11.47140741,  9.6401329 ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 6.181009597466173}
done in step count: 8
reward sum = 0.9081446739905483
running average episode reward sum: 0.5518151169043614
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.44074739,  4.46431582,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6401932866274327}
episode index:1043
target Thresh 8.848936650392472
target distance 4.0
model initialize at round 1043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.39406189,  7.19915915,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.821213940363564}
done in step count: 5
reward sum = 0.9479794966559927
running average episode reward sum: 0.5521945847010584
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.48380241, 10.25973523,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8843396998089242}
episode index:1044
target Thresh 8.854010913395811
target distance 6.0
model initialize at round 1044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([14.        ,  5.81653142,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.788040200439636}
done in step count: 38
reward sum = 0.6201077155367045
running average episode reward sum: 0.5522595733430063
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.17098446, 10.74721214,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.7665257084517895}
episode index:1045
target Thresh 8.859082639901825
target distance 2.0
model initialize at round 1045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.49889302,  9.48933303,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.591610155267202}
done in step count: 8
reward sum = 0.8968554844573008
running average episode reward sum: 0.5525890149406301
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.4062078 , 10.49201556,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.6504252185263966}
episode index:1046
target Thresh 8.864151831178447
target distance 5.0
model initialize at round 1046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 7.42488527, 10.49691093,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.737724102240329}
done in step count: 6
reward sum = 0.927823891301638
running average episode reward sum: 0.5529474054624649
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.560269 , 9.9839932, 0.       ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.1323179645078951}
episode index:1047
target Thresh 8.869218488492972
target distance 6.0
model initialize at round 1047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([10.20636952, 11.        ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.20636951923373}
done in step count: 2
reward sum = 0.9682614976185119
running average episode reward sum: 0.5533436975351329
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.72174529, 11.80829367,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0836304336872284}
episode index:1048
target Thresh 8.87428261311207
target distance 6.0
model initialize at round 1048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([13.21633899, 10.12824988,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 4.2182890523432475}
done in step count: 2
reward sum = 0.9716069379342429
running average episode reward sum: 0.5537424232171149
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.54542518, 10.66752723,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.8620215914960755}
episode index:1049
target Thresh 8.879344206301766
target distance 5.0
model initialize at round 1049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.        ,  8.92689991,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.926899909973139}
done in step count: 27
reward sum = 0.7186054694751756
running average episode reward sum: 0.5538994356421225
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.28306317,  3.0423707 ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.1962659787268948}
episode index:1050
target Thresh 8.88440326932746
target distance 8.0
model initialize at round 1050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([13.13710422,  4.03500712,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 8.694217093144335}
done in step count: 7
reward sum = 0.9071320090582548
running average episode reward sum: 0.554235527529293
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.48715883, 9.88241388, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.0079573257666605}
episode index:1051
target Thresh 8.88945980345392
target distance 5.0
model initialize at round 1051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 8.23846245, 11.        ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.23846244812016}
done in step count: 2
reward sum = 0.9702647290489921
running average episode reward sum: 0.5546309925497489
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.3942511 , 11.88654567,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9702562366157845}
episode index:1052
target Thresh 8.89451380994528
target distance 5.0
model initialize at round 1052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.        ,  7.73637164,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 3.8678770669741973}
done in step count: 7
reward sum = 0.9110655372176351
running average episode reward sum: 0.5549694868941628
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.44199074,  3.98470646,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.558218802333536}
episode index:1053
target Thresh 8.899565290065038
target distance 5.0
model initialize at round 1053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.80967665,  9.        ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.006031100741701}
done in step count: 4
reward sum = 0.9491166494927697
running average episode reward sum: 0.5553434405588674
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.29993469,  5.67041418,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.7737688632917686}
episode index:1054
target Thresh 8.904614245076068
target distance 4.0
model initialize at round 1054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([13.5309    ,  9.92397612,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.4702701132240743}
done in step count: 10
reward sum = 0.8921417456183048
running average episode reward sum: 0.5556626806584498
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.69307914,  9.92810219,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.3152296099711404}
episode index:1055
target Thresh 8.909660676240604
target distance 8.0
model initialize at round 1055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 8.01989322, 11.86205117,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 7.738130002259484}
done in step count: 42
reward sum = 0.5779878514945505
running average episode reward sum: 0.5556838219187112
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.93905382, 7.52927664, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5327740599624324}
episode index:1056
target Thresh 8.914704584820257
target distance 7.0
model initialize at round 1056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2., 4., 0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 5.000000000000019}
done in step count: 3
reward sum = 0.9524656847658566
running average episode reward sum: 0.5560592068409886
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.17545233, 9.96533395, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.2695465747699273}
episode index:1057
target Thresh 8.919745972076003
target distance 2.0
model initialize at round 1057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.        , 7.18844485, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 3.1884448528289546}
done in step count: 41
reward sum = 0.5739242452196551
running average episode reward sum: 0.5560760925105338
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.6974403 , 3.28022252, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.0022487641883389}
episode index:1058
target Thresh 8.924784839268192
target distance 7.0
model initialize at round 1058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([9.75868833, 9.88136607, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.866330467146855}
done in step count: 14
reward sum = 0.8410925044702362
running average episode reward sum: 0.5563452298211662
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.22996335, 11.81508083,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8469001742831549}
episode index:1059
target Thresh 8.929821187656536
target distance 3.0
model initialize at round 1059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.57324457,  8.29150987,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.4130134729677}
done in step count: 3
reward sum = 0.9581474671831882
running average episode reward sum: 0.5567242885356586
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.90164732,  7.68495279,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.6919780197850793}
episode index:1060
target Thresh 8.934855018500125
target distance 6.0
model initialize at round 1060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([2.60351348, 5.87145519, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 7.613141404003174}
done in step count: 11
reward sum = 0.8746022059479229
running average episode reward sum: 0.557023890719836
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.06450466, 10.90420682,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.9065047322612617}
episode index:1061
target Thresh 8.939886333057416
target distance 7.0
model initialize at round 1061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([8.97559996, 8.13944615, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 5.460936332161773}
done in step count: 14
reward sum = 0.8287441180751554
running average episode reward sum: 0.5572797478077413
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.9342544 ,  6.65277138,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1397112601088257}
episode index:1062
target Thresh 8.944915132586239
target distance 4.0
model initialize at round 1062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.92176509,  6.52376223,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.6422347345058492}
done in step count: 1
reward sum = 0.9837948002624533
running average episode reward sum: 0.5576809849219978
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.00659668,  8.52376223,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.47628345922413967}
episode index:1063
target Thresh 8.949941418343792
target distance 7.0
model initialize at round 1063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.15077257,  9.05094945,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 7.101906464633856}
done in step count: 22
reward sum = 0.7589616770655343
running average episode reward sum: 0.5578701585048393
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.56121093,  2.47159573,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.6441571037926453}
episode index:1064
target Thresh 8.95496519158665
target distance 6.0
model initialize at round 1064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.12916172,  7.3739109 ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 4.7572189861563645}
done in step count: 52
reward sum = 0.49469531532366234
running average episode reward sum: 0.5578108394032608
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.05271528,  2.2117049 ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.232378800018479}
episode index:1065
target Thresh 8.95998645357075
target distance 1.0
model initialize at round 1065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.53529453,  9.64211631,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.5764548955375486}
done in step count: 1
reward sum = 0.9853779998661089
running average episode reward sum: 0.5582119343005055
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.53529453,  9.37191468,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7813080925103505}
episode index:1066
target Thresh 8.965005205551412
target distance 3.0
model initialize at round 1066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.        ,  7.91476774,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.243248076363367}
done in step count: 2
reward sum = 0.97028368637424
running average episode reward sum: 0.5585981308816431
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.12754142, 11.13225269,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8824254890388306}
episode index:1067
target Thresh 8.970021448783323
target distance 4.0
model initialize at round 1067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 3.05439615, 10.99684072,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 3.1097063267009872}
done in step count: 6
reward sum = 0.9307659317655618
running average episode reward sum: 0.5589466026053171
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.62620184, 9.37815128, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.8825103859003977}
episode index:1068
target Thresh 8.975035184520543
target distance 6.0
model initialize at round 1068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.91922313,  7.50728822,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.50788057367559}
done in step count: 15
reward sum = 0.8382289718900444
running average episode reward sum: 0.5592078583296247
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.11084419,  1.60873553,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.9714349896839306}
episode index:1069
target Thresh 8.980046414016508
target distance 4.0
model initialize at round 1069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.12492483,  4.19560309,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 3.3735069163372904}
done in step count: 35
reward sum = 0.6447154194791401
running average episode reward sum: 0.5592877719381756
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.20844312,  7.05363238,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7933717432386337}
episode index:1070
target Thresh 8.985055138524027
target distance 7.0
model initialize at round 1070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.0990195135928}
done in step count: 45
reward sum = 0.5391918865257609
running average episode reward sum: 0.5592690082729912
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.45451919,  8.52911592,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7206116349673788}
episode index:1071
target Thresh 8.990061359295273
target distance 4.0
model initialize at round 1071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([14.16927338,  5.4657371 ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 3.7226622790701844}
done in step count: 7
reward sum = 0.913041008351294
running average episode reward sum: 0.5595990194670942
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.83079793,  9.98175562,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.2861062569361101}
episode index:1072
target Thresh 8.995065077581812
target distance 7.0
model initialize at round 1072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 5.000000000000018}
done in step count: 6
reward sum = 0.9228268567512262
running average episode reward sum: 0.5599375356248614
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.20732134, 9.07867994, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.9443584058308684}
episode index:1073
target Thresh 9.000066294634566
target distance 5.0
model initialize at round 1073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 7.        , 10.03712213,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.1742120775640057}
done in step count: 2
reward sum = 0.9703787237914602
running average episode reward sum: 0.5603196968801375
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.13226259, 9.23671412, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.899445270037586}
episode index:1074
target Thresh 9.005065011703842
target distance 8.0
model initialize at round 1074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.76165643, 4.99999475, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 6.004737351051391}
done in step count: 25
reward sum = 0.7138249501153641
running average episode reward sum: 0.5604624924645424
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.01589992, 11.86726237,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8674081088927469}
episode index:1075
target Thresh 9.010061230039318
target distance 1.0
model initialize at round 1075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.57362738, 8.        , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.080636530961875}
done in step count: 14
reward sum = 0.8495346153759545
running average episode reward sum: 0.5607311468538653
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.9993768 , 6.15497994, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.15498119720461775}
episode index:1076
target Thresh 9.01505495089005
target distance 9.0
model initialize at round 1076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([12.3680222 , 10.27786076,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.398078967810205}
done in step count: 21
reward sum = 0.770445321087101
running average episode reward sum: 0.5609258675356046
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.88072851, 5.86054802, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.18350079898249474}
episode index:1077
target Thresh 9.020046175504467
target distance 1.0
model initialize at round 1077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([7.48159909, 9.73982477, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 2.495200433861355}
done in step count: 4
reward sum = 0.9484511840584686
running average episode reward sum: 0.5612853529869246
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.85109161, 10.19267033,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.24350680910431377}
episode index:1078
target Thresh 9.025034905130376
target distance 5.0
model initialize at round 1078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4.86758859, 6.04909228, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 5.717210495339177}
done in step count: 5
reward sum = 0.9292783330137117
running average episode reward sum: 0.5616264030147529
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.94616555, 9.80607505, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.2012586305027336}
episode index:1079
target Thresh 9.03002114101496
target distance 5.0
model initialize at round 1079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([10.78872514, 10.54713464,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 4.819880159968368}
done in step count: 14
reward sum = 0.8424296969767786
running average episode reward sum: 0.5618864060647177
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.3965739 , 10.76623313,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.9753134211609565}
episode index:1080
target Thresh 9.035004884404776
target distance 3.0
model initialize at round 1080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.4142135623730951}
done in step count: 1
reward sum = 0.9808993265739228
running average episode reward sum: 0.5622740220873904
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.76535296,  3.10521078,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9250443111438197}
episode index:1081
target Thresh 9.039986136545764
target distance 5.0
model initialize at round 1081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 6.        , 10.34252274,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.8062334123281247}
done in step count: 27
reward sum = 0.7135128006955797
running average episode reward sum: 0.5624137991471022
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.40846095, 7.88615501, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.4240295131617676}
episode index:1082
target Thresh 9.044964898683231
target distance 9.0
model initialize at round 1082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([9.10887185, 8.13329047, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 9.388999465664137}
done in step count: 18
reward sum = 0.781942872875308
running average episode reward sum: 0.562616503739649
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.82271852, 1.13319305, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.195081609843061}
episode index:1083
target Thresh 9.049941172061873
target distance 2.0
model initialize at round 1083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([15.88281059,  5.        ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.131894867904988}
done in step count: 31
reward sum = 0.6835532119294102
running average episode reward sum: 0.5627280689686064
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.92129275,  5.6051512 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0023402161942532}
episode index:1084
target Thresh 9.054914957925755
target distance 3.0
model initialize at round 1084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11., 11.,  0.]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.4142135623730951}
done in step count: 21
reward sum = 0.7386830237683717
running average episode reward sum: 0.5628902394338595
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.61657784, 10.05722599,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.6192277864275393}
episode index:1085
target Thresh 9.059886257518324
target distance 1.0
model initialize at round 1085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.67444123, 11.89277225,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.920566405103353}
done in step count: 6
reward sum = 0.9312492260113453
running average episode reward sum: 0.5632294281876141
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.13972543, 10.63353895,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.6487640502039285}
episode index:1086
target Thresh 9.064855072082407
target distance 8.0
model initialize at round 1086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 8.        , 10.20825291,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 6.120447294666409}
done in step count: 33
reward sum = 0.6554675989530884
running average episode reward sum: 0.5633142839104894
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.81821397, 8.32136327, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.702562434826037}
episode index:1087
target Thresh 9.069821402860208
target distance 7.0
model initialize at round 1087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([12.        , 11.12793231,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 5.125644476178335}
done in step count: 3
reward sum = 0.9544672725026557
running average episode reward sum: 0.5636737995250044
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.02051126, 9.27606944, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.2179793263053407}
episode index:1088
target Thresh 9.074785251093305
target distance 7.0
model initialize at round 1088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 8.        , 10.17007339,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 5.002891659629323}
done in step count: 3
reward sum = 0.9592620374112745
running average episode reward sum: 0.5640370577783435
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.54558969, 9.85430557, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.4771955597701826}
episode index:1089
target Thresh 9.079746618022664
target distance 5.0
model initialize at round 1089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([11.        , 10.84068634,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.5196770005392954}
done in step count: 2
reward sum = 0.9723071023613659
running average episode reward sum: 0.5644116174522729
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.93388224, 9.22600569, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.23547851239798478}
episode index:1090
target Thresh 9.084705504888626
target distance 3.0
model initialize at round 1090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([5.        , 9.96841609, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.3920595291253828}
done in step count: 1
reward sum = 0.9850758206495367
running average episode reward sum: 0.5647971941738102
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.9163295 , 9.38341093, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.392434317800533}
episode index:1091
target Thresh 9.089661912930913
target distance 4.0
model initialize at round 1091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.08298814, 4.08167303, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 2.083326575312295}
done in step count: 1
reward sum = 0.9828121803919627
running average episode reward sum: 0.5651799917802371
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.85523184, 2.09217691, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8601849125446294}
episode index:1092
target Thresh 9.094615843388626
target distance 5.0
model initialize at round 1092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.6055512754640153}
done in step count: 2
reward sum = 0.9626476679655964
running average episode reward sum: 0.5655436401573509
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.86635116,  5.02158734,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.3068495208777142}
episode index:1093
target Thresh 9.099567297500249
target distance 1.0
model initialize at round 1093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.,  8.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.4142135623730698}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.5659352821683638
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.,  8.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.4142135623730698}
episode index:1094
target Thresh 9.104516276503645
target distance 7.0
model initialize at round 1094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.        ,  7.15043473,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 5.150434732437177}
done in step count: 4
reward sum = 0.9439091850823843
running average episode reward sum: 0.5662804638148605
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.79412375,  2.01673585,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.2065553631622367}
episode index:1095
target Thresh 9.109462781636056
target distance 5.0
model initialize at round 1095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([16.,  7.,  0.]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 3.1622776601683924}
done in step count: 23
reward sum = 0.7525487554761089
running average episode reward sum: 0.5664504166357194
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.575488  ,  3.08290879,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0827015870388865}
episode index:1096
target Thresh 9.114406814134114
target distance 7.0
model initialize at round 1096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([7.45697963, 8.93587744, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 8.825249152495504}
done in step count: 14
reward sum = 0.8457597958009406
running average episode reward sum: 0.5667050286495436
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.3659864 , 1.16259666, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9138875198231691}
episode index:1097
target Thresh 9.119348375233823
target distance 9.0
model initialize at round 1097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([14.90149689,  5.45120239,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 9.58282898164206}
done in step count: 27
reward sum = 0.7215927377637134
running average episode reward sum: 0.5668460921368972
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.07837626, 8.78345198, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.23029520570445652}
episode index:1098
target Thresh 9.124287466170573
target distance 8.0
model initialize at round 1098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([10.00000001,  8.37650599,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 6.548489972233796}
done in step count: 7
reward sum = 0.9142598700561446
running average episode reward sum: 0.5671622102241759
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.56325755, 11.39781229,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6895750051028448}
episode index:1099
target Thresh 9.129224088179141
target distance 5.0
model initialize at round 1099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.97701799,  6.20518982,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.794865253104437}
done in step count: 8
reward sum = 0.9002646363702141
running average episode reward sum: 0.5674650306115815
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.81967692, 11.86094826,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8796296463129133}
episode index:1100
target Thresh 9.134158242493678
target distance 5.0
model initialize at round 1100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([10.00494245, 11.86740305,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 4.1535141000645}
done in step count: 7
reward sum = 0.9093070394762274
running average episode reward sum: 0.5677755138167264
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.63356084, 8.11180889, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.0910008235743747}
episode index:1101
target Thresh 9.139089930347723
target distance 7.0
model initialize at round 1101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([15.50623143,  3.32020795,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 8.656685642606917}
done in step count: 39
reward sum = 0.5928726908105347
running average episode reward sum: 0.5677982880245248
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.53923429, 10.94538782,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.0516953848474588}
episode index:1102
target Thresh 9.1440191529742
target distance 3.0
model initialize at round 1102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([13.23104823, 11.87834142,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 2.5801854312384593}
done in step count: 12
reward sum = 0.870475713374
running average episode reward sum: 0.5680727009214872
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.11591314, 10.25262072,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.2779443916677032}
episode index:1103
target Thresh 9.148945911605415
target distance 9.0
model initialize at round 1103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([10.39550662,  9.67774868,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 8.75070573649921}
done in step count: 4
reward sum = 0.940904848346027
running average episode reward sum: 0.5684104111999514
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.24184573, 5.21635624, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7884211600730469}
episode index:1104
target Thresh 9.153870207473057
target distance 2.0
model initialize at round 1104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([1.68811905, 7.83298056, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.3179061678869552}
done in step count: 3
reward sum = 0.963792443809008
running average episode reward sum: 0.5687682229941677
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.07966158, 7.28111665, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.7232836466112744}
episode index:1105
target Thresh 9.158792041808198
target distance 3.0
model initialize at round 1105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.73138642,  1.51482749,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.4955086438420064}
done in step count: 18
reward sum = 0.8119083467730849
running average episode reward sum: 0.5689880603574398
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.81047622,  5.40191547,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.4443594398045617}
episode index:1106
target Thresh 9.163711415841298
target distance 8.0
model initialize at round 1106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.72613347,  9.2722857 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 6.400336229254746}
done in step count: 5
reward sum = 0.9316303953896133
running average episode reward sum: 0.569315650542654
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.88764433,  3.28883153,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9334538559635697}
episode index:1107
target Thresh 9.1686283308022
target distance 1.0
model initialize at round 1107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([5.        , 7.53031421, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.0691140998571607}
done in step count: 34
reward sum = 0.6585462820844501
running average episode reward sum: 0.5693961836036123
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.8404898 , 7.46356067, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.49023667652671615}
episode index:1108
target Thresh 9.173542787920136
target distance 1.0
model initialize at round 1108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.53070104,  5.        ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.1320969901707316}
done in step count: 0
reward sum = 0.9967887676989045
running average episode reward sum: 0.5697815691618587
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.53070104,  5.        ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.1320969901707316}
episode index:1109
target Thresh 9.178454788423716
target distance 2.0
model initialize at round 1109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.86200495, 11.86725219,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8781622776089898}
done in step count: 0
reward sum = 0.9957360170955992
running average episode reward sum: 0.570165311907745
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.86200495, 11.86725219,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8781622776089898}
episode index:1110
target Thresh 9.183364333540945
target distance 7.0
model initialize at round 1110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([4.       , 8.3064115, 0.       ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 5.670802681957477}
done in step count: 42
reward sum = 0.5646532252734073
running average episode reward sum: 0.5701603505336368
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.1918255, 2.7928214, 0.       ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8343074919896466}
episode index:1111
target Thresh 9.188271424499202
target distance 7.0
model initialize at round 1111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.        , 10.20784199,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 8.207841992378263}
done in step count: 34
reward sum = 0.6462040354552998
running average episode reward sum: 0.5702287351423793
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.53028161,  1.4359311 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7741913904174526}
episode index:1112
target Thresh 9.193176062525268
target distance 1.0
model initialize at round 1112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([12.19615734, 10.26841334,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.225902969847293}
done in step count: 1
reward sum = 0.9883519319920602
running average episode reward sum: 0.5706044073767456
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.32248133,  9.10881391,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.947737756415273}
episode index:1113
target Thresh 9.198078248845295
target distance 7.0
model initialize at round 1113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 9.06303811, 10.09310997,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.937691970812563}
done in step count: 5
reward sum = 0.9390241407258034
running average episode reward sum: 0.5709351252702366
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.24272109, 10.54677986,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9340447299361464}
episode index:1114
target Thresh 9.202977984684836
target distance 6.0
model initialize at round 1114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 6.98172991, 11.86420492,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 4.93458856710787}
done in step count: 13
reward sum = 0.8573293399985953
running average episode reward sum: 0.5711919810681992
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.96937334,  9.82895756,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8295231376170312}
episode index:1115
target Thresh 9.20787527126882
target distance 5.0
model initialize at round 1115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([14.02619884, 11.86775028,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 3.5561735756854214}
done in step count: 80
reward sum = 0.29845926031452696
running average episode reward sum: 0.5709475969098179
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.76110125,  9.74627895,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.3484924491185035}
episode index:1116
target Thresh 9.212770109821573
target distance 2.0
model initialize at round 1116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.97082305, 10.28151178,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.7190803929629207}
done in step count: 0
reward sum = 0.9958551883691628
running average episode reward sum: 0.5713279976183759
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.97082305, 10.28151178,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.7190803929629207}
episode index:1117
target Thresh 9.2176625015668
target distance 8.0
model initialize at round 1117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([14.30704176,  7.74598563,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 7.413866156863287}
done in step count: 4
reward sum = 0.9458569275013904
running average episode reward sum: 0.5716629966612051
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.90968421, 8.16997429, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.834924916430015}
episode index:1118
target Thresh 9.222552447727601
target distance 5.0
model initialize at round 1118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([12.        , 11.07766551,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 3.187689280135006}
done in step count: 2
reward sum = 0.9697450346624529
running average episode reward sum: 0.5720187446844412
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.        , 10.06537807,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0021348671814532}
episode index:1119
target Thresh 9.227439949526465
target distance 5.0
model initialize at round 1119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.40161675,  7.        ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 3.0590950477731624}
done in step count: 3
reward sum = 0.9620324784682278
running average episode reward sum: 0.5723669712324625
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.70827969,  4.6333104 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6972681048820502}
episode index:1120
target Thresh 9.232325008185265
target distance 9.0
model initialize at round 1120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([14.      ,  4.656708,  0.      ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 8.237972164258954}
done in step count: 4
reward sum = 0.9347059616458846
running average episode reward sum: 0.5726901995914397
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.60365409, 9.22740409, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.6450665712326867}
episode index:1121
target Thresh 9.237207624925265
target distance 7.0
model initialize at round 1121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.904457013955612}
done in step count: 3
reward sum = 0.9546523608839921
running average episode reward sum: 0.573030629325212
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.85483083,  9.94455211,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.8566272352283183}
episode index:1122
target Thresh 9.242087800967123
target distance 2.0
model initialize at round 1122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.87402831, 3.15273001, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8872721921200344}
done in step count: 0
reward sum = 0.9944060517112969
running average episode reward sum: 0.5734058523193225
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.87402831, 3.15273001, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8872721921200344}
episode index:1123
target Thresh 9.24696553753088
target distance 9.0
model initialize at round 1123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([10.66747372, 11.8906994 ,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 8.194284360072638}
done in step count: 14
reward sum = 0.845179091010279
running average episode reward sum: 0.573647643456948
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.50743386, 9.55812343, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.7543148482044467}
episode index:1124
target Thresh 9.25184083583597
target distance 1.0
model initialize at round 1124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([10.81305008,  8.30060939,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 2.4849706320506524}
done in step count: 17
reward sum = 0.8209173818222957
running average episode reward sum: 0.5738674387799393
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.75164195, 9.06282469, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.9695252906341862}
episode index:1125
target Thresh 9.256713697101217
target distance 5.0
model initialize at round 1125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 5.28898692, 10.92467254,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 3.824478707462924}
done in step count: 5
reward sum = 0.9399754621329852
running average episode reward sum: 0.574192579120395
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.18335574, 9.62006934, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.4218609148503143}
episode index:1126
target Thresh 9.261584122544837
target distance 3.0
model initialize at round 1126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.51679251, 4.02327919, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.0349237437097303}
done in step count: 1
reward sum = 0.9864125082587095
running average episode reward sum: 0.5745583465819196
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.86760791, 5.17991202, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.193854172861451}
episode index:1127
target Thresh 9.26645211338444
target distance 2.0
model initialize at round 1127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.00494245, 11.86740305,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.867417127752727}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.5749301920195291
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.00494245, 11.86740305,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.867417127752727}
episode index:1128
target Thresh 9.271317670837018
target distance 2.0
model initialize at round 1128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.11940832,  4.77236809,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9095372428375444}
done in step count: 0
reward sum = 0.9948064669610865
running average episode reward sum: 0.5753020930602214
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.11940832,  4.77236809,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9095372428375444}
episode index:1129
target Thresh 9.276180796118965
target distance 7.0
model initialize at round 1129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([10.        ,  8.73613679,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 6.88701617047153}
done in step count: 35
reward sum = 0.6352578418666303
running average episode reward sum: 0.5753551512450058
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.32566933,  3.8637954 ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.687948790916739}
episode index:1130
target Thresh 9.281041490446057
target distance 7.0
model initialize at round 1130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 8.774297  , 10.92308474,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.9876870516061595}
done in step count: 25
reward sum = 0.7344343836825926
running average episode reward sum: 0.5754958048545881
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.74943431,  7.78757209,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7789591821142974}
episode index:1131
target Thresh 9.285899755033475
target distance 9.0
model initialize at round 1131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([13.90772843,  7.02536821,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 9.123968245153925}
done in step count: 70
reward sum = 0.3581488926988223
running average episode reward sum: 0.5753038022820124
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.54250138, 8.14705283, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.0108544017095638}
episode index:1132
target Thresh 9.29075559109578
target distance 4.0
model initialize at round 1132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.48863554,  6.28921545,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.006762351413379}
done in step count: 12
reward sum = 0.8507044283445385
running average episode reward sum: 0.5755468743261982
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.8669944 , 10.17041877,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8835846531325259}
episode index:1133
target Thresh 9.29560899984693
target distance 8.0
model initialize at round 1133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([7.91489518, 8.14410551, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 6.1917265889506785}
done in step count: 3
reward sum = 0.9512224215828482
running average episode reward sum: 0.5758781578775709
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.87287556,  6.22752504,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7828653720824095}
episode index:1134
target Thresh 9.30045998250028
target distance 5.0
model initialize at round 1134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2., 5., 0.]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 3.0000000000000195}
done in step count: 3
reward sum = 0.9531441173504809
running average episode reward sum: 0.5762105507934061
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.3954094 , 1.09569224, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.986975745909416}
episode index:1135
target Thresh 9.305308540268578
target distance 7.0
model initialize at round 1135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([7.97379352, 8.13224956, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 5.786773209275114}
done in step count: 57
reward sum = 0.46645218118173776
running average episode reward sum: 0.5761139325102972
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.53036812, 10.76344498,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.5258444423910925}
episode index:1136
target Thresh 9.31015467436396
target distance 7.0
model initialize at round 1136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 8.00000087, 11.75805787,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 5.05713955486212}
done in step count: 3
reward sum = 0.9502858532134137
running average episode reward sum: 0.576443019511795
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.53618404, 11.91096585,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0570487744110224}
episode index:1137
target Thresh 9.314998385997956
target distance 9.0
model initialize at round 1137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.000000000000018}
done in step count: 4
reward sum = 0.9320106169762221
running average episode reward sum: 0.5767554690701996
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.14354486, 11.87032725,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.2210589364066264}
episode index:1138
target Thresh 9.319839676381502
target distance 9.0
model initialize at round 1138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.        , 10.85721719,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.523110852427328}
done in step count: 28
reward sum = 0.6891075515793883
running average episode reward sum: 0.5768541100557212
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.32086657, 2.62115211, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.4964686031342173}
episode index:1139
target Thresh 9.324678546724915
target distance 8.0
model initialize at round 1139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([11.        ,  9.63949502,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 6.0108205632930005}
done in step count: 5
reward sum = 0.9339831239597697
running average episode reward sum: 0.5771673811205493
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.5709343 , 9.18480092, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.9212203395487742}
episode index:1140
target Thresh 9.329514998237915
target distance 6.0
model initialize at round 1140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([10.14541692,  8.12857994,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 5.77537784805498}
done in step count: 8
reward sum = 0.9025539610922003
running average episode reward sum: 0.5774525577901125
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.39702174,  4.38784328,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8592546984419319}
episode index:1141
target Thresh 9.334349032129616
target distance 2.0
model initialize at round 1141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 3.41401696, 10.74109483,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.8370594219132332}
done in step count: 2
reward sum = 0.975029101134626
running average episode reward sum: 0.5778006983709746
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.88379842, 8.90544525, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.888842083330899}
episode index:1142
target Thresh 9.33918064960852
target distance 4.0
model initialize at round 1142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.        , 7.44519091, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 2.4451909065246777}
done in step count: 7
reward sum = 0.9185480035068091
running average episode reward sum: 0.57809881499839
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.81765813, 5.46373099, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9400059805670854}
episode index:1143
target Thresh 9.34400985188254
target distance 4.0
model initialize at round 1143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 5.        , 10.52958637,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.2247181624709462}
done in step count: 11
reward sum = 0.8691938340288319
running average episode reward sum: 0.5783532686863536
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.68410982, 8.73190907, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7971684246647104}
episode index:1144
target Thresh 9.348836640158972
target distance 9.0
model initialize at round 1144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([10.,  9.,  0.]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 8.062257748298576}
done in step count: 13
reward sum = 0.8466705143738901
running average episode reward sum: 0.5785876068921942
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.91272126,  1.15189113,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.2459328846004212}
episode index:1145
target Thresh 9.353661015644514
target distance 7.0
model initialize at round 1145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.90901203, 2.44481997, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 6.645347240554593}
done in step count: 3
reward sum = 0.949581897199377
running average episode reward sum: 0.5789113366394081
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.435854 , 8.5362546, 0.       ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.6364184973466701}
episode index:1146
target Thresh 9.358482979545256
target distance 6.0
model initialize at round 1146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.38838018,  6.37405348,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.829801314368027}
done in step count: 2
reward sum = 0.9678332770549588
running average episode reward sum: 0.5792504141811827
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.77610748, 10.49892537,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.5488202263927515}
episode index:1147
target Thresh 9.363302533066697
target distance 4.0
model initialize at round 1147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.50480741,  5.        ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.0627240542158365}
done in step count: 1
reward sum = 0.9851287731243621
running average episode reward sum: 0.5796039667586594
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.8649638 ,  3.67403457,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.096578762918531}
episode index:1148
target Thresh 9.36811967741372
target distance 7.0
model initialize at round 1148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.04814053,  9.        ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 5.000231745652016}
done in step count: 29
reward sum = 0.6839485049635737
running average episode reward sum: 0.5796947801078369
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.26771978,  4.12788682,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.29669668032897595}
episode index:1149
target Thresh 9.372934413790611
target distance 2.0
model initialize at round 1149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.80920398, 7.79982919, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.215241994627407}
done in step count: 1
reward sum = 0.9888701931525598
running average episode reward sum: 0.5800505848148324
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.46597791, 8.43804073, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.7300230320808467}
episode index:1150
target Thresh 9.377746743401058
target distance 8.0
model initialize at round 1150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.86059092, 3.95840422, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 8.613548834987961}
done in step count: 20
reward sum = 0.7641619052779948
running average episode reward sum: 0.5802105425215772
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.53385131, 10.59521139,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.7995460113492583}
episode index:1151
target Thresh 9.382556667448139
target distance 1.0
model initialize at round 1151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.29450107,  7.        ,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.635766795852435}
done in step count: 1
reward sum = 0.985257661969634
running average episode reward sum: 0.5805621459238759
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.51093495,  6.80013159,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9493498206791464}
episode index:1152
target Thresh 9.387364187134336
target distance 6.0
model initialize at round 1152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([12.        ,  8.53170419,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 5.4347348456054005}
done in step count: 2
reward sum = 0.964634660533026
running average episode reward sum: 0.5808952530484286
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.6403268 ,  4.53170541,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8323034584082758}
episode index:1153
target Thresh 9.39216930366153
target distance 5.0
model initialize at round 1153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.74054539,  5.46846223,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 5.5808886023781845}
done in step count: 5
reward sum = 0.9376606286307142
running average episode reward sum: 0.5812044084865415
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.75709152, 10.42540264,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.6238322316202954}
episode index:1154
target Thresh 9.396972018230999
target distance 9.0
model initialize at round 1154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.        ,  7.81289196,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.691401540533095}
done in step count: 7
reward sum = 0.904931545481094
running average episode reward sum: 0.5814846917220345
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.55643646, 10.94207393,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.44732990219100904}
episode index:1155
target Thresh 9.401772332043425
target distance 6.0
model initialize at round 1155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 6.523056363717578}
done in step count: 5
reward sum = 0.9286479836965026
running average episode reward sum: 0.5817850059884484
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.99692029, 9.05686913, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.05695245739932455}
episode index:1156
target Thresh 9.406570246298882
target distance 3.0
model initialize at round 1156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.        ,  7.89727664,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.3435420978067618}
done in step count: 7
reward sum = 0.9114874538531691
running average episode reward sum: 0.5820699692104576
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.58443881,  6.76561697,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6296857337211885}
episode index:1157
target Thresh 9.411365762196851
target distance 4.0
model initialize at round 1157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4., 7., 0.]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.236067977499816}
done in step count: 1
reward sum = 0.9796307716383813
running average episode reward sum: 0.5824132859655767
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.62723517, 5.        , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.1804338031958337}
episode index:1158
target Thresh 9.416158880936214
target distance 2.0
model initialize at round 1158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.25770187,  7.        ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.2577018737792933}
done in step count: 0
reward sum = 0.9965867448929437
running average episode reward sum: 0.5827706401147807
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.25770187,  7.        ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.2577018737792933}
episode index:1159
target Thresh 9.420949603715243
target distance 5.0
model initialize at round 1159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([8.       , 9.0134778, 0.       ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.5980926116552334}
done in step count: 11
reward sum = 0.8633212832721252
running average episode reward sum: 0.5830124941175027
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.5906018 , 11.02301293,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5910499840389185}
episode index:1160
target Thresh 9.425737931731625
target distance 7.0
model initialize at round 1160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([7., 9., 0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 5.3851648071345295}
done in step count: 9
reward sum = 0.8848449281057773
running average episode reward sum: 0.5832724703741679
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.14278429, 7.1634912 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8726672587469897}
episode index:1161
target Thresh 9.430523866182437
target distance 6.0
model initialize at round 1161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 7.52769566, 10.88549209,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 4.853512805676262}
done in step count: 18
reward sum = 0.7982817219237801
running average episode reward sum: 0.5834575041534704
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.76662352,  8.70800692,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.37379745590774954}
episode index:1162
target Thresh 9.43530740826417
target distance 7.0
model initialize at round 1162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 8.        , 10.17558968,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.136342190440767}
done in step count: 4
reward sum = 0.9460012239384582
running average episode reward sum: 0.5837692356408178
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.00161501, 8.39178659, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.1690577989868254}
episode index:1163
target Thresh 9.440088559172706
target distance 9.0
model initialize at round 1163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([13.       ,  9.3974458,  0.       ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 7.025885820766248}
done in step count: 6
reward sum = 0.9241397255120408
running average episode reward sum: 0.5840616501510165
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.77994382, 10.54405317,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.5868718508448498}
episode index:1164
target Thresh 9.444867320103327
target distance 9.0
model initialize at round 1164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([8.        , 9.77672231, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 7.953843814267355}
done in step count: 7
reward sum = 0.9153344712049075
running average episode reward sum: 0.5843460045038525
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.85292587,  5.90194889,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8585432748223634}
episode index:1165
target Thresh 9.449643692250733
target distance 7.0
model initialize at round 1165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([8.       , 9.5027988, 0.       ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 5.025217072819596}
done in step count: 53
reward sum = 0.48103889956880586
running average episode reward sum: 0.5842574049284366
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.33480722,  9.40561071,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.5259428906943795}
episode index:1166
target Thresh 9.454417676809008
target distance 4.0
model initialize at round 1166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2., 5., 0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.0000000000000187}
done in step count: 3
reward sum = 0.962081507532016
running average episode reward sum: 0.5845811616573171
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.18793643, 6.97026765, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.1902737826155376}
episode index:1167
target Thresh 9.459189274971655
target distance 6.0
model initialize at round 1167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([2.        , 7.42214119, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.779240013305988}
done in step count: 14
reward sum = 0.8430786816971844
running average episode reward sum: 0.5848024780272143
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.13710115, 1.32243498, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.097127514472574}
episode index:1168
target Thresh 9.463958487931569
target distance 4.0
model initialize at round 1168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.93754482,  4.6551435 ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.8598248208687425}
done in step count: 4
reward sum = 0.9510552585190223
running average episode reward sum: 0.5851157823732295
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.86515059,  1.14143303,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.2188612694121965}
episode index:1169
target Thresh 9.468725316881057
target distance 1.0
model initialize at round 1169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.        ,  9.99582154,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.4171712629001623}
done in step count: 1
reward sum = 0.986822145429829
running average episode reward sum: 0.5854591211450727
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.6149717 , 10.29793185,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.933321957216777}
episode index:1170
target Thresh 9.473489763011825
target distance 6.0
model initialize at round 1170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.14334836, 7.01053379, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 5.3434636847030585}
done in step count: 32
reward sum = 0.6694073371926508
running average episode reward sum: 0.5855308104841398
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.99635063, 1.13270001, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8673076683931703}
episode index:1171
target Thresh 9.478251827514985
target distance 6.0
model initialize at round 1171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.        ,  4.05584574,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 5.944154262542762}
done in step count: 4
reward sum = 0.9460722900419518
running average episode reward sum: 0.5858384397329093
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.41807613,  9.5172154 ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6386459262480226}
episode index:1172
target Thresh 9.48301151158105
target distance 9.0
model initialize at round 1172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([4.85238878, 3.99673078, 0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 9.561185678803387}
done in step count: 17
reward sum = 0.8038412818273477
running average episode reward sum: 0.5860242904081816
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.25117745,  8.80664078,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.31698248880661734}
episode index:1173
target Thresh 9.487768816399944
target distance 2.0
model initialize at round 1173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.55949032,  5.8059231 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.3186542581716638}
done in step count: 1
reward sum = 0.9835604781160004
running average episode reward sum: 0.5863629072631287
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.87280235,  7.77544704,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.1675196161456518}
episode index:1174
target Thresh 9.492523743160993
target distance 1.0
model initialize at round 1174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.        , 10.33787477,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0555374746136055}
done in step count: 0
reward sum = 0.9966711927438967
running average episode reward sum: 0.5867121058039633
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.        , 10.33787477,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0555374746136055}
episode index:1175
target Thresh 9.497276293052929
target distance 7.0
model initialize at round 1175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([13.04928422, 10.91587722,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 6.345425525911125}
done in step count: 67
reward sum = 0.383096638069263
running average episode reward sum: 0.5865389633994269
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.4254342 , 9.63251127, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.762276037311131}
episode index:1176
target Thresh 9.502026467263887
target distance 7.0
model initialize at round 1176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([3.06153023, 7.45160306, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.793169034146049}
done in step count: 8
reward sum = 0.9027303044064879
running average episode reward sum: 0.586807605150495
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.26345609, 10.46738998,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5942074909795859}
episode index:1177
target Thresh 9.506774266981413
target distance 3.0
model initialize at round 1177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.34766114, 6.        , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.6781509293701158}
done in step count: 32
reward sum = 0.6727225821937751
running average episode reward sum: 0.5868805380681887
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.23484543, 7.43591436, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.4951501860876302}
episode index:1178
target Thresh 9.511519693392456
target distance 3.0
model initialize at round 1178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 9.21801281, 11.88010974,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.127073285786398}
done in step count: 18
reward sum = 0.7629756076034312
running average episode reward sum: 0.5870298977539692
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.72657827, 8.9274869 , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.7301877329900103}
episode index:1179
target Thresh 9.516262747683374
target distance 7.0
model initialize at round 1179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([4., 6., 0.]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 5.8309518948453265}
done in step count: 3
reward sum = 0.9555738882071471
running average episode reward sum: 0.5873422231696074
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.8059054 , 8.52453835, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.5135528132614321}
episode index:1180
target Thresh 9.52100343103993
target distance 3.0
model initialize at round 1180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.        ,  5.98699105,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.9869910478591608}
done in step count: 12
reward sum = 0.8622314463429969
running average episode reward sum: 0.5875749828844028
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.8893446 ,  4.36849296,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9626634285077781}
episode index:1181
target Thresh 9.525741744647295
target distance 1.0
model initialize at round 1181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.       , 10.8942647,  0.       ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.005574439851638}
done in step count: 0
reward sum = 0.9964002183296673
running average episode reward sum: 0.5879208587181127
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.       , 10.8942647,  0.       ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.005574439851638}
episode index:1182
target Thresh 9.530477689690043
target distance 4.0
model initialize at round 1182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([13.25496125, 10.18798319,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 4.417691626381237}
done in step count: 20
reward sum = 0.7717702011220047
running average episode reward sum: 0.5880762681368817
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.47246671, 9.78369775, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.9150994226010339}
episode index:1183
target Thresh 9.535211267352167
target distance 8.0
model initialize at round 1183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([10.        , 10.81062809,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 7.690392875853145}
done in step count: 4
reward sum = 0.9423542485214468
running average episode reward sum: 0.5883754894040985
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.43056083, 5.52458045, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7418117827700723}
episode index:1184
target Thresh 9.539942478817057
target distance 9.0
model initialize at round 1184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 5.37367296, 10.80570447,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.628514873054481}
done in step count: 17
reward sum = 0.8071109203929473
running average episode reward sum: 0.5885600762656924
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.61390877, 11.90392045,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0926830072068394}
episode index:1185
target Thresh 9.544671325267519
target distance 3.0
model initialize at round 1185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.83713746, 4.        , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.3041468966806147}
done in step count: 29
reward sum = 0.6947008531269018
running average episode reward sum: 0.5886495710185264
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.16243377, 5.20128228, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8614126461644618}
episode index:1186
target Thresh 9.54939780788576
target distance 1.0
model initialize at round 1186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.77169263, 8.50197637, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5478611035375814}
done in step count: 0
reward sum = 0.9993643825986992
running average episode reward sum: 0.58899558181177
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.77169263, 8.50197637, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5478611035375814}
episode index:1187
target Thresh 9.554121927853407
target distance 7.0
model initialize at round 1187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([10.07896948,  8.72446656,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 6.51776959428602}
done in step count: 4
reward sum = 0.9469401568476326
running average episode reward sum: 0.5892968819591067
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.42734039,  5.7460708 ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.49709138491853}
episode index:1188
target Thresh 9.558843686351487
target distance 6.0
model initialize at round 1188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([14.2828058 ,  6.88207424,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.297535897816575}
done in step count: 2
reward sum = 0.9658837306781358
running average episode reward sum: 0.5896136076518896
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.2828058 ,  9.67977679,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.427225966462694}
episode index:1189
target Thresh 9.56356308456044
target distance 5.0
model initialize at round 1189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.49972787, 7.        , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 3.354223673082839}
done in step count: 6
reward sum = 0.9287442224139942
running average episode reward sum: 0.5898985913617738
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.91191344, 4.52544157, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0524613803236902}
episode index:1190
target Thresh 9.56828012366011
target distance 9.0
model initialize at round 1190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.        , 3.63764453, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.9501118279127505}
done in step count: 13
reward sum = 0.8324075991683638
running average episode reward sum: 0.5901022093364225
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.2987021 , 11.85252458,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9033388664920643}
episode index:1191
target Thresh 9.572994804829769
target distance 8.0
model initialize at round 1191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([ 6.97506418, 11.85642417,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 6.304455754396309}
done in step count: 41
reward sum = 0.584962130572518
running average episode reward sum: 0.5900978971898085
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.27807663, 10.73138478,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.0276658251385364}
episode index:1192
target Thresh 9.577707129248076
target distance 6.0
model initialize at round 1192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.75582111,  4.        ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 4.007445986023886}
done in step count: 33
reward sum = 0.6641926760602156
running average episode reward sum: 0.5901600051352154
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.63905662,  8.85208312,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.065100465372857}
episode index:1193
target Thresh 9.582417098093114
target distance 8.0
model initialize at round 1193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13167914, 8.04814758, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 6.110161234420105}
done in step count: 15
reward sum = 0.8140086468457053
running average episode reward sum: 0.590347483059596
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.16986028, 1.93788257, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8324605246803428}
episode index:1194
target Thresh 9.58712471254238
target distance 2.0
model initialize at round 1194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.6407442894293542e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.5906852675927723
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.6407442894293542e-14}
episode index:1195
target Thresh 9.591829973772773
target distance 5.0
model initialize at round 1195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([13.85895574,  7.        ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 3.685657406664748}
done in step count: 2
reward sum = 0.9692588065826331
running average episode reward sum: 0.5910018006521284
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.43923527,  3.        ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.14649774635}
episode index:1196
target Thresh 9.596532882960613
target distance 7.0
model initialize at round 1196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.        ,  3.66250229,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.605187220680077}
done in step count: 12
reward sum = 0.8684847547025962
running average episode reward sum: 0.591233615985504
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.28436675, 10.71097648,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7717937172184591}
episode index:1197
target Thresh 9.601233441281625
target distance 8.0
model initialize at round 1197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([4.61082049, 4.        , 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 6.891047653519641}
done in step count: 3
reward sum = 0.9531185866918024
running average episode reward sum: 0.591535690251536
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.32366997, 10.09408193,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.6828423817653057}
episode index:1198
target Thresh 9.605931649910946
target distance 9.0
model initialize at round 1198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([11.02619884, 11.86775028,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 7.588903857503009}
done in step count: 7
reward sum = 0.8994248644787952
running average episode reward sum: 0.5917924785536437
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.59089236, 8.0605521 , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.109827073471382}
episode index:1199
target Thresh 9.610627510023136
target distance 5.0
model initialize at round 1199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.44683176, 8.12947047, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.4936967499272686}
done in step count: 2
reward sum = 0.9660331956527668
running average episode reward sum: 0.592104345817893
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.87443442, 4.18074427, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.198255186643251}
episode index:1200
target Thresh 9.615321022792148
target distance 6.0
model initialize at round 1200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([9.86088789, 8.78366339, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 6.126292671578661}
done in step count: 20
reward sum = 0.8009248837156364
running average episode reward sum: 0.5922782180392899
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.51723953, 7.38837285, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6468154325528178}
episode index:1201
target Thresh 9.620012189391367
target distance 2.0
model initialize at round 1201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.87960812,  6.21323976,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9050865371490998}
done in step count: 0
reward sum = 0.9945437084797654
running average episode reward sum: 0.5926128815088744
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.87960812,  6.21323976,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9050865371490998}
episode index:1202
target Thresh 9.62470101099359
target distance 1.0
model initialize at round 1202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 5.        , 11.56181907,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 3.0521534480670884}
done in step count: 6
reward sum = 0.9317680266037883
running average episode reward sum: 0.5928948059852625
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.75623375, 10.4279782 ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9482080056798152}
episode index:1203
target Thresh 9.629387488771012
target distance 9.0
model initialize at round 1203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.        ,  3.47940159,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 9.620779618511184}
done in step count: 11
reward sum = 0.8709255245543807
running average episode reward sum: 0.593125728508991
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.41222569, 10.45875208,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6803523563321768}
episode index:1204
target Thresh 9.63407162389526
target distance 5.0
model initialize at round 1204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([3.3680563, 5.       , 0.       ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 3.2972076113870057}
done in step count: 15
reward sum = 0.8364814380636887
running average episode reward sum: 0.5933276834546796
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.07553858, 1.18424336, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8192465884124334}
episode index:1205
target Thresh 9.638753417537364
target distance 6.0
model initialize at round 1205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([4., 8., 0.]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 4.123105625617683}
done in step count: 2
reward sum = 0.9676433077321812
running average episode reward sum: 0.5936380612525879
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.28159547, 9.80663562, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.0801694734688705}
episode index:1206
target Thresh 9.643432870867775
target distance 5.0
model initialize at round 1206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([2.04918277, 5.41099548, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 3.9294500941408117}
done in step count: 2
reward sum = 0.9717327250902029
running average episode reward sum: 0.5939513128382031
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.28568038, 2.1636399 , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.32922833924330575}
episode index:1207
target Thresh 9.648109985056355
target distance 9.0
model initialize at round 1207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 7.97599587, 11.86065362,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 7.266269051081504}
done in step count: 18
reward sum = 0.8056418890778849
running average episode reward sum: 0.5941265533814478
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.93409641, 10.8084562 ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.235369386657543}
episode index:1208
target Thresh 9.652784761272379
target distance 5.0
model initialize at round 1208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 4.2426406871192865}
done in step count: 5
reward sum = 0.9374985966833008
running average episode reward sum: 0.5944105666513418
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.13514173,  8.95394993,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.2876336045998462}
episode index:1209
target Thresh 9.657457200684547
target distance 6.0
model initialize at round 1209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([13.27991956,  6.00000071,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 4.354156912617386}
done in step count: 7
reward sum = 0.9051565453175314
running average episode reward sum: 0.5946673815097436
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.48616519,  1.08699276,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.0343784691327003}
episode index:1210
target Thresh 9.66212730446097
target distance 8.0
model initialize at round 1210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([9.01865235, 8.13526401, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 6.080455944850008}
done in step count: 31
reward sum = 0.6912616871034707
running average episode reward sum: 0.5947471455936361
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.37283073, 9.01677048, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.6273934475312788}
episode index:1211
target Thresh 9.666795073769165
target distance 2.0
model initialize at round 1211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.        ,  9.55686361,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.5568636059760532}
done in step count: 0
reward sum = 0.9968527224381276
running average episode reward sum: 0.5950789158715606
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.        ,  9.55686361,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.5568636059760532}
episode index:1212
target Thresh 9.671460509776084
target distance 4.0
model initialize at round 1212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([11.00000004, 11.6940629 ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 2.1170081374375447}
done in step count: 1
reward sum = 0.983651049149778
running average episode reward sum: 0.5953992556351865
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.84624702, 11.16453922,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.862094640187031}
episode index:1213
target Thresh 9.67612361364808
target distance 1.0
model initialize at round 1213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([16.41199255,  8.96947527,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.4121856935024484}
done in step count: 16
reward sum = 0.8256847569550835
running average episode reward sum: 0.5955889471519245
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.94424856,  8.64318128,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.009418124174582}
episode index:1214
target Thresh 9.680784386550929
target distance 4.0
model initialize at round 1214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.89029449,  4.07458806,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.35273499022085}
done in step count: 2
reward sum = 0.9726269303262476
running average episode reward sum: 0.59589926647964
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.85125962,  2.96054702,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.2834693326970001}
episode index:1215
target Thresh 9.685442829649828
target distance 2.0
model initialize at round 1215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8675403 , 4.96499127, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.710885001912724}
done in step count: 22
reward sum = 0.7775345645863428
running average episode reward sum: 0.5960486376129515
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.26320495, 3.63639677, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9735850191916349}
episode index:1216
target Thresh 9.690098944109387
target distance 3.0
model initialize at round 1216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.4142135623730951}
done in step count: 1
reward sum = 0.9810249216445033
running average episode reward sum: 0.5963649698101835
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.        , 10.91729808,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.3569951233838817}
episode index:1217
target Thresh 9.694752731093633
target distance 8.0
model initialize at round 1217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([1.15934321, 2.69974884, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.145753870670214}
done in step count: 14
reward sum = 0.8405664780050129
running average episode reward sum: 0.5965654636592761
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.40613336, 10.26566805,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9444156923760694}
episode index:1218
target Thresh 9.699404191766012
target distance 6.0
model initialize at round 1218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 1.12875846, 11.85957598,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 6.914683250108354}
done in step count: 29
reward sum = 0.7040090729919518
running average episode reward sum: 0.596653604438056
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.88949075, 4.87301931, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8985087016141554}
episode index:1219
target Thresh 9.704053327289394
target distance 3.0
model initialize at round 1219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 3.12547135, 10.54260892,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.780565746606891}
done in step count: 31
reward sum = 0.6857652840887756
running average episode reward sum: 0.5967266467984255
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.91199135, 8.54041801, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0600848344434872}
episode index:1220
target Thresh 9.708700138826055
target distance 6.0
model initialize at round 1220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([13.13178528,  4.0759242 ,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 7.11208426513001}
done in step count: 26
reward sum = 0.6916152937386267
running average episode reward sum: 0.5968043606779834
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.03265473, 9.492069  , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.4931513275504859}
episode index:1221
target Thresh 9.713344627537701
target distance 1.0
model initialize at round 1221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.8608618 , 8.13209562, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 2.056732803614623}
done in step count: 6
reward sum = 0.9263054640455998
running average episode reward sum: 0.5970740015154364
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.10679197, 9.16441855, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.8423781144627821}
episode index:1222
target Thresh 9.71798679458546
target distance 3.0
model initialize at round 1222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([3.66722703, 6.41864717, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 3.479254541291501}
done in step count: 2
reward sum = 0.9724604013786199
running average episode reward sum: 0.5973809405177775
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.51231393, 8.30455718, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.8637744405151189}
episode index:1223
target Thresh 9.722626641129867
target distance 4.0
model initialize at round 1223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.56649839,  5.29880381,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.367576687994078}
done in step count: 2
reward sum = 0.9753109195907818
running average episode reward sum: 0.5976897068405495
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.85411578,  3.19600607,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8763173768473258}
episode index:1224
target Thresh 9.727264168330887
target distance 6.0
model initialize at round 1224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([5.98292699, 7.92532716, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 4.158340687967094}
done in step count: 2
reward sum = 0.9702593253280238
running average episode reward sum: 0.5979938453046209
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.88651261, 8.14801652, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.8595087135030582}
episode index:1225
target Thresh 9.7318993773479
target distance 3.0
model initialize at round 1225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.09149599,  5.65082598,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.3504235777707456}
done in step count: 25
reward sum = 0.7450276385195165
running average episode reward sum: 0.5981137749891354
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.87737573,  8.6884164 ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.9310598848356334}
episode index:1226
target Thresh 9.73653226933971
target distance 9.0
model initialize at round 1226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 5.        , 10.29145551,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 7.0060649663504515}
done in step count: 6
reward sum = 0.9145963996274337
running average episode reward sum: 0.5983717070385554
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.68227607,  9.73586614,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.7316196657881492}
episode index:1227
target Thresh 9.741162845464538
target distance 4.0
model initialize at round 1227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([5.49682753, 7.50317247, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 2.916567342690601}
done in step count: 15
reward sum = 0.7955144964924085
running average episode reward sum: 0.598532246769381
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.43673169, 9.53713229, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.7783201704206384}
episode index:1228
target Thresh 9.74579110688003
target distance 6.0
model initialize at round 1228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.61708117,  7.        ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.10880008570387}
done in step count: 2
reward sum = 0.9651694300169893
running average episode reward sum: 0.5988305683179957
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.61708117, 11.        ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6170811653138397}
episode index:1229
target Thresh 9.75041705474325
target distance 7.0
model initialize at round 1229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.43702129, 6.        , 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 5.019062422783911}
done in step count: 3
reward sum = 0.9575905759842462
running average episode reward sum: 0.5991222431209765
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.03560803, 11.86733042,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8680610518632553}
episode index:1230
target Thresh 9.755040690210686
target distance 3.0
model initialize at round 1230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.28405941,  5.        ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0395622856542373}
done in step count: 13
reward sum = 0.8463420598950983
running average episode reward sum: 0.5993230715667718
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.73229787,  6.5186159 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.5836324832925667}
episode index:1231
target Thresh 9.759662014438247
target distance 3.0
model initialize at round 1231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([1.12861253, 5.88510901, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.178318843905091}
done in step count: 34
reward sum = 0.6614829346385966
running average episode reward sum: 0.5993735260010833
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.87349053, 6.18722921, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.1931395822867443}
episode index:1232
target Thresh 9.764281028581264
target distance 3.0
model initialize at round 1232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.97716907, 11.86097253,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.3369580433547037}
done in step count: 1
reward sum = 0.9795591728882986
running average episode reward sum: 0.5996818679693616
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.93530937, 11.86890904,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.2766387668453232}
episode index:1233
target Thresh 9.768897733794487
target distance 4.0
model initialize at round 1233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([11.36223459, 11.        ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.820958409004294}
done in step count: 82
reward sum = 0.3189419760187184
running average episode reward sum: 0.599454364005058
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.44180166, 10.37169614,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6706291131535106}
episode index:1234
target Thresh 9.773512131232096
target distance 2.0
model initialize at round 1234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([11.51077986, 10.03915471,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.772286745095607}
done in step count: 2
reward sum = 0.978224362157242
running average episode reward sum: 0.5997610603598371
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.57933569, 10.33141848,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8846643906680823}
episode index:1235
target Thresh 9.778124222047694
target distance 5.0
model initialize at round 1235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.86595866, 5.97936665, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 3.0236059470851706}
done in step count: 2
reward sum = 0.9678747953548184
running average episode reward sum: 0.6000588870062732
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.97420927, 9.99409786, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.3918743719865128}
episode index:1236
target Thresh 9.782734007394295
target distance 8.0
model initialize at round 1236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 9.27518594, 11.        ,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 6.955426558797818}
done in step count: 9
reward sum = 0.8930871099276718
running average episode reward sum: 0.6002957732010359
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.43354571, 8.40299067, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5919149944474359}
episode index:1237
target Thresh 9.787341488424351
target distance 9.0
model initialize at round 1237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.        ,  9.73658657,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.715019229269304}
done in step count: 87
reward sum = 0.3088674335228687
running average episode reward sum: 0.6000603706649469
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.77890779, 3.00445826, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7789205456540554}
episode index:1238
target Thresh 9.79194666628973
target distance 2.0
model initialize at round 1238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.        , 10.78476602,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.7847660183906875}
done in step count: 0
reward sum = 0.9969652558551473
running average episode reward sum: 0.600380713590847
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.        , 10.78476602,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.7847660183906875}
episode index:1239
target Thresh 9.796549542141731
target distance 8.0
model initialize at round 1239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([14.32552648, 10.03050125,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 8.325582347623143}
done in step count: 23
reward sum = 0.7412880538962368
running average episode reward sum: 0.6004943485427061
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.19103837, 9.05768405, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.9614858336862923}
episode index:1240
target Thresh 9.801150117131067
target distance 8.0
model initialize at round 1240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([10.48161469,  8.09230197,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 7.665393948403627}
done in step count: 22
reward sum = 0.7564244528940796
running average episode reward sum: 0.6006199972972197
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.61906637, 4.72878114, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9562244089131589}
episode index:1241
target Thresh 9.805748392407885
target distance 1.0
model initialize at round 1241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.86740305,  3.00494245,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.175463780705617}
done in step count: 29
reward sum = 0.7033338342634211
running average episode reward sum: 0.6007026976490444
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.3861943 ,  5.47933293,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.7787923308237635}
episode index:1242
target Thresh 9.81034436912175
target distance 5.0
model initialize at round 1242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 8.4722923, 10.0214355,  0.       ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 5.527749257393248}
done in step count: 5
reward sum = 0.938480444233184
running average episode reward sum: 0.600974441612507
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.7522695,  9.8996781,  0.       ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7589294361922909}
episode index:1243
target Thresh 9.814938048421665
target distance 5.0
model initialize at round 1243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([12.        ,  9.92313588,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 3.1874184444698286}
done in step count: 20
reward sum = 0.7595299923932554
running average episode reward sum: 0.6011018978430381
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.78003261, 10.4139035 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6260149862337636}
episode index:1244
target Thresh 9.819529431456042
target distance 6.0
model initialize at round 1244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([6.        , 9.27327108, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 4.197763601362329}
done in step count: 12
reward sum = 0.8503258792981674
running average episode reward sum: 0.6013020777478213
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.14156005, 7.26109189, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.1326536708639363}
episode index:1245
target Thresh 9.824118519372728
target distance 1.0
model initialize at round 1245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.21527839, 3.06806259, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.2257815313583559}
done in step count: 0
reward sum = 0.999534684635157
running average episode reward sum: 0.6016216865815992
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.21527839, 3.06806259, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.2257815313583559}
episode index:1246
target Thresh 9.828705313318999
target distance 8.0
model initialize at round 1246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([10., 11.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 6.0000000000000195}
done in step count: 3
reward sum = 0.9493100953461232
running average episode reward sum: 0.6019005064763583
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.06877575, 11.23827874,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.2480057726972486}
episode index:1247
target Thresh 9.833289814441546
target distance 1.0
model initialize at round 1247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.08311749, 5.89873028, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.210708840325991}
done in step count: 3
reward sum = 0.9638854489831772
running average episode reward sum: 0.6021905585136234
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.60784577, 6.31260896, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9175962773832516}
episode index:1248
target Thresh 9.8378720238865
target distance 9.0
model initialize at round 1248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([8.97380116, 8.13224972, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 8.701003238415892}
done in step count: 14
reward sum = 0.8335117783520324
running average episode reward sum: 0.602375763653606
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.2591543 ,  3.45253142,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5214840746428336}
episode index:1249
target Thresh 9.842451942799416
target distance 4.0
model initialize at round 1249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([14.37459123,  7.12339711,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 3.7300841400500904}
done in step count: 11
reward sum = 0.8701897659832262
running average episode reward sum: 0.6025900148554697
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.46368703,  9.01563822,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.0881055927654688}
episode index:1250
target Thresh 9.847029572325265
target distance 8.0
model initialize at round 1250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2., 9., 0.]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082762530298236}
done in step count: 13
reward sum = 0.8350470772142509
running average episode reward sum: 0.6027758318517596
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.75980306, 2.14435124, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1443056810921681}
episode index:1251
target Thresh 9.851604913608465
target distance 6.0
model initialize at round 1251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([10.58833945, 11.47968459,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.432878356586489}
done in step count: 4
reward sum = 0.9464881652451848
running average episode reward sum: 0.6030503624694861
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.56949405, 11.90839551,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0052451274782406}
episode index:1252
target Thresh 9.856177967792842
target distance 5.0
model initialize at round 1252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([5.        , 9.08208108, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.179531648560539}
done in step count: 36
reward sum = 0.6335888079268533
running average episode reward sum: 0.6030747347324209
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.24051398, 4.05849447, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.24752489822257934}
episode index:1253
target Thresh 9.86074873602166
target distance 4.0
model initialize at round 1253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([5.        , 8.00549322, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.167489466045525}
done in step count: 17
reward sum = 0.8108358781246596
running average episode reward sum: 0.6032404134751579
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.54673508, 4.45554296, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.70843669907174}
episode index:1254
target Thresh 9.865317219437621
target distance 1.0
model initialize at round 1254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.73730212, 3.        , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0339294820209166}
done in step count: 0
reward sum = 0.9965922891459552
running average episode reward sum: 0.603553841264537
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.73730212, 3.        , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0339294820209166}
episode index:1255
target Thresh 9.869883419182838
target distance 3.0
model initialize at round 1255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.4142135623730951}
done in step count: 1
reward sum = 0.9808154140876977
running average episode reward sum: 0.603854208758823
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.53759212,  6.00000002,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.101735456836317}
episode index:1256
target Thresh 9.874447336398859
target distance 6.0
model initialize at round 1256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([8.70920771, 8.11341653, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 7.357955670805561}
done in step count: 7
reward sum = 0.9085687750366724
running average episode reward sum: 0.6040966228926956
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.78080283,  2.49911935,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5467438380696126}
episode index:1257
target Thresh 9.87900897222667
target distance 3.0
model initialize at round 1257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.        ,  7.79613495,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 2.4201283363092974}
done in step count: 30
reward sum = 0.6856616253449496
running average episode reward sum: 0.6041614599375703
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.74495711, 10.10836726,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.2771107033799081}
episode index:1258
target Thresh 9.883568327806675
target distance 5.0
model initialize at round 1258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([12.00494245, 11.86740305,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 3.1276296443239566}
done in step count: 2
reward sum = 0.9681365033855156
running average episode reward sum: 0.6044505584629459
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.00488004, 11.54281407,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.1335390863281485}
episode index:1259
target Thresh 9.888125404278714
target distance 6.0
model initialize at round 1259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 6.        , 10.59325218,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.043754215015549}
done in step count: 29
reward sum = 0.7015841960026845
running average episode reward sum: 0.6045276486514696
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.64960706, 10.06790106,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.6531461414950518}
episode index:1260
target Thresh 9.892680202782058
target distance 5.0
model initialize at round 1260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([12.        ,  9.57011759,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 4.092155865372601}
done in step count: 2
reward sum = 0.9689907780954389
running average episode reward sum: 0.6048166757168494
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.70051217,  6.61747062,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9338025864912898}
episode index:1261
target Thresh 9.897232724455403
target distance 3.0
model initialize at round 1261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.        , 10.81438166,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.017081200546037}
done in step count: 20
reward sum = 0.7839757025693659
running average episode reward sum: 0.6049586400804409
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.77642494, 10.69349538,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8347339485102296}
episode index:1262
target Thresh 9.901782970436884
target distance 8.0
model initialize at round 1262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([10.60810864,  9.5086385 ,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 8.378177678087937}
done in step count: 9
reward sum = 0.8936051154288005
running average episode reward sum: 0.6051871804409701
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.64840593, 6.16820123, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.3897563942283804}
episode index:1263
target Thresh 9.906330941864057
target distance 9.0
model initialize at round 1263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([2.80100846, 7.16488218, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 8.401852154575023}
done in step count: 4
reward sum = 0.9476558070082958
running average episode reward sum: 0.6054581208100899
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.14666353,  8.147142  ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.2064617256490557}
episode index:1264
target Thresh 9.910876639873923
target distance 3.0
model initialize at round 1264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.89063429,  5.3017817 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.577296750217843}
done in step count: 1
reward sum = 0.97924834594952
running average episode reward sum: 0.6057536071540736
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.9119822 ,  3.45285777,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.063520638487707}
episode index:1265
target Thresh 9.915420065602898
target distance 5.0
model initialize at round 1265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([5.        , 9.20212436, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.0068013329422256}
done in step count: 2
reward sum = 0.9693526242882549
running average episode reward sum: 0.6060408101691874
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.99999999, 9.36396867, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.064177225312111}
episode index:1266
target Thresh 9.919961220186844
target distance 8.0
model initialize at round 1266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([ 3.97380116, 11.86775028,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 6.6737593664686665}
done in step count: 5
reward sum = 0.9340944271844346
running average episode reward sum: 0.6062997317295783
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.22065738,  9.61862706,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.6568021885924822}
episode index:1267
target Thresh 9.924500104761046
target distance 4.0
model initialize at round 1267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 8.48493433, 11.31468216,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.504780126788575}
done in step count: 1
reward sum = 0.9844361368950572
running average episode reward sum: 0.6065979465601504
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.48969677, 11.89719698,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.02213763415583}
episode index:1268
target Thresh 9.92903672046023
target distance 2.0
model initialize at round 1268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 3.78028178, 10.40654421,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 3.413622641696314}
done in step count: 6
reward sum = 0.9325994654490026
running average episode reward sum: 0.6068548429501337
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.06954157, 7.29893374, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9772994765979796}
episode index:1269
target Thresh 9.933571068418544
target distance 5.0
model initialize at round 1269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.89680642, 6.        , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.0017742945100383}
done in step count: 2
reward sum = 0.972147103486658
running average episode reward sum: 0.6071424746513436
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.80473295, 9.41891623, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9072408293280543}
episode index:1270
target Thresh 9.938103149769578
target distance 1.0
model initialize at round 1270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.29261541, 11.25945926,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.2930047953023633}
done in step count: 5
reward sum = 0.9374327530067736
running average episode reward sum: 0.6074023411173982
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.97115963, 10.57238811,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.5731142299353579}
episode index:1271
target Thresh 9.942632965646355
target distance 4.0
model initialize at round 1271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([13.12503134,  6.19503879,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 3.0221685721115636}
done in step count: 2
reward sum = 0.9718950592993222
running average episode reward sum: 0.607688891996472
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.73001655,  8.07707724,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.9616015163871219}
episode index:1272
target Thresh 9.947160517181324
target distance 1.0
model initialize at round 1272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.66423488,  7.69247282,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.6662614597874525}
done in step count: 58
reward sum = 0.46337000708150894
running average episode reward sum: 0.607575522880278
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.42551921,  9.99214579,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5745344793719265}
episode index:1273
target Thresh 9.951685805506374
target distance 5.0
model initialize at round 1273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.88281453,  9.25602365,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.257329835211098}
done in step count: 6
reward sum = 0.9241979638212435
running average episode reward sum: 0.6078240491290543
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.42786827,  4.47177671,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7415578074878503}
episode index:1274
target Thresh 9.95620883175283
target distance 5.0
model initialize at round 1274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 9.25934303, 11.        ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 4.844979723953682}
done in step count: 6
reward sum = 0.9266777638138253
running average episode reward sum: 0.6080741304739051
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.41911882,  9.10574289,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.9876013221993266}
episode index:1275
target Thresh 9.960729597051445
target distance 6.0
model initialize at round 1275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([12.        ,  8.21760536,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 5.133515777222843}
done in step count: 2
reward sum = 0.965785436762868
running average episode reward sum: 0.6083544684882382
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.00018798,  5.88508333,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8850833478370399}
episode index:1276
target Thresh 9.965248102532415
target distance 2.0
model initialize at round 1276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.        , 5.59290969, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.592909693717993}
done in step count: 10
reward sum = 0.8894093938358428
running average episode reward sum: 0.608574558484595
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.06576982, 4.96546919, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9677067894565382}
episode index:1277
target Thresh 9.969764349325361
target distance 3.0
model initialize at round 1277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.31455398,  7.        ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0483053962727453}
done in step count: 1
reward sum = 0.9839482253075681
running average episode reward sum: 0.6088682780986975
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.80692276,  8.45677745,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9272378272851574}
episode index:1278
target Thresh 9.974278338559348
target distance 4.0
model initialize at round 1278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.        , 10.03336078,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 3.0333607792854558}
done in step count: 3
reward sum = 0.9579678122115602
running average episode reward sum: 0.609141225349763
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.87160586,  7.85928785,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.2239576703170454}
episode index:1279
target Thresh 9.978790071362873
target distance 6.0
model initialize at round 1279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([10.43991303, 10.92470223,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 7.497158139511905}
done in step count: 27
reward sum = 0.6912337100711317
running average episode reward sum: 0.6092053601034516
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.8678177 , 9.21229102, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1720038408132267}
episode index:1280
target Thresh 9.98329954886387
target distance 7.0
model initialize at round 1280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([1.94690794, 9.        , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.405107493564871}
done in step count: 4
reward sum = 0.948823745271303
running average episode reward sum: 0.6094704798420683
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.85488545, 4.1026658 , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.8610281029117681}
episode index:1281
target Thresh 9.987806772189705
target distance 8.0
model initialize at round 1281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 7.915692916515819}
done in step count: 28
reward sum = 0.6714871392412263
running average episode reward sum: 0.6095188547713968
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.06213165, 10.76188626,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.2083326170405366}
episode index:1282
target Thresh 9.992311742467189
target distance 1.0
model initialize at round 1282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.00059613,  6.89816909,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.3436948405774731}
done in step count: 0
reward sum = 0.996992182891195
running average episode reward sum: 0.6098208604831036
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.00059613,  6.89816909,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.3436948405774731}
episode index:1283
target Thresh 9.99681446082256
target distance 2.0
model initialize at round 1283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.        ,  2.55159721,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.5515972077846354}
done in step count: 0
reward sum = 0.9968492013000361
running average episode reward sum: 0.6101222844245497
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.        ,  2.55159721,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.5515972077846354}
episode index:1284
target Thresh 10.001314928381497
target distance 10.0
model initialize at round 1284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.50423312, 7.29151845, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.648238067708773}
done in step count: 31
reward sum = 0.6688972095153177
running average episode reward sum: 0.6101680236658655
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.20609586,  9.2863214 ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.3527824384219357}
episode index:1285
target Thresh 10.005813146269121
target distance 5.0
model initialize at round 1285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.24431992, 7.45972753, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.951701121082513}
done in step count: 3
reward sum = 0.9601625633862921
running average episode reward sum: 0.6104401811617601
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.54631903, 11.90562087,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0129045280494315}
episode index:1286
target Thresh 10.010309115609987
target distance 6.0
model initialize at round 1286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 6., 10.,  0.]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 4.000000000000019}
done in step count: 2
reward sum = 0.9651901935804758
running average episode reward sum: 0.6107158221970506
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.97405056, 10.93868077,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.9390393841213108}
episode index:1287
target Thresh 10.014802837528082
target distance 7.0
model initialize at round 1287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([10.        , 11.35836458,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.1812309674246295}
done in step count: 8
reward sum = 0.8948835177549336
running average episode reward sum: 0.610936449289875
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.08985597,  9.47500368,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.050706088488859}
episode index:1288
target Thresh 10.019294313146842
target distance 5.0
model initialize at round 1288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([8.        , 8.55337679, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 3.0330631856691754}
done in step count: 2
reward sum = 0.973154670619997
running average episode reward sum: 0.6112174564437386
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.96064615, 8.42931254, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.5720427431053475}
episode index:1289
target Thresh 10.023783543589131
target distance 8.0
model initialize at round 1289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([14.        ,  4.28367472,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 7.631757607231909}
done in step count: 8
reward sum = 0.900817323412002
running average episode reward sum: 0.6114419524646442
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.40163887, 9.76539012, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.971523584174546}
episode index:1290
target Thresh 10.028270529977261
target distance 1.0
model initialize at round 1290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.90102005,  2.51204276,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.2024388540107107}
done in step count: 1
reward sum = 0.9882982551106753
running average episode reward sum: 0.6117338628462444
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.11014247,  2.55567166,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9946225900423366}
episode index:1291
target Thresh 10.032755273432977
target distance 5.0
model initialize at round 1291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 4.4593184 , 10.77710366,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.594912215679445}
done in step count: 47
reward sum = 0.5299930048721022
running average episode reward sum: 0.6116705959283077
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.86493279, 10.90463959,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.9146671138197293}
episode index:1292
target Thresh 10.037237775077465
target distance 2.0
model initialize at round 1292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.62597859,  3.88880324,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6357781944865838}
done in step count: 0
reward sum = 0.9979915705313958
running average episode reward sum: 0.6119693747176373
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.62597859,  3.88880324,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6357781944865838}
episode index:1293
target Thresh 10.041718036031346
target distance 5.0
model initialize at round 1293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 9.14443777, 11.85812388,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 4.232344627091436}
done in step count: 3
reward sum = 0.9612127353328626
running average episode reward sum: 0.6122392691230587
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.29158444, 10.06378896,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.1740288349262198}
episode index:1294
target Thresh 10.046196057414694
target distance 10.0
model initialize at round 1294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 5.33164573, 11.00052226,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 9.873153682665473}
done in step count: 12
reward sum = 0.8533961944497365
running average episode reward sum: 0.6124254906870176
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.64229589,  8.64311093,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5052940108062113}
episode index:1295
target Thresh 10.05067184034701
target distance 5.0
model initialize at round 1295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([11.01987984, 11.86205781,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.1405123685672125}
done in step count: 12
reward sum = 0.8576665048889601
running average episode reward sum: 0.6126147198646426
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.47410581, 10.44638568,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7635794080183462}
episode index:1296
target Thresh 10.055145385947236
target distance 8.0
model initialize at round 1296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.3358891 , 4.90192699, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.322778782258135}
done in step count: 11
reward sum = 0.8653980016072574
running average episode reward sum: 0.6128096183085459
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.45192716, 11.5972505 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.748963494860843}
episode index:1297
target Thresh 10.059616695333762
target distance 10.0
model initialize at round 1297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([7.        , 8.51897228, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.319738347296258}
done in step count: 8
reward sum = 0.8890454826885658
running average episode reward sum: 0.6130224348450481
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.15602804,  1.12735581,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.214000228265777}
episode index:1298
target Thresh 10.06408576962442
target distance 9.0
model initialize at round 1298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([3.63078797, 3.53594756, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.189241022249337}
done in step count: 4
reward sum = 0.9417238043118423
running average episode reward sum: 0.613275476699911
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.03917743, 11.67177371,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.6729151384990502}
episode index:1299
target Thresh 10.068552609936468
target distance 1.0
model initialize at round 1299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.54636109, 11.36367708,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.4690560336304426}
done in step count: 3
reward sum = 0.9649711759332991
running average episode reward sum: 0.6135460118531675
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.95617878, 10.0535012 ,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.06915690661041074}
episode index:1300
target Thresh 10.073017217386624
target distance 1.0
model initialize at round 1300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.20634091, 2.6091938 , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8846605208832068}
done in step count: 0
reward sum = 0.9985173111136911
running average episode reward sum: 0.6138419160032524
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.20634091, 2.6091938 , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8846605208832068}
episode index:1301
target Thresh 10.077479593091038
target distance 6.0
model initialize at round 1301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.29794977,  4.54716636,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 4.462790771207256}
done in step count: 24
reward sum = 0.7573994189059563
running average episode reward sum: 0.6139521752220717
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.65785072,  8.23555599,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.8375206096725718}
episode index:1302
target Thresh 10.081939738165307
target distance 4.0
model initialize at round 1302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.68922961, 6.48189557, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.4957366847691866}
done in step count: 25
reward sum = 0.71739637445267
running average episode reward sum: 0.6140315644770452
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.41892851, 2.71986739, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5039596952270798}
episode index:1303
target Thresh 10.086397653724461
target distance 5.0
model initialize at round 1303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([10.98631527,  8.1326958 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 6.497400184000901}
done in step count: 4
reward sum = 0.9404405526433548
running average episode reward sum: 0.6142818781182771
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.12457688,  4.94929147,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.2913248734685505}
episode index:1304
target Thresh 10.09085334088298
target distance 6.0
model initialize at round 1304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.74632585,  5.        ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 4.364590928836091}
done in step count: 2
reward sum = 0.9707121932269607
running average episode reward sum: 0.6145550047965213
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.8701064 ,  8.91671824,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.874082945992096}
episode index:1305
target Thresh 10.095306800754791
target distance 1.0
model initialize at round 1305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.06660438,  8.73251253,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0996337750387748}
done in step count: 29
reward sum = 0.7057057204323651
running average episode reward sum: 0.6146247986063496
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.42705832,  9.0265556 ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.42788316885501604}
episode index:1306
target Thresh 10.099758034453256
target distance 10.0
model initialize at round 1306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.86880333, 4.90649005, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.582611185376088}
done in step count: 30
reward sum = 0.6710629080377624
running average episode reward sum: 0.6146679800213698
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.61562089,  2.09331644,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.3955442505843158}
episode index:1307
target Thresh 10.104207043091181
target distance 10.0
model initialize at round 1307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.25323987,  6.24037403,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 11.09548105024508}
done in step count: 16
reward sum = 0.8132894783791712
running average episode reward sum: 0.6148198313198084
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.08607857, 1.28463148, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.160605227314571}
episode index:1308
target Thresh 10.108653827780822
target distance 6.0
model initialize at round 1308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([4.97169663, 6.78156946, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.970103001917724}
done in step count: 19
reward sum = 0.7900959414250859
running average episode reward sum: 0.6149537320914702
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.67734147, 10.96121437,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.0139238614342203}
episode index:1309
target Thresh 10.113098389633874
target distance 9.0
model initialize at round 1309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([4.4870882 , 4.32950246, 0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 8.846320755148888}
done in step count: 53
reward sum = 0.4737292850843316
running average episode reward sum: 0.6148459271700907
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.92763491,  8.72905451,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.9663943240504553}
episode index:1310
target Thresh 10.117540729761476
target distance 6.0
model initialize at round 1310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([15.30841017, 10.97200036,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.3966659345626935}
done in step count: 5
reward sum = 0.9380722191259769
running average episode reward sum: 0.6150924765918724
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.62992787, 10.50032797,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.6223194202775366}
episode index:1311
target Thresh 10.121980849274216
target distance 7.0
model initialize at round 1311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([13.77463444, 11.87777988,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 6.451977982681744}
done in step count: 5
reward sum = 0.9285431120281673
running average episode reward sum: 0.6153313871371745
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.96104221, 8.50076272, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.082977369596708}
episode index:1312
target Thresh 10.126418749282118
target distance 10.0
model initialize at round 1312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.05504394,  9.29508328,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.13810382980255}
done in step count: 99
reward sum = -0.1470582348518813
running average episode reward sum: 0.6147507400526436
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([1.16353554, 9.78409961, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.350901680442326}
episode index:1313
target Thresh 10.130854430894665
target distance 9.0
model initialize at round 1313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 6.18340242, 11.12443376,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.197657290870804}
done in step count: 14
reward sum = 0.8348378137279703
running average episode reward sum: 0.6149182340204331
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.4369483 ,  5.83956979,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.585461412994511}
episode index:1314
target Thresh 10.135287895220774
target distance 3.0
model initialize at round 1314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.34335208, 4.88241529, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 2.5075824362079016}
done in step count: 14
reward sum = 0.8480346664071565
running average episode reward sum: 0.615095508873959
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.45844608, 3.91414644, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.022661486159681}
episode index:1315
target Thresh 10.139719143368808
target distance 8.0
model initialize at round 1315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([5., 8., 0.]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 6.324555320336782}
done in step count: 31
reward sum = 0.6380068026507315
running average episode reward sum: 0.6151129186716617
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.61763689, 10.32267528,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.5003207810706932}
episode index:1316
target Thresh 10.144148176446583
target distance 10.0
model initialize at round 1316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([13.89988613,  7.85887849,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 8.972743807938636}
done in step count: 6
reward sum = 0.912739320487316
running average episode reward sum: 0.6153389068279378
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.37673395, 9.84618275, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.9262579127090182}
episode index:1317
target Thresh 10.148574995561354
target distance 7.0
model initialize at round 1317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([9.00961673, 8.14600668, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 7.1817577548786735}
done in step count: 5
reward sum = 0.9266588761051573
running average episode reward sum: 0.6155751131779205
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.53746217, 2.51285555, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.6717521590366134}
episode index:1318
target Thresh 10.152999601819829
target distance 1.0
model initialize at round 1318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.75528395,  9.58579832,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.6032530151470352}
done in step count: 3
reward sum = 0.9656047153962574
running average episode reward sum: 0.6158404881606485
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.02394247, 10.17902327,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8213257818492355}
episode index:1319
target Thresh 10.157421996328159
target distance 7.0
model initialize at round 1319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([3.45678532, 3.60122514, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 7.314279868727906}
done in step count: 16
reward sum = 0.816505026829722
running average episode reward sum: 0.6159925067505493
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.91919363, 10.62369037,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.6289032919153534}
episode index:1320
target Thresh 10.161842180191943
target distance 8.0
model initialize at round 1320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 6.708203932499395}
done in step count: 3
reward sum = 0.9512441186636245
running average episode reward sum: 0.6162462929821263
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.05522798, 9.54481706, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.5476091271896418}
episode index:1321
target Thresh 10.166260154516225
target distance 6.0
model initialize at round 1321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 4.49730361, 10.        ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.61240445156632}
done in step count: 2
reward sum = 0.966744531082308
running average episode reward sum: 0.6165114202424139
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.09144748, 10.78614804,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9333811389005222}
episode index:1322
target Thresh 10.1706759204055
target distance 10.0
model initialize at round 1322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 7.89964318, 10.77593744,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 9.948730263431388}
done in step count: 7
reward sum = 0.9033004793324578
running average episode reward sum: 0.6167281920179921
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.34001614,  4.21014253,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0292975820372094}
episode index:1323
target Thresh 10.17508947896371
target distance 7.0
model initialize at round 1323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.        , 9.49876595, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.588955798954813}
done in step count: 8
reward sum = 0.9006539343948123
running average episode reward sum: 0.6169426374427479
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.93676875, 4.58299076, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.5864097716050155}
episode index:1324
target Thresh 10.17950083129424
target distance 4.0
model initialize at round 1324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.86741309, 4.99418645, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.185336006504669}
done in step count: 18
reward sum = 0.7940440913556948
running average episode reward sum: 0.6170762989173991
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.28797849, 7.91388721, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.1585182147105233}
episode index:1325
target Thresh 10.183909978499933
target distance 5.0
model initialize at round 1325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.61602384, 4.        , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.024473126444503}
done in step count: 2
reward sum = 0.9725815522240626
running average episode reward sum: 0.6173444024266801
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.52389699, 6.73428059, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.587430732334154}
episode index:1326
target Thresh 10.188316921683077
target distance 4.0
model initialize at round 1326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.00510037,  4.79484594,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.2051599597111737}
done in step count: 4
reward sum = 0.9518480357303333
running average episode reward sum: 0.6175964775082955
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.83072627,  7.0964903 ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.19484346338661376}
episode index:1327
target Thresh 10.192721661945402
target distance 6.0
model initialize at round 1327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([7.12992108, 9.        , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 5.079016239270893}
done in step count: 56
reward sum = 0.48926871240417863
running average episode reward sum: 0.6174998451550544
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.07528495, 4.79489843, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.2184822133155638}
episode index:1328
target Thresh 10.1971242003881
target distance 8.0
model initialize at round 1328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([5.03428686, 8.97679973, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.181053824450146}
done in step count: 7
reward sum = 0.915827363022427
running average episode reward sum: 0.6177243203377989
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.85639478,  7.27892993,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9006741462450851}
episode index:1329
target Thresh 10.201524538111801
target distance 3.0
model initialize at round 1329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.4142135623730956}
done in step count: 33
reward sum = 0.6287978091350186
running average episode reward sum: 0.6177326462692253
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.80172431, 10.81469179,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.1430155666446813}
episode index:1330
target Thresh 10.205922676216595
target distance 5.0
model initialize at round 1330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.02279246, 10.        ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.145221244329375}
done in step count: 2
reward sum = 0.969728546389225
running average episode reward sum: 0.6179971059988422
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.96815999, 11.54373696,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.1103979715002354}
episode index:1331
target Thresh 10.210318615802008
target distance 1.0
model initialize at round 1331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.97315015, 8.14000554, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 2.0991904608951826}
done in step count: 30
reward sum = 0.6804968007981782
running average episode reward sum: 0.6180440276916345
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.1211665 , 9.30914513, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.7013998624178966}
episode index:1332
target Thresh 10.214712357967027
target distance 5.0
model initialize at round 1332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([12.,  9.,  0.]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 4.242640687119313}
done in step count: 2
reward sum = 0.9622794006099732
running average episode reward sum: 0.6183022687815958
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.,  5.,  0.]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.414213562373014}
episode index:1333
target Thresh 10.219103903810092
target distance 5.0
model initialize at round 1333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.1276296443239566}
done in step count: 13
reward sum = 0.8551608274540473
running average episode reward sum: 0.6184798239230295
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.87199228, 11.77241221,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.1648996332487944}
episode index:1334
target Thresh 10.223493254429089
target distance 8.0
model initialize at round 1334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 8.        , 10.08002654,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 7.255798821914254}
done in step count: 6
reward sum = 0.9249001202049351
running average episode reward sum: 0.6187093522348511
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.3845084, 6.1468471, 0.       ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.4115954094292267}
episode index:1335
target Thresh 10.227880410921351
target distance 7.0
model initialize at round 1335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.4634254 ,  5.86590481,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.497196177489543}
done in step count: 4
reward sum = 0.9423422738692682
running average episode reward sum: 0.6189515924456553
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.90151859, 11.85361836,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.2415313460681834}
episode index:1336
target Thresh 10.23226537438367
target distance 1.0
model initialize at round 1336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([ 9.37737614, 11.68852858,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 2.7148846534542033}
done in step count: 6
reward sum = 0.9298505917434603
running average episode reward sum: 0.619184127224487
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.6001458 , 9.64910149, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.7623753160887675}
episode index:1337
target Thresh 10.23664814591229
target distance 2.0
model initialize at round 1337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([ 9.35718203, 10.04126167,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.2236996442826937}
done in step count: 1
reward sum = 0.9867756030907666
running average episode reward sum: 0.6194588592692302
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.9892568 , 9.50411391, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.50422837407477}
episode index:1338
target Thresh 10.241028726602899
target distance 10.0
model initialize at round 1338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([13.02561867, 11.86773409,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 8.522584929953588}
done in step count: 35
reward sum = 0.6342552240684431
running average episode reward sum: 0.6194699095790129
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.10759441, 8.27076548, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.7371292563252916}
episode index:1339
target Thresh 10.245407117550647
target distance 3.0
model initialize at round 1339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([4.92579076, 7.00003699, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.169964056285123}
done in step count: 1
reward sum = 0.9836276286619486
running average episode reward sum: 0.6197416690708659
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.40797546, 5.96431374, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.4095332494436782}
episode index:1340
target Thresh 10.249783319850124
target distance 4.0
model initialize at round 1340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([12.        ,  9.67138338,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.3371079047125822}
done in step count: 1
reward sum = 0.9799742102575727
running average episode reward sum: 0.6200102988554943
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.       ,  7.7532357,  0.       ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.753235697746411}
episode index:1341
target Thresh 10.254157334595387
target distance 9.0
model initialize at round 1341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([3.99503899, 6.        , 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 9.43818842958549}
done in step count: 27
reward sum = 0.7278913176816446
running average episode reward sum: 0.6200906870960503
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.49422262, 11.89472501,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0221491270935263}
episode index:1342
target Thresh 10.258529162879938
target distance 4.0
model initialize at round 1342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 9.        , 10.79704311,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.01027149838257}
done in step count: 1
reward sum = 0.983942485222328
running average episode reward sum: 0.6203616117409694
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.        , 11.07319123,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.07319122552885204}
episode index:1343
target Thresh 10.262898805796734
target distance 10.0
model initialize at round 1343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([7.        , 8.60076141, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 9.765681154521774}
done in step count: 61
reward sum = 0.4437157924832933
running average episode reward sum: 0.620230178839736
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.89223563,  2.80287082,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9137528829799363}
episode index:1344
target Thresh 10.267266264438188
target distance 2.0
model initialize at round 1344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.        , 4.06025648, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.9397435188293377}
done in step count: 51
reward sum = 0.5249968858713011
running average episode reward sum: 0.6201593734174545
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.51805329, 6.04062416, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5196436649317776}
episode index:1345
target Thresh 10.271631539896159
target distance 8.0
model initialize at round 1345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.088354702269999}
done in step count: 3
reward sum = 0.9522091936520014
running average episode reward sum: 0.6204060671917744
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.92546074, 11.21494093,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.2274987984308829}
episode index:1346
target Thresh 10.275994633261973
target distance 1.0
model initialize at round 1346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([5.        , 9.02899611, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.0004202988708868}
done in step count: 0
reward sum = 0.9962928635099486
running average episode reward sum: 0.6206851219774597
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([5.        , 9.02899611, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.0004202988708868}
episode index:1347
target Thresh 10.2803555456264
target distance 2.0
model initialize at round 1347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7., 9., 0.]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 2.763362727283444e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.6209620617981035
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7., 9., 0.]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 2.763362727283444e-14}
episode index:1348
target Thresh 10.284714278079665
target distance 6.0
model initialize at round 1348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.36867386,  9.        ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.049515118350706}
done in step count: 3
reward sum = 0.9575236541361831
running average episode reward sum: 0.6212115514884949
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.23978293,  4.40398574,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6424397659312253}
episode index:1349
target Thresh 10.289070831711456
target distance 10.0
model initialize at round 1349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([7.97727227, 9.27419865, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 9.601110957039035}
done in step count: 41
reward sum = 0.6032545679685112
running average episode reward sum: 0.6211982500192209
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.38640199,  4.26186507,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6671400423940599}
episode index:1350
target Thresh 10.293425207610907
target distance 5.0
model initialize at round 1350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([11.57653284,  8.5023725 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.8976260108694705}
done in step count: 5
reward sum = 0.9415899242001721
running average episode reward sum: 0.6214354015175043
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.02781664,  4.93321059,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9744748940613193}
episode index:1351
target Thresh 10.297777406866619
target distance 6.0
model initialize at round 1351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.19390029, 5.2314347 , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.836218744020601}
done in step count: 3
reward sum = 0.9627059361633968
running average episode reward sum: 0.6216878205520057
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.19014484, 9.13123322, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1876957149614968}
episode index:1352
target Thresh 10.302127430566637
target distance 1.0
model initialize at round 1352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.        ,  3.62366515,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.068469894909857}
done in step count: 0
reward sum = 0.9967082813583374
running average episode reward sum: 0.6219649975370806
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.        ,  3.62366515,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.068469894909857}
episode index:1353
target Thresh 10.306475279798464
target distance 9.0
model initialize at round 1353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([3.47374439, 8.4917587 , 0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 8.541390045009473}
done in step count: 7
reward sum = 0.9139806175604422
running average episode reward sum: 0.622180666384956
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.05651138,  8.7622883 ,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.24433662442266416}
episode index:1354
target Thresh 10.310820955649067
target distance 7.0
model initialize at round 1354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([12.,  9.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 5.385164807134529}
done in step count: 3
reward sum = 0.9515765255891205
running average episode reward sum: 0.6224237629600143
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.91712699, 11.06131887,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.1030918935417651}
episode index:1355
target Thresh 10.315164459204865
target distance 6.0
model initialize at round 1355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([10.24525321,  9.70346045,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 7.432338186755814}
done in step count: 19
reward sum = 0.7823092484826725
running average episode reward sum: 0.6225416726101047
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.85092456,  5.97451619,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9858525657600712}
episode index:1356
target Thresh 10.31950579155173
target distance 7.0
model initialize at round 1356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4.       , 9.3299526, 0.       ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 5.044696573773193}
done in step count: 3
reward sum = 0.9575731098850507
running average episode reward sum: 0.6227885638682292
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.47992349, 10.09146082,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.48856077966257466}
episode index:1357
target Thresh 10.323844953774998
target distance 6.0
model initialize at round 1357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([5., 9., 0.]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.472135954999604}
done in step count: 2
reward sum = 0.9652500814642567
running average episode reward sum: 0.6230407446617462
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.        , 10.61886853,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.38113147020340143}
episode index:1358
target Thresh 10.328181946959461
target distance 8.0
model initialize at round 1358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([10.95434438,  8.13247235,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 7.6272726986086985}
done in step count: 47
reward sum = 0.542281979020352
running average episode reward sum: 0.6229813195214655
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.24207465, 4.97151634, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.7584603799550534}
episode index:1359
target Thresh 10.332516772189365
target distance 6.0
model initialize at round 1359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([6.        , 9.06467116, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 6.453750378398974}
done in step count: 14
reward sum = 0.8258689832512578
running average episode reward sum: 0.6231305016271491
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.10661539, 4.41992009, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9871519326529931}
episode index:1360
target Thresh 10.336849430548419
target distance 6.0
model initialize at round 1360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([ 3.86306763, 11.        ,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 7.206649963571453}
done in step count: 21
reward sum = 0.7550583133559357
running average episode reward sum: 0.6232274360957227
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.93613826, 10.01596064,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.06582600738076314}
episode index:1361
target Thresh 10.341179923119784
target distance 3.0
model initialize at round 1361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.05914005,  8.03945378,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.9623651008946615}
done in step count: 0
reward sum = 0.9940272893665959
running average episode reward sum: 0.6234996826840273
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.05914005,  8.03945378,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.9623651008946615}
episode index:1362
target Thresh 10.345508250986082
target distance 4.0
model initialize at round 1362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([11.        ,  9.37637234,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 2.035105927541824}
done in step count: 1
reward sum = 0.9790828753375131
running average episode reward sum: 0.6237605654372581
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.03350067, 8.13208241, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.868563893261189}
episode index:1363
target Thresh 10.349834415229399
target distance 3.0
model initialize at round 1363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.11647344,  6.26670127,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.872549586027313}
done in step count: 34
reward sum = 0.6714781533935092
running average episode reward sum: 0.623795549006141
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.48025338,  8.6197943 ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6125354563144539}
episode index:1364
target Thresh 10.354158416931275
target distance 4.0
model initialize at round 1364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.76374769, 10.63192654,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.788149813069979}
done in step count: 1
reward sum = 0.9837698990390613
running average episode reward sum: 0.6240592664786926
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.76374769, 11.02798888,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7642603709817509}
episode index:1365
target Thresh 10.358480257172712
target distance 10.0
model initialize at round 1365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.02619884, 11.86775028,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.072970849349376}
done in step count: 5
reward sum = 0.9292129125024815
running average episode reward sum: 0.6242826586060893
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.19104127, 10.48997328,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.5446320036356123}
episode index:1366
target Thresh 10.362799937034167
target distance 10.0
model initialize at round 1366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 6.        , 10.76011267,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 8.839595427053624}
done in step count: 7
reward sum = 0.9159269892474785
running average episode reward sum: 0.6244960048611307
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.29853493,  7.64398282,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9522432034039005}
episode index:1367
target Thresh 10.36711745759556
target distance 8.0
model initialize at round 1367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3., 9., 0.]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.082762530298236}
done in step count: 17
reward sum = 0.7929021134101231
running average episode reward sum: 0.6246191087416489
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.13165623, 3.26957694, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.3000084788926797}
episode index:1368
target Thresh 10.371432819936274
target distance 6.0
model initialize at round 1368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([14.42655373,  7.        ,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.347371119615982}
done in step count: 3
reward sum = 0.9554158881839868
running average episode reward sum: 0.6248607426199853
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.22545528, 9.0220108 , 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.2475505544518106}
episode index:1369
target Thresh 10.37574602513515
target distance 6.0
model initialize at round 1369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([12.        , 11.29267675,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 4.2036904229301575}
done in step count: 2
reward sum = 0.9701274642076168
running average episode reward sum: 0.6251127621247938
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.        , 9.64995372, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.3500462770461308}
episode index:1370
target Thresh 10.380057074270482
target distance 10.0
model initialize at round 1370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([10.        ,  9.33742458,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 10.206025193975826}
done in step count: 16
reward sum = 0.8003370670344301
running average episode reward sum: 0.6252405697870183
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.84106514, 3.96035874, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.2765889993067203}
episode index:1371
target Thresh 10.384365968420042
target distance 8.0
model initialize at round 1371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.        ,  9.09483826,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 6.176330094410075}
done in step count: 10
reward sum = 0.8811762964091167
running average episode reward sum: 0.6254271118618157
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.35107366,  3.1282315 ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.3737593213113866}
episode index:1372
target Thresh 10.388672708661051
target distance 2.0
model initialize at round 1372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([ 5.59643686, 10.53845584,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.5905060808266487}
done in step count: 1
reward sum = 0.9854347883793816
running average episode reward sum: 0.6256893170158707
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.33354068, 8.86048615, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.36154321774567083}
episode index:1373
target Thresh 10.392977296070185
target distance 8.0
model initialize at round 1373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([9.99505755, 8.13259695, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 6.372386023266191}
done in step count: 5
reward sum = 0.9260810922336231
running average episode reward sum: 0.6259079427620263
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.31251862,  6.50150425,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.5909098101983743}
episode index:1374
target Thresh 10.397279731723604
target distance 2.0
model initialize at round 1374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.6340965 , 7.21199429, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.814173194708082}
done in step count: 15
reward sum = 0.8399228471098281
running average episode reward sum: 0.6260635899651883
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.60705736, 7.88090835, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6186286887306797}
episode index:1375
target Thresh 10.401580016696911
target distance 1.0
model initialize at round 1375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.68946588, 6.25930488, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.4045620605044578}
done in step count: 0
reward sum = 0.9984540909492593
running average episode reward sum: 0.6263342225967174
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.68946588, 6.25930488, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.4045620605044578}
episode index:1376
target Thresh 10.405878152065174
target distance 7.0
model initialize at round 1376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([6.        , 9.39500272, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 8.407500536386221}
done in step count: 21
reward sum = 0.7639814095702633
running average episode reward sum: 0.6264341842430309
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.27136137, 1.17889254, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.0977849121872176}
episode index:1377
target Thresh 10.410174138902933
target distance 5.0
model initialize at round 1377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.30461836, 7.8153379 , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.6078236686953655}
done in step count: 5
reward sum = 0.9438282725814802
running average episode reward sum: 0.6266645137701271
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.61359656, 10.29584417,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9339893860077291}
episode index:1378
target Thresh 10.41446797828418
target distance 9.0
model initialize at round 1378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([11.        , 10.47714126,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.993722024894756}
done in step count: 7
reward sum = 0.9039102277325467
running average episode reward sum: 0.6268655621486351
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.83513527, 2.05604836, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8370139376427288}
episode index:1379
target Thresh 10.418759671282379
target distance 2.0
model initialize at round 1379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([3.38055551, 2.97622561, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.718734275546365}
done in step count: 1
reward sum = 0.9830574542102742
running average episode reward sum: 0.6271236722153465
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.38055556, 4.97622561, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.1561694799907276}
episode index:1380
target Thresh 10.42304921897045
target distance 10.0
model initialize at round 1380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([11.        , 10.41889215,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 9.13928923880461}
done in step count: 4
reward sum = 0.9397836104396562
running average episode reward sum: 0.6273500733291947
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.93766403, 6.09588797, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.11436903148615883}
episode index:1381
target Thresh 10.42733662242078
target distance 3.0
model initialize at round 1381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([5.        , 8.37944829, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.5810412989456983}
done in step count: 16
reward sum = 0.8262745524352483
running average episode reward sum: 0.6274940128943944
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.21451063, 6.17018602, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.27382127303952886}
episode index:1382
target Thresh 10.431621882705223
target distance 10.0
model initialize at round 1382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 5.45762241, 10.        ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 8.600710139836872}
done in step count: 12
reward sum = 0.8698598010829603
running average episode reward sum: 0.6276692593066783
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.24844437,  9.86275451,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8978139782844863}
episode index:1383
target Thresh 10.435905000895092
target distance 3.0
model initialize at round 1383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([11.92395624, 11.78210945,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.3302501137960447}
done in step count: 1
reward sum = 0.9833449311072742
running average episode reward sum: 0.6279262503990197
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.21817908, 11.19766229,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.29440192362317547}
episode index:1384
target Thresh 10.440185978061166
target distance 5.0
model initialize at round 1384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([4.27390337, 6.02947247, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 4.765273329161668}
done in step count: 3
reward sum = 0.9638464892289601
running average episode reward sum: 0.6281687920877056
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.34854993, 8.18296287, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.044957825334493}
episode index:1385
target Thresh 10.444464815273692
target distance 4.0
model initialize at round 1385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.15750834, 3.78164303, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.285928643681368}
done in step count: 9
reward sum = 0.8984934638155683
running average episode reward sum: 0.6283638315333967
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.15105516, 9.14114613, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8605983831392899}
episode index:1386
target Thresh 10.448741513602373
target distance 1.0
model initialize at round 1386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 2.92862153, 10.2171253 ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.3269305279752737}
done in step count: 2
reward sum = 0.9769988060093594
running average episode reward sum: 0.6286151905633
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.11124716, 11.83830055,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.2217321434578172}
episode index:1387
target Thresh 10.45301607411639
target distance 1.0
model initialize at round 1387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.22996354, 10.89214754,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.178508966082328}
done in step count: 0
reward sum = 0.9972751708385659
running average episode reward sum: 0.628880795736409
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.22996354, 10.89214754,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.178508966082328}
episode index:1388
target Thresh 10.457288497884381
target distance 6.0
model initialize at round 1388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 4.007136883679184}
done in step count: 2
reward sum = 0.9688869310513999
running average episode reward sum: 0.6291255805710491
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.84835185,  8.2802087 ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.7355927419369254}
episode index:1389
target Thresh 10.461558785974452
target distance 7.0
model initialize at round 1389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 5.385164807134515}
done in step count: 3
reward sum = 0.9532010649118876
running average episode reward sum: 0.6293587284015101
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.68326783,  9.30311924,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.7474865918278284}
episode index:1390
target Thresh 10.465826939454175
target distance 5.0
model initialize at round 1390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([13.        ,  9.14159107,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 3.120395149260515}
done in step count: 2
reward sum = 0.9660343606002564
running average episode reward sum: 0.6296007669580871
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86911695,  9.92684327,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8721904511932339}
episode index:1391
target Thresh 10.470092959390588
target distance 4.0
model initialize at round 1391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([12.        ,  9.93894684,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.000931654995181}
done in step count: 1
reward sum = 0.9832951403186557
running average episode reward sum: 0.6298548577435472
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.        ,  9.62401295,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.37598705291748047}
episode index:1392
target Thresh 10.474356846850197
target distance 8.0
model initialize at round 1392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 9., 11.,  0.]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 6.082762530298234}
done in step count: 21
reward sum = 0.7514379829939533
running average episode reward sum: 0.6299421392404966
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.51474983,  9.48672295,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7269255176856588}
episode index:1393
target Thresh 10.478618602898974
target distance 8.0
model initialize at round 1393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 7.99505752, 11.85296361,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 6.065218964323948}
done in step count: 27
reward sum = 0.7151166292720316
running average episode reward sum: 0.6300032400224417
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.84461198, 11.85227055,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.1998893627616936}
episode index:1394
target Thresh 10.482878228602354
target distance 5.0
model initialize at round 1394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.        , 5.71965313, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 3.7196531295776394}
done in step count: 2
reward sum = 0.9716008683644901
running average episode reward sum: 0.6302481128742997
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.50635445, 2.9098103 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.0412250576420279}
episode index:1395
target Thresh 10.48713572502525
target distance 9.0
model initialize at round 1395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 3.98159749, 11.86315561,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.071280742413106}
done in step count: 4
reward sum = 0.9417964422489579
running average episode reward sum: 0.6304712850300122
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.92700217, 10.94303939,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9287505222518518}
episode index:1396
target Thresh 10.491391093232034
target distance 6.0
model initialize at round 1396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([5.39760911, 8.97919273, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.526381252375894}
done in step count: 4
reward sum = 0.9513249228039384
running average episode reward sum: 0.6307009583569798
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.39996201, 4.39128632, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.559530692157162}
episode index:1397
target Thresh 10.495644334286546
target distance 8.0
model initialize at round 1397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([14.71036458,  7.        ,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 7.35044167370859}
done in step count: 4
reward sum = 0.9418278224339363
running average episode reward sum: 0.630923509761899
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.25396562, 10.79952596,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.0935305503427848}
episode index:1398
target Thresh 10.499895449252094
target distance 4.0
model initialize at round 1398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.15863562, 8.        , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.006281450534729}
done in step count: 1
reward sum = 0.9815544436708552
running average episode reward sum: 0.6311741394501827
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.22451597, 6.25740671, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8170885441234487}
episode index:1399
target Thresh 10.504144439191464
target distance 9.0
model initialize at round 1399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.10030756,  2.62519028,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 10.36104194463134}
done in step count: 33
reward sum = 0.586563015382892
running average episode reward sum: 0.6311422743615632
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.41973158, 11.89367029,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0655318040660948}
episode index:1400
target Thresh 10.508391305166896
target distance 7.0
model initialize at round 1400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.61843729,  9.19564772,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.3761948174647936}
done in step count: 35
reward sum = 0.6525214019892208
running average episode reward sum: 0.6311575342670791
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.65654968,  4.82885784,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0573848847645027}
episode index:1401
target Thresh 10.512636048240113
target distance 3.0
model initialize at round 1401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.34062666,  5.        ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.1978201891131042}
done in step count: 1
reward sum = 0.9811431449864042
running average episode reward sum: 0.6314071673703026
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.87124634,  6.88784588,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.243921414642353}
episode index:1402
target Thresh 10.516878669472298
target distance 3.0
model initialize at round 1402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 5.36237025, 10.27312505,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.378106495657835}
done in step count: 20
reward sum = 0.7693446163960936
running average episode reward sum: 0.6315054834423095
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.54907045, 9.66545875, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.5614760039331967}
episode index:1403
target Thresh 10.521119169924106
target distance 7.0
model initialize at round 1403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([12.78877415,  8.65802073,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 6.0747607694023555}
done in step count: 3
reward sum = 0.957527429440619
running average episode reward sum: 0.6317376928055561
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.78370102,  2.65802073,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8550655546005878}
episode index:1404
target Thresh 10.52535755065566
target distance 8.0
model initialize at round 1404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.98143065, 3.13746452, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 6.932358854606255}
done in step count: 3
reward sum = 0.9567998089023411
running average episode reward sum: 0.6319690537422799
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.19114686, 9.12503979, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.1915530880037826}
episode index:1405
target Thresh 10.529593812726558
target distance 5.0
model initialize at round 1405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([7.        , 9.43187392, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 4.558262671913943}
done in step count: 46
reward sum = 0.5531068596127765
running average episode reward sum: 0.6319129639882759
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.10323812, 6.33486395, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9572438246631094}
episode index:1406
target Thresh 10.533827957195866
target distance 10.0
model initialize at round 1406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 5.72132099, 10.        ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 8.805482735458822}
done in step count: 15
reward sum = 0.8395956313396411
running average episode reward sum: 0.632060570717026
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.59905966,  6.96749365,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.4022559113604811}
episode index:1407
target Thresh 10.538059985122121
target distance 8.0
model initialize at round 1407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([8.        , 9.73731613, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 7.06877159536622}
done in step count: 3
reward sum = 0.9538146473391385
running average episode reward sum: 0.6322890892373543
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.9999982 , 6.47596898, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.47596897848350134}
episode index:1408
target Thresh 10.542289897563325
target distance 1.0
model initialize at round 1408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.26411152, 10.62393707,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.776156565733269}
done in step count: 1
reward sum = 0.9864567330412161
running average episode reward sum: 0.6325404502336665
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.18154651, 11.32637125,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8811267281028765}
episode index:1409
target Thresh 10.546517695576963
target distance 10.0
model initialize at round 1409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([12.20962025, 11.87361256,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.09441376173673}
done in step count: 5
reward sum = 0.9282037778354935
running average episode reward sum: 0.6327501405369301
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.78494969, 6.16047138, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.2683238753525058}
episode index:1410
target Thresh 10.550743380219977
target distance 3.0
model initialize at round 1410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.        , 8.89878154, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.344547601636666}
done in step count: 38
reward sum = 0.6462850050762537
running average episode reward sum: 0.6327597329285243
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.04538392, 7.95427612, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9557104836885232}
episode index:1411
target Thresh 10.554966952548796
target distance 8.0
model initialize at round 1411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([8.        , 9.31720006, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 8.017020427042178}
done in step count: 3
reward sum = 0.9475327445368518
running average episode reward sum: 0.6329826599905698
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.79767532, 3.89807592, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.22654755159539988}
episode index:1412
target Thresh 10.559188413619305
target distance 6.0
model initialize at round 1412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13259695, 6.00494245, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 4.097798445291378}
done in step count: 4
reward sum = 0.946141237334384
running average episode reward sum: 0.6332042867261279
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.0761484 , 1.14795907, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8554369210153837}
episode index:1413
target Thresh 10.563407764486875
target distance 2.0
model initialize at round 1413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.58932447,  9.4748522 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.07751804742351}
done in step count: 9
reward sum = 0.9037466316168083
running average episode reward sum: 0.6333956179459941
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.37919675, 10.70539919,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.48018726945824675}
episode index:1414
target Thresh 10.567625006206342
target distance 1.0
model initialize at round 1414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.89898374,  6.64130221,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 2.790098404677478}
done in step count: 4
reward sum = 0.9513256670794321
running average episode reward sum: 0.6336203034930848
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.86310327,  4.132686  ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8732427134400546}
episode index:1415
target Thresh 10.57184013983202
target distance 5.0
model initialize at round 1415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.80402529,  9.        ,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.105874542027325}
done in step count: 2
reward sum = 0.9710484094919747
running average episode reward sum: 0.6338586001781124
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.86285174,  5.21904206,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.163790540910588}
episode index:1416
target Thresh 10.576053166417683
target distance 9.0
model initialize at round 1416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([4.33215427, 9.15094316, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 7.273968867686254}
done in step count: 4
reward sum = 0.9443450000388879
running average episode reward sum: 0.6340777154920578
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.77207655, 1.65302277, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.4151413026869007}
episode index:1417
target Thresh 10.580264087016598
target distance 7.0
model initialize at round 1417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 5.3851648071345135}
done in step count: 37
reward sum = 0.6309553490836608
running average episode reward sum: 0.6340755135411351
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.78659646,  2.76903403,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.3144619993253751}
episode index:1418
target Thresh 10.58447290268149
target distance 2.0
model initialize at round 1418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.        ,  6.74729961,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.2527003884315304}
done in step count: 0
reward sum = 0.9969521071003895
running average episode reward sum: 0.6343312405274347
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.        ,  6.74729961,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.2527003884315304}
episode index:1419
target Thresh 10.588679614464564
target distance 6.0
model initialize at round 1419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 7.        , 10.31630826,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 4.2110173871939285}
done in step count: 2
reward sum = 0.9692842152979247
running average episode reward sum: 0.6345671229040336
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.        ,  9.27007467,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.27007466554645454}
episode index:1420
target Thresh 10.592884223417498
target distance 7.0
model initialize at round 1420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.18278164, 4.        , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.003339797247472}
done in step count: 3
reward sum = 0.957976670963468
running average episode reward sum: 0.6347947158301838
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 3.16851033, 10.        ,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0140984812712819}
episode index:1421
target Thresh 10.597086730591442
target distance 9.0
model initialize at round 1421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 8.       , 11.4025172,  0.       ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 9.486423271713232}
done in step count: 39
reward sum = 0.6110104916573299
running average episode reward sum: 0.634777989934141
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.65479546,  5.44288246,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.7905074119352492}
episode index:1422
target Thresh 10.601287137037028
target distance 2.0
model initialize at round 1422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.40934885,  1.62804532,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.4576057257896822}
done in step count: 26
reward sum = 0.6987684716507162
running average episode reward sum: 0.6348229586493319
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.7020081 ,  1.95312147,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7035715769696416}
episode index:1423
target Thresh 10.605485443804353
target distance 9.0
model initialize at round 1423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.22827864,  3.47625911,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 8.187091052507528}
done in step count: 5
reward sum = 0.9329809110771284
running average episode reward sum: 0.635032339233902
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.20616825, 11.55294859,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5901334545917345}
episode index:1424
target Thresh 10.609681651942994
target distance 2.0
model initialize at round 1424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.37385893, 9.52554911, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.6449591998862292}
done in step count: 0
reward sum = 0.9978095974505934
running average episode reward sum: 0.635286919765984
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.37385893, 9.52554911, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.6449591998862292}
episode index:1425
target Thresh 10.613875762502005
target distance 4.0
model initialize at round 1425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([12.        , 10.33223403,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.108532996508657}
done in step count: 1
reward sum = 0.984524336469185
running average episode reward sum: 0.63553182679032
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.2408874 , 10.29160884,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.03830148291684}
episode index:1426
target Thresh 10.618067776529912
target distance 7.0
model initialize at round 1426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([7.        , 8.81917849, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 5.740014579145403}
done in step count: 3
reward sum = 0.9607067745439068
running average episode reward sum: 0.6357596999141839
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.73837947, 6.99606065, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.2398955856628737}
episode index:1427
target Thresh 10.622257695074723
target distance 2.0
model initialize at round 1427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([4.        , 5.16207409, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.3046531463346307}
done in step count: 0
reward sum = 0.9959871875282477
running average episode reward sum: 0.636011960059572
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([4.        , 5.16207409, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.3046531463346307}
episode index:1428
target Thresh 10.62644551918391
target distance 7.0
model initialize at round 1428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([12.00007785,  8.00016787,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 6.403206691334559}
done in step count: 3
reward sum = 0.951846519838184
running average episode reward sum: 0.636232977946051
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.60727011,  3.09299047,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.40358889332492454}
episode index:1429
target Thresh 10.630631249904438
target distance 7.0
model initialize at round 1429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 4.9477669, 10.       ,  0.       ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.150248469355958}
done in step count: 3
reward sum = 0.9486652729739946
running average episode reward sum: 0.6364514620684483
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.05833235, 11.86775709,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8697154930321983}
episode index:1430
target Thresh 10.634814888282731
target distance 10.0
model initialize at round 1430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 8.246211251235342}
done in step count: 10
reward sum = 0.8667671875968481
running average episode reward sum: 0.6366124094657427
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.20332707, 10.74638442,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.091685611062001}
episode index:1431
target Thresh 10.638996435364705
target distance 1.0
model initialize at round 1431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.71918583, 3.30433595, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7502033500951361}
done in step count: 0
reward sum = 0.9974556214937895
running average episode reward sum: 0.6368643949490025
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.71918583, 3.30433595, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7502033500951361}
episode index:1432
target Thresh 10.64317589219574
target distance 5.0
model initialize at round 1432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([4.24788499, 7.        , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 3.7487313751193216}
done in step count: 5
reward sum = 0.9350818494095904
running average episode reward sum: 0.6370725020351578
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.62189887, 3.8727923 , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6347755545880633}
episode index:1433
target Thresh 10.64735325982071
target distance 3.0
model initialize at round 1433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 6.        , 10.68403959,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.3472479046839916}
done in step count: 42
reward sum = 0.5717361492364196
running average episode reward sum: 0.6370269397249774
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.62158245, 8.31486518, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.4922803333730482}
episode index:1434
target Thresh 10.651528539283948
target distance 7.0
model initialize at round 1434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 5.099019513592799}
done in step count: 25
reward sum = 0.7105622222997086
running average episode reward sum: 0.6370781838243326
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.22651314, 9.00809841, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0174364696151843}
episode index:1435
target Thresh 10.655701731629279
target distance 6.0
model initialize at round 1435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([12.14612992,  7.82335934,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.428663852803338}
done in step count: 8
reward sum = 0.9038503074849106
running average episode reward sum: 0.6372639582837063
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.08591354,  4.42359637,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0074660975471956}
episode index:1436
target Thresh 10.659872837899997
target distance 7.0
model initialize at round 1436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([8.02619884, 8.13224972, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 7.928881475384448}
done in step count: 56
reward sum = 0.44425419668031696
running average episode reward sum: 0.6371296439054158
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.41564181, 2.03401827, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.5853475385517947}
episode index:1437
target Thresh 10.664041859138885
target distance 4.0
model initialize at round 1437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.10871923,  9.        ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.2867571646991256}
done in step count: 1
reward sum = 0.9795538524738194
running average episode reward sum: 0.637367769224309
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.10871923, 10.55415809,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9965723349783883}
episode index:1438
target Thresh 10.668208796388189
target distance 10.0
model initialize at round 1438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([12.00000005,  8.14604014,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.088300660951072}
done in step count: 26
reward sum = 0.7062377103563485
running average episode reward sum: 0.6374156288081396
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.04986638, 2.36870144, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.019163697512444}
episode index:1439
target Thresh 10.672373650689652
target distance 10.0
model initialize at round 1439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.39335155, 5.61900628, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.897314052883843}
done in step count: 58
reward sum = 0.49344037375717503
running average episode reward sum: 0.637315645992132
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.74387326,  7.13422565,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.141451994401865}
episode index:1440
target Thresh 10.676536423084485
target distance 7.0
model initialize at round 1440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.       , 8.8097055, 0.       ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.144320788201692}
done in step count: 17
reward sum = 0.8191357545438647
running average episode reward sum: 0.6374418223339443
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.23348977, 3.82235994, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8548645223505363}
episode index:1441
target Thresh 10.680697114613379
target distance 9.0
model initialize at round 1441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([9.        , 9.78857052, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 7.044277356135112}
done in step count: 8
reward sum = 0.9023349284822602
running average episode reward sum: 0.637625520743201
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.86873472,  8.08208914,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.2638276600606424}
episode index:1442
target Thresh 10.68485572631651
target distance 1.0
model initialize at round 1442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.27492154,  6.76274371,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.3631423930745209}
done in step count: 0
reward sum = 0.9983445978520724
running average episode reward sum: 0.637875499313616
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.27492154,  6.76274371,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.3631423930745209}
episode index:1443
target Thresh 10.689012259233527
target distance 4.0
model initialize at round 1443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.85817242,  3.87570089,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 3.2400161801162723}
done in step count: 4
reward sum = 0.9515197409156289
running average episode reward sum: 0.6380927044670799
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.01257069,  6.04712904,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9529538796014226}
episode index:1444
target Thresh 10.693166714403565
target distance 10.0
model initialize at round 1444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 11.522389298803166}
done in step count: 31
reward sum = 0.640495995262816
running average episode reward sum: 0.6380943676441011
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.70457244, 11.87528249,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9237948222078863}
episode index:1445
target Thresh 10.697319092865241
target distance 3.0
model initialize at round 1445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([4.62292027, 9.        , 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.7018662020668363}
done in step count: 26
reward sum = 0.7005325904831259
running average episode reward sum: 0.6381375476045706
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.42672929, 9.24113551, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.8706165650793632}
episode index:1446
target Thresh 10.701469395656646
target distance 6.0
model initialize at round 1446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([9.10424805, 8.57411337, 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 5.0991705081342005}
done in step count: 28
reward sum = 0.7018426704912367
running average episode reward sum: 0.6381815732596409
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.35930872, 10.75953794,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8402384404226564}
episode index:1447
target Thresh 10.705617623815357
target distance 3.0
model initialize at round 1447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2., 10.,  0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.4142135623730954}
done in step count: 1
reward sum = 0.9828156285640086
running average episode reward sum: 0.6384195802039119
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.50460672, 11.15779328,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.5199165494812739}
episode index:1448
target Thresh 10.709763778378433
target distance 7.0
model initialize at round 1448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([9.85872495, 9.96279633, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 6.216287959245668}
done in step count: 13
reward sum = 0.8476249051667953
running average episode reward sum: 0.6385639593101664
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.23390407,  8.99442504,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7661162155803841}
episode index:1449
target Thresh 10.713907860382404
target distance 2.0
model initialize at round 1449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.13259695, 9.99505755, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8674171277527281}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.638809087614232
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.13259695, 9.99505755, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8674171277527281}
episode index:1450
target Thresh 10.7180498708633
target distance 1.0
model initialize at round 1450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 1.52705825, 11.76221888,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.802414499673566}
done in step count: 21
reward sum = 0.7674469782860148
running average episode reward sum: 0.6388977422597674
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.93958748, 9.57777438, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.580924178723227}
episode index:1451
target Thresh 10.722189810856625
target distance 1.0
model initialize at round 1451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.86779043, 11.76417134,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.9660520700631237}
done in step count: 51
reward sum = 0.48421501645758175
running average episode reward sum: 0.6387912114568733
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.58544693, 10.53329024,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.7919258718108256}
episode index:1452
target Thresh 10.726327681397354
target distance 2.0
model initialize at round 1452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5., 9., 0.]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 2.5757174171303632e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.6390356772440368
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5., 9., 0.]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 2.5757174171303632e-14}
episode index:1453
target Thresh 10.730463483519964
target distance 8.0
model initialize at round 1453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([10.        , 11.29405224,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 8.001686644331834}
done in step count: 4
reward sum = 0.9387137198467019
running average episode reward sum: 0.639241783188055
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.91292352,  6.4857654 ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0341167176111759}
episode index:1454
target Thresh 10.734597218258397
target distance 1.0
model initialize at round 1454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([12.9738028,  9.       ,  0.       ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 3.1374357559494106}
done in step count: 31
reward sum = 0.6437073156789914
running average episode reward sum: 0.6392448522825506
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.9792076 , 10.19688299,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.9988045021281942}
episode index:1455
target Thresh 10.738728886646097
target distance 1.0
model initialize at round 1455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.87151066,  4.13655794,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 2.1895926747379564}
done in step count: 11
reward sum = 0.8764026374677472
running average episode reward sum: 0.6394077353767712
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.55991683,  2.87840105,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.572968727064263}
episode index:1456
target Thresh 10.742858489715971
target distance 8.0
model initialize at round 1456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([10.68229723, 10.24858576,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 7.9185590376069195}
done in step count: 5
reward sum = 0.9361688111872875
running average episode reward sum: 0.6396114149071834
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.8024302 , 5.65711492, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8726192790622423}
episode index:1457
target Thresh 10.746986028500428
target distance 10.0
model initialize at round 1457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.86756395, 5.04767055, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.597793363857635}
done in step count: 8
reward sum = 0.889143143887824
running average episode reward sum: 0.6397825614977052
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.3153606 ,  7.82873914,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.35886291422647254}
episode index:1458
target Thresh 10.751111504031346
target distance 2.0
model initialize at round 1458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.13869613, 7.02176193, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8615787502651718}
done in step count: 0
reward sum = 0.9954425062686205
running average episode reward sum: 0.6400263311651287
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.13869613, 7.02176193, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8615787502651718}
episode index:1459
target Thresh 10.7552349173401
target distance 5.0
model initialize at round 1459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([13.08773736,  4.51924471,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 5.441625679237932}
done in step count: 14
reward sum = 0.8095684003658478
running average episode reward sum: 0.6401424558700607
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.18761939,  9.91278121,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.9318640314065365}
episode index:1460
target Thresh 10.759356269457538
target distance 1.0
model initialize at round 1460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.        ,  8.44315624,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0937949785732353}
done in step count: 0
reward sum = 0.99543797556391
running average episode reward sum: 0.6403856423996254
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.        ,  8.44315624,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0937949785732353}
episode index:1461
target Thresh 10.763475561414003
target distance 7.0
model initialize at round 1461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([10.00494245, 11.86740305,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 5.768140878289246}
done in step count: 40
reward sum = 0.5571014165658489
running average episode reward sum: 0.6403286764448828
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.00248954, 8.40441803, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.1617852610029118}
episode index:1462
target Thresh 10.767592794239313
target distance 1.0
model initialize at round 1462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.34833044, 10.95047957,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.6535483841309861}
done in step count: 0
reward sum = 0.9999071602254488
running average episode reward sum: 0.6405744580469201
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.34833044, 10.95047957,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.6535483841309861}
episode index:1463
target Thresh 10.771707968962778
target distance 7.0
model initialize at round 1463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([1.57275593, 8.64761519, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 7.435598798006364}
done in step count: 10
reward sum = 0.8857849099735035
running average episode reward sum: 0.640741951525012
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.01174533, 8.43576735, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.1379832050993461}
episode index:1464
target Thresh 10.775821086613195
target distance 7.0
model initialize at round 1464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([8.        , 9.78510153, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 5.723354833379066}
done in step count: 3
reward sum = 0.9549296029505646
running average episode reward sum: 0.640956414085712
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.       , 7.0444423, 0.       ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0009870716827962}
episode index:1465
target Thresh 10.779932148218839
target distance 5.0
model initialize at round 1465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([4.954319  , 7.05521678, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 4.249982388855833}
done in step count: 2
reward sum = 0.9646660268526492
running average episode reward sum: 0.6411772255541752
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.42247514, 3.50386567, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.7613699728504436}
episode index:1466
target Thresh 10.784041154807479
target distance 10.0
model initialize at round 1466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.05844296,  7.17913022,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 9.239640448056495}
done in step count: 8
reward sum = 0.8958955885919626
running average episode reward sum: 0.6413508577034852
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.45118237, 9.61237139, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7606341121761289}
episode index:1467
target Thresh 10.788148107406364
target distance 4.0
model initialize at round 1467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 4.42080927, 11.        ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.6192207842488}
done in step count: 1
reward sum = 0.9830362677179066
running average episode reward sum: 0.6415836134323779
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.7579236 , 10.76732388,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0785333221648552}
episode index:1468
target Thresh 10.792253007042232
target distance 7.0
model initialize at round 1468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.14525706,  7.55741715,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 5.622763637907222}
done in step count: 6
reward sum = 0.9224148049978105
running average episode reward sum: 0.6417747851080522
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.4384838 ,  1.11084191,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.9913980828788905}
episode index:1469
target Thresh 10.796355854741313
target distance 7.0
model initialize at round 1469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 9.        , 10.10046411,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.0802721211863515}
done in step count: 7
reward sum = 0.9149265015448609
running average episode reward sum: 0.641960602602227
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.63103014, 11.23657459,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.6739188209504826}
episode index:1470
target Thresh 10.800456651529311
target distance 4.0
model initialize at round 1470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.86343774, 4.9817518 , 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.195188039632376}
done in step count: 1
reward sum = 0.9822695171718256
running average episode reward sum: 0.6421919478874545
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.94374872, 6.66500972, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0014390339796198}
episode index:1471
target Thresh 10.80455539843143
target distance 5.0
model initialize at round 1471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.08466196, 5.19143307, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 3.192555825855472}
done in step count: 2
reward sum = 0.9702379133624971
running average episode reward sum: 0.6424148052009566
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.42150368, 1.28481872, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9198599120596548}
episode index:1472
target Thresh 10.808652096472356
target distance 9.0
model initialize at round 1472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([8., 9., 0.]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 9.899494936611692}
done in step count: 6
reward sum = 0.910239595571786
running average episode reward sum: 0.6425966278692328
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.46886376,  2.87977794,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.9969164727023063}
episode index:1473
target Thresh 10.812746746676265
target distance 8.0
model initialize at round 1473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([10.        , 10.25349796,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 6.4092318609593635}
done in step count: 5
reward sum = 0.9388166913119549
running average episode reward sum: 0.6427975912772671
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.98392699, 7.53105702, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.0899632313849739}
episode index:1474
target Thresh 10.816839350066815
target distance 5.0
model initialize at round 1474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([6.56182384, 9.        , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.9449894058600403}
done in step count: 2
reward sum = 0.9717671937122653
running average episode reward sum: 0.6430206215162061
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.76163064, 6.20748637, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7893869935335548}
episode index:1475
target Thresh 10.820929907667162
target distance 7.0
model initialize at round 1475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([14.57623494,  3.60169768,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 8.508145147150367}
done in step count: 7
reward sum = 0.9039468526286332
running average episode reward sum: 0.6431974008055777
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.06175206, 8.40439637, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.1113293295059041}
episode index:1476
target Thresh 10.825018420499942
target distance 10.0
model initialize at round 1476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([13.79257202,  3.53269923,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 10.914911911560571}
done in step count: 76
reward sum = 0.37826381006699616
running average episode reward sum: 0.6430180280291806
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.98268273, 9.07977759, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.920385341261439}
episode index:1477
target Thresh 10.829104889587287
target distance 2.0
model initialize at round 1477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.76804617,  2.88468683,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.1391777962320884}
done in step count: 30
reward sum = 0.7093837912034915
running average episode reward sum: 0.6430629304399886
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.99716052,  3.32060509,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.2066095227993587}
episode index:1478
target Thresh 10.83318931595081
target distance 9.0
model initialize at round 1478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([10.27162778, 10.81264305,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 11.425377375505478}
done in step count: 22
reward sum = 0.7690424343970823
running average episode reward sum: 0.6431481092797162
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.08889079, 2.61497658, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0992343608439843}
episode index:1479
target Thresh 10.837271700611618
target distance 3.0
model initialize at round 1479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([4.52417362, 8.17586327, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 2.3463883600222157}
done in step count: 1
reward sum = 0.9846809363569624
running average episode reward sum: 0.643378874703417
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.52417362, 9.3963418 , 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.7994755798234789}
episode index:1480
target Thresh 10.841352044590312
target distance 2.0
model initialize at round 1480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 3.12477207, 11.04570663,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.5357781640254182}
done in step count: 6
reward sum = 0.9303430080844437
running average episode reward sum: 0.6435726384666723
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.12127704, 10.4162891 ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9723428710275426}
episode index:1481
target Thresh 10.845430348906973
target distance 10.0
model initialize at round 1481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 8.06225774829857}
done in step count: 6
reward sum = 0.9113812927358655
running average episode reward sum: 0.6437533460606462
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.87823695, 10.80582772,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.191913782280815}
episode index:1482
target Thresh 10.849506614581179
target distance 4.0
model initialize at round 1482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.35794384,  7.66199087,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.3991971200927305}
done in step count: 19
reward sum = 0.7861194664790883
running average episode reward sum: 0.6438493447932278
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.1099494 , 10.25294305,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7551046030085187}
episode index:1483
target Thresh 10.853580842631997
target distance 8.0
model initialize at round 1483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([12.51881927,  8.        ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 6.180121062524904}
done in step count: 4
reward sum = 0.9431199828034564
running average episode reward sum: 0.6440510096436389
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.88371355,  2.16899802,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8997277171227684}
episode index:1484
target Thresh 10.857653034077979
target distance 8.0
model initialize at round 1484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.91373301, 8.        , 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 6.406453459446336}
done in step count: 3
reward sum = 0.9525248969948399
running average episode reward sum: 0.6442587361671078
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.91373301,  9.88632062,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.14270597603780447}
episode index:1485
target Thresh 10.861723189937182
target distance 1.0
model initialize at round 1485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.37445635, 3.5242359 , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.6054494445635425}
done in step count: 0
reward sum = 0.999500403866694
running average episode reward sum: 0.644497795162868
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.37445635, 3.5242359 , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.6054494445635425}
episode index:1486
target Thresh 10.865791311227138
target distance 2.0
model initialize at round 1486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.        ,  9.70079458,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7007945775985309}
done in step count: 0
reward sum = 0.9957340487816737
running average episode reward sum: 0.644733999771892
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.        ,  9.70079458,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7007945775985309}
episode index:1487
target Thresh 10.86985739896488
target distance 7.0
model initialize at round 1487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([2.47633064, 9.95911074, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 6.593796732620892}
done in step count: 10
reward sum = 0.8880409246463643
running average episode reward sum: 0.6448975124902216
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.0784874, 9.0560025, 0.       ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.09641862431184599}
episode index:1488
target Thresh 10.87392145416693
target distance 6.0
model initialize at round 1488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 8.0938306 , 11.85682032,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 4.495245338913486}
done in step count: 5
reward sum = 0.9420590214701908
running average episode reward sum: 0.6450970836849698
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.95677764, 10.85550123,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.2834741170441688}
episode index:1489
target Thresh 10.877983477849302
target distance 6.0
model initialize at round 1489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([7.827317 , 8.6657002, 0.       ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 6.061381636964978}
done in step count: 4
reward sum = 0.9439503534819714
running average episode reward sum: 0.6452976563492631
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.06098708, 5.37965553, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0128591094132648}
episode index:1490
target Thresh 10.882043471027497
target distance 6.0
model initialize at round 1490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([13.00000868, 11.80421139,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 4.885045642529901}
done in step count: 2
reward sum = 0.9628364315828778
running average episode reward sum: 0.6455106266881186
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.02794016, 8.13221206, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.8682376228501938}
episode index:1491
target Thresh 10.88610143471652
target distance 5.0
model initialize at round 1491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([12.64096689,  8.74795842,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.044106166926247}
done in step count: 3
reward sum = 0.9617251593119307
running average episode reward sum: 0.6457225667233893
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.14111006, 11.13178152,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.1930761983626576}
episode index:1492
target Thresh 10.890157369930858
target distance 7.0
model initialize at round 1492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([10.17228117,  8.12676523,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 7.042058710391682}
done in step count: 6
reward sum = 0.9148950675364417
running average episode reward sum: 0.6459028564091315
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.7441041 ,  3.76695143,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0685997399415537}
episode index:1493
target Thresh 10.8942112776845
target distance 6.0
model initialize at round 1493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.35450602, 4.22397947, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.819443386627105}
done in step count: 5
reward sum = 0.9395490644231564
running average episode reward sum: 0.6460994067491677
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.78003226, 9.92291796, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.2083988968914627}
episode index:1494
target Thresh 10.898263158990916
target distance 1.0
model initialize at round 1494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.8774675 , 11.87013244,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8787176270806613}
done in step count: 0
reward sum = 0.9960626161808368
running average episode reward sum: 0.6463334958524665
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.8774675 , 11.87013244,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8787176270806613}
episode index:1495
target Thresh 10.902313014863083
target distance 1.0
model initialize at round 1495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.74819422, 8.44980204, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.462709116473136}
done in step count: 2
reward sum = 0.9762321068805377
running average episode reward sum: 0.6465540163143836
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.37716301, 6.9073984 , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9826615875776257}
episode index:1496
target Thresh 10.906360846313458
target distance 1.0
model initialize at round 1496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.12637814, 1.82254448, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8914626238862812}
done in step count: 0
reward sum = 0.995973379992077
running average episode reward sum: 0.6467874293829725
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.12637814, 1.82254448, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8914626238862812}
episode index:1497
target Thresh 10.910406654354004
target distance 2.0
model initialize at round 1497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.92261797,  1.13137467,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.1097321698183618}
done in step count: 11
reward sum = 0.880625331030712
running average episode reward sum: 0.6469435294508281
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.79298904,  1.13278577,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.1751136725966054}
episode index:1498
target Thresh 10.91445043999617
target distance 5.0
model initialize at round 1498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([13.13270874,  4.99858882,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 4.158644071796738}
done in step count: 3
reward sum = 0.9559299778562766
running average episode reward sum: 0.6471496578353547
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.51821671,  8.30096633,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.8489777424623456}
episode index:1499
target Thresh 10.918492204250906
target distance 9.0
model initialize at round 1499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([6.55389128, 8.08695571, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 9.937719336702395}
done in step count: 10
reward sum = 0.8774199293761397
running average episode reward sum: 0.6473031713497153
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.88689801,  5.39791696,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9720730379081137}
episode index:1500
target Thresh 10.922531948128649
target distance 1.0
model initialize at round 1500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.27107095, 11.88270023,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.9021145124702679}
done in step count: 22
reward sum = 0.774657152261893
running average episode reward sum: 0.6473880174395968
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.150772  ,  9.65921135,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.915054697163352}
episode index:1501
target Thresh 10.926569672639337
target distance 1.0
model initialize at round 1501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.01542544, 1.69873363, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0296351217951014}
done in step count: 0
reward sum = 0.9969780265936153
running average episode reward sum: 0.6476207671128019
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.01542544, 1.69873363, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0296351217951014}
episode index:1502
target Thresh 10.930605378792402
target distance 6.0
model initialize at round 1502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.02731264, 5.08537912, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.914696769688814}
done in step count: 4
reward sum = 0.9491088010700921
running average episode reward sum: 0.6478213579537583
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.10221838, 9.63503679, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9691284675467426}
episode index:1503
target Thresh 10.934639067596768
target distance 4.0
model initialize at round 1503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.00402069,  2.84343839,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.7346773112910574}
done in step count: 3
reward sum = 0.9606844231972095
running average episode reward sum: 0.6480293786088404
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.86398347,  6.17414425,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.881358980205121}
episode index:1504
target Thresh 10.938670740060862
target distance 4.0
model initialize at round 1504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 6.48644745, 10.1077165 ,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.074037885633926}
done in step count: 49
reward sum = 0.5245044703918329
running average episode reward sum: 0.6479473022578656
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.94422462, 8.72727037, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.19183988938047}
episode index:1505
target Thresh 10.942700397192597
target distance 1.0
model initialize at round 1505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 9.28160584, 10.41323948,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.34658100220016}
done in step count: 30
reward sum = 0.7008591314712023
running average episode reward sum: 0.6479824362746075
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.60572956, 9.18289347, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.0171388235970036}
episode index:1506
target Thresh 10.946728039999389
target distance 6.0
model initialize at round 1506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.06773749,  6.54251528,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 4.913726525364388}
done in step count: 2
reward sum = 0.9671769018057433
running average episode reward sum: 0.648194244148218
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.80758158, 10.344917  ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.6827580716033196}
episode index:1507
target Thresh 10.950753669488149
target distance 8.0
model initialize at round 1507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3., 9., 0.]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.0000000000000195}
done in step count: 6
reward sum = 0.917204194709653
running average episode reward sum: 0.6483726327095983
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.12611555, 3.68056132, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6921479895481955}
episode index:1508
target Thresh 10.954777286665287
target distance 8.0
model initialize at round 1508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([8.        , 9.31347564, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 6.430565257465726}
done in step count: 58
reward sum = 0.4549962066149724
running average episode reward sum: 0.6482444839845523
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.09672978,  7.69911298,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.1422154078242241}
episode index:1509
target Thresh 10.958798892536704
target distance 5.0
model initialize at round 1509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([3.78854972, 7.        , 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 3.2353688781815015}
done in step count: 2
reward sum = 0.9727674243984155
running average episode reward sum: 0.648459399839131
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.84111148, 9.48863471, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.9843591764003986}
episode index:1510
target Thresh 10.9628184881078
target distance 2.0
model initialize at round 1510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.        , 9.95918939, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.040810607373718}
done in step count: 0
reward sum = 0.9969987510335167
running average episode reward sum: 0.6486900678412452
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.        , 9.95918939, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.040810607373718}
episode index:1511
target Thresh 10.966836074383478
target distance 1.0
model initialize at round 1511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.89569287, 11.65457013,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.8814537041003854}
done in step count: 2
reward sum = 0.9731703512058663
running average episode reward sum: 0.6489046712032588
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.874665  , 10.27180062,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9159227283231264}
episode index:1512
target Thresh 10.97085165236813
target distance 1.0
model initialize at round 1512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 3.96794415, 11.        ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.20744290042512}
done in step count: 1
reward sum = 0.9838478767249171
running average episode reward sum: 0.6491260480740596
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.41138446, 10.06403732,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.41633875076650123}
episode index:1513
target Thresh 10.974865223065658
target distance 9.0
model initialize at round 1513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.13608289,  3.67999589,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.407713230066687}
done in step count: 10
reward sum = 0.8796406052306791
running average episode reward sum: 0.6492783033958275
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.88163922, 10.54049403,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9941999038673682}
episode index:1514
target Thresh 10.978876787479448
target distance 2.0
model initialize at round 1514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.91804779,  4.        ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.918047785758958}
done in step count: 0
reward sum = 0.9963678909624804
running average episode reward sum: 0.6495074054338253
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.91804779,  4.        ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.918047785758958}
episode index:1515
target Thresh 10.982886346612393
target distance 3.0
model initialize at round 1515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.66814089, 8.6277231 , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 2.3953764152595194}
done in step count: 11
reward sum = 0.8818497666326106
running average episode reward sum: 0.6496606655665422
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.0883894 , 11.52764963,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.0533033836698982}
episode index:1516
target Thresh 10.986893901466882
target distance 4.0
model initialize at round 1516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 1.53495336, 10.        ,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 5.4650466442108145}
done in step count: 7
reward sum = 0.9197243170697089
running average episode reward sum: 0.6498386903862543
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.48051715, 9.83815995, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.5441090235711025}
episode index:1517
target Thresh 10.990899453044804
target distance 1.0
model initialize at round 1517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.        ,  3.72450268,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 2.0188855271776074}
done in step count: 11
reward sum = 0.8802469331450498
running average episode reward sum: 0.6499904744723931
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.03918962,  3.5663418 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0541423199957025}
episode index:1518
target Thresh 10.994903002347549
target distance 4.0
model initialize at round 1518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.        ,  7.37989044,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.6201095581054936}
done in step count: 1
reward sum = 0.9828814917151375
running average episode reward sum: 0.6502096258991493
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.20601463,  9.27225399,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7563440248342543}
episode index:1519
target Thresh 10.998904550376
target distance 6.0
model initialize at round 1519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([12.        , 10.61469898,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.046956242837057}
done in step count: 2
reward sum = 0.9707157469343497
running average episode reward sum: 0.6504204851893041
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.        ,  9.65536398,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.34463602304456664}
episode index:1520
target Thresh 11.00290409813055
target distance 4.0
model initialize at round 1520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([14.70322192,  9.        ,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 3.8358639923331834}
done in step count: 3
reward sum = 0.9605224713511233
running average episode reward sum: 0.6506243655220864
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.23993289, 10.24852665,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.34544650609455163}
episode index:1521
target Thresh 11.006901646611084
target distance 4.0
model initialize at round 1521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.3718009,  4.       ,  0.       ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.034265447003709}
done in step count: 1
reward sum = 0.9832091854932786
running average episode reward sum: 0.6508428838006483
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.69353977,  2.23900443,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.733566991446837}
episode index:1522
target Thresh 11.010897196816988
target distance 10.0
model initialize at round 1522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 8.        , 10.70802355,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 8.817564212040569}
done in step count: 4
reward sum = 0.9354677535212907
running average episode reward sum: 0.6510297681537149
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.17418872,  6.16254741,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8553762608243431}
episode index:1523
target Thresh 11.014890749747146
target distance 3.0
model initialize at round 1523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([16.89480519,  4.59496724,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.4692443626866405}
done in step count: 22
reward sum = 0.7582026041845321
running average episode reward sum: 0.6511000915369372
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.88966472,  8.34085502,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.35826807862209376}
episode index:1524
target Thresh 11.01888230639995
target distance 3.0
model initialize at round 1524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.02619884, 11.86775028,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.343902748034275}
done in step count: 1
reward sum = 0.9809527709775493
running average episode reward sum: 0.6513163883759145
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.03694737, 11.83644712,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.2755838501952914}
episode index:1525
target Thresh 11.022871867773294
target distance 5.0
model initialize at round 1525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([2.90774512, 7.        , 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 5.07410583049573}
done in step count: 19
reward sum = 0.7890217031788294
running average episode reward sum: 0.6514066277696255
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 7.23211178, 10.8510159 ,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.8821020009904845}
episode index:1526
target Thresh 11.026859434864559
target distance 7.0
model initialize at round 1526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4., 4., 0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.0990195135928}
done in step count: 3
reward sum = 0.9540238721702257
running average episode reward sum: 0.651604805401846
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.59946925, 8.69777451, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.6713446409397182}
episode index:1527
target Thresh 11.030845008670644
target distance 9.0
model initialize at round 1527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([9., 9., 0.]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 8.062257748298574}
done in step count: 5
reward sum = 0.9239913273850457
running average episode reward sum: 0.6517830688324633
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.48927431,  4.05084433,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0678416689626793}
episode index:1528
target Thresh 11.034828590187935
target distance 3.0
model initialize at round 1528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.65974855,  8.8876847 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 2.139543633774361}
done in step count: 2
reward sum = 0.973750957471147
running average episode reward sum: 0.6519936429911544
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.86534439, 11.06162589,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8675359723516542}
episode index:1529
target Thresh 11.038810180412337
target distance 3.0
model initialize at round 1529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.25421169, 9.49005282, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.599339000597616}
done in step count: 1
reward sum = 0.986741783162884
running average episode reward sum: 0.6522124326252535
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.49961138, 7.49005282, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.7003859928411225}
episode index:1530
target Thresh 11.04278978033924
target distance 11.0
model initialize at round 1530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4.89638534, 5.65941528, 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.085458617765461}
done in step count: 22
reward sum = 0.7496606703109813
running average episode reward sum: 0.652276082682527
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.42434681,  9.59881691,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.5839675369529237}
episode index:1531
target Thresh 11.046767390963549
target distance 9.0
model initialize at round 1531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.86740305,  3.99505755,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 7.058442237262244}
done in step count: 5
reward sum = 0.9374597392068735
running average episode reward sum: 0.6524622338943575
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.82548496, 10.11997595,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.2065934448322087}
episode index:1532
target Thresh 11.050743013279662
target distance 2.0
model initialize at round 1532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.90703624,  4.43082824,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.8124609634464763}
done in step count: 1
reward sum = 0.9865733206250663
running average episode reward sum: 0.6526801798087285
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.61212551,  5.01483183,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.159850839563171}
episode index:1533
target Thresh 11.054716648281486
target distance 10.0
model initialize at round 1533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.1351047 ,  5.05060231,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.135244846570517}
done in step count: 9
reward sum = 0.8757159306034835
running average episode reward sum: 0.6528255746919064
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.20059421, 5.30099481, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8541940629219533}
episode index:1534
target Thresh 11.058688296962433
target distance 2.0
model initialize at round 1534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.34667331, 2.03091455, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 4.022496129612359}
done in step count: 5
reward sum = 0.9375984149088936
running average episode reward sum: 0.65301109445752
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.8201206 , 5.41591318, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.6111579241935944}
episode index:1535
target Thresh 11.062657960315413
target distance 5.0
model initialize at round 1535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([11.        ,  9.02875471,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.000137802403999}
done in step count: 2
reward sum = 0.9662123099067659
running average episode reward sum: 0.6532150014988282
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.94620192,  9.60473716,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.122944838446149}
episode index:1536
target Thresh 11.066625639332841
target distance 2.0
model initialize at round 1536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.47475791, 4.        , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.525242089877512}
done in step count: 7
reward sum = 0.9182731341239389
running average episode reward sum: 0.653387453114069
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.12835121, 3.31050051, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7013441250026126}
episode index:1537
target Thresh 11.070591335006638
target distance 9.0
model initialize at round 1537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.        ,  3.09926319,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 9.92076822234858}
done in step count: 7
reward sum = 0.902212148901068
running average episode reward sum: 0.6535492377017068
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.45760141, 10.89574819,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.4693266369271833}
episode index:1538
target Thresh 11.074555048328225
target distance 1.0
model initialize at round 1538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.13273489,  4.98569008,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.8673199437447165}
done in step count: 4
reward sum = 0.954072798840523
running average episode reward sum: 0.6537445096712577
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.7981885 ,  5.17849727,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.2694237513960772}
episode index:1539
target Thresh 11.078516780288536
target distance 2.0
model initialize at round 1539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.76858521,  9.52135241,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.9287257685709708}
done in step count: 0
reward sum = 0.9979171584752986
running average episode reward sum: 0.6539679984042474
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.76858521,  9.52135241,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.9287257685709708}
episode index:1540
target Thresh 11.082476531878001
target distance 2.0
model initialize at round 1540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.49587452, 2.06747407, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5004440961923566}
done in step count: 0
reward sum = 0.997651001363878
running average episode reward sum: 0.6541910243633386
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.49587452, 2.06747407, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5004440961923566}
episode index:1541
target Thresh 11.086434304086554
target distance 8.0
model initialize at round 1541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 8.72534589000672}
done in step count: 39
reward sum = 0.602032496610575
running average episode reward sum: 0.6541571991183628
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.06453336, 9.44748724, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.45211656370264447}
episode index:1542
target Thresh 11.090390097903645
target distance 8.0
model initialize at round 1542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([3.9526577, 9.       , 0.       ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 6.047342300415034}
done in step count: 3
reward sum = 0.9520939609121485
running average episode reward sum: 0.6543502884001475
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.86752021, 9.38941878, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.41133670000663786}
episode index:1543
target Thresh 11.094343914318218
target distance 8.0
model initialize at round 1543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([ 4.        , 11.27510774,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 6.416861790219383}
done in step count: 5
reward sum = 0.9347487821741883
running average episode reward sum: 0.6545318936422292
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.76684034,  9.75967157,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.0794188240059621}
episode index:1544
target Thresh 11.09829575431873
target distance 2.0
model initialize at round 1544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 4.09283495, 11.        ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.3194736743108106}
done in step count: 9
reward sum = 0.8898976552723252
running average episode reward sum: 0.6546842339410189
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.35853078, 9.94724838, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.3623907489380765}
episode index:1545
target Thresh 11.102245618893138
target distance 9.0
model initialize at round 1545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([9.95104766, 9.        , 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 8.630870161373904}
done in step count: 42
reward sum = 0.5749374582028701
running average episode reward sum: 0.6546326512917704
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.80030488,  2.32774256,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8648139051936323}
episode index:1546
target Thresh 11.106193509028909
target distance 7.0
model initialize at round 1546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([3.00778389, 6.37357605, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 7.004113406622472}
done in step count: 3
reward sum = 0.9532032810440388
running average episode reward sum: 0.6548256510524377
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.00778389, 9.51116353, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.48889843594468374}
episode index:1547
target Thresh 11.110139425713019
target distance 8.0
model initialize at round 1547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.58952904, 4.29145288, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 6.734400410175712}
done in step count: 11
reward sum = 0.8683377686774078
running average episode reward sum: 0.6549635787770017
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.97727083, 11.70973561,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7100994655864518}
episode index:1548
target Thresh 11.11408336993194
target distance 3.0
model initialize at round 1548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.93939561, 7.65861487, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.7100279927630642}
done in step count: 1
reward sum = 0.9842507698814091
running average episode reward sum: 0.6551761592748097
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.20940012, 9.65861487, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.0289906347850117}
episode index:1549
target Thresh 11.118025342671665
target distance 11.0
model initialize at round 1549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.13510725,  5.96856792,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.324517352654814}
done in step count: 36
reward sum = 0.6317424571682178
running average episode reward sum: 0.6551610407573215
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.82245559, 4.85113285, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8694533586443768}
episode index:1550
target Thresh 11.121965344917685
target distance 7.0
model initialize at round 1550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([13.00016787,  7.00007785,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 5.831055790466416}
done in step count: 3
reward sum = 0.9499942342705141
running average episode reward sum: 0.6553511330806698
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.19430894, 9.9564534 , 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.8068670169484667}
episode index:1551
target Thresh 11.125903377654998
target distance 4.0
model initialize at round 1551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 7.        , 10.00646603,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 2.238967143954293}
done in step count: 7
reward sum = 0.9151920156356612
running average episode reward sum: 0.6555185563297388
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.45437682, 8.51518059, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.6644607995029697}
episode index:1552
target Thresh 11.129839441868116
target distance 10.0
model initialize at round 1552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([7.64517635, 8.10142108, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 10.345550607979181}
done in step count: 5
reward sum = 0.9182658263225493
running average episode reward sum: 0.6556877432389421
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.89170028,  2.34682955,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9567758987296818}
episode index:1553
target Thresh 11.133773538541053
target distance 2.0
model initialize at round 1553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([16.,  7.,  0.]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.472135954999554}
done in step count: 37
reward sum = 0.6381085939501058
running average episode reward sum: 0.6556764310450625
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.53054342, 10.95244176,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.4718593762213806}
episode index:1554
target Thresh 11.137705668657336
target distance 2.0
model initialize at round 1554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.        ,  6.31515861,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 2.3151586055755695}
done in step count: 1
reward sum = 0.9846495497491649
running average episode reward sum: 0.6558879893207564
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.85610069,  4.84358194,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.2018897094385133}
episode index:1555
target Thresh 11.141635833199992
target distance 5.0
model initialize at round 1555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13310072, 9.0023202 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 3.1249705537792907}
done in step count: 2
reward sum = 0.967621499367405
running average episode reward sum: 0.656088332193537
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.16400598, 5.03638702, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.2757099929096065}
episode index:1556
target Thresh 11.145564033151567
target distance 8.0
model initialize at round 1556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.50890076,  1.59691155,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.416849387762664}
done in step count: 9
reward sum = 0.8946634128101313
running average episode reward sum: 0.6562415596056222
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.1086784 , 10.44725001,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5633325374503149}
episode index:1557
target Thresh 11.149490269494109
target distance 2.0
model initialize at round 1557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.60334644, 10.6895349 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.6331284051314032}
done in step count: 9
reward sum = 0.8957451840434439
running average episode reward sum: 0.6563952846534001
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.21047708, 10.9671935 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.21301846982253206}
episode index:1558
target Thresh 11.153414543209179
target distance 8.0
model initialize at round 1558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.1304671, 9.8889023, 0.       ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 6.9435626510718915}
done in step count: 5
reward sum = 0.9306555405127652
running average episode reward sum: 0.6565712052793523
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.91889531, 3.4030211 , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.41110093512209367}
episode index:1559
target Thresh 11.15733685527784
target distance 10.0
model initialize at round 1559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([11.80047309,  9.87355775,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 9.257735187395205}
done in step count: 22
reward sum = 0.759536900094291
running average episode reward sum: 0.6566372089298748
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.88737285, 6.14076208, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8665879509985487}
episode index:1560
target Thresh 11.161257206680677
target distance 11.0
model initialize at round 1560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.01287961, 5.48950028, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 11.406497306404342}
done in step count: 8
reward sum = 0.8947058944403176
running average episode reward sum: 0.6567897192985552
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.98040421, 10.1277545 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.3122517416524098}
episode index:1561
target Thresh 11.165175598397775
target distance 3.0
model initialize at round 1561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.        , 11.18485251,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.016941714509091}
done in step count: 27
reward sum = 0.7036284467550926
running average episode reward sum: 0.6568197056797694
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.33051473, 11.86158298,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9228029107524913}
episode index:1562
target Thresh 11.16909203140873
target distance 7.0
model initialize at round 1562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 8.94148183, 11.        ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 6.760594828275}
done in step count: 8
reward sum = 0.906832559370954
running average episode reward sum: 0.6569796627198791
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.92120862,  8.13301969,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.15460374622506715}
episode index:1563
target Thresh 11.173006506692653
target distance 7.0
model initialize at round 1563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.99036793, 4.35579133, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.644215651430284}
done in step count: 18
reward sum = 0.7718429057112038
running average episode reward sum: 0.6570531046911011
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.83972324, 11.8717923 ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8864030958262759}
episode index:1564
target Thresh 11.176919025228164
target distance 8.0
model initialize at round 1564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 6.082762530298235}
done in step count: 27
reward sum = 0.7126191744576895
running average episode reward sum: 0.6570886101669903
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.45732817, 10.4110801 ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6807933321752134}
episode index:1565
target Thresh 11.180829587993387
target distance 4.0
model initialize at round 1565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.71518505, 8.34363222, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 3.161985059867397}
done in step count: 2
reward sum = 0.9746272611489182
running average episode reward sum: 0.6572913806976302
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.89710224, 10.72952259,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.289388979122149}
episode index:1566
target Thresh 11.184738195965968
target distance 9.0
model initialize at round 1566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([7.41446018, 9.83598447, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 7.804566182434994}
done in step count: 4
reward sum = 0.9437769246364305
running average episode reward sum: 0.6574742049120136
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.41311976,  8.58076581,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7127109197052125}
episode index:1567
target Thresh 11.188644850123056
target distance 9.0
model initialize at round 1567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 9.01051829, 11.85415933,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 10.527829091686842}
done in step count: 26
reward sum = 0.7251511192270222
running average episode reward sum: 0.6575173662094084
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.10648542, 3.44018701, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.5698506180458519}
episode index:1568
target Thresh 11.192549551441317
target distance 5.0
model initialize at round 1568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([2.86742902, 7.45497012, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 3.5475078443246906}
done in step count: 19
reward sum = 0.7790682244149885
running average episode reward sum: 0.6575948364823246
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.33096233, 11.84673619,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9091194834421943}
episode index:1569
target Thresh 11.196452300896926
target distance 7.0
model initialize at round 1569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([13.13224963,  5.97380131,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 7.335854506752836}
done in step count: 5
reward sum = 0.9225682097056017
running average episode reward sum: 0.6577636093315113
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.83515002, 9.87427847, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.20731960174187217}
episode index:1570
target Thresh 11.20035309946557
target distance 10.0
model initialize at round 1570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.0294826, 5.       , 0.       ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.412070743248965}
done in step count: 30
reward sum = 0.6884432348739774
running average episode reward sum: 0.6577831380555995
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.34858695,  8.01331337,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.3488410930789624}
episode index:1571
target Thresh 11.204251948122447
target distance 5.0
model initialize at round 1571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.23431051, 9.        , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.096171894211441}
done in step count: 2
reward sum = 0.9701806483316938
running average episode reward sum: 0.6579818642071747
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.73408329, 5.27603722, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0310191056192497}
episode index:1572
target Thresh 11.20814884784227
target distance 8.0
model initialize at round 1572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([8.16126525, 8.12528608, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 6.517517192578555}
done in step count: 10
reward sum = 0.8777902362949934
running average episode reward sum: 0.658121602523823
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.85645627, 5.42262755, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.0328970372007018}
episode index:1573
target Thresh 11.212043799599265
target distance 1.0
model initialize at round 1573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.65643656,  9.12151539,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.9898777295116703}
done in step count: 14
reward sum = 0.8543193665091849
running average episode reward sum: 0.6582462516750207
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.84642653, 10.64370678,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9183587108984932}
episode index:1574
target Thresh 11.215936804367168
target distance 7.0
model initialize at round 1574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.22924125, 5.93627119, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.122052176853686}
done in step count: 9
reward sum = 0.8933062978924644
running average episode reward sum: 0.6583954961488095
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.74982223, 10.13815337,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.142371647816058}
episode index:1575
target Thresh 11.219827863119232
target distance 8.0
model initialize at round 1575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([13.12114365,  5.79390581,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 6.909952206001335}
done in step count: 17
reward sum = 0.8016869156313483
running average episode reward sum: 0.6584864171002579
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.25283165, 9.29619764, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.803737259707}
episode index:1576
target Thresh 11.223716976828221
target distance 8.0
model initialize at round 1576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([9., 9., 0.]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 7.211102550928006}
done in step count: 3
reward sum = 0.9466566068092447
running average episode reward sum: 0.6586691502579681
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.8170896 , 4.91749021, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.20065911173220155}
episode index:1577
target Thresh 11.227604146466415
target distance 2.0
model initialize at round 1577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.25309062,  1.71613181,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.3677386975509527}
done in step count: 23
reward sum = 0.7481000544292012
running average episode reward sum: 0.658725823834756
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.68381709,  4.14058036,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9157367278688229}
episode index:1578
target Thresh 11.231489373005605
target distance 11.0
model initialize at round 1578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 5.12074423, 11.52669251,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.893285581671343}
done in step count: 8
reward sum = 0.8944097492159664
running average episode reward sum: 0.658875085345447
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.63475499, 11.89968877,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9710014418887398}
episode index:1579
target Thresh 11.2353726574171
target distance 4.0
model initialize at round 1579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 7.73421593, 11.88511135,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 2.9474433387134273}
done in step count: 1
reward sum = 0.9801402832977124
running average episode reward sum: 0.6590784177492143
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.73088072, 10.28458389,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.3916799384228643}
episode index:1580
target Thresh 11.239254000671716
target distance 5.0
model initialize at round 1580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([14.       ,  9.1875813,  0.       ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 3.0058587365985345}
done in step count: 2
reward sum = 0.9688456007075158
running average episode reward sum: 0.6592743489212309
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.13778883,  8.12899553,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.2255843042624053}
episode index:1581
target Thresh 11.24313340373979
target distance 8.0
model initialize at round 1581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.02619884, 8.13224972, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.915494910492205}
done in step count: 4
reward sum = 0.9356353992725802
running average episode reward sum: 0.6594490398506565
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.90661128, 3.49617821, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.504890356646476}
episode index:1582
target Thresh 11.247010867591179
target distance 9.0
model initialize at round 1582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([4.87224919, 5.87183842, 0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 8.708945295101431}
done in step count: 7
reward sum = 0.9081810832289687
running average episode reward sum: 0.659606166852159
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.23308128,  9.68753899,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.7259729661211831}
episode index:1583
target Thresh 11.250886393195241
target distance 8.0
model initialize at round 1583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([13.        , 11.36389339,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 6.448875247504485}
done in step count: 4
reward sum = 0.9487669685517776
running average episode reward sum: 0.6597887178633329
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.31441832, 9.27277   , 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.41624794488478867}
episode index:1584
target Thresh 11.254759981520863
target distance 7.0
model initialize at round 1584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([10.        ,  9.77250105,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 5.717233778686851}
done in step count: 4
reward sum = 0.949758243667114
running average episode reward sum: 0.6599716639363953
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.1091221 ,  7.58638051,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.5964475928844656}
episode index:1585
target Thresh 11.258631633536439
target distance 3.0
model initialize at round 1585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.        , 10.73696959,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.2422254935640866}
done in step count: 1
reward sum = 0.9841488649078461
running average episode reward sum: 0.6601760631803874
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.81286097, 10.80651766,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.1450823928843847}
episode index:1586
target Thresh 11.262501350209885
target distance 4.0
model initialize at round 1586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.2360679774998142}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.6603763681187795
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([15.,  6.,  0.]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9999999999999627}
episode index:1587
target Thresh 11.266369132508625
target distance 4.0
model initialize at round 1587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13779888, 8.01960476, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.195949488284541}
done in step count: 1
reward sum = 0.9822317824740339
running average episode reward sum: 0.6605790478507412
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13193598, 6.23029337, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.898092524725618}
episode index:1588
target Thresh 11.270234981399609
target distance 3.0
model initialize at round 1588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([13.33005369,  8.29365563,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.7617993549994604}
done in step count: 11
reward sum = 0.8805989590300096
running average episode reward sum: 0.6607175122378899
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.76890635,  8.66763016,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.404813525068359}
episode index:1589
target Thresh 11.2740988978493
target distance 1.0
model initialize at round 1589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.15081165, 2.02980207, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8497111374407291}
done in step count: 0
reward sum = 0.9987081624707487
running average episode reward sum: 0.6609300849738855
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.15081165, 2.02980207, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8497111374407291}
episode index:1590
target Thresh 11.277960882823674
target distance 3.0
model initialize at round 1590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([13.00488448, 10.63758463,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 2.094516671838835}
done in step count: 31
reward sum = 0.7001724222987141
running average episode reward sum: 0.6609547501764781
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.83026487,  9.16384325,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8532104785508394}
episode index:1591
target Thresh 11.281820937288227
target distance 10.0
model initialize at round 1591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.86949885, 4.92084086, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.969148876110221}
done in step count: 13
reward sum = 0.8402689217969388
running average episode reward sum: 0.6610673847063904
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.95441456, 11.86355938,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8647617182422835}
episode index:1592
target Thresh 11.285679062207976
target distance 11.0
model initialize at round 1592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.414114271867186}
done in step count: 6
reward sum = 0.9076209842018637
running average episode reward sum: 0.6612221578385282
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.06099255,  9.72212075,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7246919833887979}
episode index:1593
target Thresh 11.289535258547449
target distance 1.0
model initialize at round 1593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([16.        ,  3.89363766,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.005640565994234}
done in step count: 0
reward sum = 0.9964010589757011
running average episode reward sum: 0.6614324331842855
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([16.        ,  3.89363766,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.005640565994234}
episode index:1594
target Thresh 11.2933895272707
target distance 5.0
model initialize at round 1594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.        , 10.50156713,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.041124024199186}
done in step count: 15
reward sum = 0.8173287575120298
running average episode reward sum: 0.6615301738264973
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.4308256 , 11.91004969,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0068769183155275}
episode index:1595
target Thresh 11.297241869341287
target distance 2.0
model initialize at round 1595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([12.70807793, 11.88748113,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.5673816970641077}
done in step count: 2
reward sum = 0.9742807510473971
running average episode reward sum: 0.6617261328347811
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.79388995, 11.86191292,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8862139928707594}
episode index:1596
target Thresh 11.301092285722303
target distance 6.0
model initialize at round 1596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([12.45945004,  7.55315264,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.806713336111275}
done in step count: 2
reward sum = 0.968777911530407
running average episode reward sum: 0.6619184006987107
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.12890843,  3.67443541,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.1016640326044056}
episode index:1597
target Thresh 11.304940777376352
target distance 2.0
model initialize at round 1597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([8.5737865, 8.8343385, 0.       ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.5824814914317382}
done in step count: 5
reward sum = 0.9461914010844422
running average episode reward sum: 0.6620962936901911
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.97657188, 8.13270019, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.3061016830443934}
episode index:1598
target Thresh 11.308787345265555
target distance 7.0
model initialize at round 1598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([10.97380131,  8.13224963,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 7.928881302838386}
done in step count: 3
reward sum = 0.9466566068092447
running average episode reward sum: 0.6622742551117791
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.86781219,  2.93113555,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.2728359686122475}
episode index:1599
target Thresh 11.312631990351555
target distance 2.0
model initialize at round 1599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.,  9.,  0.]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9999999999999805}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.6624815837024626
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.,  9.,  0.]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9999999999999805}
episode index:1600
target Thresh 11.316474713595511
target distance 1.0
model initialize at round 1600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.06622207,  2.        ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.1838388002643194}
done in step count: 6
reward sum = 0.9272629958860632
running average episode reward sum: 0.6626469687194418
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.12718835,  4.41836487,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0488563263228436}
episode index:1601
target Thresh 11.320315515958104
target distance 10.0
model initialize at round 1601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([11.0000152, 11.7834522,  0.       ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 8.196398288377702}
done in step count: 11
reward sum = 0.8767403792709506
running average episode reward sum: 0.6627806100493742
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.9358972 , 9.52030814, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0516690770978556}
episode index:1602
target Thresh 11.324154398399537
target distance 7.0
model initialize at round 1602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([3.99035764, 3.93844068, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 6.767602864714702}
done in step count: 5
reward sum = 0.9317415698360514
running average episode reward sum: 0.6629483960504887
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.24049006, 9.27134422, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.7673165647342183}
episode index:1603
target Thresh 11.327991361879533
target distance 1.0
model initialize at round 1603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.15343624,  7.25405431,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.26340606619039}
done in step count: 1
reward sum = 0.9881748647949901
running average episode reward sum: 0.6631511556943444
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.31989938,  6.91063362,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9651886913384682}
episode index:1604
target Thresh 11.331826407357326
target distance 5.0
model initialize at round 1604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4.52145517, 6.        , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 3.363751749100089}
done in step count: 5
reward sum = 0.9398921626245379
running average episode reward sum: 0.6633235799977276
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.29774773, 8.87387291, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.3233600990980047}
episode index:1605
target Thresh 11.335659535791685
target distance 8.0
model initialize at round 1605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 9.43726385, 11.06626582,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 6.880331406750998}
done in step count: 21
reward sum = 0.7505332139913862
running average episode reward sum: 0.6633778823850213
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.86525014,  9.43947238,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9704606025559394}
episode index:1606
target Thresh 11.339490748140884
target distance 3.0
model initialize at round 1606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 9.0022504, 11.8668826,  0.       ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.0370251431332513}
done in step count: 1
reward sum = 0.9783929100529792
running average episode reward sum: 0.6635739091601724
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.06620269, 9.84324774, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.2581908357186244}
episode index:1607
target Thresh 11.34332004536273
target distance 11.0
model initialize at round 1607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.76008222, 3.18925072, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.65797039068354}
done in step count: 24
reward sum = 0.7215925626665891
running average episode reward sum: 0.6636099904123529
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.57467416,  6.70184243,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.907101530025395}
episode index:1608
target Thresh 11.347147428414548
target distance 9.0
model initialize at round 1608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4., 5., 0.]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 9.219544457292914}
done in step count: 7
reward sum = 0.8982566066895791
running average episode reward sum: 0.6637558242322891
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.89819656, 11.87014428,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8760793392247435}
episode index:1609
target Thresh 11.350972898253184
target distance 2.0
model initialize at round 1609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.08774036, 8.        , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.0877403616905359}
done in step count: 0
reward sum = 0.9963758369851998
running average episode reward sum: 0.6639624205135021
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.08774036, 8.        , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.0877403616905359}
episode index:1610
target Thresh 11.354796455835004
target distance 3.0
model initialize at round 1610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2., 10.,  0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.4142135623730954}
done in step count: 9
reward sum = 0.889436053371228
running average episode reward sum: 0.6641023793172622
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.16378948, 11.72718541,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.108172663192903}
episode index:1611
target Thresh 11.3586181021159
target distance 4.0
model initialize at round 1611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([ 8.63496673, 11.30118656,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 4.554843785126088}
done in step count: 10
reward sum = 0.8825912759657647
running average episode reward sum: 0.6642379183350342
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.08881617,  9.14785439,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.8567616096609982}
episode index:1612
target Thresh 11.362437838051282
target distance 1.0
model initialize at round 1612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.93161845, 3.        , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0023352912558612}
done in step count: 0
reward sum = 0.9963490657547094
running average episode reward sum: 0.6644438148926409
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.93161845, 3.        , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0023352912558612}
episode index:1613
target Thresh 11.366255664596078
target distance 6.0
model initialize at round 1613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([7.53118384, 9.65676558, 0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 5.5081114033599}
done in step count: 67
reward sum = 0.37026174036177606
running average episode reward sum: 0.6642615459493132
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.60822327,  8.59306967,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.731797676708827}
episode index:1614
target Thresh 11.370071582704757
target distance 8.0
model initialize at round 1614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([7.90350706, 8.13061746, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 7.267863362583136}
done in step count: 7
reward sum = 0.9159625231567914
running average episode reward sum: 0.6644173979475841
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.83588076, 2.68108394, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.0782263121667006}
episode index:1615
target Thresh 11.373885593331288
target distance 2.0
model initialize at round 1615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.        , 10.99097198,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.009028017520854803}
done in step count: 0
reward sum = 0.996263480757106
running average episode reward sum: 0.6646227482463524
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.        , 10.99097198,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.009028017520854803}
episode index:1616
target Thresh 11.37769769742918
target distance 1.0
model initialize at round 1616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([16.        ,  4.66064203,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 2.1062877032751732}
done in step count: 20
reward sum = 0.7831650689269815
running average episode reward sum: 0.6646960582776947
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.0496345 ,  4.40684152,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.4098580313054511}
episode index:1617
target Thresh 11.381507895951456
target distance 5.0
model initialize at round 1617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([11.        ,  9.57418323,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 5.4702058628218}
done in step count: 5
reward sum = 0.9349120289093975
running average episode reward sum: 0.6648630644400135
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.08501592,  4.16792762,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8364042969965354}
episode index:1618
target Thresh 11.385316189850666
target distance 1.0
model initialize at round 1618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.70879155, 4.58717048, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.5052034938829967}
done in step count: 0
reward sum = 0.9993646340227861
running average episode reward sum: 0.6650696744274025
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.70879155, 4.58717048, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.5052034938829967}
episode index:1619
target Thresh 11.389122580078883
target distance 1.0
model initialize at round 1619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.13061595, 3.89055014, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.117112136614489}
done in step count: 9
reward sum = 0.9037480665123411
running average episode reward sum: 0.6652170067681957
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.70124692, 4.6322789 , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7918118788038456}
episode index:1620
target Thresh 11.392927067587708
target distance 2.0
model initialize at round 1620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.5121479338940402e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.6654198340312663
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.5121479338940402e-14}
episode index:1621
target Thresh 11.396729653328256
target distance 7.0
model initialize at round 1621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([13.65592861,  9.8065604 ,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 6.704619702789392}
done in step count: 3
reward sum = 0.9530948822850813
running average episode reward sum: 0.6655971922607692
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.65592471, 8.86271292, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.6701380232202782}
episode index:1622
target Thresh 11.40053033825118
target distance 11.0
model initialize at round 1622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.184084079649052}
done in step count: 33
reward sum = 0.6181124179761491
running average episode reward sum: 0.6655679348520911
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.79185715, 5.02501759, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.792252249108747}
episode index:1623
target Thresh 11.404329123306653
target distance 5.0
model initialize at round 1623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 9.11106944, 10.98939574,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.111087513413729}
done in step count: 3
reward sum = 0.9601724864746684
running average episode reward sum: 0.665749341595701
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.87288276, 10.1507194 ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.858741130058813}
episode index:1624
target Thresh 11.408126009444363
target distance 7.0
model initialize at round 1624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([7.        , 9.85050333, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 6.31081419940385}
done in step count: 6
reward sum = 0.9264141818255865
running average episode reward sum: 0.6659097507281502
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.4814018 , 6.01704198, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.518878136742083}
episode index:1625
target Thresh 11.411920997613535
target distance 9.0
model initialize at round 1625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([4.84939536, 5.06768174, 0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 8.160531452568495}
done in step count: 6
reward sum = 0.9210941114078157
running average episode reward sum: 0.6660666906793677
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.33900598,  8.14559752,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.0802391890883554}
episode index:1626
target Thresh 11.415714088762916
target distance 6.0
model initialize at round 1626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.7559737 , 7.82321358, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.3012952416052945}
done in step count: 36
reward sum = 0.5668865100177899
running average episode reward sum: 0.6660057317484142
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.63119119, 10.98841379,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.36899075268573556}
episode index:1627
target Thresh 11.419505283840785
target distance 1.0
model initialize at round 1627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.        ,  8.70133722,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.9734610029894823}
done in step count: 3
reward sum = 0.9612338456407555
running average episode reward sum: 0.6661870757987165
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.13375497,  7.86428404,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.2236696260150257}
episode index:1628
target Thresh 11.423294583794931
target distance 11.0
model initialize at round 1628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.295630140987008}
done in step count: 49
reward sum = 0.5149368559177183
running average episode reward sum: 0.6660942272905023
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.86357685, 3.90318405, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.2496025024642705}
episode index:1629
target Thresh 11.427081989572685
target distance 7.0
model initialize at round 1629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([8.41638803, 9.        , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 8.134496629379672}
done in step count: 12
reward sum = 0.860916760608329
running average episode reward sum: 0.6662137503170775
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.32102885, 3.73949396, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.7272312013665031}
episode index:1630
target Thresh 11.430867502120897
target distance 5.0
model initialize at round 1630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 9.        , 10.77019352,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.008788962279221}
done in step count: 2
reward sum = 0.9695951746893792
running average episode reward sum: 0.6663997597740808
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.99991455, 11.83165888,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.3005712570185208}
episode index:1631
target Thresh 11.434651122385944
target distance 7.0
model initialize at round 1631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 9., 11.,  0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 5.099019513592799}
done in step count: 3
reward sum = 0.95493345523796
running average episode reward sum: 0.6665765573815955
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([15.        , 10.49378568,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.1152687107247115}
episode index:1632
target Thresh 11.438432851313733
target distance 3.0
model initialize at round 1632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([16.        ,  5.20910835,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.9799596832502537}
done in step count: 13
reward sum = 0.8482462552132585
running average episode reward sum: 0.6666878064310944
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.12171966,  2.62762767,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9539588637517096}
episode index:1633
target Thresh 11.442212689849697
target distance 7.0
model initialize at round 1633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.89916749,  5.3930612 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.678579477824638}
done in step count: 3
reward sum = 0.9586726992047078
running average episode reward sum: 0.6668664997559253
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.8375381, 10.1216565,  0.       ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.2136545500918383}
episode index:1634
target Thresh 11.445990638938792
target distance 7.0
model initialize at round 1634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 9.        , 10.47372389,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 5.02239129523361}
done in step count: 3
reward sum = 0.9597572829518991
running average episode reward sum: 0.6670456378496231
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.10560024,  9.17689633,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.215504249283414}
episode index:1635
target Thresh 11.449766699525508
target distance 5.0
model initialize at round 1635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([12.        ,  8.92923307,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 3.0008345435543387}
done in step count: 4
reward sum = 0.9455782499598352
running average episode reward sum: 0.6672158900575145
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.38369098, 9.18286711, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.6428663865643307}
episode index:1636
target Thresh 11.45354087255386
target distance 9.0
model initialize at round 1636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.49230289, 4.        , 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.435626723583588}
done in step count: 4
reward sum = 0.9410099827850331
running average episode reward sum: 0.6673831436266822
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.64190315, 11.47477503,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.7984052728873232}
episode index:1637
target Thresh 11.457313158967388
target distance 3.0
model initialize at round 1637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.45369017, 4.91765022, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.9939500505943841}
done in step count: 9
reward sum = 0.8964296174713027
running average episode reward sum: 0.6675229766387973
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.17931887, 2.87141627, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8306932589327992}
episode index:1638
target Thresh 11.461083559709168
target distance 11.0
model initialize at round 1638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([ 3.35346367, 11.89373806,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 10.071215577614879}
done in step count: 5
reward sum = 0.9270898526941089
running average episode reward sum: 0.6676813456906919
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.19973539,  8.08179061,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.2180032556677722}
episode index:1639
target Thresh 11.4648520757218
target distance 4.0
model initialize at round 1639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([5.        , 8.49047577, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 2.505726120199568}
done in step count: 1
reward sum = 0.9842011998876437
running average episode reward sum: 0.6678743456017876
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.00129104, 9.17250508, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.296983976397236}
episode index:1640
target Thresh 11.468618707947412
target distance 6.0
model initialize at round 1640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([8.00494245, 8.13259695, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 4.537348765210968}
done in step count: 2
reward sum = 0.9646051471491073
running average episode reward sum: 0.6680551687593423
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.59440334, 6.46836137, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7567547198597411}
episode index:1641
target Thresh 11.47238345732766
target distance 3.0
model initialize at round 1641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([5., 8., 0.]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.1622776601683684}
done in step count: 5
reward sum = 0.9393883305763646
running average episode reward sum: 0.6682204142902906
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.75937034, 11.43234501,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8738223607229961}
episode index:1642
target Thresh 11.476146324803734
target distance 9.0
model initialize at round 1642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([6.        , 8.36320114, 0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 7.028905518383978}
done in step count: 26
reward sum = 0.7305004221540194
running average episode reward sum: 0.6682583205640967
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.85970053,  9.34207845,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.36973180358217483}
episode index:1643
target Thresh 11.479907311316353
target distance 8.0
model initialize at round 1643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([8.02084403, 8.13394917, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 8.595108794295074}
done in step count: 4
reward sum = 0.9339698847586724
running average episode reward sum: 0.6684199456031445
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.11509051, 2.69244446, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.1236298920209475}
episode index:1644
target Thresh 11.483666417805757
target distance 1.0
model initialize at round 1644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([1.11637485, 6.38024793, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 2.3351933634043167}
done in step count: 28
reward sum = 0.7039203684121375
running average episode reward sum: 0.6684415264072837
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.84176768, 5.08752069, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8463053258678022}
episode index:1645
target Thresh 11.487423645211726
target distance 4.0
model initialize at round 1645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 4.274333  , 10.99765348,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.725668009869596}
done in step count: 1
reward sum = 0.9849547380595544
running average episode reward sum: 0.6686338187594418
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.274333  , 10.61129624,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8232151665090954}
episode index:1646
target Thresh 11.491178994473568
target distance 5.0
model initialize at round 1646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 8.00020186, 11.85854715,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.120627247519554}
done in step count: 1
reward sum = 0.9811383955960776
running average episode reward sum: 0.668823560457582
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.99985846, 11.23474814,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0270460742462606}
episode index:1647
target Thresh 11.49493246653012
target distance 7.0
model initialize at round 1647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([10.95348144, 11.14229119,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 5.482404687203581}
done in step count: 8
reward sum = 0.9075264855524161
running average episode reward sum: 0.6689684044655279
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.88910563,  9.70151527,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.1325336607359922}
episode index:1648
target Thresh 11.49868406231975
target distance 1.0
model initialize at round 1648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.07417321,  2.86594383,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.153208096975536}
done in step count: 0
reward sum = 0.9991211356651399
running average episode reward sum: 0.6691686183716524
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.07417321,  2.86594383,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.153208096975536}
episode index:1649
target Thresh 11.502433782780356
target distance 10.0
model initialize at round 1649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([12.        ,  8.91038841,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 8.904557120810544}
done in step count: 4
reward sum = 0.9335684354021239
running average episode reward sum: 0.6693288606850042
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.88986696, 5.99026756, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.996372983081431}
episode index:1650
target Thresh 11.506181628849367
target distance 2.0
model initialize at round 1650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.99921679, 11.        ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9992167949676558}
done in step count: 0
reward sum = 0.9962488250676164
running average episode reward sum: 0.6695268739886884
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.99921679, 11.        ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9992167949676558}
episode index:1651
target Thresh 11.509927601463747
target distance 11.0
model initialize at round 1651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.14223988,  5.96823298,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.914218943312228}
done in step count: 36
reward sum = 0.6113079818748631
running average episode reward sum: 0.6694916325285711
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.67614696, 9.0748013 , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1459351365200603}
episode index:1652
target Thresh 11.513671701559991
target distance 6.0
model initialize at round 1652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.05916226,  4.3660177 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 6.718001626814433}
done in step count: 28
reward sum = 0.7111250162308995
running average episode reward sum: 0.6695168190885846
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.3594487 , 11.25642611,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.4415401671388574}
episode index:1653
target Thresh 11.51741393007412
target distance 8.0
model initialize at round 1653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 9.60803688, 11.        ,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 8.178155360616916}
done in step count: 9
reward sum = 0.8915197844515654
running average episode reward sum: 0.6696510409539793
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.99376153, 7.28047128, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7195557596660453}
episode index:1654
target Thresh 11.52115428794169
target distance 2.0
model initialize at round 1654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.53472692, 6.40308344, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.6633164248102532}
done in step count: 17
reward sum = 0.8162696814520958
running average episode reward sum: 0.6697396322775432
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.20986276, 7.67725981, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8535092800185966}
episode index:1655
target Thresh 11.524892776097795
target distance 7.0
model initialize at round 1655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 5.459776886690954}
done in step count: 2
reward sum = 0.9622794006099732
running average episode reward sum: 0.6699162867270192
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.03947577, 8.13987485, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.289349476177342}
episode index:1656
target Thresh 11.528629395477058
target distance 11.0
model initialize at round 1656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([4., 4., 0.]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 10.295630140987026}
done in step count: 19
reward sum = 0.771342609075343
running average episode reward sum: 0.6699774975431619
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.96926366,  8.53752406,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.4634961892641605}
episode index:1657
target Thresh 11.532364147013627
target distance 5.0
model initialize at round 1657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 5.        , 11.57264207,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.3872116959382135}
done in step count: 2
reward sum = 0.969641884299222
running average episode reward sum: 0.6701582360152706
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.13268433, 10.01269724,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8674086032894895}
episode index:1658
target Thresh 11.536097031641194
target distance 11.0
model initialize at round 1658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.89496394, 5.65964076, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.240418016785735}
done in step count: 16
reward sum = 0.8000819699926708
running average episode reward sum: 0.6702365505022972
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.7136199 ,  4.72713592,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7814986916513365}
episode index:1659
target Thresh 11.53982805029298
target distance 4.0
model initialize at round 1659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.06456262, 8.        , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.2079499754914633}
done in step count: 1
reward sum = 0.9823113055194665
running average episode reward sum: 0.6704245473426691
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.26074737, 10.        ,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.26074737310411944}
episode index:1660
target Thresh 11.54355720390174
target distance 10.0
model initialize at round 1660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([ 4.18616935, 11.87423137,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 9.270642738799209}
done in step count: 28
reward sum = 0.6924701109049556
running average episode reward sum: 0.6704378198071858
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.57853762,  9.20712515,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.4696076667877609}
episode index:1661
target Thresh 11.547284493399765
target distance 5.0
model initialize at round 1661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.86179903,  8.0204422 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.1409821196186614}
done in step count: 12
reward sum = 0.8673368986663528
running average episode reward sum: 0.6705562909737677
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.88812709,  5.38319045,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9672665837693215}
episode index:1662
target Thresh 11.55100991971887
target distance 5.0
model initialize at round 1662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.87547947,  6.201193  ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 3.8983841482138306}
done in step count: 2
reward sum = 0.9656599958818123
running average episode reward sum: 0.6707337435924736
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86857146, 10.12370879,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8773370195940088}
episode index:1663
target Thresh 11.55473348379042
target distance 9.0
model initialize at round 1663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 9.560133908571554}
done in step count: 5
reward sum = 0.9211576141759779
running average episode reward sum: 0.6708842387070071
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.06099255, 10.73358246,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.7361136539093572}
episode index:1664
target Thresh 11.558455186545299
target distance 8.0
model initialize at round 1664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([3.02386904, 8.28068316, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 6.363590869969782}
done in step count: 4
reward sum = 0.950717092325445
running average episode reward sum: 0.6710523064869581
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.7616539, 2.037148 , 0.       ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7625592681384873}
episode index:1665
target Thresh 11.562175028913938
target distance 3.0
model initialize at round 1665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.        ,  9.09627193,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.3478591991246232}
done in step count: 2
reward sum = 0.9753503225892779
running average episode reward sum: 0.6712349583573677
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.13987637,  9.58608717,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9545347031155812}
episode index:1666
target Thresh 11.565893011826295
target distance 4.0
model initialize at round 1666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.        ,  9.26302814,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.6489755049132273}
done in step count: 1
reward sum = 0.9810367372193959
running average episode reward sum: 0.6714208022559053
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.        , 10.81865525,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.18134474754321417}
episode index:1667
target Thresh 11.569609136211866
target distance 2.0
model initialize at round 1667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.39472681,  3.89679396,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.8387880287961453}
done in step count: 2
reward sum = 0.976666176753499
running average episode reward sum: 0.6716038030799446
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.07157928,  3.60019565,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.1055314793164295}
episode index:1668
target Thresh 11.573323402999682
target distance 1.0
model initialize at round 1668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.12058073,  6.24123349,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.8948378287570504}
done in step count: 12
reward sum = 0.8635467298855017
running average episode reward sum: 0.6717188078293787
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.7033309,  5.1250538,  0.       ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9238741279264058}
episode index:1669
target Thresh 11.577035813118313
target distance 7.0
model initialize at round 1669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.        , 8.57217717, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.486598589005165}
done in step count: 4
reward sum = 0.9470557206976471
running average episode reward sum: 0.6718836802322937
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.53609788, 3.9091754 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0554623830575374}
episode index:1670
target Thresh 11.580746367495856
target distance 8.0
model initialize at round 1670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.56949646, 8.09458244, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.313426131333802}
done in step count: 7
reward sum = 0.9033991538746149
running average episode reward sum: 0.672022229288932
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5569669 , 3.13627266, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.4635176026944509}
episode index:1671
target Thresh 11.584455067059952
target distance 11.0
model initialize at round 1671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([11.        ,  9.01885885,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 10.827126204873135}
done in step count: 5
reward sum = 0.9210355158870411
running average episode reward sum: 0.672171160680438
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.13266141, 2.9617143 , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8681831735479926}
episode index:1672
target Thresh 11.588161912737776
target distance 8.0
model initialize at round 1672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([8.32985628, 9.        , 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 8.971667475557997}
done in step count: 6
reward sum = 0.913569669576524
running average episode reward sum: 0.6723154514807345
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.66217589,  2.32883857,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9428332689201062}
episode index:1673
target Thresh 11.59186690545604
target distance 11.0
model initialize at round 1673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([6., 9., 0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 9.000000000000018}
done in step count: 5
reward sum = 0.9264069678358446
running average episode reward sum: 0.6724672385275416
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.81462383,  9.06401387,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8171350954256293}
episode index:1674
target Thresh 11.595570046140992
target distance 3.0
model initialize at round 1674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([13.51364017,  9.51519173,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 2.565893490261987}
done in step count: 9
reward sum = 0.9000362355586068
running average episode reward sum: 0.6726031006153214
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.19915513,  8.12562362,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8967702140153805}
episode index:1675
target Thresh 11.599271335718417
target distance 1.0
model initialize at round 1675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.        ,  7.59409213,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0792410288676137}
done in step count: 0
reward sum = 0.9955175674340114
running average episode reward sum: 0.6727957703449268
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.        ,  7.59409213,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0792410288676137}
episode index:1676
target Thresh 11.602970775113638
target distance 6.0
model initialize at round 1676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.        , 3.31576216, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 4.684237837791439}
done in step count: 9
reward sum = 0.8890947008698546
running average episode reward sum: 0.6729247500291993
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.84993917, 8.81169384, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8254484505221767}
episode index:1677
target Thresh 11.606668365251513
target distance 6.0
model initialize at round 1677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([11., 11.,  0.]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 4.123105625617674}
done in step count: 6
reward sum = 0.9258307119799734
running average episode reward sum: 0.6730754687192771
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.08643534, 10.8364864 ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.840940287645902}
episode index:1678
target Thresh 11.61036410705644
target distance 2.0
model initialize at round 1678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.46554792, 11.3724252 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6514135054722886}
done in step count: 0
reward sum = 0.9969764635877212
running average episode reward sum: 0.6732683817596992
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.46554792, 11.3724252 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6514135054722886}
episode index:1679
target Thresh 11.61405800145236
target distance 2.0
model initialize at round 1679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.3211273,  8.       ,  0.       ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.32112729549413643}
done in step count: 0
reward sum = 0.994886039715512
running average episode reward sum: 0.6734598208418158
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.3211273,  8.       ,  0.       ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.32112729549413643}
episode index:1680
target Thresh 11.617750049362735
target distance 7.0
model initialize at round 1680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([13.01633036, 10.05722857,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 5.016656795420735}
done in step count: 8
reward sum = 0.9058433375196439
running average episode reward sum: 0.6735980620771982
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.97968638, 9.31568052, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.1950224068901012}
episode index:1681
target Thresh 11.621440251710592
target distance 3.0
model initialize at round 1681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.00040858,  7.25773493,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.6065740457647704}
done in step count: 1
reward sum = 0.9814283146380276
running average episode reward sum: 0.6737810764960809
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.10005195,  5.46136785,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.5478457681445679}
episode index:1682
target Thresh 11.625128609418466
target distance 11.0
model initialize at round 1682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([12.        ,  9.34662259,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.468351020089141}
done in step count: 7
reward sum = 0.9068231356112516
running average episode reward sum: 0.6739195447427327
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.55646028, 4.4063567 , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.6015423935585065}
episode index:1683
target Thresh 11.628815123408458
target distance 11.0
model initialize at round 1683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.12942211,  6.13087975,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.374806484400851}
done in step count: 23
reward sum = 0.7355970980425954
running average episode reward sum: 0.6739561703682077
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.50146363, 4.2551046 , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.5600150606384549}
episode index:1684
target Thresh 11.63249979460219
target distance 3.0
model initialize at round 1684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([12.        , 10.20689809,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.0211791326596915}
done in step count: 1
reward sum = 0.9843194076635188
running average episode reward sum: 0.6741403622004303
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.1580978 ,  9.01102602,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.0015310474904857}
episode index:1685
target Thresh 11.636182623920833
target distance 11.0
model initialize at round 1685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.51886666,  7.        ,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.51886665821072}
done in step count: 5
reward sum = 0.9233450627452915
running average episode reward sum: 0.6742881704451188
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.07545234, 6.24231773, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.761429894182153}
episode index:1686
target Thresh 11.639863612285096
target distance 10.0
model initialize at round 1686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([6., 9., 0.]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 8.944271909999184}
done in step count: 4
reward sum = 0.9311900409465235
running average episode reward sum: 0.674440453711569
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.11835089,  5.98204971,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9891554783853168}
episode index:1687
target Thresh 11.643542760615222
target distance 4.0
model initialize at round 1687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([15.14844418, 11.09500384,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.1498772131416386}
done in step count: 6
reward sum = 0.935605225481391
running average episode reward sum: 0.6745951721782573
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.2751843 , 11.87247906,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9148475897032319}
episode index:1688
target Thresh 11.647220069830999
target distance 7.0
model initialize at round 1688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([10.11669987, 11.8714989 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.1903880254729495}
done in step count: 4
reward sum = 0.9477643977121695
running average episode reward sum: 0.6747569064740145
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.8457848 , 11.82909438,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.1843772261453243}
episode index:1689
target Thresh 11.650895540851756
target distance 8.0
model initialize at round 1689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.87662395, 3.81683756, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.461672783396965}
done in step count: 3
reward sum = 0.9545407279853892
running average episode reward sum: 0.6749224590311219
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.40169577, 9.79209668, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.452308835886061}
episode index:1690
target Thresh 11.65456917459636
target distance 2.0
model initialize at round 1690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.74203748,  7.        ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7420374751090648}
done in step count: 0
reward sum = 0.9965870355633559
running average episode reward sum: 0.6751126805429682
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.74203748,  7.        ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7420374751090648}
episode index:1691
target Thresh 11.65824097198322
target distance 10.0
model initialize at round 1691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.246211251235342}
done in step count: 4
reward sum = 0.9319479266987085
running average episode reward sum: 0.675264474423675
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.37975996, 11.9029478 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9795572237855267}
episode index:1692
target Thresh 11.661910933930283
target distance 7.0
model initialize at round 1692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([12.24916213, 11.73625597,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 8.597583451291765}
done in step count: 42
reward sum = 0.5745028882412072
running average episode reward sum: 0.6752049578340811
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.86530926,  4.0863897 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8696110035973064}
episode index:1693
target Thresh 11.665579061355043
target distance 1.0
model initialize at round 1693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.62493086, 9.29216886, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.9442264066065573}
done in step count: 0
reward sum = 0.9975195808554292
running average episode reward sum: 0.6753952262065848
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.62493086, 9.29216886, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.9442264066065573}
episode index:1694
target Thresh 11.669245355174528
target distance 11.0
model initialize at round 1694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([11.        ,  9.38045573,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 10.009415188862882}
done in step count: 4
reward sum = 0.933902164500153
running average episode reward sum: 0.6755477376746045
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.86798957, 4.96839017, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8685649499487702}
episode index:1695
target Thresh 11.672909816305316
target distance 5.0
model initialize at round 1695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([9.55730081, 9.61984205, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 6.588870074902078}
done in step count: 13
reward sum = 0.849431962473345
running average episode reward sum: 0.6756502637505472
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.08503335, 11.86718583,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.260624936945039}
episode index:1696
target Thresh 11.676572445663519
target distance 5.0
model initialize at round 1696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.64717364, 4.        , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 3.422452484778062}
done in step count: 2
reward sum = 0.9684053416478982
running average episode reward sum: 0.6758227770551419
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.43462807, 8.        , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.0903676254873582}
episode index:1697
target Thresh 11.680233244164796
target distance 11.0
model initialize at round 1697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([12.04312424, 11.77653307,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.121133608440896}
done in step count: 7
reward sum = 0.9148910472146644
running average episode reward sum: 0.6759635710893938
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.07349823, 7.63057377, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.6348427098895367}
episode index:1698
target Thresh 11.683892212724347
target distance 10.0
model initialize at round 1698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([11.        ,  8.70939314,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 8.321471010004178}
done in step count: 7
reward sum = 0.909495293106583
running average episode reward sum: 0.676101023544966
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.95998968, 11.48536478,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.4870110837536027}
episode index:1699
target Thresh 11.687549352256909
target distance 1.0
model initialize at round 1699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.03228939, 3.11842537, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8821657633317435}
done in step count: 0
reward sum = 0.9992871337254774
running average episode reward sum: 0.6762911330215428
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.03228939, 3.11842537, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8821657633317435}
episode index:1700
target Thresh 11.691204663676773
target distance 1.0
model initialize at round 1700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.08584356, 8.8375653 , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.5906950269057791}
done in step count: 15
reward sum = 0.8363256297878111
running average episode reward sum: 0.676385215618113
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.36363874, 10.97325914,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.038973766618564}
episode index:1701
target Thresh 11.694858147897767
target distance 4.0
model initialize at round 1701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 5.96314073, 11.24322049,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 2.0513293031657773}
done in step count: 2
reward sum = 0.973856877775623
running average episode reward sum: 0.6765599933279589
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.52627732, 11.28334569,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.5977061093599059}
episode index:1702
target Thresh 11.698509805833258
target distance 2.0
model initialize at round 1702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.00047017, 5.21228516, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.21228568169074724}
done in step count: 0
reward sum = 0.9976030567408637
running average episode reward sum: 0.6767485095131691
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.00047017, 5.21228516, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.21228568169074724}
episode index:1703
target Thresh 11.702159638396163
target distance 5.0
model initialize at round 1703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.54174447,  6.24880707,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 3.790110216036119}
done in step count: 6
reward sum = 0.92962997794032
running average episode reward sum: 0.6768969141307907
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.24892142, 10.44380975,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5088506338318164}
episode index:1704
target Thresh 11.705807646498936
target distance 6.0
model initialize at round 1704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([5., 9., 0.]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.123105625617685}
done in step count: 2
reward sum = 0.9679798067676773
running average episode reward sum: 0.6770676372349765
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.47098935, 5.        , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.4709893465042896}
episode index:1705
target Thresh 11.709453831053587
target distance 2.0
model initialize at round 1705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.        ,  8.31126446,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.31126445531851843}
done in step count: 0
reward sum = 0.9966442325044103
running average episode reward sum: 0.6772549623201286
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.        ,  8.31126446,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.31126445531851843}
episode index:1706
target Thresh 11.713098192971659
target distance 3.0
model initialize at round 1706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 5.39546597, 10.04652494,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.477012234819131}
done in step count: 1
reward sum = 0.9844488487056539
running average episode reward sum: 0.6774349235892473
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.29842031, 8.38702005, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.4887117777175419}
episode index:1707
target Thresh 11.716740733164237
target distance 4.0
model initialize at round 1707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.43252915, 6.00352275, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 2.0823368096015797}
done in step count: 1
reward sum = 0.9832242443306087
running average episode reward sum: 0.6776139571494002
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.42170864, 4.02060401, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.42221168311437784}
episode index:1708
target Thresh 11.720381452541963
target distance 9.0
model initialize at round 1708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 9.074346404723803}
done in step count: 23
reward sum = 0.7380552905759469
running average episode reward sum: 0.6776493236405802
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.72612749,  9.00192574,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.2738792776979412}
episode index:1709
target Thresh 11.72402035201501
target distance 2.0
model initialize at round 1709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([5.27297966, 7.25922469, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.2991053272533257}
done in step count: 2
reward sum = 0.9767010741961686
running average episode reward sum: 0.6778242077052326
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.99421495, 6.80769577, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.01264223324078}
episode index:1710
target Thresh 11.72765743249311
target distance 4.0
model initialize at round 1710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([5., 9., 0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.0000000000000178}
done in step count: 5
reward sum = 0.9373659723047018
running average episode reward sum: 0.6779758978072777
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.48934054, 10.65985715,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.5959457387318928}
episode index:1711
target Thresh 11.731292694885534
target distance 3.0
model initialize at round 1711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([11.23692942, 10.59483606,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 3.2911308060012963}
done in step count: 10
reward sum = 0.8825978183369131
running average episode reward sum: 0.6780954199571199
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.0389745 , 10.23452656,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.23774296999733396}
episode index:1712
target Thresh 11.73492614010109
target distance 2.0
model initialize at round 1712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.13259695, 8.00494245, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8674171277527281}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.6782798359409192
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.13259695, 8.00494245, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8674171277527281}
episode index:1713
target Thresh 11.738557769048144
target distance 10.0
model initialize at round 1713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.88972282, 3.43207137, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.191325838336942}
done in step count: 64
reward sum = 0.41206217712954685
running average episode reward sum: 0.6781245164200259
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.5153445 ,  8.73832328,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9003894867281698}
episode index:1714
target Thresh 11.7421875826346
target distance 5.0
model initialize at round 1714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([13.15175268,  6.60391203,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 5.363810569393695}
done in step count: 6
reward sum = 0.9288392015317287
running average episode reward sum: 0.6782707057407907
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.8358932 , 9.61351746, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.4198806962422825}
episode index:1715
target Thresh 11.745815581767918
target distance 4.0
model initialize at round 1715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([2.20710504, 7.35735786, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 4.590109368789993}
done in step count: 3
reward sum = 0.9627783221715766
running average episode reward sum: 0.6784365027200627
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.27030534, 10.09403485,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9454299690318836}
episode index:1716
target Thresh 11.74944176735509
target distance 6.0
model initialize at round 1716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.74962461,  9.        ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.190875638342168}
done in step count: 17
reward sum = 0.8131716603301405
running average episode reward sum: 0.678514973982503
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.51675963,  4.23814706,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9205761289042342}
episode index:1717
target Thresh 11.753066140302668
target distance 7.0
model initialize at round 1717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.        , 8.00743234, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 5.392066269572071}
done in step count: 5
reward sum = 0.9319631662987656
running average episode reward sum: 0.6786624991235487
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.87492631, 2.21022197, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.1786625427214972}
episode index:1718
target Thresh 11.756688701516744
target distance 6.0
model initialize at round 1718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([7.22042147, 8.12064111, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 5.22181526287066}
done in step count: 3
reward sum = 0.9594839251188843
running average episode reward sum: 0.6788258623731096
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.21585223, 7.93967491, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.2241234062725353}
episode index:1719
target Thresh 11.760309451902957
target distance 8.0
model initialize at round 1719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.        , 4.49559569, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.849139810245287}
done in step count: 4
reward sum = 0.9382757531257032
running average episode reward sum: 0.6789767053328495
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.89199202, 10.39789236,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6117183456942846}
episode index:1720
target Thresh 11.763928392366495
target distance 11.0
model initialize at round 1720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 6.        , 10.97621119,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 9.052788978426381}
done in step count: 27
reward sum = 0.7106125561183575
running average episode reward sum: 0.6789950875819986
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.43765936, 10.75348311,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9401934845302415}
episode index:1721
target Thresh 11.767545523812094
target distance 3.0
model initialize at round 1721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 8.02232742, 11.24950472,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.6144396530471956}
done in step count: 1
reward sum = 0.9837036864502664
running average episode reward sum: 0.6791720379878454
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.02232742, 10.56663802,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.1300098744506297}
episode index:1722
target Thresh 11.771160847144035
target distance 11.0
model initialize at round 1722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([5., 9., 0.]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.219544457292901}
done in step count: 46
reward sum = 0.5514073846555224
running average episode reward sum: 0.6790978855483025
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.34267489,  7.57990163,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8765627156517911}
episode index:1723
target Thresh 11.77477436326615
target distance 5.0
model initialize at round 1723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 9.99505755, 11.86740305,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.1276296443239566}
done in step count: 2
reward sum = 0.9681534932113867
running average episode reward sum: 0.6792655512140003
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.77781823, 11.84591187,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.14915973884329}
episode index:1724
target Thresh 11.778386073081819
target distance 3.0
model initialize at round 1724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 8.01100802, 10.62580276,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.1890190552044833}
done in step count: 1
reward sum = 0.9841382073913478
running average episode reward sum: 0.6794422889856974
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.19212145, 9.87873423, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.22719163011963556}
episode index:1725
target Thresh 11.781995977493969
target distance 10.0
model initialize at round 1725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 5.90338846, 11.87012936,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 8.30978352415879}
done in step count: 7
reward sum = 0.9074438380004284
running average episode reward sum: 0.6795743872180351
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.92811331,  9.4782941 ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.526635304801031}
episode index:1726
target Thresh 11.785604077405077
target distance 8.0
model initialize at round 1726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([13.12887323,  3.19284652,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 7.961465479859136}
done in step count: 8
reward sum = 0.8843924112864369
running average episode reward sum: 0.6796929848000086
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.50738987, 10.59301073,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.7804525663082735}
episode index:1727
target Thresh 11.789210373717165
target distance 10.0
model initialize at round 1727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([10.        , 11.53126526,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.695404519561556}
done in step count: 10
reward sum = 0.8654587069639399
running average episode reward sum: 0.679800488111446
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.0959982 , 3.39032776, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9846700028451842}
episode index:1728
target Thresh 11.792814867331808
target distance 6.0
model initialize at round 1728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([10.81831133,  9.0399344 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 5.568772735345925}
done in step count: 5
reward sum = 0.9309754290231861
running average episode reward sum: 0.6799457599107009
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.8884732,  6.3128297,  0.       ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.1232041861688522}
episode index:1729
target Thresh 11.796417559150134
target distance 10.0
model initialize at round 1729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([16.29035449,  9.93367159,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 10.290568254140064}
done in step count: 33
reward sum = 0.6474694834039944
running average episode reward sum: 0.6799269874965351
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.17768416, 9.81001004, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.2601304419636956}
episode index:1730
target Thresh 11.800018450072809
target distance 7.0
model initialize at round 1730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([4.6352126 , 3.73031067, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 5.775971351138341}
done in step count: 6
reward sum = 0.9221713178707212
running average episode reward sum: 0.6800669322281205
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.7558979 , 8.14369545, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.8904174946973228}
episode index:1731
target Thresh 11.803617541000058
target distance 6.0
model initialize at round 1731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([3.20167029, 8.        , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 4.176602863998522}
done in step count: 11
reward sum = 0.8672326481211398
running average episode reward sum: 0.6801749955744791
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.13434699, 4.84360969, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.2087317550228105}
episode index:1732
target Thresh 11.807214832831658
target distance 8.0
model initialize at round 1732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 4.        , 10.45879984,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 6.017515873870609}
done in step count: 34
reward sum = 0.6460234047871791
running average episode reward sum: 0.6801552889439038
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.41419629, 10.67370403,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.7908449260669476}
episode index:1733
target Thresh 11.810810326466926
target distance 2.0
model initialize at round 1733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.70906138,  8.        ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.709061384201009}
done in step count: 0
reward sum = 0.9966229243381461
running average episode reward sum: 0.6803377962307516
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.70906138,  8.        ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.709061384201009}
episode index:1734
target Thresh 11.814404022804741
target distance 2.0
model initialize at round 1734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.65662503,  8.91726315,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 2.1837923936718338}
done in step count: 8
reward sum = 0.9090594048996641
running average episode reward sum: 0.6804696242472755
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.15295336, 11.35127761,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9169972586359926}
episode index:1735
target Thresh 11.817995922743524
target distance 8.0
model initialize at round 1735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.64110768, 10.10370612,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 7.232511942715284}
done in step count: 15
reward sum = 0.8348459219594592
running average episode reward sum: 0.6805585506860498
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.65525444,  2.21936713,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0191889247342654}
episode index:1736
target Thresh 11.821586027181246
target distance 2.0
model initialize at round 1736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([9.61769152, 9.40449226, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.7238199770665081}
done in step count: 1
reward sum = 0.9857648473467308
running average episode reward sum: 0.6807342595499879
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.61769152, 10.58390188,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.6979263458790296}
episode index:1737
target Thresh 11.825174337015442
target distance 3.0
model initialize at round 1737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([13.58311343,  8.58008575,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 2.1265946024647753}
done in step count: 2
reward sum = 0.9742334089012684
running average episode reward sum: 0.6809031313275203
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.40261835,  9.9219099 ,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.41012144369365583}
episode index:1738
target Thresh 11.828760853143182
target distance 7.0
model initialize at round 1738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4.        , 5.91754901, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 6.4549520565489775}
done in step count: 5
reward sum = 0.926371412657822
running average episode reward sum: 0.6810442861758988
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.47135254, 10.96892123,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.0774885469516828}
episode index:1739
target Thresh 11.8323455764611
target distance 10.0
model initialize at round 1739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.109325747416417}
done in step count: 5
reward sum = 0.9229075476804355
running average episode reward sum: 0.6811832880503267
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.08250979, 11.34310545,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.35288697197997027}
episode index:1740
target Thresh 11.835928507865374
target distance 8.0
model initialize at round 1740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([16.87170314, 10.78422989,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 8.906297386902086}
done in step count: 27
reward sum = 0.7203098962614803
running average episode reward sum: 0.6812057616908844
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.67640037, 9.01384911, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.1958306947959083}
episode index:1741
target Thresh 11.839509648251738
target distance 5.0
model initialize at round 1741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([6.99505019, 8.13259643, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 3.12763685364986}
done in step count: 2
reward sum = 0.968390666273077
running average episode reward sum: 0.6813706209931704
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.46279313,  9.73882334,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.8718012378243545}
episode index:1742
target Thresh 11.843088998515476
target distance 1.0
model initialize at round 1742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.46551454, 9.58420157, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.6719358044877566}
done in step count: 2
reward sum = 0.9747962286844893
running average episode reward sum: 0.6815389661496198
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.14022014, 8.03526563, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8605028005103148}
episode index:1743
target Thresh 11.846666559551426
target distance 9.0
model initialize at round 1743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([10.        ,  9.14956093,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.005809385957383}
done in step count: 16
reward sum = 0.8073114872670005
running average episode reward sum: 0.6816110834209027
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.28392014, 2.19032575, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.3418106752801983}
episode index:1744
target Thresh 11.85024233225398
target distance 7.0
model initialize at round 1744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([4.51039982, 7.        , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.59482861652843}
done in step count: 3
reward sum = 0.9529827625202242
running average episode reward sum: 0.6817665972771201
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.3683761 , 1.10310648, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9695973028414997}
episode index:1745
target Thresh 11.85381631751708
target distance 8.0
model initialize at round 1745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([10.,  9.,  0.]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 6.324555320336783}
done in step count: 4
reward sum = 0.9370930318347972
running average episode reward sum: 0.6819128323484589
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.86926452,  7.12958715,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8788706590467219}
episode index:1746
target Thresh 11.85738851623422
target distance 1.0
model initialize at round 1746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.58736348, 9.        , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1597395653443665}
done in step count: 0
reward sum = 0.9968722985361025
running average episode reward sum: 0.6820931182478223
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.58736348, 9.        , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1597395653443665}
episode index:1747
target Thresh 11.860958929298453
target distance 11.0
model initialize at round 1747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([12.54457903, 11.30203867,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.63297947363701}
done in step count: 25
reward sum = 0.7257692204194196
running average episode reward sum: 0.6821181045762956
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.67413784, 9.14701604, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.913108860598244}
episode index:1748
target Thresh 11.864527557602381
target distance 6.0
model initialize at round 1748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([12.,  9.,  0.]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 5.656854249492408}
done in step count: 2
reward sum = 0.963205552605416
running average episode reward sum: 0.6822788178113037
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.7440145,  5.076859 ,  0.       ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.26727491629123473}
episode index:1749
target Thresh 11.868094402038162
target distance 8.0
model initialize at round 1749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4.62761965, 3.05961705, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 8.2028425252914}
done in step count: 4
reward sum = 0.9372680772444844
running average episode reward sum: 0.6824245259595512
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.73722526, 10.74865305,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.7934304981077406}
episode index:1750
target Thresh 11.871659463497506
target distance 5.0
model initialize at round 1750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.0338003,  5.       ,  0.       ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 3.000190404048764}
done in step count: 4
reward sum = 0.9502989816241898
running average episode reward sum: 0.6825775096578177
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.66363543,  2.41781187,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7842058063768618}
episode index:1751
target Thresh 11.87522274287168
target distance 5.0
model initialize at round 1751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.6695739, 8.1888864, 0.       ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 4.201898465152716}
done in step count: 32
reward sum = 0.6633834386107703
running average episode reward sum: 0.6825665541378136
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.73550075, 4.77454047, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0681171761156762}
episode index:1752
target Thresh 11.878784241051502
target distance 10.0
model initialize at round 1752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([6.48931944, 9.89011335, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.950129933410624}
done in step count: 31
reward sum = 0.6904139117931938
running average episode reward sum: 0.6825710306681362
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.333107  ,  2.65970851,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7486952487401498}
episode index:1753
target Thresh 11.882343958927345
target distance 6.0
model initialize at round 1753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([10.       , 10.1643188,  0.       ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 4.166009873724471}
done in step count: 9
reward sum = 0.8922496672172215
running average episode reward sum: 0.6826905737904561
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.28287905, 8.77551889, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.3611264665194124}
episode index:1754
target Thresh 11.885901897389141
target distance 4.0
model initialize at round 1754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 6.91445792, 10.8765378 ,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.011396178411606}
done in step count: 20
reward sum = 0.7794451356343565
running average episode reward sum: 0.6827457045949256
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.67465523, 10.6438181 ,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9325563923538069}
episode index:1755
target Thresh 11.889458057326376
target distance 9.0
model initialize at round 1755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([14.41923136, 11.83554109,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 8.883903987234314}
done in step count: 28
reward sum = 0.7002991909479366
running average episode reward sum: 0.6827557008855594
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.42695487, 9.80370197, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9870752655154327}
episode index:1756
target Thresh 11.893012439628087
target distance 3.0
model initialize at round 1756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([14.11013818,  8.9566648 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.7203152112975464}
done in step count: 3
reward sum = 0.9658007475902447
running average episode reward sum: 0.6829167965296714
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.04142046,  7.35401395,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.35642885828753995}
episode index:1757
target Thresh 11.896565045182872
target distance 10.0
model initialize at round 1757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([12.85351315,  7.11185932,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 8.89794858151328}
done in step count: 17
reward sum = 0.7964939225554898
running average episode reward sum: 0.6829814024034062
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.3944531 , 7.90869461, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.6123918021783853}
episode index:1758
target Thresh 11.90011587487888
target distance 4.0
model initialize at round 1758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([6.        , 9.59623456, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 2.08698242698727}
done in step count: 1
reward sum = 0.9813889238783798
running average episode reward sum: 0.6831510485213567
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.        , 8.60461843, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.39538156986237105}
episode index:1759
target Thresh 11.90366492960382
target distance 1.0
model initialize at round 1759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 8.81554592, 10.80437511,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.822333769245796}
done in step count: 2
reward sum = 0.9723668418841646
running average episode reward sum: 0.6833153756766766
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.27173865, 10.45097759,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.912025327979527}
episode index:1760
target Thresh 11.90721221024496
target distance 3.0
model initialize at round 1760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.53912199, 9.58080053, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.6466143660169714}
done in step count: 1
reward sum = 0.9864653194880476
running average episode reward sum: 0.6834875221524355
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.88511574, 8.79292166, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.1883411297010267}
episode index:1761
target Thresh 11.91075771768911
target distance 2.0
model initialize at round 1761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2., 9., 0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.732141342546872e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.6836637494385042
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2., 9., 0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.732141342546872e-14}
episode index:1762
target Thresh 11.914301452822654
target distance 9.0
model initialize at round 1762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([4.31839108, 7.62506151, 0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 8.040363695124542}
done in step count: 14
reward sum = 0.843300915825717
running average episode reward sum: 0.6837542980297617
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.20019737,  9.10389875,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.2011168568950843}
episode index:1763
target Thresh 11.917843416531523
target distance 1.0
model initialize at round 1763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.86015081, 11.21247083,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.4865883543463634}
done in step count: 2
reward sum = 0.9767464975955021
running average episode reward sum: 0.6839203933809895
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.22256386, 10.86318535,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.8914166336748626}
episode index:1764
target Thresh 11.921383609701213
target distance 11.0
model initialize at round 1764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([5.        , 9.47797489, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.127790555792227}
done in step count: 5
reward sum = 0.928360859005605
running average episode reward sum: 0.6840588865626465
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.42770255, 11.30009719,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5224823423547642}
episode index:1765
target Thresh 11.924922033216765
target distance 11.0
model initialize at round 1765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.29934001, 11.27081585,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.21609807516376}
done in step count: 13
reward sum = 0.8501000754999867
running average episode reward sum: 0.684152907620935
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.86562867, 4.99761668, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.865631950671635}
episode index:1766
target Thresh 11.928458687962792
target distance 2.0
model initialize at round 1766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([15.3248111 ,  6.46341926,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.4035247993756146}
done in step count: 9
reward sum = 0.9055917440128731
running average episode reward sum: 0.6842782267134034
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.9876001 ,  6.20630163,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.2066739479418139}
episode index:1767
target Thresh 11.931993574823451
target distance 6.0
model initialize at round 1767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([11.02099649, 11.86157592,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 4.112265295041119}
done in step count: 4
reward sum = 0.9501847285398691
running average episode reward sum: 0.6844286263185089
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.56300779, 10.54052089,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.7267040799158612}
episode index:1768
target Thresh 11.93552669468247
target distance 6.0
model initialize at round 1768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 3.58953691, 10.77631736,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 4.478264546693158}
done in step count: 3
reward sum = 0.95937832668092
running average episode reward sum: 0.6845840529439258
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.96164274, 9.49726546, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.0851261600180284}
episode index:1769
target Thresh 11.93905804842312
target distance 3.0
model initialize at round 1769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 7.62427211, 11.        ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.3757278919219926}
done in step count: 1
reward sum = 0.9833837233977097
running average episode reward sum: 0.6847528663170636
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.10095775, 11.15778717,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.1873212743308233}
episode index:1770
target Thresh 11.942587636928252
target distance 10.0
model initialize at round 1770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([11.        , 10.65267706,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 8.026580052772896}
done in step count: 5
reward sum = 0.9329120863512322
running average episode reward sum: 0.6848929901002562
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.77862663, 10.67246992,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7079703123264456}
episode index:1771
target Thresh 11.94611546108025
target distance 4.0
model initialize at round 1771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.94835985,  6.42528963,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 2.6434781642675573}
done in step count: 12
reward sum = 0.8648250068776381
running average episode reward sum: 0.6849945318704466
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.1173293 ,  4.66526105,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6755282633301231}
episode index:1772
target Thresh 11.94964152176108
target distance 1.0
model initialize at round 1772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.22843552, 5.63767469, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.8343892907562953}
done in step count: 3
reward sum = 0.9664105871445218
running average episode reward sum: 0.685153254969868
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.35605303, 6.50284167, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.6115064729059794}
episode index:1773
target Thresh 11.953165819852256
target distance 7.0
model initialize at round 1773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.59967697,  7.9009428 ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 6.064819086767891}
done in step count: 4
reward sum = 0.9480347000037078
running average episode reward sum: 0.6853014406773279
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.82683297,  1.58447602,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9253717859974807}
episode index:1774
target Thresh 11.95668835623485
target distance 2.0
model initialize at round 1774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.        , 10.69431448,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.3056855201720978}
done in step count: 0
reward sum = 0.9948469732128842
running average episode reward sum: 0.6854758325266437
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.        , 10.69431448,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.3056855201720978}
episode index:1775
target Thresh 11.960209131789492
target distance 10.0
model initialize at round 1775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 5.97885596, 11.85584935,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 8.23303883602824}
done in step count: 17
reward sum = 0.8108462813253206
running average episode reward sum: 0.6855464239955618
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.96577719,  9.76015993,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.9951124778037305}
episode index:1776
target Thresh 11.963728147396383
target distance 7.0
model initialize at round 1776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.92256284, 9.37196374, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.450606994335422}
done in step count: 4
reward sum = 0.9465558630989142
running average episode reward sum: 0.6856933060659631
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.65424179, 4.55670025, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.6553349585138002}
episode index:1777
target Thresh 11.967245403935276
target distance 9.0
model initialize at round 1777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([11.00000547,  8.23757986,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 8.74255793641563}
done in step count: 5
reward sum = 0.9244945761551225
running average episode reward sum: 0.6858276149917727
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.10029965, 3.40380994, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.4160799030098291}
episode index:1778
target Thresh 11.97076090228548
target distance 7.0
model initialize at round 1778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([9.25159691, 8.11958974, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 7.745754350708723}
done in step count: 22
reward sum = 0.7231140653766733
running average episode reward sum: 0.6858485742106512
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.18673669,  1.12359601,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8960773077215125}
episode index:1779
target Thresh 11.974274643325874
target distance 11.0
model initialize at round 1779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.11220249,  5.67487569,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.644862177839336}
done in step count: 15
reward sum = 0.8065525115778427
running average episode reward sum: 0.6859163854114192
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.48716796, 9.24067952, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.5433776327425762}
episode index:1780
target Thresh 11.977786627934892
target distance 9.0
model initialize at round 1780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([5.94478322, 8.045681  , 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.648796278614387}
done in step count: 4
reward sum = 0.934094265129014
running average episode reward sum: 0.6860557329014347
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.87442884, 11.8657334 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.2304958778021609}
episode index:1781
target Thresh 11.981296856990532
target distance 6.0
model initialize at round 1781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 7.37743652, 10.63186824,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 7.946296424773938}
done in step count: 6
reward sum = 0.9237178404229532
running average episode reward sum: 0.6861891010874738
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.32559079, 4.37489307, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7716039118856735}
episode index:1782
target Thresh 11.98480533137035
target distance 8.0
model initialize at round 1782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([11.32609802,  9.        ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 7.035449930391614}
done in step count: 7
reward sum = 0.9149706905990662
running average episode reward sum: 0.6863174138129431
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.97146383,  3.81936771,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8198644716952603}
episode index:1783
target Thresh 11.98831205195146
target distance 1.0
model initialize at round 1783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.        , 9.80568777, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.0187037069881393}
done in step count: 0
reward sum = 0.996971681599542
running average episode reward sum: 0.6864915473711194
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.        , 9.80568777, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.0187037069881393}
episode index:1784
target Thresh 11.99181701961055
target distance 6.0
model initialize at round 1784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([4.56041431, 9.        , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 4.749286415949154}
done in step count: 18
reward sum = 0.7944345634470505
running average episode reward sum: 0.6865520196490331
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.81876889, 5.34325784, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8878110327843725}
episode index:1785
target Thresh 11.995320235223858
target distance 11.0
model initialize at round 1785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.178937723926285}
done in step count: 20
reward sum = 0.7410104630712158
running average episode reward sum: 0.6865825114986536
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.89146915,  4.06081997,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.12441066964104087}
episode index:1786
target Thresh 11.998821699667188
target distance 9.0
model initialize at round 1786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([8.13414168, 8.50452828, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 8.254961476575048}
done in step count: 11
reward sum = 0.8721382864915006
running average episode reward sum: 0.6866863479703899
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.51996105,  5.97176512,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.4808685914466075}
episode index:1787
target Thresh 12.002321413815906
target distance 9.0
model initialize at round 1787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([14.        ,  3.81080174,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 8.713654717470078}
done in step count: 31
reward sum = 0.6575262271696015
running average episode reward sum: 0.6866700391779957
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.48830083, 8.64182164, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.6055818982937903}
episode index:1788
target Thresh 12.005819378544942
target distance 3.0
model initialize at round 1788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.05399799,  9.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0014568304718612}
done in step count: 1
reward sum = 0.9829121333058286
running average episode reward sum: 0.686835630063478
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.82961643, 11.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.2993319116560027}
episode index:1789
target Thresh 12.009315594728786
target distance 10.0
model initialize at round 1789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([8.        , 8.91852939, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 8.90813519151985}
done in step count: 27
reward sum = 0.7138016434932583
running average episode reward sum: 0.6868506948754499
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.87287084,  5.48381198,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.997986745230311}
episode index:1790
target Thresh 12.012810063241492
target distance 8.0
model initialize at round 1790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([7.97012069, 8.14064636, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 6.136816647120404}
done in step count: 3
reward sum = 0.9544438179245642
running average episode reward sum: 0.6870001047710664
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.94110483,  7.12628686,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.13934494090365748}
episode index:1791
target Thresh 12.016302784956675
target distance 8.0
model initialize at round 1791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([7.41865191, 8.09322718, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 8.154091359504534}
done in step count: 10
reward sum = 0.8751234449791586
running average episode reward sum: 0.6871050843135933
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.47061041, 2.56658274, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7754155918452516}
episode index:1792
target Thresh 12.019793760747522
target distance 10.0
model initialize at round 1792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 2.54188107, 11.91136046,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 9.896061512623758}
done in step count: 5
reward sum = 0.9287822312083236
running average episode reward sum: 0.6872398735756651
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.95040661,  9.84036577,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.8418278476555577}
episode index:1793
target Thresh 12.023282991486772
target distance 3.0
model initialize at round 1793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.92486632,  9.        ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0028185628843007}
done in step count: 1
reward sum = 0.9838251721396921
running average episode reward sum: 0.6874051942549092
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.63850376, 10.70369182,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9501943138770059}
episode index:1794
target Thresh 12.026770478046732
target distance 4.0
model initialize at round 1794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([10.       , 10.9886291,  0.       ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 2.0000323240651205}
done in step count: 1
reward sum = 0.9836516246635281
running average episode reward sum: 0.6875702340490086
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.0000003 , 11.70238676,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7023867568811683}
episode index:1795
target Thresh 12.030256221299275
target distance 4.0
model initialize at round 1795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.1186473, 4.2292805, 0.       ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 2.3971804530418392}
done in step count: 1
reward sum = 0.9816376545994652
running average episode reward sum: 0.6877339686929677
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.21507575, 2.23378035, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8189989821000973}
episode index:1796
target Thresh 12.033740222115837
target distance 11.0
model initialize at round 1796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 9.00000000000002}
done in step count: 5
reward sum = 0.9285037131012934
running average episode reward sum: 0.6878679529692104
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.51956375, 11.87018913,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9940060888424439}
episode index:1797
target Thresh 12.03722248136742
target distance 7.0
model initialize at round 1797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 9.9403336 , 10.11072481,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 8.187997222645418}
done in step count: 8
reward sum = 0.9078941883481573
running average episode reward sum: 0.6879903257363845
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.85378833,  2.78076682,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8814860727505873}
episode index:1798
target Thresh 12.040702999924587
target distance 12.0
model initialize at round 1798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([12.24475706,  9.        ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 10.438153435416789}
done in step count: 8
reward sum = 0.8880122960226476
running average episode reward sum: 0.6881015108227025
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.148182  , 11.10282704,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8580019270875721}
episode index:1799
target Thresh 12.044181778657466
target distance 11.0
model initialize at round 1799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.18772204862523}
done in step count: 8
reward sum = 0.8863899272846938
running average episode reward sum: 0.6882116710540703
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.77626237, 5.95340245, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7776596971127838}
episode index:1800
target Thresh 12.047658818435755
target distance 7.0
model initialize at round 1800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.        , 8.05555964, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 6.137573013415628}
done in step count: 7
reward sum = 0.9178611158505434
running average episode reward sum: 0.6883391832388547
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.36568066, 2.99600079, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.180838089341404}
episode index:1801
target Thresh 12.051134120128713
target distance 3.0
model initialize at round 1801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.48464489,  9.93046832,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.8297727896150784}
done in step count: 2
reward sum = 0.9730887183719246
running average episode reward sum: 0.6884972018488064
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.20184257, 11.42005388,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.4660318518225135}
episode index:1802
target Thresh 12.054607684605163
target distance 7.0
model initialize at round 1802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 8.        , 10.64600563,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 5.012515537496385}
done in step count: 3
reward sum = 0.9532219467894166
running average episode reward sum: 0.6886440264438927
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.96891139, 11.04330724,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9698787551450292}
episode index:1803
target Thresh 12.058079512733498
target distance 5.0
model initialize at round 1803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([11.        ,  9.85298812,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 4.883187225866561}
done in step count: 2
reward sum = 0.9659907497813688
running average episode reward sum: 0.6887977663127051
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.84860229,  6.33975625,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9140898011793153}
episode index:1804
target Thresh 12.061549605381677
target distance 9.0
model initialize at round 1804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([5.24346268, 9.        , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 7.350722740281815}
done in step count: 11
reward sum = 0.8573261028896332
running average episode reward sum: 0.6888911338121937
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.33177212, 1.12251069, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.1029578346342745}
episode index:1805
target Thresh 12.065017963417217
target distance 8.0
model initialize at round 1805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.86520576, 8.17471528, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 6.278126054162694}
done in step count: 3
reward sum = 0.9566339779165443
running average episode reward sum: 0.6890393856638571
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.39341951, 2.27408624, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.4794811494714369}
episode index:1806
target Thresh 12.068484587707216
target distance 1.0
model initialize at round 1806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.56135923,  8.07470751,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 2.0054613835277997}
done in step count: 7
reward sum = 0.9226009030916454
running average episode reward sum: 0.689168639408975
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.92325296,  9.929848  ,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.10397793578255587}
episode index:1807
target Thresh 12.071949479118322
target distance 7.0
model initialize at round 1807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 7.        , 10.08032942,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 5.08387588155543}
done in step count: 18
reward sum = 0.8047670032121685
running average episode reward sum: 0.6892325765570962
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.62463578, 11.20978254,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.4300081553105555}
episode index:1808
target Thresh 12.075412638516767
target distance 4.0
model initialize at round 1808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([14.       ,  9.0102049,  0.       ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.000026034845862}
done in step count: 1
reward sum = 0.9840292847095691
running average episode reward sum: 0.6893955377003536
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.       ,  8.9809019,  0.       ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.01909809559574782}
episode index:1809
target Thresh 12.078874066768332
target distance 6.0
model initialize at round 1809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([10.        , 11.34785199,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 4.638147148885158}
done in step count: 2
reward sum = 0.9697930651506039
running average episode reward sum: 0.6895504534613757
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.        ,  9.51521939,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.5152193903924189}
episode index:1810
target Thresh 12.08233376473838
target distance 6.0
model initialize at round 1810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.61937731, 5.        , 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 4.231562243507813}
done in step count: 2
reward sum = 0.9684508526411868
running average episode reward sum: 0.6897044569948819
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.50877814, 9.01969533, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.4916165369125529}
episode index:1811
target Thresh 12.085791733291831
target distance 7.0
model initialize at round 1811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([6.06249273, 9.4385097 , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.760790149498181}
done in step count: 4
reward sum = 0.9461365059351786
running average episode reward sum: 0.6898459757856877
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.82562384, 2.36596799, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0409857442959916}
episode index:1812
target Thresh 12.089247973293178
target distance 11.0
model initialize at round 1812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.14208245,  6.51438534,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.155118352808437}
done in step count: 9
reward sum = 0.8875081934029343
running average episode reward sum: 0.6899550007264584
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.33833033, 6.43873093, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.5540327048043655}
episode index:1813
target Thresh 12.092702485606486
target distance 8.0
model initialize at round 1813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([8.54512979, 8.08980585, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 8.874180787683638}
done in step count: 8
reward sum = 0.8988477368899338
running average episode reward sum: 0.6900701565898341
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.8874454 ,  2.94669931,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.297612773479798}
episode index:1814
target Thresh 12.096155271095377
target distance 3.0
model initialize at round 1814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([12.96294402, 11.85845475,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.0847985027099916}
done in step count: 10
reward sum = 0.8753450553605966
running average episode reward sum: 0.6901722364238676
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.69535455, 11.56093556,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8934017289099643}
episode index:1815
target Thresh 12.09960633062305
target distance 1.0
model initialize at round 1815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.14352624, 7.02525302, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.2975665554492684}
done in step count: 0
reward sum = 0.9960719151560516
running average episode reward sum: 0.6903406833835218
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.14352624, 7.02525302, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.2975665554492684}
episode index:1816
target Thresh 12.10305566505227
target distance 3.0
model initialize at round 1816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.83361006,  8.38569808,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.6228543685641796}
done in step count: 1
reward sum = 0.9848023401463962
running average episode reward sum: 0.6905027426332538
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.59837154, 10.29124677,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.49611501242757766}
episode index:1817
target Thresh 12.106503275245371
target distance 11.0
model initialize at round 1817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([11.        ,  8.98267531,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 9.053488320677639}
done in step count: 6
reward sum = 0.9183708265576065
running average episode reward sum: 0.6906280826134101
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.16512663, 8.80071503, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.1567878394958542}
episode index:1818
target Thresh 12.109949162064256
target distance 7.0
model initialize at round 1818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 8.01839381, 11.86442674,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 5.092299087527839}
done in step count: 3
reward sum = 0.953084591076659
running average episode reward sum: 0.6907723687642972
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.07886486, 11.16575346,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9359295627799735}
episode index:1819
target Thresh 12.113393326370389
target distance 6.0
model initialize at round 1819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([ 2.08299959, 10.        ,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 6.988912269632944}
done in step count: 21
reward sum = 0.7672833509892291
running average episode reward sum: 0.6908144077655195
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.64879259, 9.50136995, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.8199412547421936}
episode index:1820
target Thresh 12.116835769024824
target distance 7.0
model initialize at round 1820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.68229175,  5.89423192,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 5.7674566963587575}
done in step count: 4
reward sum = 0.9504500129748669
running average episode reward sum: 0.6909569863515763
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.1438521 , 10.00185447,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0084581924946574}
episode index:1821
target Thresh 12.120276490888164
target distance 1.0
model initialize at round 1821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.54585612,  4.70744324,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.4030901206152502}
done in step count: 2
reward sum = 0.9750224262810179
running average episode reward sum: 0.6911128949355112
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.47393632,  5.13555372,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.011933975726635}
episode index:1822
target Thresh 12.123715492820592
target distance 11.0
model initialize at round 1822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([3.4093678 , 8.11947405, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.975377903740613}
done in step count: 30
reward sum = 0.6779420869393679
running average episode reward sum: 0.6911056701368298
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.40871364, 11.89663195,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.98539113769505}
episode index:1823
target Thresh 12.127152775681857
target distance 4.0
model initialize at round 1823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.,  6.,  0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.0000000000000187}
done in step count: 2
reward sum = 0.9690740415418659
running average episode reward sum: 0.6912580650772931
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.61873743,  8.74469697,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9681991424331219}
episode index:1824
target Thresh 12.130588340331279
target distance 4.0
model initialize at round 1824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([13.80208969, 10.        ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 2.975181782230628}
done in step count: 10
reward sum = 0.891308330412362
running average episode reward sum: 0.6913676816610383
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.08969583,  9.94299183,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.310681988640946}
episode index:1825
target Thresh 12.13402218762775
target distance 4.0
model initialize at round 1825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([5.        , 9.05435026, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.6508979021531704}
done in step count: 9
reward sum = 0.8955108796153509
running average episode reward sum: 0.6914794796883956
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.41516546, 6.1287876 , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.5988469612454699}
episode index:1826
target Thresh 12.137454318429732
target distance 2.0
model initialize at round 1826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.39062715, 2.14056993, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.053544175039902}
done in step count: 0
reward sum = 0.9959565067863649
running average episode reward sum: 0.6916461337809506
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.39062715, 2.14056993, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.053544175039902}
episode index:1827
target Thresh 12.140884733595257
target distance 7.0
model initialize at round 1827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 5.0990195135928005}
done in step count: 5
reward sum = 0.9336627173842912
running average episode reward sum: 0.6917785279732938
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.78902692, 9.38941578, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.4428930857167117}
episode index:1828
target Thresh 12.144313433981935
target distance 9.0
model initialize at round 1828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([12.05521678,  8.045681  ,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 7.119466869678083}
done in step count: 4
reward sum = 0.9368798706372644
running average episode reward sum: 0.6919125363618471
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.05522186, 9.56388028, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.1002575582042948}
episode index:1829
target Thresh 12.147740420446931
target distance 1.0
model initialize at round 1829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.12773107,  3.76203862,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.1582555570856061}
done in step count: 0
reward sum = 0.9985915149860609
running average episode reward sum: 0.6920801205031719
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.12773107,  3.76203862,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.1582555570856061}
episode index:1830
target Thresh 12.151165693846998
target distance 4.0
model initialize at round 1830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.        , 9.02650881, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.187437146945014}
done in step count: 11
reward sum = 0.8768140926782978
running average episode reward sum: 0.6921810128964953
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.13285324, 6.72753384, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1319226974770178}
episode index:1831
target Thresh 12.154589255038452
target distance 4.0
model initialize at round 1831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.84218165, 7.00000007, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 2.170085175398437}
done in step count: 2
reward sum = 0.9705075779148892
running average episode reward sum: 0.6923329378774006
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.56809868, 9.74523793, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9370782686565756}
episode index:1832
target Thresh 12.158011104877184
target distance 12.0
model initialize at round 1832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([12.14300156, 11.87329704,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 10.314539372978707}
done in step count: 11
reward sum = 0.8519816613319244
running average episode reward sum: 0.6924200348350953
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.14335657, 10.78012036,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.15863097547338}
episode index:1833
target Thresh 12.161431244218658
target distance 11.0
model initialize at round 1833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([14.02619884, 11.86775028,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 9.217415937331792}
done in step count: 5
reward sum = 0.9285626372124065
running average episode reward sum: 0.6925487930697612
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.01306739, 9.49452231, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.5056465637588727}
episode index:1834
target Thresh 12.164849673917908
target distance 10.0
model initialize at round 1834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([11.35674703,  9.29870629,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 9.397558019935785}
done in step count: 25
reward sum = 0.7276710757891655
running average episode reward sum: 0.6925679332783278
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.60811432, 5.48562859, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.6240268585400269}
episode index:1835
target Thresh 12.168266394829537
target distance 3.0
model initialize at round 1835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 5.09013939, 10.02064323,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.2959536074722133}
done in step count: 1
reward sum = 0.9825632634770674
running average episode reward sum: 0.6927258828045798
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.11518788, 8.84645152, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.2244887337130428}
episode index:1836
target Thresh 12.171681407807734
target distance 3.0
model initialize at round 1836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.31309485,  5.        ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0478684973905799}
done in step count: 39
reward sum = 0.5837652414241798
running average episode reward sum: 0.6926665683563595
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.89323989,  4.62040619,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0875575109115758}
episode index:1837
target Thresh 12.175094713706244
target distance 3.0
model initialize at round 1837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6., 9., 0.]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.0000000000000187}
done in step count: 1
reward sum = 0.9810020588979176
running average episode reward sum: 0.6928234429431613
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([8.        , 8.80601241, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.0186418341395078}
episode index:1838
target Thresh 12.1785063133784
target distance 10.0
model initialize at round 1838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([5.99998179, 8.00003594, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.434015616465787}
done in step count: 6
reward sum = 0.9057929148104228
running average episode reward sum: 0.6929392501600549
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.15001038,  2.84472853,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8640553081434662}
episode index:1839
target Thresh 12.181916207677094
target distance 10.0
model initialize at round 1839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.0594033 , 4.10125959, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.677810615347157}
done in step count: 11
reward sum = 0.8623551041159097
running average episode reward sum: 0.6930313239937266
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.17881773,  8.05682275,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8231458847550226}
episode index:1840
target Thresh 12.185324397454806
target distance 9.0
model initialize at round 1840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([14.3785007 ,  4.84120822,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 9.003077541269896}
done in step count: 7
reward sum = 0.9144668881778714
running average episode reward sum: 0.693151604039454
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.63990075, 9.19905106, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.0251790965380856}
episode index:1841
target Thresh 12.188730883563585
target distance 8.0
model initialize at round 1841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([4.        , 6.10026598, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 7.155971312726442}
done in step count: 4
reward sum = 0.9430558525046228
running average episode reward sum: 0.6932872740983385
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.20268083, 10.88112178,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.9041322449363796}
episode index:1842
target Thresh 12.192135666855046
target distance 8.0
model initialize at round 1842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.90979666,  4.56732514,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 6.4966942228897}
done in step count: 4
reward sum = 0.9455427004719168
running average episode reward sum: 0.693424146277597
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.76534828, 10.16050781,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.136004017611774}
episode index:1843
target Thresh 12.19553874818039
target distance 11.0
model initialize at round 1843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([12.19817761,  7.86121899,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 9.19922450606054}
done in step count: 17
reward sum = 0.7985119362768293
running average episode reward sum: 0.6934811353177267
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.22599808, 8.07969444, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.23963792300222791}
episode index:1844
target Thresh 12.198940128390387
target distance 6.0
model initialize at round 1844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([10.27427241,  8.11313696,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 6.2650137191488104}
done in step count: 99
reward sum = -0.22196841590003222
running average episode reward sum: 0.6929849566991806
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([9.47155808, 8.09007689, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 6.876946931420835}
episode index:1845
target Thresh 12.202339808335376
target distance 3.0
model initialize at round 1845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([16.        ,  3.94735587,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.6494706747973704}
done in step count: 8
reward sum = 0.8938299951609388
running average episode reward sum: 0.6930937568283582
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.28649404,  7.28634533,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.4050586213161392}
episode index:1846
target Thresh 12.205737788865287
target distance 7.0
model initialize at round 1846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.52489722, 9.71510744, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.739161101658216}
done in step count: 4
reward sum = 0.9453007866897275
running average episode reward sum: 0.6932303063843199
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.79638487, 3.67307305, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.38514977501559644}
episode index:1847
target Thresh 12.209134070829608
target distance 4.0
model initialize at round 1847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([6.35959554, 9.        , 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 3.093168456309288}
done in step count: 3
reward sum = 0.9578146690526972
running average episode reward sum: 0.6933734797407421
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.88620371, 6.3829163 , 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0798839299997118}
episode index:1848
target Thresh 12.21252865507741
target distance 2.0
model initialize at round 1848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([8.58998561, 9.55938756, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.1455578355894875}
done in step count: 10
reward sum = 0.8882508843575928
running average episode reward sum: 0.6934788758492424
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.29895956, 10.40922653,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.916772051646211}
episode index:1849
target Thresh 12.21592154245734
target distance 1.0
model initialize at round 1849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.44150791,  6.37587708,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7644989517019212}
done in step count: 0
reward sum = 0.9997478403692932
running average episode reward sum: 0.6936444266408749
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.44150791,  6.37587708,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7644989517019212}
episode index:1850
target Thresh 12.219312733817622
target distance 8.0
model initialize at round 1850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.34836054,  8.        ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 6.035282428172424}
done in step count: 4
reward sum = 0.938177361656914
running average episode reward sum: 0.6937765351957187
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.22406124,  1.11968065,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.9083862554975779}
episode index:1851
target Thresh 12.222702230006053
target distance 7.0
model initialize at round 1851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([14.49908197, 10.48745322,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 6.6671270823069095}
done in step count: 4
reward sum = 0.9513046890964051
running average episode reward sum: 0.6939155892744988
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.40851271, 9.21915317, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.46358466925161496}
episode index:1852
target Thresh 12.226090031870003
target distance 3.0
model initialize at round 1852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([15.06725991,  9.13758731,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 2.3892097467013396}
done in step count: 39
reward sum = 0.6074078421348872
running average episode reward sum: 0.6938689040358914
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.03708096,  7.44069939,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.0589754661091646}
episode index:1853
target Thresh 12.229476140256429
target distance 9.0
model initialize at round 1853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([12.1717093, 11.       ,  0.       ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 8.704988964311998}
done in step count: 20
reward sum = 0.786658761423672
running average episode reward sum: 0.6939189525026593
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.9568761 , 7.33192757, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1670187015232059}
episode index:1854
target Thresh 12.232860556011852
target distance 12.0
model initialize at round 1854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([15.11186242,  6.99568486,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.26417035928133}
done in step count: 27
reward sum = 0.7045397662968151
running average episode reward sum: 0.6939246780087479
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.13054066, 8.51935576, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9934678767495366}
episode index:1855
target Thresh 12.23624327998238
target distance 11.0
model initialize at round 1855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([12.62679801, 11.8913647 ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 9.667976566776714}
done in step count: 6
reward sum = 0.9125973082697343
running average episode reward sum: 0.6940424973138455
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.97194468, 11.82923792,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.2776196576560184}
episode index:1856
target Thresh 12.239624313013692
target distance 7.0
model initialize at round 1856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.13259695, 8.00494245, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.341969964170626}
done in step count: 4
reward sum = 0.9463874090156094
running average episode reward sum: 0.6941783857961835
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.37462898, 2.24116942, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9833172259384445}
episode index:1857
target Thresh 12.243003655951046
target distance 4.0
model initialize at round 1857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.03434968, 8.        , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.0002949533787575}
done in step count: 1
reward sum = 0.9831835832656666
running average episode reward sum: 0.6943339321887935
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.79089773, 6.        , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.2091022729873857}
episode index:1858
target Thresh 12.246381309639283
target distance 2.0
model initialize at round 1858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.68254995,  9.29181528,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.7374318883240818}
done in step count: 24
reward sum = 0.7250783900524566
running average episode reward sum: 0.694350470358704
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.18754598, 11.87372339,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.193094337323172}
episode index:1859
target Thresh 12.24975727492281
target distance 7.0
model initialize at round 1859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 7.        , 10.55203009,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 6.133263220792345}
done in step count: 5
reward sum = 0.9320766241117342
running average episode reward sum: 0.6944782801187863
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.79641425, 6.70034719, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.36226918919118734}
episode index:1860
target Thresh 12.253131552645623
target distance 9.0
model initialize at round 1860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([13.8544991 , 11.44385183,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 7.986104447133366}
done in step count: 4
reward sum = 0.9450241492828182
running average episode reward sum: 0.6946129098174235
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.79891587, 10.26541203,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.8418492259633612}
episode index:1861
target Thresh 12.256504143651284
target distance 7.0
model initialize at round 1861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.29391646, 5.17574644, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.86689726182886}
done in step count: 12
reward sum = 0.8664299128396338
running average episode reward sum: 0.6947051853292506
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.01322942, 11.86491334,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.865014513573124}
episode index:1862
target Thresh 12.259875048782948
target distance 6.0
model initialize at round 1862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([12.00002797,  8.02701369,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 6.350179693571422}
done in step count: 4
reward sum = 0.947909669398601
running average episode reward sum: 0.6948410975590249
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.77710232,  2.22524237,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.3168872025226236}
episode index:1863
target Thresh 12.263244268883339
target distance 8.0
model initialize at round 1863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([3.64385498, 3.96184874, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 6.481565402337548}
done in step count: 3
reward sum = 0.957141113907111
running average episode reward sum: 0.6949818164519154
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5.24550818, 9.96191796, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.7554522768955807}
episode index:1864
target Thresh 12.266611804794765
target distance 3.0
model initialize at round 1864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([5.        , 9.06045577, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.457589255806575}
done in step count: 1
reward sum = 0.9839771453155766
running average episode reward sum: 0.6951367737328075
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.        , 8.80066136, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.281038096936272}
episode index:1865
target Thresh 12.269977657359103
target distance 10.0
model initialize at round 1865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([12.05521678,  8.045681  ,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.014102953246663}
done in step count: 10
reward sum = 0.8731294664424776
running average episode reward sum: 0.6952321610279358
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.34116328, 3.95340029, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.344331118922271}
episode index:1866
target Thresh 12.273341827417822
target distance 4.0
model initialize at round 1866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.71341968,  7.55325663,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.651053952125376}
done in step count: 4
reward sum = 0.948675281555146
running average episode reward sum: 0.6953679098873504
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.39567554,  4.82521296,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6290934460983215}
episode index:1867
target Thresh 12.276704315811962
target distance 6.0
model initialize at round 1867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([11.        , 10.26913619,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 4.0662220682301875}
done in step count: 2
reward sum = 0.9696141943024514
running average episode reward sum: 0.69551472267344
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.        , 10.42123252,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.5787674784660997}
episode index:1868
target Thresh 12.280065123382148
target distance 10.0
model initialize at round 1868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.88565203, 3.28362462, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.204294161201151}
done in step count: 36
reward sum = 0.6250739651606453
running average episode reward sum: 0.6954770336646049
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.12671169,  2.61569974,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0685123480303567}
episode index:1869
target Thresh 12.283424250968578
target distance 4.0
model initialize at round 1869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3., 7., 0.]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 2.2360679774998156}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.6956281475505643
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13259695, 5.00494245, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.867417127752728}
episode index:1870
target Thresh 12.286781699411035
target distance 2.0
model initialize at round 1870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.,  2.,  0.]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.732141342546872e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.6957876194119512
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.,  2.,  0.]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.732141342546872e-14}
episode index:1871
target Thresh 12.290137469548883
target distance 4.0
model initialize at round 1871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.75450563,  8.79640603,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.8043351893832384}
done in step count: 3
reward sum = 0.9618620357143648
running average episode reward sum: 0.6959297531813435
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.90317903,  5.3895671 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9836131788732503}
episode index:1872
target Thresh 12.293491562221064
target distance 4.0
model initialize at round 1872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([14.        ,  8.08679461,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.1986232227403777}
done in step count: 20
reward sum = 0.7745744296408734
running average episode reward sum: 0.6959717417966449
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.25095643,  8.73390087,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.36577024543630826}
episode index:1873
target Thresh 12.296843978266102
target distance 7.0
model initialize at round 1873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([11.        ,  9.82286537,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 5.741826266950702}
done in step count: 3
reward sum = 0.9552033703585217
running average episode reward sum: 0.6961100724415552
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.49882925,  7.32262427,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5940682059666976}
episode index:1874
target Thresh 12.300194718522096
target distance 8.0
model initialize at round 1874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.64160757, 4.        , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.2205205119062095}
done in step count: 3
reward sum = 0.9551835318497286
running average episode reward sum: 0.6962482449532396
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.43027538, 9.99231932, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.4303439240408149}
episode index:1875
target Thresh 12.303543783826736
target distance 9.0
model initialize at round 1875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([10.67985012, 11.87052334,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 8.198780524324558}
done in step count: 9
reward sum = 0.8901852296375122
running average episode reward sum: 0.6963516228768453
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.71974538, 8.55471446, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8463525391878733}
episode index:1876
target Thresh 12.306891175017288
target distance 11.0
model initialize at round 1876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([5.77010143, 9.17563009, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 9.760924860584558}
done in step count: 6
reward sum = 0.9215983947565707
running average episode reward sum: 0.6964716264846661
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.74933509,  5.45813986,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.5970304093126279}
episode index:1877
target Thresh 12.310236892930599
target distance 5.0
model initialize at round 1877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([13.87476075,  5.06224966,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 3.7274676210347804}
done in step count: 2
reward sum = 0.9692580690807105
running average episode reward sum: 0.6966168801814692
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.51904563,  1.08861552,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.0305041363120513}
episode index:1878
target Thresh 12.3135809384031
target distance 1.0
model initialize at round 1878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.07278368,  1.13184636,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8711992924162443}
done in step count: 0
reward sum = 0.9961397181717623
running average episode reward sum: 0.6967762856301069
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.07278368,  1.13184636,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8711992924162443}
episode index:1879
target Thresh 12.316923312270797
target distance 1.0
model initialize at round 1879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.15447289,  6.        ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.3095480510258093}
done in step count: 0
reward sum = 0.99646381289442
running average episode reward sum: 0.69693569388929
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.15447289,  6.        ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.3095480510258093}
episode index:1880
target Thresh 12.32026401536929
target distance 7.0
model initialize at round 1880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3., 7., 0.]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 5.0000000000000195}
done in step count: 3
reward sum = 0.9572165742280283
running average episode reward sum: 0.697074067563048
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.94345775, 2.29765499, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.3029777494701253}
episode index:1881
target Thresh 12.32360304853375
target distance 11.0
model initialize at round 1881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.13552562,  5.9815731 ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.30988453035578}
done in step count: 6
reward sum = 0.9015416772331843
running average episode reward sum: 0.6971827113513955
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.05216425, 11.86845694,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.2855388261571818}
episode index:1882
target Thresh 12.326940412598937
target distance 5.0
model initialize at round 1882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 6.1657052, 11.       ,  0.       ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 3.165705204010023}
done in step count: 2
reward sum = 0.9627559221136234
running average episode reward sum: 0.6973237486380458
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.23690008, 11.88245906,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.1666428266541988}
episode index:1883
target Thresh 12.330276108399193
target distance 1.0
model initialize at round 1883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.27556032, 7.        , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 3.086229552511187}
done in step count: 17
reward sum = 0.8041156382027214
running average episode reward sum: 0.697380432231233
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.88334796, 3.24433129, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.1624709954879473}
episode index:1884
target Thresh 12.333610136768446
target distance 7.0
model initialize at round 1884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([ 8.       , 11.4282099,  0.       ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 5.199979184660569}
done in step count: 3
reward sum = 0.954808120437144
running average episode reward sum: 0.6975169986440743
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([14.       ,  9.6454334,  0.       ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.0609983395842073}
episode index:1885
target Thresh 12.336942498540195
target distance 6.0
model initialize at round 1885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.84592611,  2.58615303,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 6.469391310337969}
done in step count: 4
reward sum = 0.9510356613047171
running average episode reward sum: 0.6976514199922507
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.97163886,  8.47942889,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.1023050187229533}
episode index:1886
target Thresh 12.340273194547532
target distance 7.0
model initialize at round 1886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([6.15777268, 8.00176568, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 5.447351910271024}
done in step count: 4
reward sum = 0.946168652410682
running average episode reward sum: 0.6977831196384714
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.85574643, 3.99749098, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.3142641336148038}
episode index:1887
target Thresh 12.343602225623133
target distance 1.0
model initialize at round 1887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([5., 8., 0.]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.4142135623730687}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.69794001417267
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([5., 8., 0.]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.4142135623730687}
episode index:1888
target Thresh 12.346929592599258
target distance 7.0
model initialize at round 1888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.86775037, 5.97380131, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.183499109751565}
done in step count: 3
reward sum = 0.9466566068092447
running average episode reward sum: 0.6980716799178456
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.06886445, 11.86781219,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8705402410026093}
episode index:1889
target Thresh 12.350255296307747
target distance 4.0
model initialize at round 1889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([13.29770926, 11.88882569,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 3.800335359011915}
done in step count: 11
reward sum = 0.8652179292751414
running average episode reward sum: 0.6981601170868177
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.65793367, 10.23135537,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.6974252828276111}
episode index:1890
target Thresh 12.353579337580024
target distance 4.0
model initialize at round 1890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([13.70741379,  8.        ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.5011900204330213}
done in step count: 4
reward sum = 0.9479616817602998
running average episode reward sum: 0.6982922173325466
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.560359  ,  7.54930168,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7035741238843981}
episode index:1891
target Thresh 12.356901717247101
target distance 12.0
model initialize at round 1891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.182309676547058}
done in step count: 24
reward sum = 0.7206229259173021
running average episode reward sum: 0.6983040200326442
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.08963055,  5.95813241,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9113316786810174}
episode index:1892
target Thresh 12.360222436139571
target distance 12.0
model initialize at round 1892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.1279704 ,  5.87499534,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.300067573402595}
done in step count: 52
reward sum = 0.4947656403126194
running average episode reward sum: 0.6981964984374409
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.08415946, 3.77378393, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.24136388426613004}
episode index:1893
target Thresh 12.36354149508762
target distance 7.0
model initialize at round 1893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([13.08813335,  1.46548842,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 8.572146704273875}
done in step count: 14
reward sum = 0.8304568684572948
running average episode reward sum: 0.6982663296782117
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.61636083, 8.11211682, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.0808502300142022}
episode index:1894
target Thresh 12.366858894921005
target distance 5.0
model initialize at round 1894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4.75632834, 9.        , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 4.07398403799528}
done in step count: 8
reward sum = 0.9026073329487758
running average episode reward sum: 0.6983741613422066
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.503575  , 6.75901659, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9069420991101721}
episode index:1895
target Thresh 12.370174636469077
target distance 2.0
model initialize at round 1895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.        , 9.71800792, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7180079221725233}
done in step count: 0
reward sum = 0.9966133481260813
running average episode reward sum: 0.6985314604913543
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.        , 9.71800792, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7180079221725233}
episode index:1896
target Thresh 12.373488720560776
target distance 6.0
model initialize at round 1896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.74999988, 6.        , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.06970512705604}
done in step count: 2
reward sum = 0.9690112645631963
running average episode reward sum: 0.6986740434139015
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.80366302, 10.        ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.19633698463439053}
episode index:1897
target Thresh 12.376801148024622
target distance 8.0
model initialize at round 1897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([7.33379173, 9.94869804, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 7.289249125629516}
done in step count: 31
reward sum = 0.6836881500055414
running average episode reward sum: 0.6986661477903986
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.99530349,  6.22283672,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7771774747047401}
episode index:1898
target Thresh 12.38011191968872
target distance 5.0
model initialize at round 1898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([12.        ,  8.93078166,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 3.184843458213141}
done in step count: 2
reward sum = 0.9707094722193838
running average episode reward sum: 0.6988094038854112
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.26903129, 9.44833526, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.91577794562163}
episode index:1899
target Thresh 12.383421036380764
target distance 3.0
model initialize at round 1899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2., 9., 0.]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0000000000000195}
done in step count: 1
reward sum = 0.9796197094419994
running average episode reward sum: 0.6989571987830726
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.139486  , 7.02456194, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.3007550681119144}
episode index:1900
target Thresh 12.38672849892803
target distance 10.0
model initialize at round 1900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.09530933,  3.40449422,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 11.235005486870048}
done in step count: 17
reward sum = 0.7902647610421984
running average episode reward sum: 0.6990052301151395
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.95909837, 10.48088184,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0729012202770567}
episode index:1901
target Thresh 12.390034308157386
target distance 3.0
model initialize at round 1901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.86736283,  4.96766774,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.1503568561943167}
done in step count: 3
reward sum = 0.9599687055303859
running average episode reward sum: 0.6991424348866512
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.86926947,  3.90425725,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.2543167841558038}
episode index:1902
target Thresh 12.393338464895287
target distance 2.0
model initialize at round 1902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 4.40326893, 11.        ,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.126611834837579}
done in step count: 29
reward sum = 0.6829634803415048
running average episode reward sum: 0.6991339330713358
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.32995011, 8.28426573, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9804297018401947}
episode index:1903
target Thresh 12.396640969967768
target distance 2.0
model initialize at round 1903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([2.48308516, 8.        , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.5169148445129506}
done in step count: 9
reward sum = 0.8816770461284145
running average episode reward sum: 0.6992298065550843
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.90802958, 7.542123  , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.016941036586387}
episode index:1904
target Thresh 12.399941824200459
target distance 5.0
model initialize at round 1904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([3.27907467, 9.98438263, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 3.848934200631684}
done in step count: 7
reward sum = 0.9145879668229329
running average episode reward sum: 0.6993428554581119
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.41335936, 9.05987382, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.5896881525012528}
episode index:1905
target Thresh 12.403241028418572
target distance 4.0
model initialize at round 1905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 4.        , 11.30612302,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.052573238330408}
done in step count: 7
reward sum = 0.9167276369590419
running average episode reward sum: 0.6994569083340305
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.74271226, 9.09586731, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7488738463555156}
episode index:1906
target Thresh 12.406538583446906
target distance 1.0
model initialize at round 1906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.87292278, 2.13799812, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 2.0564642407588245}
done in step count: 17
reward sum = 0.7929807311329862
running average episode reward sum: 0.6995059507162009
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.89788159, 4.65266248, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.110026874976857}
episode index:1907
target Thresh 12.409834490109851
target distance 3.0
model initialize at round 1907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0000000000000187}
done in step count: 1
reward sum = 0.9810176231476333
running average episode reward sum: 0.6996534935214586
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([10.        , 11.12911111,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0083003912653896}
episode index:1908
target Thresh 12.413128749231385
target distance 10.0
model initialize at round 1908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14., 11.,  0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.16552506059642}
done in step count: 14
reward sum = 0.812385773215472
running average episode reward sum: 0.6997125465752532
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.11521556, 9.26526862, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9236941839951451}
episode index:1909
target Thresh 12.416421361635074
target distance 7.0
model initialize at round 1909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 4.91242845, 11.86858881,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.134080162712173}
done in step count: 18
reward sum = 0.7520829795652206
running average episode reward sum: 0.6997399656501171
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.18780821, 11.87210553,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.1917313255334143}
episode index:1910
target Thresh 12.419712328144067
target distance 1.0
model initialize at round 1910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([16.07358801,  9.14392447,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.397707942251871}
done in step count: 2
reward sum = 0.9748564988311357
running average episode reward sum: 0.6998839303456592
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.69922018,  6.94785523,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7011618482493529}
episode index:1911
target Thresh 12.423001649581108
target distance 9.0
model initialize at round 1911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.53511345,  9.        ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 7.166350068741185}
done in step count: 9
reward sum = 0.8898251514645004
running average episode reward sum: 0.6999832719885037
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.83110288,  1.13053279,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8857197442106427}
episode index:1912
target Thresh 12.426289326768527
target distance 6.0
model initialize at round 1912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.08020687,  8.        ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 4.0008040619556695}
done in step count: 3
reward sum = 0.9592618652700098
running average episode reward sum: 0.700118807060789
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.91251838,  4.04967862,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.10060317651863353}
episode index:1913
target Thresh 12.429575360528244
target distance 11.0
model initialize at round 1913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([14.        ,  5.34231603,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 10.133805797963179}
done in step count: 12
reward sum = 0.8549761781324847
running average episode reward sum: 0.7001997147781723
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.13058077, 9.76608374, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.2678957918442104}
episode index:1914
target Thresh 12.432859751681766
target distance 5.0
model initialize at round 1914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.85334826, 7.        , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.5263153226697255}
done in step count: 19
reward sum = 0.7890336216465448
running average episode reward sum: 0.7002461032412889
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.67938513, 10.11624621,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.34103822874798334}
episode index:1915
target Thresh 12.436142501050194
target distance 10.0
model initialize at round 1915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([ 5.        , 11.20679152,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 8.090509611454076}
done in step count: 7
reward sum = 0.9101181673508427
running average episode reward sum: 0.7003556398091958
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.1009373 ,  9.75660724,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.2634926471778545}
episode index:1916
target Thresh 12.439423609454211
target distance 11.0
model initialize at round 1916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([11.72464859, 10.95905936,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 10.916091791757163}
done in step count: 9
reward sum = 0.8753106614746099
running average episode reward sum: 0.7004469048178894
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13230959, 6.97671303, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.3064666030107173}
episode index:1917
target Thresh 12.442703077714096
target distance 7.0
model initialize at round 1917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([13.       , 11.5417906,  0.       ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 5.232314809430361}
done in step count: 9
reward sum = 0.888479419559004
running average episode reward sum: 0.7005449405398607
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.39479888, 9.84130866, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.6256607168844474}
episode index:1918
target Thresh 12.44598090664972
target distance 2.0
model initialize at round 1918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.40259337, 9.34766591, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7665645408239995}
done in step count: 0
reward sum = 0.9971623961623205
running average episode reward sum: 0.7006995093025614
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.40259337, 9.34766591, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7665645408239995}
episode index:1919
target Thresh 12.449257097080531
target distance 11.0
model initialize at round 1919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([11.,  9.,  0.]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 10.816653826391995}
done in step count: 16
reward sum = 0.8105028876755509
running average episode reward sum: 0.7007566985621306
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.28074656, 2.8143526 , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7428260051769815}
episode index:1920
target Thresh 12.452531649825582
target distance 5.0
model initialize at round 1920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([10.        , 10.75028998,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.010374577322258}
done in step count: 41
reward sum = 0.5281597196740678
running average episode reward sum: 0.7006668510978473
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.14118845, 10.14607008,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.2110960229153522}
episode index:1921
target Thresh 12.455804565703517
target distance 1.0
model initialize at round 1921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.96301508,  2.8737728 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.874555195690744}
done in step count: 0
reward sum = 0.9986208874900399
running average episode reward sum: 0.7008218740096018
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.96301508,  2.8737728 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.874555195690744}
episode index:1922
target Thresh 12.459075845532555
target distance 1.0
model initialize at round 1922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.12338281, 6.65590629, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9417314823455355}
done in step count: 0
reward sum = 0.9972699304151585
running average episode reward sum: 0.700976033165299
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.12338281, 6.65590629, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9417314823455355}
episode index:1923
target Thresh 12.462345490130522
target distance 5.0
model initialize at round 1923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.41375256,  5.        ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 3.0567443572820836}
done in step count: 2
reward sum = 0.9721285427501353
running average episode reward sum: 0.7011169648230874
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.9118686 ,  2.36754692,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.3779654465342558}
episode index:1924
target Thresh 12.465613500314827
target distance 2.0
model initialize at round 1924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.86775028,  3.97380116,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8681456786036896}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7012691118544548
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.86775028,  3.97380116,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8681456786036896}
episode index:1925
target Thresh 12.468879876902474
target distance 4.0
model initialize at round 1925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.184532509686136}
done in step count: 1
reward sum = 0.9804314933172259
running average episode reward sum: 0.701414055977748
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.97171161, 11.85260158,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.853070736814821}
episode index:1926
target Thresh 12.472144620710054
target distance 7.0
model initialize at round 1926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 9.00494245, 11.86740305,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 5.341969964170625}
done in step count: 13
reward sum = 0.8505086275201947
running average episode reward sum: 0.7014914273174172
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.15530452, 10.67015474,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0782475680192152}
episode index:1927
target Thresh 12.475407732553757
target distance 8.0
model initialize at round 1927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 8.        , 10.46422434,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 8.819648309250411}
done in step count: 5
reward sum = 0.9280548559334837
running average episode reward sum: 0.7016089394691889
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.14539437, 3.8159119 , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8742077653473196}
episode index:1928
target Thresh 12.47866921324936
target distance 2.0
model initialize at round 1928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.        ,  8.51300478,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.1122788968969617}
done in step count: 0
reward sum = 0.9952831125053443
running average episode reward sum: 0.7017611811348375
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.        ,  8.51300478,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.1122788968969617}
episode index:1929
target Thresh 12.481929063612231
target distance 6.0
model initialize at round 1929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 7.97380116, 11.86775028,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.118648762259487}
done in step count: 2
reward sum = 0.9679922264220959
running average episode reward sum: 0.7018991246816184
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.92393919, 11.56556115,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.5706528407174118}
episode index:1930
target Thresh 12.485187284457332
target distance 10.0
model initialize at round 1930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 7., 11.,  0.]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 8.544003745317543}
done in step count: 26
reward sum = 0.7294417494988988
running average episode reward sum: 0.7019133880813166
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.12474796,  8.16018349,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.20302906691623565}
episode index:1931
target Thresh 12.488443876599222
target distance 2.0
model initialize at round 1931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.76779068,  8.94969416,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.0756688780117785}
done in step count: 35
reward sum = 0.6434530244869968
running average episode reward sum: 0.7018831290939489
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.33651763,  9.92613251,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.6675816514421629}
episode index:1932
target Thresh 12.491698840852049
target distance 7.0
model initialize at round 1932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.830951894845326}
done in step count: 5
reward sum = 0.9312665998855566
running average episode reward sum: 0.702001796176614
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.20985918, 10.36547375,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6683295838689612}
episode index:1933
target Thresh 12.49495217802955
target distance 11.0
model initialize at round 1933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.178937723926285}
done in step count: 9
reward sum = 0.8743032582637847
running average episode reward sum: 0.7020908869015815
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.14136205, 2.87129013, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.19117912729104758}
episode index:1934
target Thresh 12.498203888945062
target distance 7.0
model initialize at round 1934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([ 5.        , 10.90123475,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 5.349270376352209}
done in step count: 6
reward sum = 0.9222296332577522
running average episode reward sum: 0.7022046536955641
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.56828433,  9.47309363,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.7394353687386225}
episode index:1935
target Thresh 12.501453974411513
target distance 12.0
model initialize at round 1935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.13224963,  5.97380131,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.820240211421464}
done in step count: 11
reward sum = 0.8451420939306432
running average episode reward sum: 0.7022784850179995
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.12012441, 2.7154486 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.1340404519632052}
episode index:1936
target Thresh 12.504702435241425
target distance 2.0
model initialize at round 1936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.85795808, 7.38870037, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.395945777629213}
done in step count: 1
reward sum = 0.98810762031587
running average episode reward sum: 0.7024260478137134
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.39735061, 6.63848752, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.7520331223706307}
episode index:1937
target Thresh 12.507949272246911
target distance 4.0
model initialize at round 1937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.74159899, 10.73571026,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.744636464479274}
done in step count: 21
reward sum = 0.7859106191160976
running average episode reward sum: 0.7024691255078839
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.12506857,  7.1788127 ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8930167855741304}
episode index:1938
target Thresh 12.51119448623968
target distance 8.0
model initialize at round 1938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([ 2.96528659, 11.86751004,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 7.596697085965003}
done in step count: 7
reward sum = 0.9141162484003308
running average episode reward sum: 0.7025782782272715
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.87006134,  8.76532967,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.9011530953571767}
episode index:1939
target Thresh 12.514438078031036
target distance 9.0
model initialize at round 1939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 7.071067811865498}
done in step count: 6
reward sum = 0.9187693656395309
running average episode reward sum: 0.7026897169321231
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.04535906,  1.13899428,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.862199685169604}
episode index:1940
target Thresh 12.517680048431881
target distance 1.0
model initialize at round 1940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.02343547,  1.66759777,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.3326083153287245}
done in step count: 2
reward sum = 0.9762543834747839
running average episode reward sum: 0.7028306569973176
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.35531939,  2.22783868,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8499911628019079}
episode index:1941
target Thresh 12.520920398252702
target distance 1.0
model initialize at round 1941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.81014472, 4.15562093, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8654600140433402}
done in step count: 0
reward sum = 0.9999548028277357
running average episode reward sum: 0.7029836560425444
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.81014472, 4.15562093, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8654600140433402}
episode index:1942
target Thresh 12.52415912830359
target distance 2.0
model initialize at round 1942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2., 6., 0.]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9999999999999809}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7031334328537451
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2., 6., 0.]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9999999999999809}
episode index:1943
target Thresh 12.527396239394225
target distance 4.0
model initialize at round 1943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([2.36729842, 8.        , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.5818044921094185}
done in step count: 4
reward sum = 0.9519438060931138
running average episode reward sum: 0.7032614217288683
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.54900235, 5.64341107, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.6546443644542628}
episode index:1944
target Thresh 12.530631732333886
target distance 1.0
model initialize at round 1944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.34038964, 9.55908865, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.5570169901495088}
done in step count: 0
reward sum = 0.999678666067846
running average episode reward sum: 0.7034138213403536
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.34038964, 9.55908865, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.5570169901495088}
episode index:1945
target Thresh 12.533865607931448
target distance 2.0
model initialize at round 1945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([5.67890775, 9.34750521, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.7865202939526696}
done in step count: 2
reward sum = 0.9738039264814109
running average episode reward sum: 0.703552767951423
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.63594908, 10.90693748,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.37575751285304115}
episode index:1946
target Thresh 12.537097866995376
target distance 8.0
model initialize at round 1946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([9.99505755, 8.13259695, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 7.289560442313219}
done in step count: 4
reward sum = 0.9373578459146146
running average episode reward sum: 0.7036728527372285
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.86009751,  4.1097446 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8670706994439059}
episode index:1947
target Thresh 12.54032851033374
target distance 8.0
model initialize at round 1947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([15.56159317,  4.14846897,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.314727406524501}
done in step count: 4
reward sum = 0.9450878797382077
running average episode reward sum: 0.7037967824225472
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.12622118, 10.77542353,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9021773765052056}
episode index:1948
target Thresh 12.543557538754195
target distance 7.0
model initialize at round 1948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 1.49272823, 10.70485377,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 7.158315153077924}
done in step count: 5
reward sum = 0.9381124603980199
running average episode reward sum: 0.7039170059617855
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.48827991, 3.45407078, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7324315545489546}
episode index:1949
target Thresh 12.546784953064003
target distance 4.0
model initialize at round 1949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 6., 10.,  0.]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 2.236067977499815}
done in step count: 3
reward sum = 0.9596212318526258
running average episode reward sum: 0.7040481363340373
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.48361465, 10.58690003,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6612907215403614}
episode index:1950
target Thresh 12.550010754070014
target distance 11.0
model initialize at round 1950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.54449213,  7.50383828,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.553761856079246}
done in step count: 22
reward sum = 0.7471405955339236
running average episode reward sum: 0.7040702237042064
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.70368769, 3.32914855, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7768623671322734}
episode index:1951
target Thresh 12.553234942578685
target distance 9.0
model initialize at round 1951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.78150058,  8.        ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.339769256056965}
done in step count: 5
reward sum = 0.9223356705953861
running average episode reward sum: 0.7041820400192121
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.15496068, 11.87430984,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8879360904767121}
episode index:1952
target Thresh 12.556457519396053
target distance 3.0
model initialize at round 1952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 7.32714605, 11.89148119,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 2.8176030471657465}
done in step count: 20
reward sum = 0.7575964092159981
running average episode reward sum: 0.704209389926635
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.23205734, 11.84106306,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.138913082411827}
episode index:1953
target Thresh 12.559678485327769
target distance 1.0
model initialize at round 1953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.42980534,  5.45101058,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.5590233062495051}
done in step count: 1
reward sum = 0.9888024334785628
running average episode reward sum: 0.7043550363153515
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.9690631 ,  4.48663437,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.4876167605977809}
episode index:1954
target Thresh 12.562897841179074
target distance 6.0
model initialize at round 1954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.60492209,  3.79024136,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 5.244760796286441}
done in step count: 5
reward sum = 0.9438657917788954
running average episode reward sum: 0.7044775482107293
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.83677569,  8.76330543,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.8696078871740117}
episode index:1955
target Thresh 12.566115587754803
target distance 5.0
model initialize at round 1955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.,  9.,  0.]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.0000000000000195}
done in step count: 2
reward sum = 0.9681660283879058
running average episode reward sum: 0.7046123582721695
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.7592532,  5.       ,  0.       ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0285713489337474}
episode index:1956
target Thresh 12.569331725859396
target distance 4.0
model initialize at round 1956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 8.       , 10.2757172,  0.       ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 2.0189155439842006}
done in step count: 1
reward sum = 0.9839685368583648
running average episode reward sum: 0.704755105425254
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.        , 10.35312273,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.3531227335332652}
episode index:1957
target Thresh 12.572546256296887
target distance 2.0
model initialize at round 1957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.0287993 , 6.64149654, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0352562678062}
done in step count: 0
reward sum = 0.9972716935843156
running average episode reward sum: 0.7049045010269694
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.0287993 , 6.64149654, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0352562678062}
episode index:1958
target Thresh 12.575759179870909
target distance 6.0
model initialize at round 1958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.12982533, 9.10201218, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.433974752827739}
done in step count: 7
reward sum = 0.9182818006041461
running average episode reward sum: 0.7050134225683564
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.27975881, 4.95048771, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.1925494794578684}
episode index:1959
target Thresh 12.578970497384693
target distance 7.0
model initialize at round 1959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 7.0707581, 11.       ,  0.       ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 5.168422172440173}
done in step count: 3
reward sum = 0.9532326590233724
running average episode reward sum: 0.7051400650359355
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.2516904 , 9.20845855, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0892681576244225}
episode index:1960
target Thresh 12.582180209641068
target distance 8.0
model initialize at round 1960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 9.30545735, 10.38057911,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 8.000398403719736}
done in step count: 10
reward sum = 0.8776042641152476
running average episode reward sum: 0.7052280121032886
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.0448655,  5.0555351,  0.       ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.3432408041517343}
episode index:1961
target Thresh 12.585388317442462
target distance 4.0
model initialize at round 1961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([5.72831597, 7.98410408, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 3.0371672206258564}
done in step count: 6
reward sum = 0.9316657349687987
running average episode reward sum: 0.7053434237867063
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.11893083, 9.27320461, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.736461869679767}
episode index:1962
target Thresh 12.588594821590902
target distance 9.0
model initialize at round 1962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 4.      , 10.245453,  0.      ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 7.109933415678396}
done in step count: 4
reward sum = 0.9464269802560574
running average episode reward sum: 0.705466237620873
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.92510951,  8.87336109,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.14712579291903607}
episode index:1963
target Thresh 12.591799722888013
target distance 7.0
model initialize at round 1963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([12.       ,  9.8103807,  0.       ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 5.003594256129829}
done in step count: 3
reward sum = 0.9583771654839457
running average episode reward sum: 0.705595011005732
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.41107726, 10.17497468,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.6143664469742737}
episode index:1964
target Thresh 12.595003022135021
target distance 9.0
model initialize at round 1964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([11.00494981,  8.13259643,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.310105302032698}
done in step count: 10
reward sum = 0.8725421354632851
running average episode reward sum: 0.7056799713744127
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.23532841, 2.58029198, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9599278218897253}
episode index:1965
target Thresh 12.598204720132752
target distance 11.0
model initialize at round 1965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([3.87662673, 6.        , 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.558536184739555}
done in step count: 29
reward sum = 0.6992971470038415
running average episode reward sum: 0.7056767247699516
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.4327381 ,  2.01055273,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0799389615279988}
episode index:1966
target Thresh 12.60140481768163
target distance 8.0
model initialize at round 1966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([13.09749908,  4.41093003,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 8.271468922205}
done in step count: 5
reward sum = 0.9233064247288446
running average episode reward sum: 0.7057873651868093
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.27774004, 10.92711916,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.1752486527357238}
episode index:1967
target Thresh 12.604603315581679
target distance 8.0
model initialize at round 1967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.708203932499393}
done in step count: 11
reward sum = 0.8547832339955828
running average episode reward sum: 0.7058630744697405
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.18212515, 11.86291443,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.1889241271693283}
episode index:1968
target Thresh 12.607800214632523
target distance 12.0
model initialize at round 1968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 5.16840254, 11.87288766,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.503163149920159}
done in step count: 6
reward sum = 0.9103281137927041
running average episode reward sum: 0.7059669165415144
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.87637052,  7.78103865,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9033102219949349}
episode index:1969
target Thresh 12.610995515633387
target distance 3.0
model initialize at round 1969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.        ,  7.71038246,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.6319048365279674}
done in step count: 1
reward sum = 0.9807534228048079
running average episode reward sum: 0.7061064020776887
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.        ,  9.36355686,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0640364616957843}
episode index:1970
target Thresh 12.614189219383098
target distance 2.0
model initialize at round 1970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([13.3533237 , 10.23277366,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 3.572745544344017}
done in step count: 4
reward sum = 0.9506915459972402
running average episode reward sum: 0.7062304939822648
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.62229968, 9.6030979 , 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.7116070642898474}
episode index:1971
target Thresh 12.617381326680079
target distance 2.0
model initialize at round 1971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.71587579, 6.96440077, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.7167603898106121}
done in step count: 0
reward sum = 0.9967214889020973
running average episode reward sum: 0.7063778017890193
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.71587579, 6.96440077, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.7167603898106121}
episode index:1972
target Thresh 12.620571838322359
target distance 7.0
model initialize at round 1972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.51455092, 8.        , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 5.023510804715866}
done in step count: 3
reward sum = 0.9568526736719645
running average episode reward sum: 0.7065047530672165
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.06327665, 2.        , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.001999967189596}
episode index:1973
target Thresh 12.623760755107568
target distance 1.0
model initialize at round 1973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([6.32174313, 9.        , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.3972911595194275}
done in step count: 3
reward sum = 0.9656060610310897
running average episode reward sum: 0.7066360100621324
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.80344415, 10.1655913 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8572468038547463}
episode index:1974
target Thresh 12.62694807783293
target distance 10.0
model initialize at round 1974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.00494981,  8.13259643,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.509088678107885}
done in step count: 5
reward sum = 0.9186231839616794
running average episode reward sum: 0.7067433453400562
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.4926525 , 2.98204971, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5076649434664761}
episode index:1975
target Thresh 12.63013380729528
target distance 6.0
model initialize at round 1975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.123105625617674}
done in step count: 3
reward sum = 0.9500394289082322
running average episode reward sum: 0.7068664708884207
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.11545965,  3.70564876,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9322308078094211}
episode index:1976
target Thresh 12.633317944291045
target distance 11.0
model initialize at round 1976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([12.48562548,  7.52070591,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 10.10359233187621}
done in step count: 10
reward sum = 0.8727196473136861
running average episode reward sum: 0.7069503622270272
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.1031633 , 10.98983534,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.10366284976456595}
episode index:1977
target Thresh 12.636500489616267
target distance 4.0
model initialize at round 1977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([16.43476057,  7.38553798,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 3.572599988722738}
done in step count: 29
reward sum = 0.6844132136303493
running average episode reward sum: 0.7069389683197488
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.37572229, 10.70265316,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7967990351054629}
episode index:1978
target Thresh 12.639681444066575
target distance 3.0
model initialize at round 1978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.84073985, 5.        , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.3064622034479771}
done in step count: 1
reward sum = 0.9820959259317708
running average episode reward sum: 0.707078006701564
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.72388935, 7.        , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.0374184741709822}
episode index:1979
target Thresh 12.642860808437211
target distance 6.0
model initialize at round 1979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([11.        , 11.30402315,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 5.87576507756408}
done in step count: 14
reward sum = 0.8251372560927015
running average episode reward sum: 0.7071376325850948
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.6346587 ,  7.85632351,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9310017258138075}
episode index:1980
target Thresh 12.64603858352302
target distance 6.0
model initialize at round 1980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.43012571, 3.72902548, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.285708354712189}
done in step count: 18
reward sum = 0.8081076456628465
running average episode reward sum: 0.7071886017991673
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.42061149, 9.34368236, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7795299050999466}
episode index:1981
target Thresh 12.649214770118439
target distance 8.0
model initialize at round 1981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([12.29377071, 11.88465217,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 6.569890676687829}
done in step count: 10
reward sum = 0.8702642644702725
running average episode reward sum: 0.7072708801355302
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.71090588, 9.93599652, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.7137812079561667}
episode index:1982
target Thresh 12.652389369017516
target distance 7.0
model initialize at round 1982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.88835878, 5.31626569, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.644194636712744}
done in step count: 4
reward sum = 0.9381582657189427
running average episode reward sum: 0.7073873135120221
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.66454728, 11.47865747,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.818984774200416}
episode index:1983
target Thresh 12.655562381013905
target distance 7.0
model initialize at round 1983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([5.17664066, 7.14612992, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 6.485066570110793}
done in step count: 4
reward sum = 0.9457620679829777
running average episode reward sum: 0.7075074620777836
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.30392186,  9.27966727,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.0017005681404516}
episode index:1984
target Thresh 12.658733806900855
target distance 8.0
model initialize at round 1984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([11.50613749,  9.        ,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.609479695741808}
done in step count: 60
reward sum = 0.40918305855147274
running average episode reward sum: 0.7073571727057301
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.8410739 , 2.87224748, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8507208800230764}
episode index:1985
target Thresh 12.661903647471224
target distance 3.0
model initialize at round 1985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([9.99505755, 8.13259695, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.327515491320984}
done in step count: 1
reward sum = 0.9807827110096273
running average episode reward sum: 0.7074948492104148
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.61250428,  8.09790169,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.9818015592870185}
episode index:1986
target Thresh 12.66507190351747
target distance 10.0
model initialize at round 1986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([10.00036196,  8.16003585,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 8.601838061848463}
done in step count: 6
reward sum = 0.9187715959895507
running average episode reward sum: 0.7076011787256534
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.88361857, 5.84190963, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.2204890776993167}
episode index:1987
target Thresh 12.66823857583166
target distance 5.0
model initialize at round 1987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.8672134 ,  3.20891186,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 3.8890112358625113}
done in step count: 2
reward sum = 0.97011681759843
running average episode reward sum: 0.7077332288458107
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.18641394,  6.7693944 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.29652841372271294}
episode index:1988
target Thresh 12.67140366520546
target distance 4.0
model initialize at round 1988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.13242061,  4.01740271,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.1960436454832903}
done in step count: 1
reward sum = 0.980177071656738
running average episode reward sum: 0.7078702041312863
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.25484743,  2.01747599,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.2554459328629904}
episode index:1989
target Thresh 12.674567172430148
target distance 6.0
model initialize at round 1989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([5.99992215, 8.00016787, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 4.122989388382346}
done in step count: 8
reward sum = 0.8969793740194839
running average episode reward sum: 0.7079652338648985
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.13262787, 8.99792583, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8673746073234451}
episode index:1990
target Thresh 12.677729098296592
target distance 5.0
model initialize at round 1990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([2.8996551, 6.       , 0.       ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.1954278103017657}
done in step count: 28
reward sum = 0.7040472072384306
running average episode reward sum: 0.707963265996176
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.42082768, 9.04577124, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5809781223607956}
episode index:1991
target Thresh 12.680889443595278
target distance 7.0
model initialize at round 1991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([14.35639632,  8.70279634,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 6.3633406446946585}
done in step count: 24
reward sum = 0.7473233484196901
running average episode reward sum: 0.7079830250736978
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.27144181, 9.50530959, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.8866424404692959}
episode index:1992
target Thresh 12.684048209116293
target distance 10.0
model initialize at round 1992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.12699302,  3.86096771,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.47433312611147}
done in step count: 6
reward sum = 0.9135543376424344
running average episode reward sum: 0.7080861717433259
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.64445026, 8.14159288, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.0733959724626663}
episode index:1993
target Thresh 12.687205395649327
target distance 3.0
model initialize at round 1993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([12.24076223,  9.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 3.407842873547343}
done in step count: 13
reward sum = 0.8452390834839177
running average episode reward sum: 0.708154954547609
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.50272601, 11.91221935,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0389540745381938}
episode index:1994
target Thresh 12.690361003983675
target distance 7.0
model initialize at round 1994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.18613672, 5.99992879, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 5.0658746120498845}
done in step count: 3
reward sum = 0.9553017604095168
running average episode reward sum: 0.7082788376583168
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.46403125, 11.91014269,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.056230193012762}
episode index:1995
target Thresh 12.693515034908245
target distance 7.0
model initialize at round 1995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([14.        ,  4.84335685,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 6.5261756488400104}
done in step count: 3
reward sum = 0.9483253474403223
running average episode reward sum: 0.7083991014407727
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.06956829, 10.55640389,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.0841072158917682}
episode index:1996
target Thresh 12.696667489211539
target distance 4.0
model initialize at round 1996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.14923614, 8.99846547, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 2.1748424368107986}
done in step count: 1
reward sum = 0.9811766461594357
running average episode reward sum: 0.7085356951036262
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.90377477, 10.9985125 ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9037759961996905}
episode index:1997
target Thresh 12.699818367681674
target distance 6.0
model initialize at round 1997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 3.63031525, 11.90086983,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.461581699626759}
done in step count: 7
reward sum = 0.9115277119340027
running average episode reward sum: 0.7086372927096474
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.97583526, 11.85837436,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8587144355258765}
episode index:1998
target Thresh 12.702967671106364
target distance 7.0
model initialize at round 1998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.18159992, 9.97000802, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 7.017890739892289}
done in step count: 20
reward sum = 0.7895329543041114
running average episode reward sum: 0.7086777607744771
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.8016382 , 3.04687872, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.803007734004449}
episode index:1999
target Thresh 12.706115400272942
target distance 6.0
model initialize at round 1999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.        , 3.68961418, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 4.3103858232498276}
done in step count: 2
reward sum = 0.9664871494027112
running average episode reward sum: 0.7088066654687911
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.11213749, 7.66711133, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9482165906078879}
episode index:2000
target Thresh 12.709261555968334
target distance 9.0
model initialize at round 2000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([11.       , 10.6696347,  0.       ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.383799710544565}
done in step count: 15
reward sum = 0.8142917547852451
running average episode reward sum: 0.7088593816553561
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.32404327, 2.39254189, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.688483403540758}
episode index:2001
target Thresh 12.712406138979087
target distance 6.0
model initialize at round 2001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.51687962,  6.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 4.033257311364788}
done in step count: 2
reward sum = 0.9692155132516622
running average episode reward sum: 0.7089894296731364
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.81394315, 10.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.18605685234075509}
episode index:2002
target Thresh 12.715549150091338
target distance 3.0
model initialize at round 2002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.4142135623730945}
done in step count: 1
reward sum = 0.9797933123058271
running average episode reward sum: 0.7091246288157389
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.29057062, 10.        ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0413603042665849}
episode index:2003
target Thresh 12.718690590090848
target distance 6.0
model initialize at round 2003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([9.3925409, 8.1003822, 0.       ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 5.553471804564125}
done in step count: 3
reward sum = 0.9532918353457741
running average episode reward sum: 0.7092464687391571
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.1541876 ,  5.86312646,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.208464274339159}
episode index:2004
target Thresh 12.721830459762971
target distance 9.0
model initialize at round 2004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([10.02620648,  8.13224956,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 7.692760548390136}
done in step count: 3
reward sum = 0.9466566068092447
running average episode reward sum: 0.709364877785576
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.88164911, 5.98204971, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.3197449641688692}
episode index:2005
target Thresh 12.724968759892676
target distance 12.0
model initialize at round 2005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.12526697,  3.1701162 ,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.128451782684133}
done in step count: 18
reward sum = 0.7791711065564905
running average episode reward sum: 0.7093996765038069
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.63159036, 7.87464799, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.38915136745198364}
episode index:2006
target Thresh 12.728105491264536
target distance 3.0
model initialize at round 2006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.02252817, 10.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.3983744761094277}
done in step count: 2
reward sum = 0.9715783609105971
running average episode reward sum: 0.7095303086335562
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.28898537, 10.4597708 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6126664140852918}
episode index:2007
target Thresh 12.73124065466274
target distance 1.0
model initialize at round 2007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.8767724 , 7.75414526, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.5234447379457887}
done in step count: 1
reward sum = 0.9832237343266199
running average episode reward sum: 0.7096666101403754
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.85231167, 9.75414526, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.1380554750560818}
episode index:2008
target Thresh 12.734374250871072
target distance 2.0
model initialize at round 2008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([14.13421285,  8.90095603,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.0719274745530925}
done in step count: 7
reward sum = 0.9238050199691457
running average episode reward sum: 0.7097731996923061
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.75700676,  7.00338212,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0258132959470032}
episode index:2009
target Thresh 12.737506280672937
target distance 5.0
model initialize at round 2009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.08040404, 10.72663116,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.092510240814682}
done in step count: 6
reward sum = 0.9259992373605812
running average episode reward sum: 0.7098807748354247
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.46479905, 10.75967649,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5232528498747344}
episode index:2010
target Thresh 12.740636744851336
target distance 8.0
model initialize at round 2010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 8.        , 11.34504747,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 6.869449949063936}
done in step count: 22
reward sum = 0.7621555178188284
running average episode reward sum: 0.7099067692377039
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.08803249,  8.62998192,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1084051405577622}
episode index:2011
target Thresh 12.74376564418889
target distance 12.0
model initialize at round 2011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.049875621120913}
done in step count: 29
reward sum = 0.6797934875616041
running average episode reward sum: 0.7098918023979047
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.56480268, 5.19090281, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.47522688465128277}
episode index:2012
target Thresh 12.746892979467825
target distance 2.0
model initialize at round 2012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.39104259,  9.76584637,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.8086259191928555}
done in step count: 11
reward sum = 0.876278848094896
running average episode reward sum: 0.7099744586550816
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.77555846,  7.21924614,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.100485130352983}
episode index:2013
target Thresh 12.75001875146997
target distance 2.0
model initialize at round 2013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.18061513,  6.11469197,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.893939856967895}
done in step count: 2
reward sum = 0.9787858343494782
running average episode reward sum: 0.7101079300432118
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.12531173,  7.5089699 ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.5067677835235864}
episode index:2014
target Thresh 12.75314296097677
target distance 11.0
model initialize at round 2014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.90908654, 5.44543523, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.518668513518136}
done in step count: 66
reward sum = 0.3632670697465099
running average episode reward sum: 0.7099358005840076
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.35069207, 10.97566737,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.3515352067584823}
episode index:2015
target Thresh 12.756265608769278
target distance 4.0
model initialize at round 2015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.44092655,  4.05658531,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.131221777652814}
done in step count: 10
reward sum = 0.892601983535156
running average episode reward sum: 0.7100264088096777
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.79221485,  2.0914084 ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.7974709130629072}
episode index:2016
target Thresh 12.759386695628157
target distance 9.0
model initialize at round 2016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 5.24439441, 11.88093912,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.79981152674226}
done in step count: 18
reward sum = 0.8015038582013475
running average episode reward sum: 0.710071762031984
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.40734774, 11.40894972,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7200531691163522}
episode index:2017
target Thresh 12.762506222333675
target distance 4.0
model initialize at round 2017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([12.95654288,  7.00409728,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.644041326090009}
done in step count: 25
reward sum = 0.7301713241593862
running average episode reward sum: 0.7100817221717894
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.86827569,  4.28036236,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.127732684075705}
episode index:2018
target Thresh 12.765624189665719
target distance 2.0
model initialize at round 2018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 4.42956269, 10.31328523,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.449678038159571}
done in step count: 5
reward sum = 0.9454483526836557
running average episode reward sum: 0.7101982980165203
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.79103059, 9.91913778, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.7951528701475119}
episode index:2019
target Thresh 12.768740598403777
target distance 8.0
model initialize at round 2019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([4.        , 2.08076835, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 9.158371398389113}
done in step count: 12
reward sum = 0.8466435825187976
running average episode reward sum: 0.710265845187066
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.42924337,  9.6373801 ,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.768442102828744}
episode index:2020
target Thresh 12.771855449326953
target distance 3.0
model initialize at round 2020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.        ,  9.46223378,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.4622337818145876}
done in step count: 1
reward sum = 0.9834687652968604
running average episode reward sum: 0.7104010272356112
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.79344679,  7.93757129,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.2157813037161598}
episode index:2021
target Thresh 12.774968743213954
target distance 2.0
model initialize at round 2021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.        ,  9.81094879,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.1890512108803328}
done in step count: 19
reward sum = 0.786957540348849
running average episode reward sum: 0.7104388890126208
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.72807505, 11.12625744,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7389412868693768}
episode index:2022
target Thresh 12.778080480843114
target distance 3.0
model initialize at round 2022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([4.96398592, 7.39317799, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.0254661594706107}
done in step count: 20
reward sum = 0.7852500941533526
running average episode reward sum: 0.7104758693414099
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.15168258, 7.84202452, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8629013258166641}
episode index:2023
target Thresh 12.781190662992358
target distance 8.0
model initialize at round 2023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 6.324555320336771}
done in step count: 3
reward sum = 0.9504692636238811
running average episode reward sum: 0.7105944431528144
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.98091513, 8.13662737, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.8635835387964599}
episode index:2024
target Thresh 12.78429929043924
target distance 10.0
model initialize at round 2024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 5.97411972, 11.86766899,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 8.909187259610203}
done in step count: 15
reward sum = 0.8206833429741862
running average episode reward sum: 0.7106488080416151
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.83501457,  8.68067299,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.0772952514732783}
episode index:2025
target Thresh 12.787406363960907
target distance 12.0
model initialize at round 2025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.93186878, 6.54070711, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.620677067351258}
done in step count: 19
reward sum = 0.7894271004031135
running average episode reward sum: 0.7106876917002338
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.3844507 ,  3.43094308,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.7514072659590397}
episode index:2026
target Thresh 12.790511884334135
target distance 8.0
model initialize at round 2026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2., 4., 0.]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 6.000000000000019}
done in step count: 4
reward sum = 0.9461516175063713
running average episode reward sum: 0.7108038554524815
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.00900396, 9.91498303, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.08549243787110684}
episode index:2027
target Thresh 12.793615852335302
target distance 9.0
model initialize at round 2027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.615773105863932}
done in step count: 7
reward sum = 0.9040380782380959
running average episode reward sum: 0.7108991385998117
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.15864521, 10.723225  ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.3190183466422915}
episode index:2028
target Thresh 12.796718268740399
target distance 1.0
model initialize at round 2028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.71786886, 4.50603914, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.5793734473119665}
done in step count: 0
reward sum = 0.9982391861143085
running average episode reward sum: 0.711040755183111
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.71786886, 4.50603914, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.5793734473119665}
episode index:2029
target Thresh 12.79981913432503
target distance 1.0
model initialize at round 2029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([16.07258832,  6.42920566,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.7869176033729146}
done in step count: 1
reward sum = 0.9867837655991012
running average episode reward sum: 0.7111765891783898
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.65095562,  4.75418019,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.695823679821249}
episode index:2030
target Thresh 12.802918449864414
target distance 2.0
model initialize at round 2030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2., 9., 0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.60805543562022e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7113158424580683
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2., 9., 0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.60805543562022e-14}
episode index:2031
target Thresh 12.806016216133381
target distance 8.0
model initialize at round 2031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([4.85575616, 3.97947944, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 7.918959052204057}
done in step count: 23
reward sum = 0.7410214894071848
running average episode reward sum: 0.711330461378811
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.94754363, 10.08109064,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.9510071638798006}
episode index:2032
target Thresh 12.809112433906364
target distance 11.0
model initialize at round 2032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([12.,  9.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.055385138137439}
done in step count: 5
reward sum = 0.9290198248548281
running average episode reward sum: 0.7114375392752575
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.        , 9.22813195, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.2632419747470016}
episode index:2033
target Thresh 12.812207103957425
target distance 4.0
model initialize at round 2033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([5.        , 9.55215061, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 2.0748181349050836}
done in step count: 1
reward sum = 0.980295537451586
running average episode reward sum: 0.7115697211819323
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.55655808, 8.08314546, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.0184610870165651}
episode index:2034
target Thresh 12.81530022706023
target distance 10.0
model initialize at round 2034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 4.       , 10.8494035,  0.       ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 8.044966519927769}
done in step count: 21
reward sum = 0.7473139461434405
running average episode reward sum: 0.7115872859116432
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.56133846, 10.30453303,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.5340077835925107}
episode index:2035
target Thresh 12.818391803988062
target distance 2.0
model initialize at round 2035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.611485  , 9.90615904, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.093177992100579}
done in step count: 0
reward sum = 0.9979381765132324
running average episode reward sum: 0.711727929767538
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.611485  , 9.90615904, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.093177992100579}
episode index:2036
target Thresh 12.821481835513811
target distance 12.0
model initialize at round 2036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 2.74410176, 10.54081392,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 12.545750836631752}
done in step count: 8
reward sum = 0.891944216532069
running average episode reward sum: 0.7118164011896118
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.7360087 ,  5.31582511,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.800908424472561}
episode index:2037
target Thresh 12.824570322409985
target distance 11.0
model initialize at round 2037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.09041975,  3.46203332,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.160161064767392}
done in step count: 28
reward sum = 0.6904076400364847
running average episode reward sum: 0.7118058964000371
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.53346035, 8.09184426, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.47549407639065316}
episode index:2038
target Thresh 12.827657265448709
target distance 1.0
model initialize at round 2038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.        , 9.71004096, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.041189822640798}
done in step count: 0
reward sum = 0.996936942726957
running average episode reward sum: 0.7119457350691528
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.        , 9.71004096, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.041189822640798}
episode index:2039
target Thresh 12.830742665401718
target distance 11.0
model initialize at round 2039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([11., 11.,  0.]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 10.816653826391974}
done in step count: 9
reward sum = 0.87656261958807
running average episode reward sum: 0.7120264296203875
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13201866, 5.07797908, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8714771084350653}
episode index:2040
target Thresh 12.833826523040361
target distance 8.0
model initialize at round 2040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.        , 10.48765528,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.489109589489933}
done in step count: 11
reward sum = 0.8599708109576376
running average episode reward sum: 0.7120989158434828
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.79258081, 3.70522151, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0609060883103383}
episode index:2041
target Thresh 12.836908839135601
target distance 12.0
model initialize at round 2041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([14.43943691,  6.        ,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.612954700501687}
done in step count: 14
reward sum = 0.8193910735964285
running average episode reward sum: 0.7121514585260259
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.35165551, 3.73208178, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7015203084164597}
episode index:2042
target Thresh 12.83998961445802
target distance 10.0
model initialize at round 2042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 8.062257748298565}
done in step count: 4
reward sum = 0.9369384079723472
running average episode reward sum: 0.712261486401428
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.97330196,  9.25034418,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.7501310766866399}
episode index:2043
target Thresh 12.843068849777811
target distance 5.0
model initialize at round 2043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([11.6848365, 11.8293761,  0.       ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 3.4173343085100414}
done in step count: 2
reward sum = 0.972341480149772
running average episode reward sum: 0.7123887271028703
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.72205331, 10.66407861,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.4360017747334306}
episode index:2044
target Thresh 12.84614654586478
target distance 4.0
model initialize at round 2044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.96611357,  6.        ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.2514264712872376}
done in step count: 1
reward sum = 0.9825969022179749
running average episode reward sum: 0.7125208582398459
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.88827401,  8.        ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.1117259860037354}
episode index:2045
target Thresh 12.849222703488355
target distance 6.0
model initialize at round 2045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 3.        , 10.91263962,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 5.996607978799042}
done in step count: 4
reward sum = 0.9502308524521879
running average episode reward sum: 0.7126370410327161
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.59252389, 5.52262007, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.790073605451687}
episode index:2046
target Thresh 12.852297323417575
target distance 3.0
model initialize at round 2046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([13.31926918,  9.41889888,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.3841775327438062}
done in step count: 6
reward sum = 0.9294437271906106
running average episode reward sum: 0.7127429553884356
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.18422221,  8.60467721,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.43613980817384657}
episode index:2047
target Thresh 12.855370406421093
target distance 9.0
model initialize at round 2047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.49577224,  3.05410242,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 8.680881977701505}
done in step count: 6
reward sum = 0.9189047864697741
running average episode reward sum: 0.7128436203450182
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.9231995 , 11.85270201,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8561536268500494}
episode index:2048
target Thresh 12.858441953267183
target distance 9.0
model initialize at round 2048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([11.        , 11.12698698,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 7.316014872521381}
done in step count: 5
reward sum = 0.9362301991508682
running average episode reward sum: 0.712952642589433
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.7515184 , 8.70463461, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8074779372759682}
episode index:2049
target Thresh 12.861511964723723
target distance 7.0
model initialize at round 2049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([8.0000328 , 8.20109797, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 5.936948392281347}
done in step count: 24
reward sum = 0.7321824289674252
running average episode reward sum: 0.7129620229730321
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.78519814, 4.62570336, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.4315527929880487}
episode index:2050
target Thresh 12.864580441558228
target distance 9.0
model initialize at round 2050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.        , 6.41253924, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 7.865740567677425}
done in step count: 28
reward sum = 0.7058584322935181
running average episode reward sum: 0.7129585594963478
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.87188947, 10.31615272,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.34112292579529446}
episode index:2051
target Thresh 12.86764738453781
target distance 8.0
model initialize at round 2051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 4.84542, 11.     ,  0.     ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 7.224127278389359}
done in step count: 6
reward sum = 0.9278987358351743
running average episode reward sum: 0.7130633061709769
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.15516296, 10.99117598,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.3023745405315539}
episode index:2052
target Thresh 12.870712794429206
target distance 11.0
model initialize at round 2052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.52564633,  8.        ,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.331405421518562}
done in step count: 16
reward sum = 0.8029461951055674
running average episode reward sum: 0.7131070874125427
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.42944395, 3.86693916, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.4495856876341183}
episode index:2053
target Thresh 12.873776671998765
target distance 3.0
model initialize at round 2053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.72353497, 7.20093846, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.8201800254783707}
done in step count: 40
reward sum = 0.5803295559807189
running average episode reward sum: 0.7130424440184668
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.8367038 , 9.50907994, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5346288745256712}
episode index:2054
target Thresh 12.876839018012463
target distance 3.0
model initialize at round 2054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.16981196,  6.72984815,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 2.8532933739230852}
done in step count: 6
reward sum = 0.9326494105095554
running average episode reward sum: 0.7131493087223555
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.63279749,  4.99924356,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.06457755768946}
episode index:2055
target Thresh 12.879899833235886
target distance 9.0
model initialize at round 2055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.06941712,  3.43240881,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 7.567909563445987}
done in step count: 21
reward sum = 0.7803376594817005
running average episode reward sum: 0.7131819878812852
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.45469862, 10.81110713,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.5770910729183696}
episode index:2056
target Thresh 12.882959118434234
target distance 6.0
model initialize at round 2056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([4.86775037, 5.97380131, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 6.523056192426002}
done in step count: 7
reward sum = 0.9050906953248142
running average episode reward sum: 0.713275283315142
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.61215656, 10.67350803,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.7771972690974408}
episode index:2057
target Thresh 12.886016874372327
target distance 6.0
model initialize at round 2057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([1.1529423 , 8.63399926, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 6.981988048784206}
done in step count: 24
reward sum = 0.7303480610284581
running average episode reward sum: 0.7132835791254983
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.32512758, 9.40260391, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.9012962136260626}
episode index:2058
target Thresh 12.88907310181461
target distance 11.0
model initialize at round 2058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.        , 10.00089079,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.296062776987718}
done in step count: 5
reward sum = 0.9262670303591837
running average episode reward sum: 0.7133870193640771
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.       , 5.4948222, 0.       ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.1157280195832417}
episode index:2059
target Thresh 12.892127801525136
target distance 9.0
model initialize at round 2059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([11.00494245, 11.86740305,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 7.569096313573254}
done in step count: 49
reward sum = 0.5363656119301446
running average episode reward sum: 0.7133010866420219
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.29059455, 9.2138152 , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.36077989346717854}
episode index:2060
target Thresh 12.89518097426758
target distance 1.0
model initialize at round 2060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.68926376, 3.69344044, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.3430021969600363}
done in step count: 4
reward sum = 0.9535385594455845
running average episode reward sum: 0.7134176501902041
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.92836267, 5.89845228, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9013037281314319}
episode index:2061
target Thresh 12.898232620805235
target distance 1.0
model initialize at round 2061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.71430928,  8.91898024,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.0476139541501515}
done in step count: 10
reward sum = 0.885021655289047
running average episode reward sum: 0.7135008723071289
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.71895847,  6.69501811,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7809707010570421}
episode index:2062
target Thresh 12.901282741901015
target distance 10.0
model initialize at round 2062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 6., 10.,  0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 8.000000000000018}
done in step count: 21
reward sum = 0.7553778942554171
running average episode reward sum: 0.713521171396779
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.66731212, 10.49744894,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8323226043248919}
episode index:2063
target Thresh 12.904331338317448
target distance 2.0
model initialize at round 2063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.        ,  7.18924935,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.189249351620699}
done in step count: 0
reward sum = 0.996973138792914
running average episode reward sum: 0.7136585027763315
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.        ,  7.18924935,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.189249351620699}
episode index:2064
target Thresh 12.907378410816683
target distance 5.0
model initialize at round 2064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 6.      , 10.834333,  0.      ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 3.1138579863528046}
done in step count: 2
reward sum = 0.9695094028405804
running average episode reward sum: 0.7137824015172827
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([10.        , 10.75126076,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.2507568611359996}
episode index:2065
target Thresh 12.910423960160491
target distance 6.0
model initialize at round 2065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([5., 8., 0.]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 4.4721359549996045}
done in step count: 2
reward sum = 0.9649977523451646
running average episode reward sum: 0.7139039965564056
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.        , 9.45051038, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.5494896173478132}
episode index:2066
target Thresh 12.913467987110256
target distance 2.0
model initialize at round 2066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9999999999999813}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7140395050245474
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9999999999999813}
episode index:2067
target Thresh 12.916510492426987
target distance 6.0
model initialize at round 2067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([15.57922924,  6.        ,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 4.759456213689417}
done in step count: 3
reward sum = 0.9577660962071243
running average episode reward sum: 0.7141573612098387
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.6110512, 10.6035369,  0.       ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.8588599219690303}
episode index:2068
target Thresh 12.919551476871305
target distance 8.0
model initialize at round 2068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.        , 11.03122652,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.093244055853397}
done in step count: 27
reward sum = 0.6674298664333621
running average episode reward sum: 0.7141347766304398
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.58225302, 3.11274072, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5930674953272682}
episode index:2069
target Thresh 12.922590941203467
target distance 7.0
model initialize at round 2069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 9.        , 11.64234499,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.041091857324258}
done in step count: 4
reward sum = 0.9457793707766349
running average episode reward sum: 0.7142466822314765
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.39856555, 10.23663512,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8611506462904935}
episode index:2070
target Thresh 12.925628886183329
target distance 2.0
model initialize at round 2070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.27650499, 4.        , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.2765049934387309}
done in step count: 0
reward sum = 0.9957779010073678
running average episode reward sum: 0.7143826219797991
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.27650499, 4.        , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.2765049934387309}
episode index:2071
target Thresh 12.928665312570384
target distance 4.0
model initialize at round 2071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.86457019, 5.01850169, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 2.1958667249671007}
done in step count: 1
reward sum = 0.9817703937819674
running average episode reward sum: 0.7145116701322132
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.84289393, 3.06515578, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8454084562116394}
episode index:2072
target Thresh 12.931700221123732
target distance 12.0
model initialize at round 2072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.88129149, 5.75178556, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.26922658409923}
done in step count: 14
reward sum = 0.8277737181208646
running average episode reward sum: 0.7145663069136838
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.5391569 ,  4.73436094,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8669846318316567}
episode index:2073
target Thresh 12.934733612602104
target distance 5.0
model initialize at round 2073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([13.32906595,  5.99999993,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 4.4813703815585235}
done in step count: 2
reward sum = 0.9712164889249016
running average episode reward sum: 0.7146900533852418
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.89112389,  8.33701383,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.1106990833067838}
episode index:2074
target Thresh 12.937765487763848
target distance 3.0
model initialize at round 2074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([8.       , 9.7415061, 0.       ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.607422438903315}
done in step count: 42
reward sum = 0.5935333030749584
running average episode reward sum: 0.7146316645899116
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.76455783, 10.65238704,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.41984257520210544}
episode index:2075
target Thresh 12.940795847366932
target distance 4.0
model initialize at round 2075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([4., 4., 0.]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.2360679774997982}
done in step count: 26
reward sum = 0.7176586421396226
running average episode reward sum: 0.714633122671583
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.2486155 , 6.08730956, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7564401033091304}
episode index:2076
target Thresh 12.943824692168949
target distance 7.0
model initialize at round 2076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.86775028,  3.97380116,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 5.100555391695826}
done in step count: 3
reward sum = 0.9549252243948387
running average episode reward sum: 0.7147488145838233
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.62592102,  8.22871357,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9933075433474534}
episode index:2077
target Thresh 12.946852022927105
target distance 3.0
model initialize at round 2077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([13.74634123, 11.58422172,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 2.3281522445171685}
done in step count: 1
reward sum = 0.9842703101495783
running average episode reward sum: 0.714878516939726
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.74634123, 10.79746035,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.32459988548338375}
episode index:2078
target Thresh 12.949877840398235
target distance 8.0
model initialize at round 2078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([12., 10.,  0.]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 6.0827625302982415}
done in step count: 16
reward sum = 0.809454869253161
running average episode reward sum: 0.71492400821068
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.41110687, 11.28589511,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.6546228941542706}
episode index:2079
target Thresh 12.952902145338793
target distance 7.0
model initialize at round 2079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([9.02183671, 8.13355538, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 7.92712719511185}
done in step count: 7
reward sum = 0.9078734151548659
running average episode reward sum: 0.715016772348634
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.00380312, 2.6467895 , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.1877478163441129}
episode index:2080
target Thresh 12.95592493850486
target distance 4.0
model initialize at round 2080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([10.99999972, 11.73477917,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 2.6475384539381897}
done in step count: 6
reward sum = 0.9265843789566325
running average episode reward sum: 0.7151184386660815
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.17542646,  9.48421728,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.9726013282529778}
episode index:2081
target Thresh 12.958946220652127
target distance 3.0
model initialize at round 2081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([6.08037066, 9.        , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.9196293354034397}
done in step count: 9
reward sum = 0.8991868470362632
running average episode reward sum: 0.7152068480841266
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.4483879, 8.37776  , 0.       ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.7669643564402882}
episode index:2082
target Thresh 12.961965992535918
target distance 4.0
model initialize at round 2082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.018435, 8.      , 0.      ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.227884612480883}
done in step count: 1
reward sum = 0.9810988329332517
running average episode reward sum: 0.7153344966606264
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.72136768, 6.00000015, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7213676780720267}
episode index:2083
target Thresh 12.964984254911176
target distance 5.0
model initialize at round 2083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([11.07606232, 11.17458415,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 5.079063724639348}
done in step count: 4
reward sum = 0.9542831526944128
running average episode reward sum: 0.7154491553247501
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.81742537, 10.7314893 ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8603965547915725}
episode index:2084
target Thresh 12.968001008532465
target distance 2.0
model initialize at round 2084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([14.12288666,  7.44804418,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.956581131989328}
done in step count: 25
reward sum = 0.7402226183663343
running average episode reward sum: 0.7154610370816046
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.29429323,  8.56540073,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9042676760545361}
episode index:2085
target Thresh 12.971016254153977
target distance 9.0
model initialize at round 2085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 3.97380116, 11.86775028,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 7.588903857503011}
done in step count: 10
reward sum = 0.8771598344520042
running average episode reward sum: 0.7155385532836037
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.71589893,  9.40210027,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8210943374433423}
episode index:2086
target Thresh 12.974029992529518
target distance 3.0
model initialize at round 2086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.        , 11.14960712,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.523678619271569}
done in step count: 1
reward sum = 0.9858508277709981
running average episode reward sum: 0.7156680752167554
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.19680262, 10.84503889,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8676531546310349}
episode index:2087
target Thresh 12.97704222441253
target distance 8.0
model initialize at round 2087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 9.32783175, 10.70764416,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 7.633115646449426}
done in step count: 27
reward sum = 0.7204303199709207
running average episode reward sum: 0.7156703559853158
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.8901877 ,  7.11186381,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.15675539220267126}
episode index:2088
target Thresh 12.980052950556063
target distance 2.0
model initialize at round 2088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([3.37818074, 5.46211684, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.4794257179210575}
done in step count: 15
reward sum = 0.825121814598438
running average episode reward sum: 0.7157227501732588
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13230967, 6.74902927, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.1462684436411748}
episode index:2089
target Thresh 12.983062171712804
target distance 10.0
model initialize at round 2089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([10.        , 10.58149493,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 10.359347251611732}
done in step count: 4
reward sum = 0.9341903105467718
running average episode reward sum: 0.7158272801064519
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.80218905, 3.90258965, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.22049477593292535}
episode index:2090
target Thresh 12.986069888635054
target distance 2.0
model initialize at round 2090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([5.08288383, 8.65202987, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.5207272328950494}
done in step count: 15
reward sum = 0.8325113853283398
running average episode reward sum: 0.7158830831218617
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.97970496, 10.78353069,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.2174186058377991}
episode index:2091
target Thresh 12.989076102074751
target distance 6.0
model initialize at round 2091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.0000000000000275}
done in step count: 3
reward sum = 0.9543791182806834
running average episode reward sum: 0.7159970869627599
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.70512509, 11.31457704,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.4311727339767948}
episode index:2092
target Thresh 12.992080812783437
target distance 9.0
model initialize at round 2092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.85634828, 4.        , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.092808982729768}
done in step count: 5
reward sum = 0.9319494878453892
running average episode reward sum: 0.7161002653673861
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.61443028, 11.87234567,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9537562473507619}
episode index:2093
target Thresh 12.995084021512294
target distance 2.0
model initialize at round 2093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.        ,  2.16093415,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.1609341502189432}
done in step count: 0
reward sum = 0.9964719766862307
running average episode reward sum: 0.7162341582572233
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.        ,  2.16093415,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.1609341502189432}
episode index:2094
target Thresh 12.998085729012129
target distance 2.0
model initialize at round 2094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.        , 9.56408906, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.4359109401703716}
done in step count: 0
reward sum = 0.9954536197874206
running average episode reward sum: 0.716367437236474
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.        , 9.56408906, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.4359109401703716}
episode index:2095
target Thresh 13.001085936033364
target distance 7.0
model initialize at round 2095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 5.000000000000018}
done in step count: 27
reward sum = 0.7201987841764657
running average episode reward sum: 0.7163692651691744
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.47763507, 10.93500382,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.0710262610894254}
episode index:2096
target Thresh 13.004084643326053
target distance 9.0
model initialize at round 2096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.00494245, 11.86740305,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.058442237262242}
done in step count: 4
reward sum = 0.9376520562223927
running average episode reward sum: 0.7164747886746837
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.49949239, 11.89037527,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.020911732223061}
episode index:2097
target Thresh 13.007081851639866
target distance 2.0
model initialize at round 2097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.      , 11.603197,  0.      ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.6031970023682796}
done in step count: 16
reward sum = 0.8212881462905383
running average episode reward sum: 0.7165247473770745
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.10496016, 10.03327061,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.11010707612605616}
episode index:2098
target Thresh 13.010077561724115
target distance 4.0
model initialize at round 2098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.40483654,  9.56558824,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.8001385450313916}
done in step count: 54
reward sum = 0.4480006428671488
running average episode reward sum: 0.7163968178370507
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.08774771, 11.71674871,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.1601435100977713}
episode index:2099
target Thresh 13.013071774327724
target distance 3.0
model initialize at round 2099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.,  7.,  0.]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0000000000000195}
done in step count: 1
reward sum = 0.9795503361798903
running average episode reward sum: 0.7165221290362616
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.89952738,  5.60804414,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9812129760897351}
episode index:2100
target Thresh 13.016064490199245
target distance 13.0
model initialize at round 2100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.52362919,  8.        ,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.886730646627827}
done in step count: 23
reward sum = 0.7391193901051107
running average episode reward sum: 0.7165328845151139
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.80000828, 1.91009858, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.2192691338255283}
episode index:2101
target Thresh 13.019055710086855
target distance 2.0
model initialize at round 2101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.       , 8.3206085, 0.       ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.3206084966659457}
done in step count: 0
reward sum = 0.9948847331690776
running average episode reward sum: 0.7166653068979179
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.       , 8.3206085, 0.       ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.3206084966659457}
episode index:2102
target Thresh 13.022045434738363
target distance 8.0
model initialize at round 2102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([9.0734563, 8.1307579, 0.       ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 7.957773011564645}
done in step count: 6
reward sum = 0.9128671653591596
running average episode reward sum: 0.7167586030740764
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.86355213, 2.93622498, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.273671662312767}
episode index:2103
target Thresh 13.0250336649012
target distance 8.0
model initialize at round 2103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([9.75971031, 9.08161402, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 7.896407821814412}
done in step count: 30
reward sum = 0.6789565258162167
running average episode reward sum: 0.7167406363073189
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.11672186, 4.33747287, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.6727304021770054}
episode index:2104
target Thresh 13.028020401322419
target distance 7.0
model initialize at round 2104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([12.00007785,  8.00016787,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 5.3852917591447325}
done in step count: 4
reward sum = 0.9414028219113986
running average episode reward sum: 0.7168473641864657
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.34237757,  2.02422153,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0341015554646564}
episode index:2105
target Thresh 13.03100564474871
target distance 9.0
model initialize at round 2105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([6.        , 9.31302214, 0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 7.033629119073641}
done in step count: 4
reward sum = 0.9454655125486283
running average episode reward sum: 0.7169559198124686
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([14.        ,  9.08750021,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.3537562034566721}
episode index:2106
target Thresh 13.033989395926376
target distance 7.0
model initialize at round 2106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86775028,  4.97380116,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 5.100555391695826}
done in step count: 5
reward sum = 0.9357485562129855
running average episode reward sum: 0.7170597606460711
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.77212405,  9.68228899,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.3909830362861597}
episode index:2107
target Thresh 13.036971655601366
target distance 7.0
model initialize at round 2107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([3.19073713, 7.59736574, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 5.976195946323464}
done in step count: 8
reward sum = 0.8975190449276288
running average episode reward sum: 0.7171453675171724
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.48579013, 8.10539409, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.0179940042967117}
episode index:2108
target Thresh 13.039952424519235
target distance 8.0
model initialize at round 2108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 6.0827625302982415}
done in step count: 3
reward sum = 0.9518335547267385
running average episode reward sum: 0.7172566468852187
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.77733421,  3.        ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.22266578674316762}
episode index:2109
target Thresh 13.04293170342518
target distance 8.0
model initialize at round 2109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([5.17664066, 7.14612992, 0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 7.3961346049999825}
done in step count: 3
reward sum = 0.9505066957963498
running average episode reward sum: 0.717367191932096
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.06298775, 10.58932594,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.1069313528673193}
episode index:2110
target Thresh 13.045909493064022
target distance 11.0
model initialize at round 2110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([12.54827142,  8.22917819,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 9.805035566906403}
done in step count: 5
reward sum = 0.9255141645017038
running average episode reward sum: 0.7174657930560039
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.70004539, 5.73485227, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.40034495905706763}
episode index:2111
target Thresh 13.048885794180205
target distance 5.0
model initialize at round 2111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([3.95155549, 7.98122966, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 5.437115562459364}
done in step count: 16
reward sum = 0.8343124467348236
running average episode reward sum: 0.7175211181761171
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.91727247, 9.74738703, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.9514210972401398}
episode index:2112
target Thresh 13.051860607517805
target distance 8.0
model initialize at round 2112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.59008145, 10.41027653,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 7.433733544110399}
done in step count: 59
reward sum = 0.4601001029927726
running average episode reward sum: 0.7173992909091111
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.10273146,  3.94306367,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.301714225575773}
episode index:2113
target Thresh 13.054833933820527
target distance 7.0
model initialize at round 2113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([10.,  9.,  0.]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 6.403124237432876}
done in step count: 2
reward sum = 0.9622794006099732
running average episode reward sum: 0.7175151282363111
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.20442761,  5.89597299,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.1982082519139585}
episode index:2114
target Thresh 13.0578057738317
target distance 2.0
model initialize at round 2114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.,  9.,  0.]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9999999999999805}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.717645853944098
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.,  9.,  0.]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9999999999999805}
episode index:2115
target Thresh 13.060776128294286
target distance 4.0
model initialize at round 2115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.8031798, 8.       , 0.       ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.009661212972769}
done in step count: 1
reward sum = 0.9835702187700857
running average episode reward sum: 0.717771527084375
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.04153377, 6.        , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9584662318229387}
episode index:2116
target Thresh 13.063744997950874
target distance 13.0
model initialize at round 2116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.94971526, 9.        , 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.046431021349337}
done in step count: 24
reward sum = 0.7306844317353429
running average episode reward sum: 0.7177776267086787
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.86384157,  4.63377102,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0713953330023347}
episode index:2117
target Thresh 13.06671238354368
target distance 8.0
model initialize at round 2117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.9999999 , 7.29248315, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 6.582601970369528}
done in step count: 3
reward sum = 0.9521548908746554
running average episode reward sum: 0.7178882864179167
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.99995242, 10.82921574,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.8292157383983607}
episode index:2118
target Thresh 13.069678285814549
target distance 8.0
model initialize at round 2118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 1.46932459, 10.44667304,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 7.465557962505799}
done in step count: 8
reward sum = 0.903667092733668
running average episode reward sum: 0.717975959285456
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.64847715, 3.92810167, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9924419447347576}
episode index:2119
target Thresh 13.072642705504961
target distance 13.0
model initialize at round 2119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.37310886,  8.        ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.533707470505432}
done in step count: 7
reward sum = 0.9018194031570952
running average episode reward sum: 0.7180626778910558
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.92276958, 9.59068749, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.41653483650589035}
episode index:2120
target Thresh 13.075605643356017
target distance 1.0
model initialize at round 2120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.0358398 ,  9.89118922,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.4693761431602055}
done in step count: 6
reward sum = 0.931325866362441
running average episode reward sum: 0.7181632263061767
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.55768681, 11.10097465,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.5667543137256985}
episode index:2121
target Thresh 13.078567100108454
target distance 1.0
model initialize at round 2121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.17081293,  2.59615344,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.6201419034280622}
done in step count: 0
reward sum = 0.9998557981808457
running average episode reward sum: 0.7182959749262872
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.17081293,  2.59615344,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.6201419034280622}
episode index:2122
target Thresh 13.081527076502635
target distance 10.0
model initialize at round 2122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([3.61291742, 6.79543436, 0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 9.918999963260108}
done in step count: 31
reward sum = 0.6890543890474804
running average episode reward sum: 0.7182822012164997
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.19820941, 10.71640677,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.743320681848813}
episode index:2123
target Thresh 13.084485573278554
target distance 12.0
model initialize at round 2123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4., 8., 0.]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.000000000000018}
done in step count: 5
reward sum = 0.9224288747924029
running average episode reward sum: 0.7183783154695956
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.72381672,  8.5033536 ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.5741446217688232}
episode index:2124
target Thresh 13.087442591175837
target distance 13.0
model initialize at round 2124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([15.30786836,  7.86588311,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.49152026357049}
done in step count: 11
reward sum = 0.8664461247932287
running average episode reward sum: 0.7184479944386891
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.54015525, 10.98104905,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0834733181998701}
episode index:2125
target Thresh 13.090398130933735
target distance 6.0
model initialize at round 2125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.36849058, 7.        , 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 4.049543696722036}
done in step count: 2
reward sum = 0.9712260513763734
running average episode reward sum: 0.7185668928662232
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.82005918, 10.39918971,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6271775685061085}
episode index:2126
target Thresh 13.093352193291137
target distance 6.0
model initialize at round 2126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([12.       ,  9.3786037,  0.       ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 5.930612981288964}
done in step count: 14
reward sum = 0.8373448507208977
running average episode reward sum: 0.71862273581773
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.85961988,  4.89986982,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.865431912341158}
episode index:2127
target Thresh 13.096304778986555
target distance 11.0
model initialize at round 2127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([5.9999921 , 8.05601967, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 9.061750372991034}
done in step count: 5
reward sum = 0.927649640048139
running average episode reward sum: 0.7187209627464097
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.72048317,  7.00271474,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.2795300114769146}
episode index:2128
target Thresh 13.099255888758139
target distance 8.0
model initialize at round 2128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 6.32455532033677}
done in step count: 3
reward sum = 0.9544183368486212
running average episode reward sum: 0.7188316707661853
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.41377919,  2.31378004,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.6649155979439503}
episode index:2129
target Thresh 13.102205523343663
target distance 12.0
model initialize at round 2129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.305877894274122}
done in step count: 14
reward sum = 0.8198946017977276
running average episode reward sum: 0.7188791181516461
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.25685447, 2.86221742, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.2914760018560013}
episode index:2130
target Thresh 13.105153683480534
target distance 9.0
model initialize at round 2130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 8.        , 10.17001224,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 7.684333255014197}
done in step count: 4
reward sum = 0.9438887920401536
running average episode reward sum: 0.7189847069240011
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.49747899,  7.01941419,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.5028958885325284}
episode index:2131
target Thresh 13.108100369905797
target distance 4.0
model initialize at round 2131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([12.91343951,  8.69591761,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.19955358740388}
done in step count: 3
reward sum = 0.9624648084582095
running average episode reward sum: 0.7190989095982666
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.57509881,  8.83275098,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9348878046282079}
episode index:2132
target Thresh 13.111045583356127
target distance 11.0
model initialize at round 2132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([11.       , 11.4427284,  0.       ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.044735322746781}
done in step count: 21
reward sum = 0.7281902866339149
running average episode reward sum: 0.7191031718472285
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.11765393, 2.37488868, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9586845747367083}
episode index:2133
target Thresh 13.113989324567815
target distance 8.0
model initialize at round 2133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([1.15078458, 4.99894494, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 10.16460354591691}
done in step count: 19
reward sum = 0.7693964451408207
running average episode reward sum: 0.7191267394542076
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.06672759, 9.02063007, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.3528351105688825}
episode index:2134
target Thresh 13.116931594276808
target distance 13.0
model initialize at round 2134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.3957932,  6.       ,  0.       ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.439584898354576}
done in step count: 7
reward sum = 0.8984947206573037
running average episode reward sum: 0.7192107525601575
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.25878017, 5.58045949, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.6355315898376483}
episode index:2135
target Thresh 13.119872393218667
target distance 7.0
model initialize at round 2135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([13.1365744 ,  3.97336088,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 8.601042058756752}
done in step count: 38
reward sum = 0.6132801595941024
running average episode reward sum: 0.7191611595859224
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.03950204, 10.79745161,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.2483931329199816}
episode index:2136
target Thresh 13.122811722128596
target distance 11.0
model initialize at round 2136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([12.38336222, 11.90298975,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.57438543698544}
done in step count: 16
reward sum = 0.8098041939605883
running average episode reward sum: 0.7192035756057515
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.81308431, 9.89744117, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.21320363559405248}
episode index:2137
target Thresh 13.125749581741422
target distance 12.0
model initialize at round 2137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.00494245, 11.86740305,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.72640106542608}
done in step count: 18
reward sum = 0.7683631888774562
running average episode reward sum: 0.7192265688766925
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.95105052, 7.06703522, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.3322613734964082}
episode index:2138
target Thresh 13.128685972791613
target distance 7.0
model initialize at round 2138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.362011362743921}
done in step count: 3
reward sum = 0.9546419912258547
running average episode reward sum: 0.7193366275126667
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.97441248, 10.31816569,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.0250410180755185}
episode index:2139
target Thresh 13.131620896013267
target distance 1.0
model initialize at round 2139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([5.        , 7.65069127, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.9299693447276711}
done in step count: 3
reward sum = 0.9654430813395142
running average episode reward sum: 0.7194516305284737
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.77484307, 6.91426872, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1984444415720596}
episode index:2140
target Thresh 13.134554352140118
target distance 2.0
model initialize at round 2140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.        ,  9.97311318,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0268868207932051}
done in step count: 8
reward sum = 0.9096878155009612
running average episode reward sum: 0.7195404844215015
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.40702411, 11.89544908,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0739876441338991}
episode index:2141
target Thresh 13.137486341905522
target distance 1.0
model initialize at round 2141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.12835895, 5.12862339, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8810800743444072}
done in step count: 0
reward sum = 0.9964181592163079
running average episode reward sum: 0.7196697457075869
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.12835895, 5.12862339, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8810800743444072}
episode index:2142
target Thresh 13.14041686604248
target distance 10.0
model initialize at round 2142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([11.14346187,  8.13162534,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.625463618402268}
done in step count: 10
reward sum = 0.8649817011130464
running average episode reward sum: 0.7197375534329277
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.38115752, 3.6046405 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8651913887672448}
episode index:2143
target Thresh 13.143345925283628
target distance 2.0
model initialize at round 2143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.        ,  9.98118204,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.9811820387839862}
done in step count: 0
reward sum = 0.9962779611708105
running average episode reward sum: 0.7198665368320591
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.        ,  9.98118204,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.9811820387839862}
episode index:2144
target Thresh 13.146273520361227
target distance 8.0
model initialize at round 2144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 8.70625269, 10.10382426,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 7.0174767668847915}
done in step count: 6
reward sum = 0.9302046834297352
running average episode reward sum: 0.7199645965740628
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.34920525,  6.81033233,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.3973891422726118}
episode index:2145
target Thresh 13.149199652007173
target distance 13.0
model initialize at round 2145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.        ,  3.04849768,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.688215030955975}
done in step count: 30
reward sum = 0.6798132595382486
running average episode reward sum: 0.7199458867245586
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.67060335, 7.46748206, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5718755371796429}
episode index:2146
target Thresh 13.152124320953
target distance 10.0
model initialize at round 2146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([14.29172134,  7.3587122 ,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 8.452601286045493}
done in step count: 7
reward sum = 0.9031765624273738
running average episode reward sum: 0.7200312293774245
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.66286978, 8.45840969, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.637947376843956}
episode index:2147
target Thresh 13.15504752792988
target distance 7.0
model initialize at round 2147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([4.87062598, 3.13181335, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 7.17546823510129}
done in step count: 10
reward sum = 0.8643596022698686
running average episode reward sum: 0.7200984213573557
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.82824562, 8.85159657, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.22698709992551944}
episode index:2148
target Thresh 13.157969273668611
target distance 8.0
model initialize at round 2148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([8.97380116, 8.13224972, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 8.59764846499265}
done in step count: 5
reward sum = 0.925018716363449
running average episode reward sum: 0.720193777474157
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.49024681,  1.35860189,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8192922831317225}
episode index:2149
target Thresh 13.16088955889963
target distance 11.0
model initialize at round 2149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([5., 9., 0.]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.48683298050515}
done in step count: 6
reward sum = 0.9099058521411354
running average episode reward sum: 0.7202820156484208
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.42778243,  5.53844212,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6293119142298161}
episode index:2150
target Thresh 13.163808384353008
target distance 5.0
model initialize at round 2150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4., 7., 0.]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 3.162277660168404}
done in step count: 17
reward sum = 0.8006613568132888
running average episode reward sum: 0.7203193840078651
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.21935133, 10.38514788,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.8704890810302868}
episode index:2151
target Thresh 13.166725750758452
target distance 12.0
model initialize at round 2151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 10.049875621120906}
done in step count: 7
reward sum = 0.8984526039588735
running average episode reward sum: 0.7204021596676936
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.26177259,  9.76528783,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0633179965824069}
episode index:2152
target Thresh 13.169641658845304
target distance 2.0
model initialize at round 2152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.66074616, 11.        ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.66074615716939}
done in step count: 0
reward sum = 0.9969136801851975
running average episode reward sum: 0.720530590471464
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.66074616, 11.        ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.66074615716939}
episode index:2153
target Thresh 13.172556109342539
target distance 4.0
model initialize at round 2153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5., 8., 0.]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 2.2360679774998147}
done in step count: 1
reward sum = 0.9801252488580638
running average episode reward sum: 0.7206511079544661
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.7348932 , 9.48615801, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.8967171274963903}
episode index:2154
target Thresh 13.175469102978772
target distance 9.0
model initialize at round 2154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([4.91296532, 5.5212277 , 0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 9.244432441363681}
done in step count: 8
reward sum = 0.8946961426065525
running average episode reward sum: 0.7207318713116132
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.59813041, 10.84532938,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.0355393475146661}
episode index:2155
target Thresh 13.178380640482247
target distance 12.0
model initialize at round 2155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([5.94478322, 8.045681  , 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 10.838584739476389}
done in step count: 24
reward sum = 0.7175414785455603
running average episode reward sum: 0.720730391537603
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.91232666,  3.47544013,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.052379683310723}
episode index:2156
target Thresh 13.181290722580856
target distance 3.0
model initialize at round 2156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.99219203,  3.04004776,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0400770728250097}
done in step count: 1
reward sum = 0.9850068677247119
running average episode reward sum: 0.720852911925265
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.38234642,  1.74916208,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.45728377314328145}
episode index:2157
target Thresh 13.184199350002114
target distance 11.0
model initialize at round 2157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.77310586,  8.98648286,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.554887196383437}
done in step count: 21
reward sum = 0.7619611912380146
running average episode reward sum: 0.7208719611742515
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.45476701, 5.83987049, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9550892528014889}
episode index:2158
target Thresh 13.18710652347318
target distance 3.0
model initialize at round 2158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([10.50603947, 11.91238497,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.760852407198071}
done in step count: 1
reward sum = 0.9849661141503583
running average episode reward sum: 0.7209942836165749
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.8453273, 11.8259019,  0.       ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.1818173234162115}
episode index:2159
target Thresh 13.190012243720844
target distance 9.0
model initialize at round 2159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([5.17664066, 7.14612992, 0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 8.327636265579082}
done in step count: 5
reward sum = 0.9259227535194718
running average episode reward sum: 0.7210891579081966
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.98662187,  9.49934185,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.50083685277216}
episode index:2160
target Thresh 13.192916511471541
target distance 10.0
model initialize at round 2160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.87135101, 4.88585854, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.012911282887437}
done in step count: 33
reward sum = 0.6514089369024937
running average episode reward sum: 0.7210569134745983
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.91106301,  8.45885592,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0596568855015045}
episode index:2161
target Thresh 13.195819327451336
target distance 12.0
model initialize at round 2161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.65131617,  5.        ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 13.105463303459555}
done in step count: 8
reward sum = 0.882886095027016
running average episode reward sum: 0.721131765084937
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.22124147, 11.86690582,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.1653285129824766}
episode index:2162
target Thresh 13.198720692385931
target distance 10.0
model initialize at round 2162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.246211251235342}
done in step count: 4
reward sum = 0.9364261365137527
running average episode reward sum: 0.7212313001618806
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.02632597, 10.67039888,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.3306508104587171}
episode index:2163
target Thresh 13.201620607000667
target distance 10.0
model initialize at round 2163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([3.16631556, 6.15855265, 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 10.073410213979457}
done in step count: 4
reward sum = 0.9400384326570812
running average episode reward sum: 0.7213324125151594
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.16631556, 10.56313646,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9412117226545819}
episode index:2164
target Thresh 13.20451907202053
target distance 6.0
model initialize at round 2164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4., 6., 0.]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 4.000000000000019}
done in step count: 2
reward sum = 0.962583450386452
running average episode reward sum: 0.7214438448652155
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.87544544, 2.16948469, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8917004945294259}
episode index:2165
target Thresh 13.207416088170126
target distance 7.0
model initialize at round 2165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([10.97845473,  8.13862257,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 6.5072354821822005}
done in step count: 3
reward sum = 0.9514439784663191
running average episode reward sum: 0.7215500314458254
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.87695008,  4.79364771,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.1827586948000486}
episode index:2166
target Thresh 13.210311656173715
target distance 4.0
model initialize at round 2166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 9.08939462, 11.8685385 ,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 4.99516380812064}
done in step count: 13
reward sum = 0.8394400165349255
running average episode reward sum: 0.7216044338385754
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.06829959, 9.35207578, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.35863936250155926}
episode index:2167
target Thresh 13.213205776755188
target distance 1.0
model initialize at round 2167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.42687833, 10.08275998,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.4348267758025491}
done in step count: 0
reward sum = 0.9988743717342722
running average episode reward sum: 0.7217323258763502
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.42687833, 10.08275998,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.4348267758025491}
episode index:2168
target Thresh 13.216098450638079
target distance 5.0
model initialize at round 2168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.87931061,  5.        ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 3.0024266734365295}
done in step count: 2
reward sum = 0.9704268762599764
running average episode reward sum: 0.7218469844980117
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.11056432,  1.13120779,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.2433405592184805}
episode index:2169
target Thresh 13.218989678545546
target distance 7.0
model initialize at round 2169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([4.954319  , 7.05521678, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.855187222340975}
done in step count: 3
reward sum = 0.9489762178044566
running average episode reward sum: 0.7219516523474616
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.14523211, 1.12878119, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.2205123352666467}
episode index:2170
target Thresh 13.221879461200409
target distance 11.0
model initialize at round 2170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([3.49372816, 4.94155979, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 11.262889721709406}
done in step count: 24
reward sum = 0.7154032309136958
running average episode reward sum: 0.7219486360317391
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.6866563 ,  9.53381388,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6189842697247404}
episode index:2171
target Thresh 13.2247677993251
target distance 7.0
model initialize at round 2171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.96490201, 4.        , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 5.092252536523918}
done in step count: 3
reward sum = 0.9567545272598695
running average episode reward sum: 0.7220567418748459
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.66263229, 9.98545406, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0416029416869763}
episode index:2172
target Thresh 13.227654693641716
target distance 11.0
model initialize at round 2172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([5.97303629, 8.52560556, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.583874084348189}
done in step count: 7
reward sum = 0.9024505874599261
running average episode reward sum: 0.7221397579105501
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.9830468 ,  2.47529903,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.5249747764064249}
episode index:2173
target Thresh 13.230540144871973
target distance 4.0
model initialize at round 2173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([16.22686863, 11.15171456,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 4.380966200069691}
done in step count: 24
reward sum = 0.7534758925303823
running average episode reward sum: 0.7221541719559134
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.28706299,  9.43148224,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.6368811492559969}
episode index:2174
target Thresh 13.233424153737237
target distance 11.0
model initialize at round 2174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([12.24206705,  7.76791273,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 9.409639674128853}
done in step count: 25
reward sum = 0.7102584990422438
running average episode reward sum: 0.7221487026810106
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.24113535, 6.7104119 , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7502208551048238}
episode index:2175
target Thresh 13.23630672095851
target distance 4.0
model initialize at round 2175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 4.04797029, 11.4085623 ,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 3.270860305754329}
done in step count: 21
reward sum = 0.775819228237337
running average episode reward sum: 0.7221733674445935
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.13519515, 9.39273892, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.056718238360079}
episode index:2176
target Thresh 13.239187847256435
target distance 8.0
model initialize at round 2176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([11.,  9.,  0.]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 7.810249675906681}
done in step count: 3
reward sum = 0.9492430765187458
running average episode reward sum: 0.7222776713991521
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.40430029,  3.44277281,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5995885990490064}
episode index:2177
target Thresh 13.242067533351289
target distance 7.0
model initialize at round 2177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.139134  , 9.00025276, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.335292866899473}
done in step count: 4
reward sum = 0.9492809922669662
running average episode reward sum: 0.7223818969826543
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.91734147, 4.73573642, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.1759351400037257}
episode index:2178
target Thresh 13.244945779962997
target distance 2.0
model initialize at round 2178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16., 11.,  0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9999999999999822}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7225065496229586
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16., 11.,  0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9999999999999822}
episode index:2179
target Thresh 13.24782258781112
target distance 2.0
model initialize at round 2179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 3.94831175, 10.60865158,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.2151151909465792}
done in step count: 64
reward sum = 0.41971225651298893
running average episode reward sum: 0.7223676531582294
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.67592815, 9.02061992, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.1899849631678405}
episode index:2180
target Thresh 13.250697957614861
target distance 2.0
model initialize at round 2180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([7.        , 9.80709255, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.1929074525833219}
done in step count: 13
reward sum = 0.8479995373383167
running average episode reward sum: 0.7224252560395591
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.66694795, 11.87610458,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9372741886879561}
episode index:2181
target Thresh 13.25357189009306
target distance 11.0
model initialize at round 2181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.13577074,  3.98171888,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.327684072641597}
done in step count: 33
reward sum = 0.628520578633555
running average episode reward sum: 0.7223822199820861
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.0969956 , 1.17277142, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.224632219739005}
episode index:2182
target Thresh 13.256444385964201
target distance 2.0
model initialize at round 2182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.      , 2.555076, 0.      ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.4449239969253447}
done in step count: 1
reward sum = 0.9832868192496795
running average episode reward sum: 0.7225017365186265
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.90982579, 3.45721473, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0594332542788922}
episode index:2183
target Thresh 13.25931544594641
target distance 9.0
model initialize at round 2183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.20519602,  9.        ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 7.003006883321627}
done in step count: 4
reward sum = 0.9464775755538747
running average episode reward sum: 0.7226042895584778
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.61729893,  2.26696002,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.4666130765014719}
episode index:2184
target Thresh 13.26218507075745
target distance 8.0
model initialize at round 2184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 8.190193112702614}
done in step count: 6
reward sum = 0.9146915959810004
running average episode reward sum: 0.7226922013691974
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.41947299, 9.66785939, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.7886658073154925}
episode index:2185
target Thresh 13.265053261114728
target distance 13.0
model initialize at round 2185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.88436359, 5.28486866, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.498803905878466}
done in step count: 7
reward sum = 0.9006294448268579
running average episode reward sum: 0.7227735999252165
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.44465354, 10.5895322 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6051450944812156}
episode index:2186
target Thresh 13.267920017735293
target distance 1.0
model initialize at round 2186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.59647276, 11.90599001,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.0847108568739077}
done in step count: 0
reward sum = 0.9950644242272033
running average episode reward sum: 0.722898104188729
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.59647276, 11.90599001,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.0847108568739077}
episode index:2187
target Thresh 13.270785341335829
target distance 13.0
model initialize at round 2187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 4.1694777 , 11.05284047,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.877277925545375}
done in step count: 35
reward sum = 0.627717505567667
running average episode reward sum: 0.7228546030010594
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.87864415,  9.44017954,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0418322780119946}
episode index:2188
target Thresh 13.273649232632671
target distance 5.0
model initialize at round 2188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3., 9., 0.]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.162277660168392}
done in step count: 3
reward sum = 0.9605439328413976
running average episode reward sum: 0.7229631865231427
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.1456424 , 5.36857319, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.062368451604912}
episode index:2189
target Thresh 13.276511692341796
target distance 11.0
model initialize at round 2189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.13224963,  5.97380131,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.178937641609224}
done in step count: 11
reward sum = 0.8542909386993334
running average episode reward sum: 0.7230231535332688
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.63109287, 5.85797877, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9339272119544229}
episode index:2190
target Thresh 13.27937272117881
target distance 2.0
model initialize at round 2190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.80617805, 9.8706857 , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.8806998496436453}
done in step count: 6
reward sum = 0.9351174503336921
running average episode reward sum: 0.7231199560420778
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.16326142, 8.12615614, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8461954996431605}
episode index:2191
target Thresh 13.282232319858974
target distance 7.0
model initialize at round 2191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 7.61860805, 11.90123451,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 7.009944714664261}
done in step count: 5
reward sum = 0.9375054298057808
running average episode reward sum: 0.7232177596341233
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.0322214 ,  9.02069877,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.03829696831251919}
episode index:2192
target Thresh 13.285090489097191
target distance 8.0
model initialize at round 2192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([13.18284501,  6.76074223,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 7.523798094605035}
done in step count: 8
reward sum = 0.8942197089334706
running average episode reward sum: 0.7232957358991936
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.49370595, 9.00638844, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.49374728149724173}
episode index:2193
target Thresh 13.287947229608
target distance 2.0
model initialize at round 2193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.86088908, 11.        ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8608890771865543}
done in step count: 0
reward sum = 0.9944028183764504
running average episode reward sum: 0.7234193033934859
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.86088908, 11.        ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8608890771865543}
episode index:2194
target Thresh 13.290802542105586
target distance 3.0
model initialize at round 2194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 8.99598394, 11.86724533,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.3267112407186834}
done in step count: 1
reward sum = 0.9821915541606112
running average episode reward sum: 0.7235371950794847
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.10038787, 10.70940622,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.30744506242542013}
episode index:2195
target Thresh 13.293656427303777
target distance 5.0
model initialize at round 2195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([8.56497169, 9.55315578, 0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 3.463970161408665}
done in step count: 2
reward sum = 0.9739847959856325
running average episode reward sum: 0.7236512422565822
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.74283028,  9.83189523,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.30723847357946016}
episode index:2196
target Thresh 13.296508885916046
target distance 13.0
model initialize at round 2196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.99855048, 6.89048236, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.432453435535551}
done in step count: 6
reward sum = 0.916863329230301
running average episode reward sum: 0.7237391858555688
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.28669451,  9.6644299 ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.44136270511710857}
episode index:2197
target Thresh 13.299359918655506
target distance 13.0
model initialize at round 2197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.39412236, 9.8692168 , 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.95528338388263}
done in step count: 7
reward sum = 0.9036743431151992
running average episode reward sum: 0.7238210489844404
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.87110724,  6.13148093,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.2301029190689718}
episode index:2198
target Thresh 13.302209526234918
target distance 4.0
model initialize at round 2198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([5.       , 7.6675663, 0.       ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 3.479240487324513}
done in step count: 5
reward sum = 0.9332839502244422
running average episode reward sum: 0.7239163026912344
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.9167346, 11.8331793,  0.       ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8373296116434321}
episode index:2199
target Thresh 13.305057709366679
target distance 9.0
model initialize at round 2199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 8.602325267042653}
done in step count: 11
reward sum = 0.8618999690813706
running average episode reward sum: 0.7239790225395936
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.95213485, 9.66553666, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.3378709824598894}
episode index:2200
target Thresh 13.307904468762839
target distance 7.0
model initialize at round 2200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.18857668,  5.99993746,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.522637504257758}
done in step count: 3
reward sum = 0.9525386789123336
running average episode reward sum: 0.724082866090876
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.93111366, 11.86796904,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.8706983328205193}
episode index:2201
target Thresh 13.310749805135087
target distance 7.0
model initialize at round 2201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([10.99505755, 11.86740305,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 5.341969964170625}
done in step count: 9
reward sum = 0.8959448932794679
running average episode reward sum: 0.7241609142412796
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.09641622,  9.60486681,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.986201748564171}
episode index:2202
target Thresh 13.313593719194754
target distance 8.0
model initialize at round 2202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([4.86276141, 4.98123072, 0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 8.725148702099753}
done in step count: 8
reward sum = 0.8905955695550594
running average episode reward sum: 0.7242364633358387
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.21393124, 10.48566584,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.5306956589306384}
episode index:2203
target Thresh 13.316436211652825
target distance 2.0
model initialize at round 2203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.55375046,  5.61231291,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8255705119414228}
done in step count: 0
reward sum = 0.9983257636434448
running average episode reward sum: 0.7243608232724573
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.55375046,  5.61231291,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8255705119414228}
episode index:2204
target Thresh 13.31927728321992
target distance 7.0
model initialize at round 2204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([7.62153888, 9.        , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 6.899398477483348}
done in step count: 4
reward sum = 0.9399492035971926
running average episode reward sum: 0.7244585957805412
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.58076279, 4.90170004, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.5890231715317941}
episode index:2205
target Thresh 13.322116934606303
target distance 12.0
model initialize at round 2205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.        , 9.65167367, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.021211432663494}
done in step count: 42
reward sum = 0.5676499423415668
running average episode reward sum: 0.7243875129820647
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.47285631,  8.62523213,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6467854567281899}
episode index:2206
target Thresh 13.324955166521892
target distance 4.0
model initialize at round 2206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.48584044, 5.        , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 2.065032700170833}
done in step count: 4
reward sum = 0.9503812329945458
running average episode reward sum: 0.7244899115865108
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.3622433, 3.9415437, 0.       ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.1372062904290465}
episode index:2207
target Thresh 13.32779197967624
target distance 7.0
model initialize at round 2207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([4.68263303, 6.50609281, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 6.362529285660516}
done in step count: 4
reward sum = 0.9467953034071084
running average episode reward sum: 0.7245905933762846
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.56288504, 9.65979379, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.5539041068871667}
episode index:2208
target Thresh 13.330627374778558
target distance 12.0
model initialize at round 2208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 6.        , 10.46108824,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 10.949945583717508}
done in step count: 33
reward sum = 0.6704935037946004
running average episode reward sum: 0.7245661039740294
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.70553209,  5.6159882 ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.4839177729587118}
episode index:2209
target Thresh 13.333461352537688
target distance 7.0
model initialize at round 2209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([13.10681674,  5.33785896,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 7.120708371418753}
done in step count: 4
reward sum = 0.9391660698419119
running average episode reward sum: 0.7246632080309832
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.14450714, 9.95760327, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.9684453221943807}
episode index:2210
target Thresh 13.336293913662125
target distance 4.0
model initialize at round 2210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.02619884, 11.86775028,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.204194245766441}
done in step count: 1
reward sum = 0.9815857906970213
running average episode reward sum: 0.724779410013193
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.22330696, 11.74647978,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7791649769730247}
episode index:2211
target Thresh 13.339125058860013
target distance 12.0
model initialize at round 2211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([15.40259731,  7.        ,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.450671683569258}
done in step count: 51
reward sum = 0.5009640848936059
running average episode reward sum: 0.724678227678148
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.66934142, 2.2313162 , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7081843793981932}
episode index:2212
target Thresh 13.341954788839134
target distance 4.0
model initialize at round 2212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([6.        , 8.63955879, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.5861463672420317}
done in step count: 1
reward sum = 0.9819072547140925
running average episode reward sum: 0.724794463117387
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.34842917, 6.63955882, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5013189854424843}
episode index:2213
target Thresh 13.344783104306925
target distance 5.0
model initialize at round 2213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.29114789,  5.87092102,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.14259484856974}
done in step count: 2
reward sum = 0.9708679020247536
running average episode reward sum: 0.7249056073987363
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.40069157,  9.87092102,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9586746836154127}
episode index:2214
target Thresh 13.347610005970461
target distance 5.0
model initialize at round 2214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([1.23036555, 6.99999227, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 3.4830521796971388}
done in step count: 3
reward sum = 0.9592400629365458
running average episode reward sum: 0.7250114017353223
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.80171506, 10.41626347,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.46107721067417173}
episode index:2215
target Thresh 13.35043549453647
target distance 4.0
model initialize at round 2215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([7.        , 9.96628797, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 2.0002841050536277}
done in step count: 1
reward sum = 0.9828920446064612
running average episode reward sum: 0.7251277738665818
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.        , 9.19852763, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.8014723658562186}
episode index:2216
target Thresh 13.353259570711325
target distance 12.0
model initialize at round 2216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([3.30660009, 8.35834169, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 11.547464824524317}
done in step count: 19
reward sum = 0.7688139682062273
running average episode reward sum: 0.7251474789610065
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.78913386,  4.47236493,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9197069465311538}
episode index:2217
target Thresh 13.35608223520104
target distance 7.0
model initialize at round 2217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4., 8., 0.]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 5.385164807134529}
done in step count: 3
reward sum = 0.9546247834986566
running average episode reward sum: 0.7252509403246393
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([10.       ,  9.0774349,  0.       ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.36056104916071}
episode index:2218
target Thresh 13.358903488711286
target distance 13.0
model initialize at round 2218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 4.        , 10.03113317,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 13.619805434473692}
done in step count: 10
reward sum = 0.8639518414445021
running average episode reward sum: 0.7253134463639003
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.89421713,  2.69279477,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7008242393811874}
episode index:2219
target Thresh 13.361723331947374
target distance 3.0
model initialize at round 2219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.        , 11.49400079,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.115363968843334}
done in step count: 5
reward sum = 0.9363741518142144
running average episode reward sum: 0.7254085187537427
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.55702114, 10.78709853,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.49148479803474615}
episode index:2220
target Thresh 13.364541765614268
target distance 7.0
model initialize at round 2220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3., 9., 0.]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.0990195135928005}
done in step count: 35
reward sum = 0.650940076126741
running average episode reward sum: 0.7253749895134783
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.99403872, 4.35599521, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.35604511550972084}
episode index:2221
target Thresh 13.36735879041657
target distance 12.0
model initialize at round 2221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.07617533, 11.53154892,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 12.007932401478326}
done in step count: 6
reward sum = 0.9078574221025839
running average episode reward sum: 0.7254571148206742
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.54172854, 4.77678658, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.5097420575642398}
episode index:2222
target Thresh 13.370174407058544
target distance 7.0
model initialize at round 2222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([10.56037855, 10.06808329,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 7.434712482664642}
done in step count: 8
reward sum = 0.9091812825618717
running average episode reward sum: 0.7255397617697255
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.35442066,  4.48812079,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6226028644585477}
episode index:2223
target Thresh 13.372988616244088
target distance 13.0
model initialize at round 2223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.        ,  9.99641001,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.000000585818734}
done in step count: 16
reward sum = 0.8122858865030158
running average episode reward sum: 0.7255787663222134
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.10070647, 10.61329184,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.088510788632426}
episode index:2224
target Thresh 13.375801418676758
target distance 6.0
model initialize at round 2224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([3.67833543, 4.99508226, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 5.176485181341114}
done in step count: 99
reward sum = -0.1307911245474476
running average episode reward sum: 0.7251938809780024
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.91034329, 2.45081289, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 7.549719489655113}
episode index:2225
target Thresh 13.378612815059752
target distance 6.0
model initialize at round 2225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([9.09948897, 9.        , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 7.70731754156506}
done in step count: 51
reward sum = 0.503256856868022
running average episode reward sum: 0.7250941788108372
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.64441285, 5.82914522, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.39450421083037096}
episode index:2226
target Thresh 13.381422806095925
target distance 10.0
model initialize at round 2226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([3.9097079 , 8.15878844, 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.256902777371184}
done in step count: 10
reward sum = 0.8793953208633556
running average episode reward sum: 0.7251634653586828
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.9780333 ,  9.70044862,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.3003557307483655}
episode index:2227
target Thresh 13.384231392487765
target distance 13.0
model initialize at round 2227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.41709077,  6.        ,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.097518825454339}
done in step count: 13
reward sum = 0.8157647614908559
running average episode reward sum: 0.7252041302133202
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.75141451, 1.11613228, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.1601059915636167}
episode index:2228
target Thresh 13.387038574937428
target distance 11.0
model initialize at round 2228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.85648583, 6.00016969, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.623142394716595}
done in step count: 27
reward sum = 0.702630691478666
running average episode reward sum: 0.7251940030537264
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.63969102,  3.11976767,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.3796931070963645}
episode index:2229
target Thresh 13.389844354146703
target distance 12.0
model initialize at round 2229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4., 6., 0.]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.770329614269018}
done in step count: 99
reward sum = -0.16516223841721314
running average episode reward sum: 0.7247947401651743
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([16.22835766,  6.6676504 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.1722855764431666}
episode index:2230
target Thresh 13.392648730817037
target distance 2.0
model initialize at round 2230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 8.00709109, 11.8672649 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.1296557722340554}
done in step count: 17
reward sum = 0.7802126662763514
running average episode reward sum: 0.724819580114126
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.9131253 , 11.85727026,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.2524815819597563}
episode index:2231
target Thresh 13.395451705649524
target distance 1.0
model initialize at round 2231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.44844186, 11.17781636,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.48240933122310486}
done in step count: 0
reward sum = 0.9984027980976323
running average episode reward sum: 0.7249421532404626
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.44844186, 11.17781636,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.48240933122310486}
episode index:2232
target Thresh 13.39825327934491
target distance 7.0
model initialize at round 2232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.52349555, 3.88439429, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 5.324423647670116}
done in step count: 3
reward sum = 0.960142142960255
running average episode reward sum: 0.7250474823894639
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.98751249, 8.74518186, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.2551239373586353}
episode index:2233
target Thresh 13.401053452603586
target distance 4.0
model initialize at round 2233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4., 9., 0.]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 2.0000000000000187}
done in step count: 1
reward sum = 0.9809445435887169
running average episode reward sum: 0.725162028970126
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.        , 8.66074693, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.33925306797030963}
episode index:2234
target Thresh 13.403852226125595
target distance 6.0
model initialize at round 2234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([16.        ,  9.92091417,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 5.021493433160465}
done in step count: 2
reward sum = 0.9700964041237885
running average episode reward sum: 0.7252716192945796
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.89764941,  5.92091417,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.286023861694186}
episode index:2235
target Thresh 13.406649600610631
target distance 1.0
model initialize at round 2235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.87315301, 5.24643075, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1533701908255216}
done in step count: 0
reward sum = 0.9967069655883363
running average episode reward sum: 0.7253930125621528
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.87315301, 5.24643075, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1533701908255216}
episode index:2236
target Thresh 13.409445576758042
target distance 12.0
model initialize at round 2236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.58486009,  6.49127281,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.594445298079412}
done in step count: 34
reward sum = 0.6577203126115577
running average episode reward sum: 0.7253627610199308
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.72546302, 6.22391084, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.759231628224122}
episode index:2237
target Thresh 13.412240155266813
target distance 9.0
model initialize at round 2237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([15.28107321,  6.59467804,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 8.95390368357134}
done in step count: 10
reward sum = 0.8814690335509108
running average episode reward sum: 0.7254325135992565
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 7.89562971, 10.29051693,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.9415692589806944}
episode index:2238
target Thresh 13.415033336835595
target distance 8.0
model initialize at round 2238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 8.8915025 , 11.38474178,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 7.292448153671451}
done in step count: 18
reward sum = 0.7823932173205741
running average episode reward sum: 0.7254579538420977
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.98559065, 8.29165049, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.2137330639650026}
episode index:2239
target Thresh 13.417825122162684
target distance 6.0
model initialize at round 2239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 8.        , 11.17173877,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 4.55153263205812}
done in step count: 16
reward sum = 0.8152921430132006
running average episode reward sum: 0.7254980583908348
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.66690862,  9.74822662,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.0023024402604355}
episode index:2240
target Thresh 13.420615511946027
target distance 8.0
model initialize at round 2240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 3.99505755, 11.86740305,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 6.288603024860095}
done in step count: 21
reward sum = 0.776221020391586
running average episode reward sum: 0.7255206924658016
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.20471227,  9.28048453,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.7480705991866043}
episode index:2241
target Thresh 13.423404506883214
target distance 10.0
model initialize at round 2241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 4.61358849, 11.90343871,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.434933283629714}
done in step count: 23
reward sum = 0.735290811856741
running average episode reward sum: 0.7255250502353784
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.78434615, 10.9519569 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7858161484854564}
episode index:2242
target Thresh 13.4261921076715
target distance 6.0
model initialize at round 2242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([10.03147922,  8.13390758,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 5.407380173234573}
done in step count: 4
reward sum = 0.9416613370939875
running average episode reward sum: 0.7256214105951013
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.08815871,  5.40338414,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.6030940545547926}
episode index:2243
target Thresh 13.428978315007782
target distance 2.0
model initialize at round 2243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 6.75613678, 11.        ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.243863224983217}
done in step count: 1
reward sum = 0.9861204309471198
running average episode reward sum: 0.7257374975025666
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.51779211, 11.1642174 ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.5094033833951868}
episode index:2244
target Thresh 13.431763129588617
target distance 1.0
model initialize at round 2244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.21751142, 10.87935477,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9058565207003882}
done in step count: 0
reward sum = 0.998308300806691
running average episode reward sum: 0.7258589098871118
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.21751142, 10.87935477,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9058565207003882}
episode index:2245
target Thresh 13.434546552110206
target distance 12.0
model initialize at round 2245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.315143201383691}
done in step count: 13
reward sum = 0.8301321662581429
running average episode reward sum: 0.7259053360920855
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.84614313,  7.87822321,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.2195221225072337}
episode index:2246
target Thresh 13.437328583268402
target distance 10.0
model initialize at round 2246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 6.        , 10.10229576,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.061710267039398}
done in step count: 25
reward sum = 0.7284823440397622
running average episode reward sum: 0.7259064829581059
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.35453965,  3.17033308,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0511737559754162}
episode index:2247
target Thresh 13.440109223758714
target distance 2.0
model initialize at round 2247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([14.63697827,  6.72951388,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.3896010144553053}
done in step count: 11
reward sum = 0.8723109165794352
running average episode reward sum: 0.7259716094855175
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.05818485,  6.0390154 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9627444545890975}
episode index:2248
target Thresh 13.442888474276305
target distance 1.0
model initialize at round 2248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.        , 10.70077264,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0438088958826413}
done in step count: 0
reward sum = 0.9966316882220597
running average episode reward sum: 0.7260919563413364
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.        , 10.70077264,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0438088958826413}
episode index:2249
target Thresh 13.445666335515984
target distance 5.0
model initialize at round 2249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 8.97381938, 10.77671593,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.03113778868644}
done in step count: 22
reward sum = 0.7673196185040402
running average episode reward sum: 0.726110279746742
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.24115215, 10.28338277,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7561048957790418}
episode index:2250
target Thresh 13.448442808172219
target distance 6.0
model initialize at round 2250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 5.82635534, 10.08974171,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 4.174609363495282}
done in step count: 3
reward sum = 0.960322786974534
running average episode reward sum: 0.7262143279507526
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.96949112, 10.05354321,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.9709685448829091}
episode index:2251
target Thresh 13.451217892939125
target distance 4.0
model initialize at round 2251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([12.5551964 ,  9.05537713,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.052280789550372}
done in step count: 3
reward sum = 0.9649496224599248
running average episode reward sum: 0.7263203382946731
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.74332786, 10.48218788,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9059060076019672}
episode index:2252
target Thresh 13.453991590510478
target distance 1.0
model initialize at round 2252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.82863188,  5.19809854,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.26193523043643113}
done in step count: 0
reward sum = 0.9984886379108604
running average episode reward sum: 0.7264411409132334
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.82863188,  5.19809854,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.26193523043643113}
episode index:2253
target Thresh 13.4567639015797
target distance 12.0
model initialize at round 2253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 4.        , 10.95914066,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 11.640934558116102}
done in step count: 74
reward sum = 0.34071443985083094
running average episode reward sum: 0.7262700110547319
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.12631258,  5.18504996,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.22404989423612306}
episode index:2254
target Thresh 13.459534826839867
target distance 12.0
model initialize at round 2254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([2.46984959, 5.        , 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 11.914040815703036}
done in step count: 10
reward sum = 0.872617484235966
running average episode reward sum: 0.7263349101559209
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.38385759,  7.23247397,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8581624885167857}
episode index:2255
target Thresh 13.462304366983714
target distance 1.0
model initialize at round 2255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.        ,  9.22313637,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.2663005594022787}
done in step count: 0
reward sum = 0.996547362374108
running average episode reward sum: 0.7264546851790672
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.        ,  9.22313637,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.2663005594022787}
episode index:2256
target Thresh 13.465072522703618
target distance 10.0
model initialize at round 2256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([12.50317247,  7.50317247,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.621980268562897}
done in step count: 56
reward sum = 0.440845711409115
running average episode reward sum: 0.7263281415486862
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.05654317, 2.11758399, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8842257360650441}
episode index:2257
target Thresh 13.467839294691633
target distance 1.0
model initialize at round 2257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.10407094, 8.1297184 , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.8764820744813718}
done in step count: 0
reward sum = 0.99623477535031
running average episode reward sum: 0.7264476750446125
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.10407094, 8.1297184 , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.8764820744813718}
episode index:2258
target Thresh 13.470604683639436
target distance 12.0
model initialize at round 2258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.34236634,  8.3590695 ,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.64835470493309}
done in step count: 27
reward sum = 0.6912056515782933
running average episode reward sum: 0.7264320743259466
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.63093636, 3.32273843, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.49027346116377185}
episode index:2259
target Thresh 13.473368690238384
target distance 5.0
model initialize at round 2259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.70763534, 7.25884283, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.2688662790312595}
done in step count: 4
reward sum = 0.9552072731651671
running average episode reward sum: 0.7265333022900347
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.83987209, 3.65346253, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6727958263568725}
episode index:2260
target Thresh 13.476131315179472
target distance 10.0
model initialize at round 2260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([12.16187799, 10.        ,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.089348289062036}
done in step count: 24
reward sum = 0.7423316476192631
running average episode reward sum: 0.7265402896165846
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.61239321, 6.87927392, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0715167109903077}
episode index:2261
target Thresh 13.478892559153365
target distance 12.0
model initialize at round 2261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4., 7., 0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.440306508910572}
done in step count: 11
reward sum = 0.8622868464179605
running average episode reward sum: 0.7266003013569918
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.64909751, 10.57211244,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6711521394982752}
episode index:2262
target Thresh 13.481652422850367
target distance 9.0
model initialize at round 2262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([6.68970768, 8.10923427, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.314247516304125}
done in step count: 9
reward sum = 0.8766041885318435
running average episode reward sum: 0.7266665867689118
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.48376299,  1.36935727,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8149913495015221}
episode index:2263
target Thresh 13.484410906960445
target distance 3.0
model initialize at round 2263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([6.212022 , 7.9771765, 0.       ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 2.0598624328743194}
done in step count: 1
reward sum = 0.9831112219640304
running average episode reward sum: 0.7267798573674963
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.51744318, 8.11532021, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.007729829343343}
episode index:2264
target Thresh 13.487168012173225
target distance 11.0
model initialize at round 2264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 6.        , 11.22793138,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 9.083381279823985}
done in step count: 8
reward sum = 0.8934453484430794
running average episode reward sum: 0.7268534403657636
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.57723159,  9.0682393 ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0231867528799108}
episode index:2265
target Thresh 13.489923739177973
target distance 13.0
model initialize at round 2265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([5.        , 7.49336398, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.281986711320654}
done in step count: 13
reward sum = 0.850768637057994
running average episode reward sum: 0.7269081249185846
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.63519714, 10.02264694,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6356007316491199}
episode index:2266
target Thresh 13.492678088663633
target distance 1.0
model initialize at round 2266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.56820244, 1.62800884, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.4849962102941139}
done in step count: 11
reward sum = 0.8837954800979566
running average episode reward sum: 0.7269773297510412
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.77492367, 2.68553359, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8362988751103853}
episode index:2267
target Thresh 13.495431061318783
target distance 2.0
model initialize at round 2267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.        ,  8.67833388,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.3216661214827976}
done in step count: 0
reward sum = 0.994887396516664
running average episode reward sum: 0.727095455882772
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.        ,  8.67833388,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.3216661214827976}
episode index:2268
target Thresh 13.49818265783167
target distance 12.0
model initialize at round 2268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.02516719, 11.86756161,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.059248382130654}
done in step count: 6
reward sum = 0.907892727897639
running average episode reward sum: 0.7271751373600814
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.33510664, 11.8934605 ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.954236932750323}
episode index:2269
target Thresh 13.500932878890193
target distance 2.0
model initialize at round 2269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.02619884, 11.86775028,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8681456786036915}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7272926813525243
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.02619884, 11.86775028,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8681456786036915}
episode index:2270
target Thresh 13.50368172518191
target distance 8.0
model initialize at round 2270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 6.772923854276528}
done in step count: 3
reward sum = 0.9487824546239836
running average episode reward sum: 0.7273902109752771
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.93900795, 9.20003434, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.8022874068035155}
episode index:2271
target Thresh 13.506429197394024
target distance 5.0
model initialize at round 2271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([14.       ,  4.1129396,  0.       ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 4.988322302199681}
done in step count: 3
reward sum = 0.9549031345856188
running average episode reward sum: 0.7274903487057394
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.41613146,  8.74898632,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.6355394031839776}
episode index:2272
target Thresh 13.50917529621341
target distance 12.0
model initialize at round 2272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([5.        , 8.61227632, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.467242277184376}
done in step count: 17
reward sum = 0.8087656204613616
running average episode reward sum: 0.7275261055344924
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.31431089,  3.65603804,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9489760029406491}
episode index:2273
target Thresh 13.511920022326596
target distance 9.0
model initialize at round 2273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([11.        , 10.48666668,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 7.1561286891437375}
done in step count: 4
reward sum = 0.9431569761595957
running average episode reward sum: 0.7276209300158578
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.8223207 , 8.11763198, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9000795873730435}
episode index:2274
target Thresh 13.514663376419755
target distance 7.0
model initialize at round 2274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 8.4158113 , 10.95754182,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 7.64439694001638}
done in step count: 13
reward sum = 0.8621792474083331
running average episode reward sum: 0.7276800765289974
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.51575611, 10.95737278,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.072872215925078}
episode index:2275
target Thresh 13.517405359178731
target distance 1.0
model initialize at round 2275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.26698059,  7.01701236,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0514717163767058}
done in step count: 1
reward sum = 0.9897589608074153
running average episode reward sum: 0.7277952254236715
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.72784835,  6.81715775,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0943080018257694}
episode index:2276
target Thresh 13.520145971289018
target distance 9.0
model initialize at round 2276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([6.83336009, 8.12900993, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 8.271000578910837}
done in step count: 5
reward sum = 0.9287510988171456
running average episode reward sum: 0.7278834800891935
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.54035612,  4.77366655,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8999068965918944}
episode index:2277
target Thresh 13.522885213435769
target distance 1.0
model initialize at round 2277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.37898862, 3.56903338, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7559016823783979}
done in step count: 0
reward sum = 0.9977863924952448
running average episode reward sum: 0.7280019624914789
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.37898862, 3.56903338, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7559016823783979}
episode index:2278
target Thresh 13.525623086303796
target distance 3.0
model initialize at round 2278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.71068072,  6.        ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.041011836596373}
done in step count: 2
reward sum = 0.9740061638808992
running average episode reward sum: 0.7281099064148616
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.33281201,  4.85544145,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.36285122143203696}
episode index:2279
target Thresh 13.528359590577564
target distance 2.0
model initialize at round 2279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.        ,  7.42285478,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.4228547811508676}
done in step count: 0
reward sum = 0.9951344596123226
running average episode reward sum: 0.7282270224469658
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.        ,  7.42285478,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.4228547811508676}
episode index:2280
target Thresh 13.531094726941202
target distance 12.0
model initialize at round 2280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.01046813, 9.6955297 , 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.993397214192575}
done in step count: 18
reward sum = 0.7940972695952607
running average episode reward sum: 0.7282559002405424
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.15053206, 10.85534372,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8684887919489773}
episode index:2281
target Thresh 13.533828496078495
target distance 10.0
model initialize at round 2281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([11.06639846, 11.86862983,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 9.25696270024688}
done in step count: 9
reward sum = 0.8944430518634122
running average episode reward sum: 0.7283287254603596
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.13413451, 9.7161046 , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.31398832432721024}
episode index:2282
target Thresh 13.536560898672883
target distance 8.0
model initialize at round 2282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.57093775, 4.        , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.2022451907036125}
done in step count: 11
reward sum = 0.8603326966258055
running average episode reward sum: 0.7283865458594685
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.04356008, 9.92626539, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.0856403696655211}
episode index:2283
target Thresh 13.539291935407466
target distance 5.0
model initialize at round 2283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 6., 11.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 3.1622776601683915}
done in step count: 2
reward sum = 0.9699943495268243
running average episode reward sum: 0.7284923286106363
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.46140528, 10.46646118,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6561104052320201}
episode index:2284
target Thresh 13.542021606965003
target distance 12.0
model initialize at round 2284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.049875621120906}
done in step count: 21
reward sum = 0.7638544281712552
running average episode reward sum: 0.7285078043653674
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.46817896, 10.49005575,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6777508208818908}
episode index:2285
target Thresh 13.544749914027916
target distance 3.0
model initialize at round 2285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.        , 8.98265147, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.2205645362806274}
done in step count: 10
reward sum = 0.8873635530704167
running average episode reward sum: 0.7285772950690879
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.10955359, 7.78849951, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.7960737775428317}
episode index:2286
target Thresh 13.547476857278276
target distance 8.0
model initialize at round 2286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 7.22731588, 11.88017857,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 8.971903815886373}
done in step count: 17
reward sum = 0.8022885047881432
running average episode reward sum: 0.7286095255936699
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.42121746, 10.31551684,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6591965603572938}
episode index:2287
target Thresh 13.550202437397825
target distance 7.0
model initialize at round 2287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 7.39205384, 11.35291705,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 7.014354641886227}
done in step count: 10
reward sum = 0.883071788971695
running average episode reward sum: 0.7286770353241673
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.14988249,  8.7151201 ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8965803576896183}
episode index:2288
target Thresh 13.552926655067955
target distance 8.0
model initialize at round 2288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([12.        , 10.26398277,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 6.005804434407576}
done in step count: 60
reward sum = 0.4237006077621156
running average episode reward sum: 0.7285437996633712
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5.08505632, 9.23233081, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.1943357714437135}
episode index:2289
target Thresh 13.555649510969719
target distance 1.0
model initialize at round 2289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.590739  ,  2.93234181,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.2201911999140789}
done in step count: 12
reward sum = 0.8692612681326212
running average episode reward sum: 0.7286052483395588
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.39167087,  3.09809787,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9832769304829087}
episode index:2290
target Thresh 13.558371005783833
target distance 1.0
model initialize at round 2290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.93034613,  9.35159582,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.3584289069776193}
done in step count: 0
reward sum = 0.9992581273961599
running average episode reward sum: 0.7287233857813119
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.93034613,  9.35159582,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.3584289069776193}
episode index:2291
target Thresh 13.561091140190669
target distance 5.0
model initialize at round 2291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.06619284,  4.98982775,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.010899940786426}
done in step count: 13
reward sum = 0.854388641733947
running average episode reward sum: 0.7287782135544152
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.9116077 ,  8.85006388,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.2464498429500939}
episode index:2292
target Thresh 13.563809914870266
target distance 3.0
model initialize at round 2292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.        ,  9.10927169,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.0059524354014062}
done in step count: 1
reward sum = 0.9864327613645999
running average episode reward sum: 0.7288905792534166
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.7736541,  8.7176645,  0.       ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.3618643423764028}
episode index:2293
target Thresh 13.566527330502309
target distance 9.0
model initialize at round 2293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([11.19136083, 11.        ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 7.191360831260633}
done in step count: 4
reward sum = 0.9389869024128035
running average episode reward sum: 0.7289821643986474
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.85175109, 11.82845641,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8416161643205604}
episode index:2294
target Thresh 13.569243387766155
target distance 3.0
model initialize at round 2294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 4.73350706, 11.77395474,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.39499398002842}
done in step count: 41
reward sum = 0.5989839063961274
running average episode reward sum: 0.7289255202775133
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.08485376, 11.59075694,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.5968198420667531}
episode index:2295
target Thresh 13.571958087340825
target distance 13.0
model initialize at round 2295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4.98320451, 6.73745274, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.041450057347916}
done in step count: 26
reward sum = 0.6810767724109059
running average episode reward sum: 0.7289046802305331
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.83580587,  6.00687024,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8358341108229753}
episode index:2296
target Thresh 13.574671429904983
target distance 8.0
model initialize at round 2296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 8.00000001, 11.63607896,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 7.582428917631064}
done in step count: 25
reward sum = 0.7219283290745744
running average episode reward sum: 0.7289016430728684
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.12280814, 7.68917045, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.1155364046993572}
episode index:2297
target Thresh 13.577383416136975
target distance 12.0
model initialize at round 2297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4.5038321, 8.       , 0.       ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.668842116641228}
done in step count: 8
reward sum = 0.8955622987129908
running average episode reward sum: 0.728974167292033
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.88203384,  6.3562049 ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9512442480717449}
episode index:2298
target Thresh 13.58009404671479
target distance 9.0
model initialize at round 2298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([8.        , 9.51877558, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 8.331826496232011}
done in step count: 28
reward sum = 0.6891800720823948
running average episode reward sum: 0.7289568579857216
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.71131677,  4.4382776 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6315616073126626}
episode index:2299
target Thresh 13.582803322316089
target distance 5.0
model initialize at round 2299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([7.01823846, 8.1358739 , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.352409506258637}
done in step count: 2
reward sum = 0.9660404504980227
running average episode reward sum: 0.729059937808553
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.48424904, 4.90939129, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.49265309252421774}
episode index:2300
target Thresh 13.585511243618189
target distance 6.0
model initialize at round 2300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.15104935,  6.39556532,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.199357175225773}
done in step count: 50
reward sum = 0.5163540191387065
running average episode reward sum: 0.7289674971659326
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.00132202, 11.8672614 ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.3226866737066216}
episode index:2301
target Thresh 13.588217811298074
target distance 5.0
model initialize at round 2301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 3.5379193441547394}
done in step count: 2
reward sum = 0.9650274923724322
running average episode reward sum: 0.7290700427763611
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.00461466, 9.28544936, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.2253059147691818}
episode index:2302
target Thresh 13.590923026032383
target distance 8.0
model initialize at round 2302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.12963466, 3.89434219, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 7.158764469447858}
done in step count: 4
reward sum = 0.9391477925805589
running average episode reward sum: 0.7291612619469231
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.09709005, 10.61780047,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9804707351513495}
episode index:2303
target Thresh 13.593626888497422
target distance 8.0
model initialize at round 2303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([2.        , 4.82011795, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 6.4954554995381555}
done in step count: 52
reward sum = 0.4973934134944565
running average episode reward sum: 0.7290606682626989
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.99461692, 11.84980759,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8498246378178946}
episode index:2304
target Thresh 13.59632939936915
target distance 2.0
model initialize at round 2304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.45503867,  3.21626198,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.2985967038715527}
done in step count: 3
reward sum = 0.9629961023096283
running average episode reward sum: 0.7291621586896173
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.16991173,  2.31976629,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8895487755007911}
episode index:2305
target Thresh 13.599030559323204
target distance 1.0
model initialize at round 2305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 1.26000522, 10.21544055,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.7532816244167304}
done in step count: 1
reward sum = 0.9871554013035906
running average episode reward sum: 0.7292740378061022
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.43444319, 10.8530164 ,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0234703160141612}
episode index:2306
target Thresh 13.601730369034867
target distance 1.0
model initialize at round 2306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([ 3.93228042, 10.38602078,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 2.4892805904170263}
done in step count: 12
reward sum = 0.8627373108692514
running average episode reward sum: 0.7293318892465284
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.98768226, 8.59944258, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.40074677271652853}
episode index:2307
target Thresh 13.604428829179097
target distance 4.0
model initialize at round 2307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([11.55349767,  8.81217861,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 3.6454489362385467}
done in step count: 14
reward sum = 0.8529129488363635
running average episode reward sum: 0.7293854338997302
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.29820027, 10.64093814,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9504338745740195}
episode index:2308
target Thresh 13.607125940430503
target distance 5.0
model initialize at round 2308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([5.85387008, 7.82335934, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 4.014465392706665}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.7294931318497124
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.79557239, 5.89597299, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.1982082519139312}
episode index:2309
target Thresh 13.609821703463368
target distance 2.0
model initialize at round 2309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.55535913,  9.10803771,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.565770187175242}
done in step count: 0
reward sum = 0.9957501188507677
running average episode reward sum: 0.729608394614648
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.55535913,  9.10803771,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.565770187175242}
episode index:2310
target Thresh 13.61251611895163
target distance 12.0
model initialize at round 2310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4., 7., 0.]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.180339887498974}
done in step count: 23
reward sum = 0.752636251272721
running average episode reward sum: 0.7296183590701468
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.90844888,  1.96612061,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.09761875358616044}
episode index:2311
target Thresh 13.615209187568894
target distance 4.0
model initialize at round 2311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5.60809407, 7.66496702, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 2.3676928191751356}
done in step count: 37
reward sum = 0.6050555701243365
running average episode reward sum: 0.7295644824313294
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.36509021, 9.06497842, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.0037710001231515}
episode index:2312
target Thresh 13.617900909988428
target distance 9.0
model initialize at round 2312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 7.47304928, 11.        ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 7.788131168369658}
done in step count: 19
reward sum = 0.7865902489910442
running average episode reward sum: 0.72958913689158
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.34866096,  9.84285508,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9121234274338115}
episode index:2313
target Thresh 13.620591286883162
target distance 5.0
model initialize at round 2313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([12.73597705,  9.00047064,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 4.735977077027186}
done in step count: 4
reward sum = 0.9513517199911545
running average episode reward sum: 0.7296849720614589
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.23220158, 9.10257268, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.253847841410806}
episode index:2314
target Thresh 13.623280318925687
target distance 13.0
model initialize at round 2314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.25195432,  9.31501317,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.730123114137633}
done in step count: 6
reward sum = 0.9211838847581552
running average episode reward sum: 0.7297676929740708
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.53608096, 6.91332239, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0590281291621897}
episode index:2315
target Thresh 13.625968006788266
target distance 3.0
model initialize at round 2315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.5641799, 3.       , 0.       ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.7497369423126277}
done in step count: 14
reward sum = 0.8251009898915633
running average episode reward sum: 0.7298088558829299
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.3829182 , 1.42777187, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8415669770172242}
episode index:2316
target Thresh 13.628654351142817
target distance 7.0
model initialize at round 2316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.13259695, 7.00494245, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 5.341969964170629}
done in step count: 3
reward sum = 0.9525352987585083
running average episode reward sum: 0.7299049829622891
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.94050056, 1.13133403, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.2802819530220286}
episode index:2317
target Thresh 13.63133935266093
target distance 7.0
model initialize at round 2317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 9.2513957 , 10.83735132,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 7.480548075731466}
done in step count: 5
reward sum = 0.9421346172881883
running average episode reward sum: 0.7299965401815842
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.32367671, 8.02812901, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0243533707122385}
episode index:2318
target Thresh 13.634023012013852
target distance 5.0
model initialize at round 2318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.87338417,  5.71833445,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.3958988024521957}
done in step count: 2
reward sum = 0.9707349003740557
running average episode reward sum: 0.7301003514623917
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.89757179,  9.71360142,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7209150605110335}
episode index:2319
target Thresh 13.636705329872497
target distance 6.0
model initialize at round 2319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([12.28892374, 11.43109655,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 6.654831692127298}
done in step count: 4
reward sum = 0.9451545732426392
running average episode reward sum: 0.7301930472476419
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.93549202,  4.25450427,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1962061692678538}
episode index:2320
target Thresh 13.639386306907449
target distance 8.0
model initialize at round 2320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([12.00007785,  8.00016787,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 7.211199045315421}
done in step count: 3
reward sum = 0.9517137939485447
running average episode reward sum: 0.7302884891893484
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.2177224 ,  2.84292704,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8705912001975775}
episode index:2321
target Thresh 13.642065943788948
target distance 12.0
model initialize at round 2321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.04987562112091}
done in step count: 11
reward sum = 0.8592675585949053
running average episode reward sum: 0.7303440357308667
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.71880217, 8.30579365, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9992992658862604}
episode index:2322
target Thresh 13.644744241186908
target distance 7.0
model initialize at round 2322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 9.54317594, 10.46221375,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 7.402707302671845}
done in step count: 13
reward sum = 0.8458301031303422
running average episode reward sum: 0.7303937499226012
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.63353509, 7.29778706, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.47220087116095305}
episode index:2323
target Thresh 13.647421199770896
target distance 13.0
model initialize at round 2323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 2.72084546, 11.60177122,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 12.79649921577916}
done in step count: 15
reward sum = 0.8261402169477113
running average episode reward sum: 0.7304349489187395
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.80333254,  8.10224562,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.22165797681899388}
episode index:2324
target Thresh 13.65009682021016
target distance 12.0
model initialize at round 2324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.13153591,  2.93845061,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.320824955413832}
done in step count: 7
reward sum = 0.8903981069516217
running average episode reward sum: 0.7305037502770332
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.86202344, 4.96242404, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8628420241902401}
episode index:2325
target Thresh 13.6527711031736
target distance 5.0
model initialize at round 2325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([1.4065738 , 2.23201168, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 6.324203404512008}
done in step count: 12
reward sum = 0.8653294370674581
running average episode reward sum: 0.7305617148887229
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.36704918, 8.31129173, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.7053575529670492}
episode index:2326
target Thresh 13.65544404932979
target distance 8.0
model initialize at round 2326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 9.        , 10.49287197,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 6.942633117080338}
done in step count: 3
reward sum = 0.9570110706403119
running average episode reward sum: 0.7306590287502406
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.82157469, 6.78126583, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.850193864475264}
episode index:2327
target Thresh 13.65811565934696
target distance 12.0
model initialize at round 2327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.04987562112091}
done in step count: 20
reward sum = 0.7762971511948754
running average episode reward sum: 0.7306786327547272
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.87585651, 10.1426893 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8662524138038153}
episode index:2328
target Thresh 13.660785933893019
target distance 4.0
model initialize at round 2328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.236067977499798}
done in step count: 1
reward sum = 0.9817137877770358
running average episode reward sum: 0.7307864194249815
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.16009259,  7.74741077,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.877066571353429}
episode index:2329
target Thresh 13.663454873635535
target distance 4.0
model initialize at round 2329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([2.94987661, 7.30537391, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.8920181400271803}
done in step count: 1
reward sum = 0.9848069492978497
running average episode reward sum: 0.7308954411116222
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.55105507, 9.30537391, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8270773577899379}
episode index:2330
target Thresh 13.666122479241741
target distance 4.0
model initialize at round 2330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.19422889, 8.        , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.3294168016647556}
done in step count: 5
reward sum = 0.9367180101938269
running average episode reward sum: 0.7309837390820565
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.2799889 , 9.63429996, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.807559597691741}
episode index:2331
target Thresh 13.668788751378539
target distance 2.0
model initialize at round 2331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.63231468, 8.16640615, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9110824288187322}
done in step count: 0
reward sum = 0.998679756146653
running average episode reward sum: 0.7310985315422042
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.63231468, 8.16640615, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9110824288187322}
episode index:2332
target Thresh 13.671453690712495
target distance 2.0
model initialize at round 2332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.        , 4.37785065, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6221493482590121}
done in step count: 0
reward sum = 0.9950264739106909
running average episode reward sum: 0.7312116596786673
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.        , 4.37785065, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6221493482590121}
episode index:2333
target Thresh 13.674117297909849
target distance 1.0
model initialize at round 2333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.        , 8.89266908, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0057434692930838}
done in step count: 0
reward sum = 0.9964023569659588
running average episode reward sum: 0.7313252803715925
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.        , 8.89266908, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0057434692930838}
episode index:2334
target Thresh 13.676779573636498
target distance 2.0
model initialize at round 2334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.        , 9.82139659, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.17860341072085184}
done in step count: 0
reward sum = 0.9959581707051075
running average episode reward sum: 0.7314386135152042
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.        , 9.82139659, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.17860341072085184}
episode index:2335
target Thresh 13.679440518558012
target distance 2.0
model initialize at round 2335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.13259695, 4.00494245, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8674171277527281}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7315510113690956
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.13259695, 4.00494245, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8674171277527281}
episode index:2336
target Thresh 13.68210013333963
target distance 10.0
model initialize at round 2336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.76553881, 7.55731487, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.346476010504338}
done in step count: 68
reward sum = 0.37508942125168376
running average episode reward sum: 0.7313984818055024
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.8873965 ,  8.32264996,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.1163671542921298}
episode index:2337
target Thresh 13.684758418646252
target distance 10.0
model initialize at round 2337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([5.        , 7.86939394, 0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 8.079496893507054}
done in step count: 13
reward sum = 0.8490984532155025
running average episode reward sum: 0.7314488239660712
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.73726823,  9.00770907,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.2628448419391393}
episode index:2338
target Thresh 13.687415375142454
target distance 2.0
model initialize at round 2338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.6892339 , 9.10720265, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9453373361943429}
done in step count: 0
reward sum = 0.9969404433707798
running average episode reward sum: 0.7315623304301176
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.6892339 , 9.10720265, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9453373361943429}
episode index:2339
target Thresh 13.690071003492468
target distance 8.0
model initialize at round 2339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([3.69418728, 4.        , 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 6.850430476726842}
done in step count: 15
reward sum = 0.8289999242445667
running average episode reward sum: 0.7316039704274743
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.63076258, 9.98775142, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.36944052456635007}
episode index:2340
target Thresh 13.692725304360206
target distance 2.0
model initialize at round 2340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.05961347,  8.53516221,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1870885768950692}
done in step count: 8
reward sum = 0.9079995983957956
running average episode reward sum: 0.7316793209733813
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.86994562,  7.16834522,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.203517785055932}
episode index:2341
target Thresh 13.695378278409244
target distance 4.0
model initialize at round 2341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([4.       , 8.1374855, 0.       ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.1780567613243815}
done in step count: 3
reward sum = 0.9650536120718289
running average episode reward sum: 0.7317789684076677
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.94597697, 8.79951006, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9669894766721617}
episode index:2342
target Thresh 13.698029926302825
target distance 4.0
model initialize at round 2342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([12.29785322,  8.0139143 ,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 3.0372134996179594}
done in step count: 2
reward sum = 0.9726851066991472
running average episode reward sum: 0.7318817879289188
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.38325552,  9.78630579,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.43880520946628276}
episode index:2343
target Thresh 13.700680248703858
target distance 2.0
model initialize at round 2343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.38754189, 5.        , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6124581098556559}
done in step count: 0
reward sum = 0.9968873585564688
running average episode reward sum: 0.731994844912975
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.38754189, 5.        , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6124581098556559}
episode index:2344
target Thresh 13.703329246274922
target distance 4.0
model initialize at round 2344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([10.        , 10.51194513,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 2.0644824568240887}
done in step count: 3
reward sum = 0.9633541554029379
running average episode reward sum: 0.732093505599751
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.22984487, 10.52313393,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.5713998388656811}
episode index:2345
target Thresh 13.705976919678276
target distance 13.0
model initialize at round 2345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.648875396109553}
done in step count: 18
reward sum = 0.778013647425002
running average episode reward sum: 0.7321130794027455
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.75438105, 10.33198308,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8241987260872253}
episode index:2346
target Thresh 13.70862326957583
target distance 9.0
model initialize at round 2346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 6.97380116, 11.86775028,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 8.020409111065447}
done in step count: 4
reward sum = 0.9437319781528883
running average episode reward sum: 0.7322032451031077
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.17308391,  8.65275291,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.0535067948245382}
episode index:2347
target Thresh 13.711268296629173
target distance 9.0
model initialize at round 2347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([5.1188682 , 7.03396667, 0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 8.122655077827032}
done in step count: 5
reward sum = 0.9278672265210643
running average episode reward sum: 0.7322865772928088
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.8122468 ,  8.57458764,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.46500208676071747}
episode index:2348
target Thresh 13.713912001499565
target distance 7.0
model initialize at round 2348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([10.99505776,  8.1325968 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 6.490593435772951}
done in step count: 3
reward sum = 0.9498625070381729
running average episode reward sum: 0.7323792022096863
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.87978983,  4.22088003,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9070932274166658}
episode index:2349
target Thresh 13.716554384847928
target distance 5.0
model initialize at round 2349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 5.53959692, 10.91815209,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 3.5405430942367606}
done in step count: 2
reward sum = 0.9701957892094027
running average episode reward sum: 0.7324804007573458
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.25112036, 11.85598582,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8920611895684066}
episode index:2350
target Thresh 13.71919544733486
target distance 13.0
model initialize at round 2350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.000000000000018}
done in step count: 6
reward sum = 0.908545308769313
running average episode reward sum: 0.7325552901269808
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.25917972, 10.93215095,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.26791345424835145}
episode index:2351
target Thresh 13.721835189620627
target distance 2.0
model initialize at round 2351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.28416234, 8.99999989, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7158376575807458}
done in step count: 0
reward sum = 0.9947919065436459
running average episode reward sum: 0.7326667852870219
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.28416234, 8.99999989, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7158376575807458}
episode index:2352
target Thresh 13.724473612365163
target distance 3.0
model initialize at round 2352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([12.00000002, 11.64008787,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 2.823130175992426}
done in step count: 10
reward sum = 0.887989970554381
running average episode reward sum: 0.732732795990493
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.26141575,  9.72710319,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.7726689087345379}
episode index:2353
target Thresh 13.727110716228076
target distance 2.0
model initialize at round 2353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.72944117, 8.36050415, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.45073864394501506}
done in step count: 0
reward sum = 0.9975849768589823
running average episode reward sum: 0.732845307537166
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.72944117, 8.36050415, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.45073864394501506}
episode index:2354
target Thresh 13.72974650186864
target distance 5.0
model initialize at round 2354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([7.        , 9.34541988, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 3.0198203416883516}
done in step count: 2
reward sum = 0.9730340688869653
running average episode reward sum: 0.7329472985186309
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.18148887,  9.38006669,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.42117561836291184}
episode index:2355
target Thresh 13.7323809699458
target distance 12.0
model initialize at round 2355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([12.05521678,  8.045681  ,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 10.055320539963583}
done in step count: 18
reward sum = 0.7878160969210994
running average episode reward sum: 0.7329705874822992
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.89477191, 8.71641784, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7241046007273887}
episode index:2356
target Thresh 13.735014121118176
target distance 12.0
model initialize at round 2356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.77032961426902}
done in step count: 12
reward sum = 0.8452517730714255
running average episode reward sum: 0.7330182248117812
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.24538289, 3.1841732 , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.8519308271110467}
episode index:2357
target Thresh 13.737645956044057
target distance 13.0
model initialize at round 2357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.        , 11.24076176,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.18464541103082}
done in step count: 8
reward sum = 0.896507153513533
running average episode reward sum: 0.7330875585389661
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.67473388, 6.63075534, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9236439236876595}
episode index:2358
target Thresh 13.740276475381398
target distance 7.0
model initialize at round 2358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.        , 3.50298774, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.563411451867789}
done in step count: 16
reward sum = 0.8059932277047893
running average episode reward sum: 0.7331184638671415
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.34283275, 10.93069382,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.349767978387491}
episode index:2359
target Thresh 13.74290567978783
target distance 11.0
model initialize at round 2359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([5.        , 9.55743194, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.577572960098538}
done in step count: 17
reward sum = 0.793744400194558
running average episode reward sum: 0.7331441528232125
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.10123884,  3.43764322,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0601965712380035}
episode index:2360
target Thresh 13.745533569920655
target distance 6.0
model initialize at round 2360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([12.        , 11.09602368,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 4.147441127303149}
done in step count: 4
reward sum = 0.9442262505392762
running average episode reward sum: 0.7332335565071244
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.5412825 , 9.54928578, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.6430902411344599}
episode index:2361
target Thresh 13.748160146436847
target distance 6.0
model initialize at round 2361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.10008715, 6.        , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.001251983789787}
done in step count: 2
reward sum = 0.9714949198811559
running average episode reward sum: 0.7333344292265885
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.50443251, 9.86535525, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5220932573099527}
episode index:2362
target Thresh 13.750785409993046
target distance 7.0
model initialize at round 2362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([7.75687465, 8.12049791, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 7.181545923568239}
done in step count: 27
reward sum = 0.6837213941216395
running average episode reward sum: 0.733313433443641
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.84768806, 2.04756061, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.849021239719332}
episode index:2363
target Thresh 13.75340936124557
target distance 2.0
model initialize at round 2363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.        ,  6.26132917,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7386708259582528}
done in step count: 0
reward sum = 0.9958067861553629
running average episode reward sum: 0.7334244712408964
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.        ,  6.26132917,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7386708259582528}
episode index:2364
target Thresh 13.756032000850407
target distance 4.0
model initialize at round 2364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 9.97603869, 10.63383061,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 3.0427861490331476}
done in step count: 1
reward sum = 0.9857448538666698
running average episode reward sum: 0.7335311606204421
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 7.97603869, 10.118128  ,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.9831611046586738}
episode index:2365
target Thresh 13.758653329463218
target distance 7.0
model initialize at round 2365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.34790468,  9.33898354,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.3503068008344545}
done in step count: 4
reward sum = 0.9506264414981103
running average episode reward sum: 0.7336229168676431
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.31345777,  4.74877407,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8117378735160066}
episode index:2366
target Thresh 13.761273347739335
target distance 12.0
model initialize at round 2366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([12.        ,  8.25135196,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 10.515288372700926}
done in step count: 9
reward sum = 0.8832405791749524
running average episode reward sum: 0.7336861266954029
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.60696011, 5.8679165 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0590937709402226}
episode index:2367
target Thresh 13.76389205633376
target distance 10.0
model initialize at round 2367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.07472301, 10.98294747,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 11.239653424808653}
done in step count: 15
reward sum = 0.8214695412815213
running average episode reward sum: 0.7337231973941302
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.2620124 , 6.94612776, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1999097630925406}
episode index:2368
target Thresh 13.76650945590117
target distance 5.0
model initialize at round 2368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.61038303,  8.        ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.694130979255659}
done in step count: 8
reward sum = 0.907043235560548
running average episode reward sum: 0.7337963590818324
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.25150782, 11.5188016 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9107116148170052}
episode index:2369
target Thresh 13.769125547095921
target distance 12.0
model initialize at round 2369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.95128477, 7.82879877, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.19904519421758}
done in step count: 11
reward sum = 0.8736243938299078
running average episode reward sum: 0.7338553582526122
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.74952487,  6.86362657,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.89921557121079}
episode index:2370
target Thresh 13.77174033057203
target distance 2.0
model initialize at round 2370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([3.52982545, 6.19303143, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.5419555886728151}
done in step count: 1
reward sum = 0.9842320622636744
running average episode reward sum: 0.7339609578747172
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.60074008, 6.875108  , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9618848712894059}
episode index:2371
target Thresh 13.774353806983193
target distance 9.0
model initialize at round 2371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.51033294,  3.15188706,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 9.589402789702655}
done in step count: 48
reward sum = 0.4954615182389124
running average episode reward sum: 0.7338604100502502
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.36597106, 11.48211209,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6052824854900671}
episode index:2372
target Thresh 13.77696597698278
target distance 13.0
model initialize at round 2372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4., 6., 0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.401754250991402}
done in step count: 23
reward sum = 0.7465088889250562
running average episode reward sum: 0.733865740214125
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.10393657,  8.29650495,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.139225597465037}
episode index:2373
target Thresh 13.779576841223838
target distance 8.0
model initialize at round 2373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 9.       , 11.3402462,  0.       ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 6.147866286616815}
done in step count: 17
reward sum = 0.7925713443517645
running average episode reward sum: 0.7338904687752613
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.38973072,  9.81155317,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.432899801767424}
episode index:2374
target Thresh 13.782186400359075
target distance 6.0
model initialize at round 2374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([9.97768837, 8.13887238, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 4.1804331307577876}
done in step count: 2
reward sum = 0.9692674058761224
running average episode reward sum: 0.7339895748540406
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.97885883,  7.41827035,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.4188042902004005}
episode index:2375
target Thresh 13.784794655040885
target distance 7.0
model initialize at round 2375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([12.26955903, 11.        ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 5.269559025764419}
done in step count: 11
reward sum = 0.8713123591740147
running average episode reward sum: 0.7340473706386871
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.57063311, 10.12124702,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9780402513069294}
episode index:2376
target Thresh 13.787401605921334
target distance 1.0
model initialize at round 2376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([12.73326249, 11.87145272,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.2598582242990957}
done in step count: 3
reward sum = 0.9647464558725976
running average episode reward sum: 0.7341444253653315
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.9270726 , 10.67818877,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6820985386743846}
episode index:2377
target Thresh 13.790007253652155
target distance 9.0
model initialize at round 2377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([12.65072433,  7.37629064,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 7.821126184573777}
done in step count: 30
reward sum = 0.6773091417776828
running average episode reward sum: 0.7341205249096596
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.26644292, 9.82664985, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.1051949915371668}
episode index:2378
target Thresh 13.792611598884765
target distance 10.0
model initialize at round 2378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.0922302 ,  5.43505028,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.412657424361987}
done in step count: 15
reward sum = 0.8019527885622777
running average episode reward sum: 0.7341490378409974
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.86888655, 2.12944811, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.229969279840861}
episode index:2379
target Thresh 13.795214642270246
target distance 8.0
model initialize at round 2379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.82772255, 9.        , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.002472783776051}
done in step count: 8
reward sum = 0.9036160036150802
running average episode reward sum: 0.7342202424484655
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.72563272, 2.53788131, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8602886275555591}
episode index:2380
target Thresh 13.79781638445936
target distance 4.0
model initialize at round 2380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 6.       , 10.0022943,  0.       ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 2.235042876512318}
done in step count: 1
reward sum = 0.982962281703553
running average episode reward sum: 0.7343247120155613
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.        , 10.65288103,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.3471189737319964}
episode index:2381
target Thresh 13.800416826102545
target distance 10.0
model initialize at round 2381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.89172178, 3.6906332 , 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.20191141035213}
done in step count: 21
reward sum = 0.7445571154389636
running average episode reward sum: 0.7343290077348826
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.83946764,  5.26093297,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8790858455556573}
episode index:2382
target Thresh 13.803015967849907
target distance 11.0
model initialize at round 2382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.14848468,  1.9612975 ,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.201502054997126}
done in step count: 61
reward sum = 0.43620984480450975
running average episode reward sum: 0.7342039052745678
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.61484112, 2.50721409, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6254480917929934}
episode index:2383
target Thresh 13.805613810351236
target distance 9.0
model initialize at round 2383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([6.31636524, 9.        , 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 10.898292624363737}
done in step count: 16
reward sum = 0.8128356755221742
running average episode reward sum: 0.7342368883996716
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.8988134 ,  3.58626861,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9894640935198092}
episode index:2384
target Thresh 13.808210354255989
target distance 10.0
model initialize at round 2384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([15.52164578,  3.32283759,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 13.845112647870053}
done in step count: 11
reward sum = 0.8661716569381005
running average episode reward sum: 0.7342922069609037
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.34450581, 10.30891996,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7721890129878176}
episode index:2385
target Thresh 13.810805600213303
target distance 8.0
model initialize at round 2385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([9.        , 9.45028114, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 6.1727882998113905}
done in step count: 37
reward sum = 0.6210876332143463
running average episode reward sum: 0.7342447616240443
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.43233924, 7.28913415, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.832014108925114}
episode index:2386
target Thresh 13.813399548871994
target distance 2.0
model initialize at round 2386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.04618776, 11.        ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9538122415542691}
done in step count: 0
reward sum = 0.9961791186856243
running average episode reward sum: 0.734354495330396
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.04618776, 11.        ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9538122415542691}
episode index:2387
target Thresh 13.81599220088054
target distance 3.0
model initialize at round 2387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 1.66986006, 11.06465197,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 4.078037270816958}
done in step count: 3
reward sum = 0.9567789160014926
running average episode reward sum: 0.7344476378851158
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.13519972, 7.08094406, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8685801406211293}
episode index:2388
target Thresh 13.818583556887113
target distance 12.0
model initialize at round 2388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([2.84715319, 4.11065769, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.153312298289906}
done in step count: 43
reward sum = 0.5679947425385309
running average episode reward sum: 0.7343779631696087
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.50519001,  4.78344261,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9322012976762781}
episode index:2389
target Thresh 13.821173617539545
target distance 7.0
model initialize at round 2389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([8.4164195 , 8.09871489, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 7.529879448111167}
done in step count: 26
reward sum = 0.7119999480085111
running average episode reward sum: 0.7343685999833488
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.48797991, 1.08703448, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.0467428608665046}
episode index:2390
target Thresh 13.823762383485356
target distance 10.0
model initialize at round 2390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([4.        , 9.29578888, 0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 8.005466323956062}
done in step count: 10
reward sum = 0.8794546126009227
running average episode reward sum: 0.7344292800388141
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.79877031,  8.00203477,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.0180510768993867}
episode index:2391
target Thresh 13.826349855371737
target distance 9.0
model initialize at round 2391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([14.60388708,  7.27992392,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 8.075760809132325}
done in step count: 19
reward sum = 0.7724798632983558
running average episode reward sum: 0.7344451874732871
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.92835775, 9.74329886, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.9631944677351368}
episode index:2392
target Thresh 13.828936033845553
target distance 6.0
model initialize at round 2392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([6.50916696, 9.        , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 6.027651834388923}
done in step count: 2
reward sum = 0.9640298115829397
running average episode reward sum: 0.7345411275585817
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.50916696, 5.14583862, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.5296412857875001}
episode index:2393
target Thresh 13.831520919553352
target distance 7.0
model initialize at round 2393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 5.3851648071345135}
done in step count: 3
reward sum = 0.9532473032579073
running average episode reward sum: 0.7346324835216975
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.48348965,  2.00000092,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.1255137030817453}
episode index:2394
target Thresh 13.834104513141355
target distance 3.0
model initialize at round 2394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.      , 8.578071, 0.      ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.738356144805552}
done in step count: 1
reward sum = 0.9823661284497096
running average episode reward sum: 0.7347359213692667
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([8.        , 9.02371562, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.3975447040659479}
episode index:2395
target Thresh 13.836686815255456
target distance 3.0
model initialize at round 2395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([12.        ,  9.69612265,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.2184361857463477}
done in step count: 1
reward sum = 0.9831235080391421
running average episode reward sum: 0.734839588976391
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.32704449,  8.85658962,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.6880666108049457}
episode index:2396
target Thresh 13.839267826541237
target distance 11.0
model initialize at round 2396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.        ,  9.77663648,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.266002041517837}
done in step count: 24
reward sum = 0.7403325997909331
running average episode reward sum: 0.7348418805954208
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.96142474, 2.52904736, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.4725298306168822}
episode index:2397
target Thresh 13.841847547643946
target distance 4.0
model initialize at round 2397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([13.        , 10.83571286,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 2.167582984044254}
done in step count: 1
reward sum = 0.9821328195314115
running average episode reward sum: 0.7349450044231672
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.        ,  9.24578781,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7542121857404922}
episode index:2398
target Thresh 13.844425979208516
target distance 3.0
model initialize at round 2398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.        ,  8.64378643,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.061549860909294}
done in step count: 3
reward sum = 0.9644846753184783
running average episode reward sum: 0.7350406858199556
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.70887007,  8.30462563,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.9929966200874862}
episode index:2399
target Thresh 13.847003121879553
target distance 1.0
model initialize at round 2399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.64113283, 10.50047261,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8127600665932578}
done in step count: 0
reward sum = 0.9977928666000196
running average episode reward sum: 0.7351501658952806
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.64113283, 10.50047261,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8127600665932578}
episode index:2400
target Thresh 13.849578976301343
target distance 13.0
model initialize at round 2400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([15.09317541,  7.        ,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 13.131307716640055}
done in step count: 11
reward sum = 0.8615178858718744
running average episode reward sum: 0.7352027971822346
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.1407993 , 8.27470991, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9020484321109342}
episode index:2401
target Thresh 13.852153543117852
target distance 10.0
model initialize at round 2401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.67173123, 8.95625186, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.549529027097405}
done in step count: 5
reward sum = 0.9247020729639873
running average episode reward sum: 0.735281689470237
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.24922156, 11.51229879,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5697029373827153}
episode index:2402
target Thresh 13.85472682297272
target distance 1.0
model initialize at round 2402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.02704263, 9.83671284, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.2832515034051417}
done in step count: 0
reward sum = 0.9994743852355359
running average episode reward sum: 0.7353916323315626
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.02704263, 9.83671284, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.2832515034051417}
episode index:2403
target Thresh 13.857298816509267
target distance 9.0
model initialize at round 2403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([7.       , 9.6099062, 0.       ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 9.105529766079735}
done in step count: 4
reward sum = 0.9359560725103336
running average episode reward sum: 0.7354750617991911
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.70215721, 1.60980469, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.4908795281142622}
episode index:2404
target Thresh 13.85986952437049
target distance 1.0
model initialize at round 2404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.7503159 , 10.38995159,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6591670607884293}
done in step count: 0
reward sum = 0.9985495979589379
running average episode reward sum: 0.7355844483007128
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.7503159 , 10.38995159,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6591670607884293}
episode index:2405
target Thresh 13.862438947199067
target distance 2.0
model initialize at round 2405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.        , 9.89184237, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.8918423652649725}
done in step count: 0
reward sum = 0.9960789899069467
running average episode reward sum: 0.7356927170212474
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.        , 9.89184237, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.8918423652649725}
episode index:2406
target Thresh 13.865007085637355
target distance 3.0
model initialize at round 2406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.85661656,  6.97384849,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.1213231713993674}
done in step count: 6
reward sum = 0.9310157573113531
running average episode reward sum: 0.7357738649399387
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.99762416,  7.64921099,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.3507970539487934}
episode index:2407
target Thresh 13.867573940327388
target distance 1.0
model initialize at round 2407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.03390217, 10.95275193,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.05815270160959986}
done in step count: 0
reward sum = 0.9985174825755634
running average episode reward sum: 0.7358829777379601
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.03390217, 10.95275193,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.05815270160959986}
episode index:2408
target Thresh 13.870139511910882
target distance 6.0
model initialize at round 2408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.96545041,  7.        ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.000149206522249}
done in step count: 2
reward sum = 0.9647996859319552
running average episode reward sum: 0.7359780033536487
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.14089701,  3.04878703,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8604871383842262}
episode index:2409
target Thresh 13.872703801029223
target distance 7.0
model initialize at round 2409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.08832004,  4.47235827,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 6.590998929669708}
done in step count: 29
reward sum = 0.6733690609710008
running average episode reward sum: 0.7359520245393821
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.00530056, 11.79461768,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7946353625068239}
episode index:2410
target Thresh 13.875266808323488
target distance 5.0
model initialize at round 2410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([12.        ,  9.53994828,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 3.0482034299027174}
done in step count: 2
reward sum = 0.9689138087366004
running average episode reward sum: 0.7360486490869546
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.36868298, 8.09929667, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.0999216621674759}
episode index:2411
target Thresh 13.87782853443443
target distance 3.0
model initialize at round 2411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.25034791,  4.5770752 ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.50405371926729}
done in step count: 9
reward sum = 0.8880278083488866
running average episode reward sum: 0.7361116586886386
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.48678528,  7.87251531,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.5288115854180153}
episode index:2412
target Thresh 13.880388980002476
target distance 13.0
model initialize at round 2412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.        ,  9.71552879,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.968133176620109}
done in step count: 8
reward sum = 0.8957881840946036
running average episode reward sum: 0.7361778321347248
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.78338534, 4.75089928, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.3301107090453079}
episode index:2413
target Thresh 13.882948145667742
target distance 12.0
model initialize at round 2413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.72934496, 9.47359788, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.793792740287625}
done in step count: 16
reward sum = 0.8127822300291071
running average episode reward sum: 0.7362095655224192
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.20287925,  5.61402772,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.4360442547848}
episode index:2414
target Thresh 13.88550603207002
target distance 7.0
model initialize at round 2414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([15.10929346,  7.        ,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 6.806134482269234}
done in step count: 14
reward sum = 0.8378730163742408
running average episode reward sum: 0.7362516621894386
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.07211825, 10.27023012,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.2796879674101597}
episode index:2415
target Thresh 13.888062639848778
target distance 10.0
model initialize at round 2415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 8.062257748298565}
done in step count: 11
reward sum = 0.8543061628904322
running average episode reward sum: 0.7363005258072784
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.80484953, 10.2789657 ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8518242895152561}
episode index:2416
target Thresh 13.890617969643168
target distance 11.0
model initialize at round 2416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([5., 8., 0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.848857801796113}
done in step count: 7
reward sum = 0.9005598857554666
running average episode reward sum: 0.7363684858238064
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.94028361,  4.6155086 ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6183986487574424}
episode index:2417
target Thresh 13.893172022092024
target distance 2.0
model initialize at round 2417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.92878842, 10.46632791,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.7357319673845992}
done in step count: 31
reward sum = 0.6944258101502727
running average episode reward sum: 0.7363511398040904
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.61763948,  8.11137996,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.082184871736999}
episode index:2418
target Thresh 13.89572479783386
target distance 13.0
model initialize at round 2418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.99906298, 6.91366102, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.197027534379458}
done in step count: 9
reward sum = 0.884849545011958
running average episode reward sum: 0.7364125281485335
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.92343592,  8.54560901,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.4607963001083688}
episode index:2419
target Thresh 13.89827629750687
target distance 3.0
model initialize at round 2419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0000000000000187}
done in step count: 1
reward sum = 0.9810346428383957
running average episode reward sum: 0.7365136116670005
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.63284624,  6.96441984,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.03194356310987}
episode index:2420
target Thresh 13.900826521748925
target distance 10.0
model initialize at round 2420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([12.77028322, 11.        ,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 8.827109821316231}
done in step count: 13
reward sum = 0.8410554500607598
running average episode reward sum: 0.7365567929302775
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.57297827, 9.19562586, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9106949635357998}
episode index:2421
target Thresh 13.903375471197586
target distance 9.0
model initialize at round 2421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.13259695, 3.99505755, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 7.058442237262246}
done in step count: 5
reward sum = 0.9350155988423231
running average episode reward sum: 0.7366387329822643
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.41592314, 10.68741321,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.5202908408463222}
episode index:2422
target Thresh 13.905923146490087
target distance 2.0
model initialize at round 2422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.08451843, 11.06763858,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.4063991713758854}
done in step count: 15
reward sum = 0.8377732526274607
running average episode reward sum: 0.7366804723630506
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.62428172, 10.47767882,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.7860691580519236}
episode index:2423
target Thresh 13.90846954826335
target distance 8.0
model initialize at round 2423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([3.02414191, 4.67365599, 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 10.180223027132831}
done in step count: 6
reward sum = 0.9214317226229112
running average episode reward sum: 0.7367566898755339
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.57449764, 11.89715218,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0653307365594353}
episode index:2424
target Thresh 13.91101467715397
target distance 6.0
model initialize at round 2424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.99249434, 4.        , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 4.000007041851484}
done in step count: 2
reward sum = 0.9702114112877482
running average episode reward sum: 0.736852959863745
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.69793758, 8.        , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6979375779628634}
episode index:2425
target Thresh 13.913558533798236
target distance 12.0
model initialize at round 2425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.85539914, 4.98185658, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.546052058251709}
done in step count: 29
reward sum = 0.6711921429194444
running average episode reward sum: 0.7368258943992173
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.13571449,  8.23864612,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.2745366884678169}
episode index:2426
target Thresh 13.916101118832108
target distance 11.0
model initialize at round 2426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([15.44576573, 11.        ,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 12.4902183019823}
done in step count: 12
reward sum = 0.8543127604051955
running average episode reward sum: 0.7368743026670401
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.8036128 , 6.26512532, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.846218041642231}
episode index:2427
target Thresh 13.918642432891232
target distance 12.0
model initialize at round 2427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([12.43820989,  9.17586255,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.66685553254171}
done in step count: 9
reward sum = 0.8746685423810707
running average episode reward sum: 0.7369310548250771
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.25444676, 1.30018131, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7446404180772296}
episode index:2428
target Thresh 13.921182476610939
target distance 9.0
model initialize at round 2428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.39792633,  3.58054721,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 7.590449241816187}
done in step count: 24
reward sum = 0.7542596269963904
running average episode reward sum: 0.7369381888605532
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.04301192, 11.8367585 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8378632435951693}
episode index:2429
target Thresh 13.923721250626237
target distance 10.0
model initialize at round 2429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([11.02619884, 11.86775028,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 8.240652819096454}
done in step count: 7
reward sum = 0.911910630888774
running average episode reward sum: 0.7370101939807294
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.16446215, 10.07233961,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8386635324641953}
episode index:2430
target Thresh 13.926258755571826
target distance 2.0
model initialize at round 2430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 3.        , 10.51442373,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.8147945415105646}
done in step count: 3
reward sum = 0.9591356101902028
running average episode reward sum: 0.7371015660153692
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.52817872, 8.74198341, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5878310308489569}
episode index:2431
target Thresh 13.928794992082073
target distance 11.0
model initialize at round 2431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([3.1582166, 8.       , 0.       ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.93919039913699}
done in step count: 32
reward sum = 0.6468657358588003
running average episode reward sum: 0.7370644624667853
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.20020851,  3.7266184 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7536960545144966}
episode index:2432
target Thresh 13.931329960791043
target distance 6.0
model initialize at round 2432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.4043276, 5.       , 0.       ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.04410998944131}
done in step count: 2
reward sum = 0.9720207205028351
running average episode reward sum: 0.73716103306195
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.01370749, 8.30362475, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6965101472327413}
episode index:2433
target Thresh 13.933863662332477
target distance 4.0
model initialize at round 2433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([4.61465371, 7.27192819, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.4638232794240102}
done in step count: 2
reward sum = 0.9723014586238373
running average episode reward sum: 0.7372576396459936
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.58201659, 4.16862404, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0148543190762744}
episode index:2434
target Thresh 13.936396097339802
target distance 2.0
model initialize at round 2434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.2922678 , 8.20679545, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.3087094502269327}
done in step count: 13
reward sum = 0.8597224838781821
running average episode reward sum: 0.7373079332165201
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.3221649 , 7.45714898, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6312507054609472}
episode index:2435
target Thresh 13.93892726644612
target distance 7.0
model initialize at round 2435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([9.28406947, 8.35094838, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 6.180519405872706}
done in step count: 36
reward sum = 0.6492953689596481
running average episode reward sum: 0.7372718032640337
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.30154869,  6.25144565,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.7423335850177765}
episode index:2436
target Thresh 13.941457170284233
target distance 11.0
model initialize at round 2436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.93197334,  8.60438895,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.404090052144923}
done in step count: 9
reward sum = 0.8836442908573191
running average episode reward sum: 0.7373318658358816
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.3559497 , 3.95421664, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0184446934530056}
episode index:2437
target Thresh 13.94398580948661
target distance 6.0
model initialize at round 2437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.000000000000026}
done in step count: 4
reward sum = 0.946253158974469
running average episode reward sum: 0.7374175595574315
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.52891958, 11.29687321,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.556821756301111}
episode index:2438
target Thresh 13.946513184685413
target distance 11.0
model initialize at round 2438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([12.82335934,  7.14612992,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.229514315196361}
done in step count: 5
reward sum = 0.9211070726951014
running average episode reward sum: 0.7374928730109525
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.01571203, 10.00348271,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.016093385379944137}
episode index:2439
target Thresh 13.949039296512487
target distance 11.0
model initialize at round 2439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 3.46651978, 11.87981522,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.920052255358057}
done in step count: 25
reward sum = 0.7292411137326964
running average episode reward sum: 0.7374894911423958
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.88508599,  8.52846588,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0028567410817666}
episode index:2440
target Thresh 13.951564145599356
target distance 12.0
model initialize at round 2440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.20354939,  8.23942792,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.538642771558948}
done in step count: 9
reward sum = 0.8901687375667661
running average episode reward sum: 0.7375520389696897
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.26823303, 11.40909531,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.4891910963339129}
episode index:2441
target Thresh 13.954087732577237
target distance 11.0
model initialize at round 2441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.26756316,  7.79870707,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 9.6809342598809}
done in step count: 5
reward sum = 0.9246379105337965
running average episode reward sum: 0.7376286507107069
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.60905747, 4.86935985, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.41219280626698}
episode index:2442
target Thresh 13.956610058077029
target distance 13.0
model initialize at round 2442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.39367867,  9.        ,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.87695280433021}
done in step count: 14
reward sum = 0.8387610279474942
running average episode reward sum: 0.7376700475085934
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.023328  , 3.08410855, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9802869204797577}
episode index:2443
target Thresh 13.959131122729303
target distance 1.0
model initialize at round 2443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.        , 8.71855915, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0388498228045533}
done in step count: 0
reward sum = 0.9966127545777856
running average episode reward sum: 0.7377759978797347
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.        , 8.71855915, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0388498228045533}
episode index:2444
target Thresh 13.961650927164333
target distance 4.0
model initialize at round 2444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 8.71601462, 10.877909  ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.2862541233553153}
done in step count: 26
reward sum = 0.7013027414827805
running average episode reward sum: 0.7377610803924558
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.49273549, 11.47051883,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6918852861204964}
episode index:2445
target Thresh 13.964169472012069
target distance 12.0
model initialize at round 2445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([5., 8., 0.]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.198039027185583}
done in step count: 16
reward sum = 0.8047834535027731
running average episode reward sum: 0.737788481199124
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.11865192,  6.35352692,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.3729069064693938}
episode index:2446
target Thresh 13.96668675790215
target distance 3.0
model initialize at round 2446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.        ,  9.12375808,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.0076289305972723}
done in step count: 9
reward sum = 0.9014342041938386
running average episode reward sum: 0.7378553572608301
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.22644753,  8.39202261,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.6487796146410327}
episode index:2447
target Thresh 13.969202785463889
target distance 10.0
model initialize at round 2447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 4.        , 10.71114159,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.005213250324031}
done in step count: 14
reward sum = 0.8345203010101855
running average episode reward sum: 0.7378948445744531
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.6929426 , 11.53278317,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6149326409622774}
episode index:2448
target Thresh 13.9717175553263
target distance 3.0
model initialize at round 2448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.08570445,  6.39175606,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.3943924039847675}
done in step count: 1
reward sum = 0.9851825166754078
running average episode reward sum: 0.7379958195324363
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.55042082,  4.73503947,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5218481767132218}
episode index:2449
target Thresh 13.974231068118074
target distance 7.0
model initialize at round 2449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([9.        , 9.31777143, 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 6.60629625153429}
done in step count: 3
reward sum = 0.9508708271290569
running average episode reward sum: 0.7380827072906391
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.63782779,  4.89157404,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6469779564094166}
episode index:2450
target Thresh 13.97674332446759
target distance 3.0
model initialize at round 2450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.17167306, 6.67373979, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.482113915885282}
done in step count: 20
reward sum = 0.7813272120890481
running average episode reward sum: 0.738100350907448
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.06545963, 10.64085302,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.36506372067677095}
episode index:2451
target Thresh 13.979254325002909
target distance 6.0
model initialize at round 2451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([ 6., 11.,  0.]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 4.4721359549995885}
done in step count: 22
reward sum = 0.7496217837604964
running average episode reward sum: 0.7381050496973554
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.43703693, 8.34967127, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.8601481684199402}
episode index:2452
target Thresh 13.981764070351783
target distance 13.0
model initialize at round 2452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([3.88413608, 7.        , 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.157062083825215}
done in step count: 8
reward sum = 0.8935821140557195
running average episode reward sum: 0.7381684321125035
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.05919108,  8.29055786,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9846549105947908}
episode index:2453
target Thresh 13.98427256114165
target distance 11.0
model initialize at round 2453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([3.19557083, 9.82974911, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 12.276875189529878}
done in step count: 35
reward sum = 0.620829454887418
running average episode reward sum: 0.7381206167183613
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.52655658,  3.57962311,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6331393215461452}
episode index:2454
target Thresh 13.986779797999633
target distance 3.0
model initialize at round 2454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([11.        , 10.36752403,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.065398475232766}
done in step count: 1
reward sum = 0.9828122054791274
running average episode reward sum: 0.738220287426614
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.06377506, 10.32117695,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.9897836956776613}
episode index:2455
target Thresh 13.989285781552535
target distance 13.0
model initialize at round 2455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 2.42337775, 10.47760814,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 14.146760563186367}
done in step count: 34
reward sum = 0.6329076894785854
running average episode reward sum: 0.7381774077043225
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.6955135 ,  3.62344393,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7909067605251514}
episode index:2456
target Thresh 13.991790512426856
target distance 10.0
model initialize at round 2456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.189726279481821}
done in step count: 8
reward sum = 0.8903720980992976
running average episode reward sum: 0.7382393510052566
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.95928471,  6.56345429,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1125232078330083}
episode index:2457
target Thresh 13.994293991248783
target distance 6.0
model initialize at round 2457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.34329522, 7.        , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.219531021195853}
done in step count: 7
reward sum = 0.9074050753732809
running average episode reward sum: 0.7383081735131363
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.5450948 , 11.90966696,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.060482111265568}
episode index:2458
target Thresh 13.99679621864418
target distance 9.0
model initialize at round 2458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4.8643949 , 5.98162736, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.558877253838157}
done in step count: 20
reward sum = 0.7784350172721397
running average episode reward sum: 0.7383244918717207
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.00467096, 10.24161367,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7584007180359408}
episode index:2459
target Thresh 13.999297195238604
target distance 12.0
model initialize at round 2459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([12.6415859 ,  7.36909703,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 10.961979793652109}
done in step count: 14
reward sum = 0.835572208831806
running average episode reward sum: 0.7383640234639808
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.27768296, 9.15690196, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8876497828121999}
episode index:2460
target Thresh 14.001796921657306
target distance 5.0
model initialize at round 2460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 6.99505755, 11.86740305,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.1276296443239593}
done in step count: 2
reward sum = 0.9666939417050588
running average episode reward sum: 0.7384568027887435
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.94623708, 11.84988707,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.2718776057327101}
episode index:2461
target Thresh 14.004295398525207
target distance 2.0
model initialize at round 2461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 5.9813584 , 11.86287856,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.481645408349189}
done in step count: 8
reward sum = 0.9093584581511568
running average episode reward sum: 0.7385262185707754
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.85234454, 8.6492949 , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.38052096687288245}
episode index:2462
target Thresh 14.006792626466932
target distance 7.0
model initialize at round 2462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([8.98035976, 8.13454161, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 6.503170184973457}
done in step count: 12
reward sum = 0.8609086500493246
running average episode reward sum: 0.7385759069310996
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.58838822,  4.95695383,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0417220797541273}
episode index:2463
target Thresh 14.009288606106791
target distance 9.0
model initialize at round 2463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 6.99505755, 11.86740305,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 8.529995966677175}
done in step count: 50
reward sum = 0.5004486325126075
running average episode reward sum: 0.7384792643684298
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.71945609,  7.77115279,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.0546533476024036}
episode index:2464
target Thresh 14.011783338068774
target distance 5.0
model initialize at round 2464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([12.99505755, 11.86740305,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 3.5379193441547394}
done in step count: 7
reward sum = 0.9071754380217109
running average episode reward sum: 0.7385477009500336
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.85348023,  9.85666716,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8654321462268744}
episode index:2465
target Thresh 14.014276822976562
target distance 11.0
model initialize at round 2465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.9013179 , 5.62741837, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.228972122753508}
done in step count: 13
reward sum = 0.8303958956463822
running average episode reward sum: 0.7385849467710784
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.7364464 ,  3.56167835,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8570176050641573}
episode index:2466
target Thresh 14.016769061453534
target distance 1.0
model initialize at round 2466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.18443894, 4.94523638, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.1857042908252478}
done in step count: 1
reward sum = 0.988908678145482
running average episode reward sum: 0.7386864156528679
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.47336137, 4.77999622, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.5219891323030739}
episode index:2467
target Thresh 14.019260054122745
target distance 11.0
model initialize at round 2467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 6.99999998, 11.64345805,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 9.02297282457902}
done in step count: 15
reward sum = 0.8162193150220323
running average episode reward sum: 0.7387178309281389
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.6134509 , 10.69292966,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6860132624809467}
episode index:2468
target Thresh 14.021749801606942
target distance 4.0
model initialize at round 2468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.77079701, 7.        , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 2.1433917134196263}
done in step count: 9
reward sum = 0.8876875644156398
running average episode reward sum: 0.7387781669886847
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.83072145, 5.68245503, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.7031359047718719}
episode index:2469
target Thresh 14.024238304528565
target distance 1.0
model initialize at round 2469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.        , 11.23697454,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.5906306944441755}
done in step count: 1
reward sum = 0.9852477839088034
running average episode reward sum: 0.7388779522586928
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.54915929,  9.82220012,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.4846340317577652}
episode index:2470
target Thresh 14.026725563509737
target distance 10.0
model initialize at round 2470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 5.974756  , 10.93766797,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 10.210782259793941}
done in step count: 21
reward sum = 0.7319701047257349
running average episode reward sum: 0.7388751566910955
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.15152611,  8.38974864,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.6287820692097069}
episode index:2471
target Thresh 14.029211579172276
target distance 9.0
model initialize at round 2471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([10.00494245, 11.86740305,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 7.249580188566641}
done in step count: 5
reward sum = 0.9322184981801236
running average episode reward sum: 0.7389533700169405
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.13669147, 10.33483954,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.3616656987047221}
episode index:2472
target Thresh 14.031696352137683
target distance 4.0
model initialize at round 2472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.56194627,  7.19878638,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.6970980135951237}
done in step count: 1
reward sum = 0.9838745616061402
running average episode reward sum: 0.7390524081049265
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.68474394,  5.22796178,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.721693035178167}
episode index:2473
target Thresh 14.03417988302715
target distance 11.0
model initialize at round 2473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([16.00177193,  5.6182394 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 12.247543943867552}
done in step count: 7
reward sum = 0.9085213063626311
running average episode reward sum: 0.7391209080638019
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.32873252, 11.01414955,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.3290368942250533}
episode index:2474
target Thresh 14.036662172461565
target distance 3.0
model initialize at round 2474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.        ,  5.10979605,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.907101539476338}
done in step count: 9
reward sum = 0.8903439729334695
running average episode reward sum: 0.7391820082920321
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.85134931,  3.15902228,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8660737483056892}
episode index:2475
target Thresh 14.039143221061495
target distance 5.0
model initialize at round 2475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([12.90886124, 11.86871574,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 4.332585221895365}
done in step count: 16
reward sum = 0.8278168956738817
running average episode reward sum: 0.7392178059040603
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.26937223, 10.59554348,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.6536309660105466}
episode index:2476
target Thresh 14.041623029447203
target distance 6.0
model initialize at round 2476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.40001535, 10.7151196 ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 5.884100189286466}
done in step count: 3
reward sum = 0.9600027855169911
running average episode reward sum: 0.7393069399289343
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.71912259,  5.44454193,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5258418476766482}
episode index:2477
target Thresh 14.044101598238646
target distance 6.0
model initialize at round 2477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([10.        , 11.55055761,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.037711441350889}
done in step count: 2
reward sum = 0.9680961012050666
running average episode reward sum: 0.7393992680811845
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.04846443, 11.53182905,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.5340327173634224}
episode index:2478
target Thresh 14.046578928055462
target distance 8.0
model initialize at round 2478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 5.07058847, 10.95594186,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 6.995038884154192}
done in step count: 4
reward sum = 0.9453593383689478
running average episode reward sum: 0.7394823499973958
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.36811211,  9.486609  ,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.6317252907523447}
episode index:2479
target Thresh 14.049055019516983
target distance 8.0
model initialize at round 2479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([11.10007453,  9.61296093,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 7.126484367059725}
done in step count: 43
reward sum = 0.5746474966139856
running average episode reward sum: 0.739415884330709
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.54150326, 9.22511965, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5864338251052664}
episode index:2480
target Thresh 14.051529873242233
target distance 13.0
model initialize at round 2480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.132598050286157}
done in step count: 26
reward sum = 0.7138728254269442
running average episode reward sum: 0.7394055888615821
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.52162428,  4.40389845,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6597164892767231}
episode index:2481
target Thresh 14.054003489849926
target distance 11.0
model initialize at round 2481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([14.40878057,  6.63533795,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 9.992302168344295}
done in step count: 16
reward sum = 0.7904183289381869
running average episode reward sum: 0.7394261419397757
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.96305325, 10.70285428,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.7038246923143272}
episode index:2482
target Thresh 14.056475869958467
target distance 4.0
model initialize at round 2482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.92357898, 7.23250043, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 3.2334036599180522}
done in step count: 7
reward sum = 0.9129464360980616
running average episode reward sum: 0.7394960252640441
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.91921565, 4.49332194, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.4998926326190159}
episode index:2483
target Thresh 14.058947014185948
target distance 8.0
model initialize at round 2483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([8.72282887, 9.82453096, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 6.773203027470054}
done in step count: 3
reward sum = 0.9597887833864854
running average episode reward sum: 0.7395847099492786
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.76113164, 8.33723822, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.009244546072531}
episode index:2484
target Thresh 14.061416923150157
target distance 13.0
model initialize at round 2484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.        , 9.57558715, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.091841684655469}
done in step count: 6
reward sum = 0.9156132055328587
running average episode reward sum: 0.7396555463660125
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.13956917, 11.57208535,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.5888643342687022}
episode index:2485
target Thresh 14.063885597468573
target distance 8.0
model initialize at round 2485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 4.58568847, 10.92735553,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 7.660725472703502}
done in step count: 4
reward sum = 0.9447493624562556
running average episode reward sum: 0.7397380458897816
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.24357201,  9.93437404,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.9655993794157282}
episode index:2486
target Thresh 14.066353037758361
target distance 11.0
model initialize at round 2486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.99983213, 7.00007785, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.055560578267828}
done in step count: 4
reward sum = 0.934639951867256
running average episode reward sum: 0.7398164141672153
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.13406639,  6.96633243,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.297551303274477}
episode index:2487
target Thresh 14.068819244636384
target distance 9.0
model initialize at round 2487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.88277883, 4.27173835, 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 9.794097306464934}
done in step count: 12
reward sum = 0.8537009991253885
running average episode reward sum: 0.7398621877142242
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.42717868, 10.59426794,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.589152044019714}
episode index:2488
target Thresh 14.071284218719189
target distance 9.0
model initialize at round 2488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([ 3.97380116, 11.86775028,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 7.270210533951995}
done in step count: 5
reward sum = 0.925060859634983
running average episode reward sum: 0.7399365945731718
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.58707832,  9.14388007,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9504975797647284}
episode index:2489
target Thresh 14.073747960623026
target distance 9.0
model initialize at round 2489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 3.55314243, 10.12214112,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 7.530928786759512}
done in step count: 4
reward sum = 0.9427387356808965
running average episode reward sum: 0.7400180412161871
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.55314243,  8.75194925,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.6062142528210618}
episode index:2490
target Thresh 14.07621047096383
target distance 11.0
model initialize at round 2490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([6., 9., 0.]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 9.48683298050515}
done in step count: 23
reward sum = 0.7422958904512074
running average episode reward sum: 0.740018955647835
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.6740497 ,  6.67856609,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.7527918294479863}
episode index:2491
target Thresh 14.078671750357222
target distance 11.0
model initialize at round 2491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.05493093, 8.47712573, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.353559142674875}
done in step count: 34
reward sum = 0.6460024256235398
running average episode reward sum: 0.7399812283083389
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.04197943,  2.32159809,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.6796995063316394}
episode index:2492
target Thresh 14.081131799418527
target distance 3.0
model initialize at round 2492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([6.        , 9.57495904, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.7409025643518616}
done in step count: 4
reward sum = 0.9507407770701668
running average episode reward sum: 0.740065768841336
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.68963122, 10.93331718,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.31745137455231576}
episode index:2493
target Thresh 14.083590618762754
target distance 14.0
model initialize at round 2493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.        , 6.74133539, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.3091396828301}
done in step count: 22
reward sum = 0.759608077472144
running average episode reward sum: 0.7400736045705384
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.89899061,  4.51186424,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.034499458282685}
episode index:2494
target Thresh 14.086048209004613
target distance 7.0
model initialize at round 2494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([10.99999978,  8.27013928,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 5.97442993906297}
done in step count: 4
reward sum = 0.9457406902362867
running average episode reward sum: 0.7401560362682
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.3601518 ,  5.12480627,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6519066868979463}
episode index:2495
target Thresh 14.088504570758499
target distance 5.0
model initialize at round 2495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.        , 11.38810623,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.025000238574367}
done in step count: 2
reward sum = 0.9721832562106303
running average episode reward sum: 0.7402489958915744
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.60481215, 11.85511429,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0473863627108237}
episode index:2496
target Thresh 14.090959704638502
target distance 1.0
model initialize at round 2496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([3.44876039, 5.62269485, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 2.9962367971336397}
done in step count: 7
reward sum = 0.9214568222522848
running average episode reward sum: 0.7403215661063766
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.11929047, 2.58766711, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9724544646277832}
episode index:2497
target Thresh 14.093413611258406
target distance 11.0
model initialize at round 2497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([11.00000001,  8.32456858,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 10.457104323013562}
done in step count: 44
reward sum = 0.5399304632300805
running average episode reward sum: 0.7402413454887319
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.39647987, 2.43810621, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.6876924603321022}
episode index:2498
target Thresh 14.095866291231685
target distance 7.0
model initialize at round 2498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([5.90669791, 8.02624481, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 5.185549670646525}
done in step count: 10
reward sum = 0.8814119838704086
running average episode reward sum: 0.7402978363404253
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.36119694,  9.29377054,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.46557959019131717}
episode index:2499
target Thresh 14.09831774517151
target distance 5.0
model initialize at round 2499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([5.84760845, 9.1333524 , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.6375203407295778}
done in step count: 5
reward sum = 0.9434096212737684
running average episode reward sum: 0.7403790810543986
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.53646424, 6.20739983, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.5078189503221779}
episode index:2500
target Thresh 14.10076797369075
target distance 5.0
model initialize at round 2500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([9.97860622, 9.88036847, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 5.588279464471177}
done in step count: 2
reward sum = 0.9697554583539968
running average episode reward sum: 0.7404707949197723
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.09740564,  6.67122625,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1248205440786934}
episode index:2501
target Thresh 14.103216977401953
target distance 2.0
model initialize at round 2501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([8.6776625 , 8.10775208, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.5952061985173913}
done in step count: 3
reward sum = 0.9582035413153115
running average episode reward sum: 0.7405578183995466
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.70574216, 8.1106136 , 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.9368008600231333}
episode index:2502
target Thresh 14.105664756917378
target distance 3.0
model initialize at round 2502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([14.28409147,  9.        ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.284091472625743}
done in step count: 3
reward sum = 0.9572512962554572
running average episode reward sum: 0.7406443919024854
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.01452428,  9.08884299,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.9894723185661168}
episode index:2503
target Thresh 14.108111312848969
target distance 2.0
model initialize at round 2503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([2.62382519, 8.97529185, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.7157750136111214}
done in step count: 6
reward sum = 0.9294617973288142
running average episode reward sum: 0.7407197982145566
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.3730372 , 9.91509442, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.3825777144595544}
episode index:2504
target Thresh 14.11055664580836
target distance 2.0
model initialize at round 2504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([ 4.90531772, 10.79434574,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 2.1019053042519107}
done in step count: 3
reward sum = 0.9670646825372455
running average episode reward sum: 0.7408101554538071
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.51648015, 9.51335692, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.7052139951188318}
episode index:2505
target Thresh 14.113000756406885
target distance 7.0
model initialize at round 2505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 5.44164038, 11.        ,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 6.856535637186675}
done in step count: 4
reward sum = 0.9466474832040566
running average episode reward sum: 0.7408922932541864
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.15829811,  9.87952413,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.2173843976549132}
episode index:2506
target Thresh 14.115443645255576
target distance 7.0
model initialize at round 2506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 1.98883462, 10.02401626,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 7.0240251353429874}
done in step count: 15
reward sum = 0.8324131935230709
running average episode reward sum: 0.7409287993970938
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.4298793 , 3.36881105, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.6790134050344504}
episode index:2507
target Thresh 14.117885312965155
target distance 3.0
model initialize at round 2507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([14.17336512,  4.765679  ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.7144888367518636}
done in step count: 18
reward sum = 0.815476525297293
running average episode reward sum: 0.7409585233707382
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.33832377,  7.24831432,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0014223844249024}
episode index:2508
target Thresh 14.120325760146034
target distance 2.0
model initialize at round 2508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.92237389, 8.        , 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.003008381229977}
done in step count: 0
reward sum = 0.994228358964695
running average episode reward sum: 0.7410594679046537
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.92237389, 8.        , 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.003008381229977}
episode index:2509
target Thresh 14.12276498740833
target distance 2.0
model initialize at round 2509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.65293928, 8.10262037, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.9621545344546257}
done in step count: 0
reward sum = 0.9949130120739358
running average episode reward sum: 0.7411606047748407
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.65293928, 8.10262037, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.9621545344546257}
episode index:2510
target Thresh 14.125202995361844
target distance 3.0
model initialize at round 2510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.56121135, 6.11286438, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.941174985126847}
done in step count: 27
reward sum = 0.6929832402516276
running average episode reward sum: 0.7411414182497418
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.86440251, 9.95940719, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.2913767341867028}
episode index:2511
target Thresh 14.127639784616083
target distance 3.0
model initialize at round 2511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([11., 11.,  0.]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0000000000000195}
done in step count: 2
reward sum = 0.967636684443507
running average episode reward sum: 0.741231583562717
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.63622242, 11.88331977,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9552946919861498}
episode index:2512
target Thresh 14.130075355780242
target distance 8.0
model initialize at round 2512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 5.97380116, 11.86775028,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 6.309006540739259}
done in step count: 11
reward sum = 0.858281033029145
running average episode reward sum: 0.7412781611391064
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.49095895,  9.85636416,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.5115388005596615}
episode index:2513
target Thresh 14.132509709463216
target distance 3.0
model initialize at round 2513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 9.45187116, 10.85568255,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.4590261757326182}
done in step count: 10
reward sum = 0.8897569537779633
running average episode reward sum: 0.7413372219158124
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.35433777, 11.36641994,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7423902505173428}
episode index:2514
target Thresh 14.13494284627359
target distance 7.0
model initialize at round 2514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 8.58490176, 11.72382559,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 6.57192255145546}
done in step count: 3
reward sum = 0.9573128948120178
running average episode reward sum: 0.7414230969348566
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.86143247,  7.97629663,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.14058025842307026}
episode index:2515
target Thresh 14.137374766819654
target distance 3.0
model initialize at round 2515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.8591618 ,  5.96929009,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.3418352783669316}
done in step count: 1
reward sum = 0.9814782140085868
running average episode reward sum: 0.741518508348638
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.49782486,  7.96911937,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0914999883133953}
episode index:2516
target Thresh 14.139805471709382
target distance 12.0
model initialize at round 2516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 10.198039027185585}
done in step count: 20
reward sum = 0.7731542934882466
running average episode reward sum: 0.7415310771945417
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.01265339, 9.74968785, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7497946248755182}
episode index:2517
target Thresh 14.14223496155045
target distance 2.0
model initialize at round 2517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.54311657,  9.        ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.45688343048094104}
done in step count: 0
reward sum = 0.9952140936162899
running average episode reward sum: 0.7416318250167903
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.54311657,  9.        ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.45688343048094104}
episode index:2518
target Thresh 14.144663236950239
target distance 11.0
model initialize at round 2518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 11.401754250991408}
done in step count: 5
reward sum = 0.9181533157844423
running average episode reward sum: 0.7417019010353562
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.96104234, 11.86634823,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.2938939843566197}
episode index:2519
target Thresh 14.147090298515808
target distance 9.0
model initialize at round 2519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([12.02375307, 11.86707891,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 7.586385742891916}
done in step count: 44
reward sum = 0.5514950635812428
running average episode reward sum: 0.7416264221316046
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.53294113, 9.54133005, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.7596476022442512}
episode index:2520
target Thresh 14.149516146853927
target distance 14.0
model initialize at round 2520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 2.38092709, 10.26294911,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 14.990119365205471}
done in step count: 18
reward sum = 0.7886363012382793
running average episode reward sum: 0.7416450694458079
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.81172651,  3.41365673,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0013482730021024}
episode index:2521
target Thresh 14.151940782571055
target distance 9.0
model initialize at round 2521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.48001707, 10.2367022 ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.5143004977181}
done in step count: 21
reward sum = 0.7800205512795211
running average episode reward sum: 0.7416602857351947
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.06832672, 11.75483139,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.7579175221940649}
episode index:2522
target Thresh 14.154364206273359
target distance 8.0
model initialize at round 2522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 8.25594018, 11.86194845,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 8.935590772471153}
done in step count: 6
reward sum = 0.9270598656692206
running average episode reward sum: 0.7417337695163815
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.59316707,  6.48327529,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.6317183219059389}
episode index:2523
target Thresh 14.156786418566686
target distance 11.0
model initialize at round 2523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.929943319783787}
done in step count: 5
reward sum = 0.9158781407424295
running average episode reward sum: 0.7418027649091019
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.99646134, 11.86732258,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.3210539945242177}
episode index:2524
target Thresh 14.159207420056592
target distance 2.0
model initialize at round 2524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.87885749, 9.        , 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.1211425065994476}
done in step count: 5
reward sum = 0.9436055075145465
running average episode reward sum: 0.7418826867873616
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.58454583, 8.96533035, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.5855730629751904}
episode index:2525
target Thresh 14.161627211348327
target distance 3.0
model initialize at round 2525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.        , 9.17073004, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.0144696872652226}
done in step count: 1
reward sum = 0.9849104987820815
running average episode reward sum: 0.741978897322593
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.92934552, 8.13868537, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.8642077033274294}
episode index:2526
target Thresh 14.164045793046842
target distance 1.0
model initialize at round 2526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.94234371,  5.85181141,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.15900976720460283}
done in step count: 0
reward sum = 0.9983452382909442
running average episode reward sum: 0.7420803481896165
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.94234371,  5.85181141,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.15900976720460283}
episode index:2527
target Thresh 14.166463165756781
target distance 8.0
model initialize at round 2527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([11.       , 10.3681339,  0.       ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 6.154006042839586}
done in step count: 5
reward sum = 0.93207339831007
running average episode reward sum: 0.7421555036683034
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.51785231, 9.76069027, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.9202286133903577}
episode index:2528
target Thresh 14.168879330082483
target distance 9.0
model initialize at round 2528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.5119589, 4.       , 0.       ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.016992526624392}
done in step count: 6
reward sum = 0.9172619789236095
running average episode reward sum: 0.7422247430812158
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.39127797, 11.90351829,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9846033502977659}
episode index:2529
target Thresh 14.171294286627994
target distance 8.0
model initialize at round 2529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 7.211102550927984}
done in step count: 13
reward sum = 0.8296134781281793
running average episode reward sum: 0.7422592840832105
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.11506884, 7.26262358, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9230787124955036}
episode index:2530
target Thresh 14.17370803599705
target distance 8.0
model initialize at round 2530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3., 8., 0.]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 6.082762530298234}
done in step count: 7
reward sum = 0.9115410163528606
running average episode reward sum: 0.7423261674227085
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.30191555, 2.25216633, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7422329561997247}
episode index:2531
target Thresh 14.176120578793093
target distance 10.0
model initialize at round 2531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.544003745317555}
done in step count: 4
reward sum = 0.9311900409465235
running average episode reward sum: 0.7424007582100401
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.08864114, 11.86997507,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8744792045384857}
episode index:2532
target Thresh 14.178531915619253
target distance 6.0
model initialize at round 2532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([3.24024606, 6.        , 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 4.369980999742208}
done in step count: 11
reward sum = 0.8733357754688514
running average episode reward sum: 0.7424524498868104
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.19602947, 10.45986472,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.9261987724869442}
episode index:2533
target Thresh 14.180942047078366
target distance 11.0
model initialize at round 2533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.        ,  8.76266003,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.27392528311482}
done in step count: 5
reward sum = 0.9271228474274159
running average episode reward sum: 0.7425253269181997
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.76628275, 11.86930316,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.1588258047021918}
episode index:2534
target Thresh 14.183350973772967
target distance 9.0
model initialize at round 2534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([13.13932426,  3.97608179,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 9.564954450462833}
done in step count: 6
reward sum = 0.9103511034792691
running average episode reward sum: 0.742591530380354
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.78251986, 9.84785001, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.8752983804072351}
episode index:2535
target Thresh 14.185758696305287
target distance 10.0
model initialize at round 2535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([ 4.78297961, 11.        ,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 8.27764605002001}
done in step count: 7
reward sum = 0.910804120315353
running average episode reward sum: 0.742657860265975
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.3537037 ,  9.45057153,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.6534355000053366}
episode index:2536
target Thresh 14.188165215277255
target distance 11.0
model initialize at round 2536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.84908438,  2.62425816,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.855589056122849}
done in step count: 36
reward sum = 0.592992199758007
running average episode reward sum: 0.7425988671006192
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.35008482, 3.18280224, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.39493801829744923}
episode index:2537
target Thresh 14.190570531290504
target distance 3.0
model initialize at round 2537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.14464867, 9.89743841, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.1120095793414309}
done in step count: 1
reward sum = 0.983712658371733
running average episode reward sum: 0.7426938685944219
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.90016264, 11.61117848,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0880403978764597}
episode index:2538
target Thresh 14.192974644946359
target distance 3.0
model initialize at round 2538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4., 4., 0.]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0000000000000195}
done in step count: 1
reward sum = 0.9793519778923704
running average episode reward sum: 0.7427870777749253
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.91291423, 2.4924188 , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0445338975078415}
episode index:2539
target Thresh 14.195377556845852
target distance 11.0
model initialize at round 2539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.79650235,  9.        ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.998572814559823}
done in step count: 5
reward sum = 0.9272644922686436
running average episode reward sum: 0.7428597066782692
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.83426062, 10.49066364,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.5356240021388624}
episode index:2540
target Thresh 14.197779267589706
target distance 6.0
model initialize at round 2540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([7.08377934, 9.33897841, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 5.275038433797783}
done in step count: 4
reward sum = 0.9457398448034484
running average episode reward sum: 0.7429395493142885
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.70560312, 6.28055739, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.4066718243264851}
episode index:2541
target Thresh 14.200179777778352
target distance 9.0
model initialize at round 2541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 4.94637084, 10.5905267 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.064031933780374}
done in step count: 10
reward sum = 0.8754789664953933
running average episode reward sum: 0.7429916891322197
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.38233194, 11.18075426,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.42290639280004594}
episode index:2542
target Thresh 14.20257908801192
target distance 8.0
model initialize at round 2542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([9.58884041, 8.10204301, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 6.746966137154808}
done in step count: 12
reward sum = 0.8583515379208398
running average episode reward sum: 0.7430370528163679
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.13528957,  5.6409164 ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9363039883054757}
episode index:2543
target Thresh 14.204977198890235
target distance 9.0
model initialize at round 2543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([2.2515496 , 9.62381792, 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 9.845108546195604}
done in step count: 34
reward sum = 0.6194790277727705
running average episode reward sum: 0.7429884844102973
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.06005257, 11.82770429,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8298799322439593}
episode index:2544
target Thresh 14.207374111012822
target distance 3.0
model initialize at round 2544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 7.99505755, 11.86740305,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.327515491320984}
done in step count: 1
reward sum = 0.980424494969061
running average episode reward sum: 0.7430817795028548
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.97163443, 11.85270983,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.292744178021993}
episode index:2545
target Thresh 14.209769824978913
target distance 2.0
model initialize at round 2545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([16.14401186,  6.        ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.144011855125555}
done in step count: 1
reward sum = 0.9838620863260633
running average episode reward sum: 0.7431763515008215
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.14401186,  6.45298654,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.4753274854219693}
episode index:2546
target Thresh 14.212164341387435
target distance 5.0
model initialize at round 2546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 9.99083114, 11.        ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.0091688632965603}
done in step count: 2
reward sum = 0.9664826271211908
running average episode reward sum: 0.7432640257354585
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.71326523, 10.33491832,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9752337843090884}
episode index:2547
target Thresh 14.214557660837022
target distance 9.0
model initialize at round 2547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 5.        , 10.86479402,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 7.053216903648264}
done in step count: 4
reward sum = 0.938051642165293
running average episode reward sum: 0.7433404729946538
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.97812506,  9.51267155,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.092802656514698}
episode index:2548
target Thresh 14.216949783925994
target distance 3.0
model initialize at round 2548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.90355858,  6.5820794 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.8219202291021628}
done in step count: 1
reward sum = 0.9816692174900643
running average episode reward sum: 0.7434339719136398
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.91068118,  5.53192916,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0546510527964177}
episode index:2549
target Thresh 14.219340711252388
target distance 8.0
model initialize at round 2549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4., 4., 0.]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 7.810249675906681}
done in step count: 3
reward sum = 0.9466566068092447
running average episode reward sum: 0.7435136670645792
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.08250979, 10.1829104 ,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.9355450229569483}
episode index:2550
target Thresh 14.221730443413936
target distance 8.0
model initialize at round 2550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([5., 9., 0.]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 6.000000000000019}
done in step count: 9
reward sum = 0.8867829251815931
running average episode reward sum: 0.7435698290630571
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.14357404,  9.57414389,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.5918232082363626}
episode index:2551
target Thresh 14.224118981008068
target distance 11.0
model initialize at round 2551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.4367851 ,  3.20024276,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.590826662442655}
done in step count: 27
reward sum = 0.6928010442829668
running average episode reward sum: 0.7435499353386136
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.05344481, 5.69857327, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.7006147016249252}
episode index:2552
target Thresh 14.22650632463192
target distance 8.0
model initialize at round 2552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([9., 9., 0.]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 8.485281374238598}
done in step count: 15
reward sum = 0.8098481365322776
running average episode reward sum: 0.7435759040817368
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.52180532,  2.09220361,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0470793106542}
episode index:2553
target Thresh 14.22889247488233
target distance 12.0
model initialize at round 2553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.        , 10.58698773,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.974489859938592}
done in step count: 18
reward sum = 0.7924525387737738
running average episode reward sum: 0.7435950413701832
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.98411586, 3.05621091, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.3635328637116015}
episode index:2554
target Thresh 14.231277432355835
target distance 7.0
model initialize at round 2554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.03235102,  8.        ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.737155096350377}
done in step count: 4
reward sum = 0.9445429639045932
running average episode reward sum: 0.743673690263543
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.80040979, 10.868682  ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.238915609640894}
episode index:2555
target Thresh 14.233661197648674
target distance 10.0
model initialize at round 2555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([13.10174611,  3.6472763 ,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 10.295406185679154}
done in step count: 9
reward sum = 0.8807035593849186
running average episode reward sum: 0.7437273013234497
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.10273835, 9.39689858, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.0811150693821872}
episode index:2556
target Thresh 14.236043771356785
target distance 10.0
model initialize at round 2556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([5.99994899, 7.99197386, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 8.94073103799352}
done in step count: 15
reward sum = 0.8104328523638503
running average episode reward sum: 0.7437533887505284
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.56730641,  3.32407593,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8824454143674498}
episode index:2557
target Thresh 14.238425154075815
target distance 12.0
model initialize at round 2557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([3.21483707, 5.        , 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 11.887797917794968}
done in step count: 14
reward sum = 0.8313313926899759
running average episode reward sum: 0.7437876256558996
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.89580325, 10.86790971,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8741419930749471}
episode index:2558
target Thresh 14.240805346401107
target distance 4.0
model initialize at round 2558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.33297923, 5.        , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.4035044483290524}
done in step count: 4
reward sum = 0.9532427821304642
running average episode reward sum: 0.7438694760492074
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.67354456, 7.22948116, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.3990423036094787}
episode index:2559
target Thresh 14.243184348927713
target distance 1.0
model initialize at round 2559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.43180913,  3.80972785,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.4718713960754154}
done in step count: 0
reward sum = 0.9993684112304211
running average episode reward sum: 0.7439692803207626
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.43180913,  3.80972785,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.4718713960754154}
episode index:2560
target Thresh 14.24556216225038
target distance 5.0
model initialize at round 2560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([12.        ,  9.96278143,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 3.1507059654285903}
done in step count: 2
reward sum = 0.9726303700379901
running average episode reward sum: 0.7440585661816439
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.51473218, 8.77669865, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.5610817343338528}
episode index:2561
target Thresh 14.247938786963562
target distance 4.0
model initialize at round 2561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([12.82826766,  7.14076384,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 2.607550907549073}
done in step count: 1
reward sum = 0.9803805257547044
running average episode reward sum: 0.7441508073836631
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.23171624,  8.72756611,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.35764876187019656}
episode index:2562
target Thresh 14.250314223661418
target distance 13.0
model initialize at round 2562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.45106328,  6.90606177,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.451417633256918}
done in step count: 24
reward sum = 0.7422054037637562
running average episode reward sum: 0.7441500483498669
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.29358391, 6.79142658, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7365640270216036}
episode index:2563
target Thresh 14.252688472937807
target distance 3.0
model initialize at round 2563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([2.47210073, 8.        , 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 2.718505967521545}
done in step count: 6
reward sum = 0.9228825922664556
running average episode reward sum: 0.7442197568303336
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.30362161, 8.93600177, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.6993129774039935}
episode index:2564
target Thresh 14.255061535386286
target distance 8.0
model initialize at round 2564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([8.35141885, 9.        , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 6.65886788003644}
done in step count: 3
reward sum = 0.9511479541521926
running average episode reward sum: 0.7443004305914728
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.18342536, 6.251336  , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7708064880102717}
episode index:2565
target Thresh 14.257433411600127
target distance 12.0
model initialize at round 2565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4.89192235, 4.32927251, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.233019159189181}
done in step count: 12
reward sum = 0.8472570918603536
running average episode reward sum: 0.7443405539980468
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.88370715,  6.35015944,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9505524544196672}
episode index:2566
target Thresh 14.259804102172293
target distance 5.0
model initialize at round 2566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.44437504,  8.74293423,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.984658000645357}
done in step count: 8
reward sum = 0.9051938695666318
running average episode reward sum: 0.7444032159830755
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.99855649, 10.86519651,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.13481121931980408}
episode index:2567
target Thresh 14.262173607695463
target distance 13.0
model initialize at round 2567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        ,  9.66055322,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.859353372375006}
done in step count: 11
reward sum = 0.8479127781247042
running average episode reward sum: 0.7444435234449686
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.41965614, 2.55092145, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7338055195556198}
episode index:2568
target Thresh 14.264541928762005
target distance 8.0
model initialize at round 2568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([13.2246449,  8.       ,  0.       ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 6.25714677255848}
done in step count: 3
reward sum = 0.9559767510122528
running average episode reward sum: 0.7445258641330057
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.78192415,  2.23160659,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.3181174178665771}
episode index:2569
target Thresh 14.26690906596401
target distance 1.0
model initialize at round 2569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.84231126, 10.10850561,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.19141369967907534}
done in step count: 0
reward sum = 0.9988718120678108
running average episode reward sum: 0.7446248314279219
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.84231126, 10.10850561,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.19141369967907534}
episode index:2570
target Thresh 14.269275019893255
target distance 11.0
model initialize at round 2570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.614260567258981}
done in step count: 10
reward sum = 0.8586756877657475
running average episode reward sum: 0.7446691919321373
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.69858327, 7.72255093, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.7828996696040277}
episode index:2571
target Thresh 14.271639791141231
target distance 5.0
model initialize at round 2571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.0000000000000187}
done in step count: 2
reward sum = 0.9685970552996757
running average episode reward sum: 0.7447562556426223
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.34363849, 11.55766062,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6550364722983234}
episode index:2572
target Thresh 14.27400338029913
target distance 9.0
model initialize at round 2572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 4.37211752, 11.        ,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 7.88572071139791}
done in step count: 14
reward sum = 0.8310105657683847
running average episode reward sum: 0.7447897784992588
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.41588243,  9.03458221,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.41731777747665905}
episode index:2573
target Thresh 14.276365787957847
target distance 12.0
model initialize at round 2573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.77032961426902}
done in step count: 39
reward sum = 0.6106667556769406
running average episode reward sum: 0.7447376716527855
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.86399534, 2.19538128, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.1806350980795826}
episode index:2574
target Thresh 14.27872701470799
target distance 8.0
model initialize at round 2574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([12.8759172,  9.       ,  0.       ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 6.764605928251671}
done in step count: 3
reward sum = 0.9507265458535483
running average episode reward sum: 0.7448176673320868
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.52119755,  3.00010806,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.4788024661843343}
episode index:2575
target Thresh 14.28108706113986
target distance 12.0
model initialize at round 2575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.049875621120906}
done in step count: 6
reward sum = 0.9134144159298077
running average episode reward sum: 0.7448831163804555
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.76203382,  9.09411019,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.2558996523904072}
episode index:2576
target Thresh 14.283445927843472
target distance 12.0
model initialize at round 2576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([2.87816644, 3.52267981, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 11.98922757818643}
done in step count: 31
reward sum = 0.6786668345507528
running average episode reward sum: 0.7448574212769127
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.88724437,  8.54998761,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.043881673050617}
episode index:2577
target Thresh 14.28580361540854
target distance 10.0
model initialize at round 2577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.52402794, 10.        ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.576382838560948}
done in step count: 33
reward sum = 0.6341721082578119
running average episode reward sum: 0.7448144867101869
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.04766677, 10.17152443,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8298457074574523}
episode index:2578
target Thresh 14.288160124424488
target distance 2.0
model initialize at round 2578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([ 7.        , 10.61402452,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.6140245199203633}
done in step count: 13
reward sum = 0.8533484577715151
running average episode reward sum: 0.7448565704523589
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.28286878, 9.32510346, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.4309373613566692}
episode index:2579
target Thresh 14.290515455480442
target distance 4.0
model initialize at round 2579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.12545419, 8.15921001, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.3295961546905675}
done in step count: 1
reward sum = 0.9801225773640838
running average episode reward sum: 0.7449477588271308
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.12994345, 6.18797744, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8901314074100132}
episode index:2580
target Thresh 14.292869609165235
target distance 8.0
model initialize at round 2580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.        ,  8.52876019,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 6.604900429296747}
done in step count: 46
reward sum = 0.5735397679886725
running average episode reward sum: 0.7448813473622573
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.94457968,  2.21305779,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.22014775476041218}
episode index:2581
target Thresh 14.295222586067405
target distance 5.0
model initialize at round 2581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([12.46993303,  9.        ,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 3.4699330329895854}
done in step count: 4
reward sum = 0.9481578895425843
running average episode reward sum: 0.7449600756899801
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.01624739, 9.16558072, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.9975901809234858}
episode index:2582
target Thresh 14.297574386775201
target distance 6.0
model initialize at round 2582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 9.71790016, 10.70169461,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.328503924574934}
done in step count: 5
reward sum = 0.9416031980832481
running average episode reward sum: 0.7450362054315183
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.35050058,  9.80806392,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6772657890302602}
episode index:2583
target Thresh 14.299925011876566
target distance 8.0
model initialize at round 2583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 8.66557384, 11.        ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 7.491659017541597}
done in step count: 39
reward sum = 0.5939486488133916
running average episode reward sum: 0.7449777350148704
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.73939778,  7.01868819,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.2612714371134638}
episode index:2584
target Thresh 14.30227446195916
target distance 1.0
model initialize at round 2584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.23551691, 10.03703196,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7653794925783473}
done in step count: 0
reward sum = 0.9976639208682255
running average episode reward sum: 0.7450754859571735
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.23551691, 10.03703196,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7653794925783473}
episode index:2585
target Thresh 14.304622737610343
target distance 10.0
model initialize at round 2585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([11.01818694, 11.86367365,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 8.514220372864733}
done in step count: 4
reward sum = 0.9404318697832624
running average episode reward sum: 0.7451510298024273
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.06752705, 9.83482762, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.837554212131476}
episode index:2586
target Thresh 14.30696983941719
target distance 2.0
model initialize at round 2586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.04500663, 2.80070871, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.2001354892320946}
done in step count: 5
reward sum = 0.9409358388703347
running average episode reward sum: 0.7452267100533233
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.15882044, 3.99529834, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8411926995852088}
episode index:2587
target Thresh 14.309315767966472
target distance 1.0
model initialize at round 2587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.4496659 , 7.55567586, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7073129109928584}
done in step count: 0
reward sum = 0.9982082950229084
running average episode reward sum: 0.7453244618249499
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.4496659 , 7.55567586, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7073129109928584}
episode index:2588
target Thresh 14.31166052384467
target distance 5.0
model initialize at round 2588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([12.2909193 ,  9.20244187,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 4.3137025634705575}
done in step count: 4
reward sum = 0.9467313330156488
running average episode reward sum: 0.7454022551317057
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.24398514,  7.63364611,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.67899642134201}
episode index:2589
target Thresh 14.314004107637974
target distance 4.0
model initialize at round 2589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([5.99999998, 8.27802852, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 2.1263214479486074}
done in step count: 1
reward sum = 0.9817828536291214
running average episode reward sum: 0.7454935217720521
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.90585435, 8.13962495, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.8655106174474199}
episode index:2590
target Thresh 14.31634651993228
target distance 12.0
model initialize at round 2590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([15.23188543,  2.41676772,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 13.0623519863851}
done in step count: 42
reward sum = 0.5806492340323173
running average episode reward sum: 0.7454298998933412
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.36770621, 7.63808265, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8983011183895152}
episode index:2591
target Thresh 14.318687761313194
target distance 13.0
model initialize at round 2591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.57896054, 8.65725827, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.2196531761237}
done in step count: 8
reward sum = 0.8973466447780133
running average episode reward sum: 0.7454885097486208
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.32933478,  2.97363964,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1822715383888323}
episode index:2592
target Thresh 14.321027832366024
target distance 3.0
model initialize at round 2592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.        ,  9.98066366,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.4279518828645468}
done in step count: 1
reward sum = 0.9852769608038546
running average episode reward sum: 0.7455809850479094
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.49689317, 10.26266551,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8891372052644636}
episode index:2593
target Thresh 14.323366733675783
target distance 7.0
model initialize at round 2593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([12.        , 10.65357834,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 5.26633851265266}
done in step count: 5
reward sum = 0.9433035252994317
running average episode reward sum: 0.7456572080780758
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.99020869, 9.76491034, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.2512398099883781}
episode index:2594
target Thresh 14.325704465827204
target distance 3.0
model initialize at round 2594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.        , 9.40671301, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.0795441035885258}
done in step count: 19
reward sum = 0.788865881018106
running average episode reward sum: 0.7456738588190931
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.39801292, 8.30656745, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.7995392318656088}
episode index:2595
target Thresh 14.32804102940472
target distance 2.0
model initialize at round 2595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.        ,  4.64731851,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6473185122012959}
done in step count: 0
reward sum = 0.9969067117164643
running average episode reward sum: 0.7457706357269889
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.        ,  4.64731851,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6473185122012959}
episode index:2596
target Thresh 14.330376424992465
target distance 11.0
model initialize at round 2596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.7694416,  9.       ,  0.       ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.820488239694052}
done in step count: 7
reward sum = 0.9095121098796478
running average episode reward sum: 0.7458336859673249
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.90828516, 9.55474329, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0115510199070352}
episode index:2597
target Thresh 14.332710653174296
target distance 1.0
model initialize at round 2597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.474612  ,  7.08047719,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.5315158809049259}
done in step count: 0
reward sum = 0.9982500361580298
running average episode reward sum: 0.7459308439158202
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.474612  ,  7.08047719,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.5315158809049259}
episode index:2598
target Thresh 14.335043714533763
target distance 6.0
model initialize at round 2598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([4., 7., 0.]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 4.4721359549996045}
done in step count: 2
reward sum = 0.9663634847936542
running average episode reward sum: 0.7460156583216985
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.        , 8.67621374, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.32378625869761635}
episode index:2599
target Thresh 14.337375609654135
target distance 8.0
model initialize at round 2599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 4.5842786 , 11.90629782,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 7.65682017553279}
done in step count: 17
reward sum = 0.8085731452045447
running average episode reward sum: 0.7460397188935766
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.98684605,  9.70900916,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.0288541157063673}
episode index:2600
target Thresh 14.339706339118385
target distance 2.0
model initialize at round 2600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.84582174, 9.48620921, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9896440698234362}
done in step count: 0
reward sum = 0.9988029187234336
running average episode reward sum: 0.7461368981322655
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.84582174, 9.48620921, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9896440698234362}
episode index:2601
target Thresh 14.342035903509196
target distance 9.0
model initialize at round 2601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4.91265594, 3.53205368, 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 9.634624014912335}
done in step count: 6
reward sum = 0.9083454049182914
running average episode reward sum: 0.7461992380656959
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.54596901, 11.63306518,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8359746894022645}
episode index:2602
target Thresh 14.344364303408957
target distance 8.0
model initialize at round 2602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([11.82153204, 11.86067325,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 8.328254012151977}
done in step count: 19
reward sum = 0.7942335053016893
running average episode reward sum: 0.7462176914914492
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.78073234, 8.02124253, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.2520020642937844}
episode index:2603
target Thresh 14.34669153939977
target distance 2.0
model initialize at round 2603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.        , 9.06881197, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.0688119679689354}
done in step count: 0
reward sum = 0.9969964484199983
running average episode reward sum: 0.746313996697643
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.        , 9.06881197, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.0688119679689354}
episode index:2604
target Thresh 14.349017612063445
target distance 5.0
model initialize at round 2604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.83810616,  6.        ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 3.217141169754588}
done in step count: 2
reward sum = 0.969375292746084
running average episode reward sum: 0.7463996248343219
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.6428062,  2.       ,  0.       ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.061879187669543}
episode index:2605
target Thresh 14.351342521981497
target distance 12.0
model initialize at round 2605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.      ,  7.585374,  0.      ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.099562699761652}
done in step count: 56
reward sum = 0.4690126380217356
running average episode reward sum: 0.7462931831663201
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.87685928, 9.6207588 , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0743480252584756}
episode index:2606
target Thresh 14.353666269735157
target distance 13.0
model initialize at round 2606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 13.038404810405304}
done in step count: 12
reward sum = 0.8492240205301176
running average episode reward sum: 0.7463326656509246
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.67720145,  3.90745041,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.6834963300815045}
episode index:2607
target Thresh 14.35598885590536
target distance 4.0
model initialize at round 2607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([11.        , 10.61537135,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 2.0925300222603447}
done in step count: 1
reward sum = 0.9819809462335367
running average episode reward sum: 0.7464230215867308
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.        , 10.26339689,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.2633968889713323}
episode index:2608
target Thresh 14.358310281072752
target distance 6.0
model initialize at round 2608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.20801842,  5.24195778,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.812253004162408}
done in step count: 3
reward sum = 0.9622609070480361
running average episode reward sum: 0.7465057497912004
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.1541515, 10.5584563,  0.       ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.954159590269293}
episode index:2609
target Thresh 14.360630545817692
target distance 3.0
model initialize at round 2609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([12.45283532,  9.48417091,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.5927725263591643}
done in step count: 6
reward sum = 0.9324838984446286
running average episode reward sum: 0.746577005786853
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.03605413,  9.37793958,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.035388796255934}
episode index:2610
target Thresh 14.36294965072024
target distance 1.0
model initialize at round 2610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.05329056, 10.30843806,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.3130078631234597}
done in step count: 0
reward sum = 0.9987138623952048
running average episode reward sum: 0.7466735729475609
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.05329056, 10.30843806,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.3130078631234597}
episode index:2611
target Thresh 14.365267596360184
target distance 12.0
model initialize at round 2611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 4.57761804, 11.90823104,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.5202768031897}
done in step count: 10
reward sum = 0.8746474524072274
running average episode reward sum: 0.7467225675415347
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.97517391,  7.41148973,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.4122379535662202}
episode index:2612
target Thresh 14.367584383316997
target distance 3.0
model initialize at round 2612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.64888418,  3.        ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.1920783008807614}
done in step count: 1
reward sum = 0.9830178988304085
running average episode reward sum: 0.7468129982079291
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.23421261,  1.17152204,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8609478909459515}
episode index:2613
target Thresh 14.369900012169882
target distance 4.0
model initialize at round 2613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([5.53841736, 7.59377575, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 2.8349348781601327}
done in step count: 1
reward sum = 0.9797165345862163
running average episode reward sum: 0.7469020967298795
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.52558696, 9.60906596, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.7720291919568404}
episode index:2614
target Thresh 14.372214483497746
target distance 3.0
model initialize at round 2614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.0297507 , 9.49136174, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.6957874211080504}
done in step count: 5
reward sum = 0.9449638410497302
running average episode reward sum: 0.7469778373586825
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.10996398, 7.45551618, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9998295342356825}
episode index:2615
target Thresh 14.374527797879209
target distance 11.0
model initialize at round 2615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 5.98148831, 11.86759849,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.759287385560093}
done in step count: 8
reward sum = 0.8966060230162388
running average episode reward sum: 0.7470350346773589
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.69793328,  6.26358195,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.40089867307245336}
episode index:2616
target Thresh 14.376839955892594
target distance 6.0
model initialize at round 2616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([12.8756851,  9.       ,  0.       ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.52909635714026}
done in step count: 2
reward sum = 0.9656201105596238
running average episode reward sum: 0.7471185597350135
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.51506615,  5.        ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5150661468507121}
episode index:2617
target Thresh 14.379150958115947
target distance 8.0
model initialize at round 2617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([7.53115618, 8.96991956, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 6.7799090643833155}
done in step count: 20
reward sum = 0.7840789524969445
running average episode reward sum: 0.747132677532096
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.53897175, 10.4290714 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.785143303270467}
episode index:2618
target Thresh 14.381460805127015
target distance 1.0
model initialize at round 2618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.32112873,  8.46878803,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.6207332906438275}
done in step count: 0
reward sum = 0.9969325064280845
running average episode reward sum: 0.7472280573827627
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.32112873,  8.46878803,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.6207332906438275}
episode index:2619
target Thresh 14.38376949750326
target distance 7.0
model initialize at round 2619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 8.        , 10.14243352,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 5.905496461228915}
done in step count: 10
reward sum = 0.8846764999998663
running average episode reward sum: 0.7472805186204028
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.22237178, 6.71267925, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8290107711549726}
episode index:2620
target Thresh 14.386077035821856
target distance 2.0
model initialize at round 2620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.        , 10.60566816,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.6056681573390286}
done in step count: 0
reward sum = 0.9968833771613327
running average episode reward sum: 0.747375750538961
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.        , 10.60566816,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.6056681573390286}
episode index:2621
target Thresh 14.388383420659686
target distance 6.0
model initialize at round 2621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([13.47587955,  8.        ,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 4.9023971428201545}
done in step count: 10
reward sum = 0.8872691141568867
running average episode reward sum: 0.7474291042245513
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.61644684, 9.36920328, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.7382530266801289}
episode index:2622
target Thresh 14.390688652593346
target distance 11.0
model initialize at round 2622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([6.        , 9.54783815, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.572535559398192}
done in step count: 8
reward sum = 0.8894847416995173
running average episode reward sum: 0.7474832619208819
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.77321503,  3.26669045,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7675769142703991}
episode index:2623
target Thresh 14.392992732199145
target distance 8.0
model initialize at round 2623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([13.       ,  8.8843255,  0.       ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 6.10284602299198}
done in step count: 23
reward sum = 0.7425790855780812
running average episode reward sum: 0.747481392951239
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.9317545 , 9.27023245, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.7329516561110551}
episode index:2624
target Thresh 14.395295660053106
target distance 7.0
model initialize at round 2624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.1325968 ,  5.99505776,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.4403507196004455}
done in step count: 2
reward sum = 0.9652213103120378
running average episode reward sum: 0.747564341491186
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.7613343 , 10.01547007,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0130451633043667}
episode index:2625
target Thresh 14.397597436730955
target distance 13.0
model initialize at round 2625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.529964086141675}
done in step count: 8
reward sum = 0.8907235936695925
running average episode reward sum: 0.7476188575811245
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.35897589, 3.55419655, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.7808025506293547}
episode index:2626
target Thresh 14.399898062808141
target distance 2.0
model initialize at round 2626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.66641808, 8.        , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.33358192443847323}
done in step count: 0
reward sum = 0.996916542237382
running average episode reward sum: 0.7477137558242368
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.66641808, 8.        , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.33358192443847323}
episode index:2627
target Thresh 14.402197538859818
target distance 11.0
model initialize at round 2627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.        ,  8.44224751,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.635822119559966}
done in step count: 45
reward sum = 0.5594866164174538
running average episode reward sum: 0.7476421321030013
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.37064131, 5.45980525, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.7794313498433547}
episode index:2628
target Thresh 14.404495865460857
target distance 2.0
model initialize at round 2628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.6407442894293542e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7477358399265474
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.6407442894293542e-14}
episode index:2629
target Thresh 14.406793043185836
target distance 14.0
model initialize at round 2629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14., 11.,  0.]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.041594578792314}
done in step count: 35
reward sum = 0.6206582361480486
running average episode reward sum: 0.7476875214460232
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.67236757, 9.28036593, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9848610802766375}
episode index:2630
target Thresh 14.409089072609053
target distance 10.0
model initialize at round 2630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([5.0908024 , 7.04159403, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.748988646893324}
done in step count: 27
reward sum = 0.7123361774010681
running average episode reward sum: 0.7476740849792635
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.32280428, 11.87791411,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.108750296931197}
episode index:2631
target Thresh 14.411383954304515
target distance 13.0
model initialize at round 2631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([1.61023188, 9.12638402, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 14.011171809801521}
done in step count: 14
reward sum = 0.8251287966612938
running average episode reward sum: 0.7477035130612095
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.19009192,  5.91028702,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.218430774855943}
episode index:2632
target Thresh 14.413677688845942
target distance 5.0
model initialize at round 2632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([5.98013973, 9.        , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 4.984126030621376}
done in step count: 3
reward sum = 0.9580445201300617
running average episode reward sum: 0.7477833995052159
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.4475795 , 6.68452015, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8796227835830904}
episode index:2633
target Thresh 14.415970276806767
target distance 11.0
model initialize at round 2633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([3.57948053, 6.        , 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 11.161864810550352}
done in step count: 33
reward sum = 0.6447210133749827
running average episode reward sum: 0.7477442717959789
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.49468719, 10.63048936,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8079962113331889}
episode index:2634
target Thresh 14.418261718760137
target distance 7.0
model initialize at round 2634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([11.        , 10.15044481,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 5.130645500400826}
done in step count: 4
reward sum = 0.9475559529193339
running average episode reward sum: 0.7478201016559878
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.25510138, 8.03023879, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.002752969733999}
episode index:2635
target Thresh 14.420552015278911
target distance 13.0
model initialize at round 2635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 4.18551397, 10.6900112 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.096749709890965}
done in step count: 12
reward sum = 0.8615132789655863
running average episode reward sum: 0.7478632326033737
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.28826777,  2.37030347,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.6925431570569726}
episode index:2636
target Thresh 14.422841166935669
target distance 8.0
model initialize at round 2636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([10.6770863 ,  8.20382051,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 8.465413714026546}
done in step count: 15
reward sum = 0.8256414397124328
running average episode reward sum: 0.7478927275624595
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.52458757, 3.03434012, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.4766510439227832}
episode index:2637
target Thresh 14.425129174302691
target distance 9.0
model initialize at round 2637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 8.98867377, 11.78860955,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 9.092122747620714}
done in step count: 15
reward sum = 0.814427143121062
running average episode reward sum: 0.7479179490998206
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.86821768,  6.91559592,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.26179151624874}
episode index:2638
target Thresh 14.427416037951986
target distance 8.0
model initialize at round 2638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.89785574, 3.64045452, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 8.813624863700127}
done in step count: 10
reward sum = 0.8741977734899874
running average episode reward sum: 0.7479658004921625
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.1971153 ,  9.14003323,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.1765061371125458}
episode index:2639
target Thresh 14.429701758455263
target distance 11.0
model initialize at round 2639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.91221052, 4.54053685, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.723980816113446}
done in step count: 9
reward sum = 0.8758195377983228
running average episode reward sum: 0.748014229938112
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.98358497,  8.10768433,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.10892827426221743}
episode index:2640
target Thresh 14.43198633638396
target distance 3.0
model initialize at round 2640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([3.12143195, 9.70519298, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.1563712151850214}
done in step count: 7
reward sum = 0.9239388059757319
running average episode reward sum: 0.7480808428029501
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.05201817, 10.6683211 ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0043308458494715}
episode index:2641
target Thresh 14.434269772309213
target distance 12.0
model initialize at round 2641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.32905200177065}
done in step count: 36
reward sum = 0.6275034141080514
running average episode reward sum: 0.748035204109273
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.62780429, 7.88677849, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.96172019777703}
episode index:2642
target Thresh 14.436552066801887
target distance 4.0
model initialize at round 2642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.94326258,  6.90797603,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 2.908529479616323}
done in step count: 4
reward sum = 0.9549759901749972
running average episode reward sum: 0.7481135017960175
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.13355661,  4.20973655,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8914670903961688}
episode index:2643
target Thresh 14.438833220432555
target distance 2.0
model initialize at round 2643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([ 9.        , 10.31120336,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.31120336055759}
done in step count: 3
reward sum = 0.9586199350479052
running average episode reward sum: 0.7481931184500462
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.49194684, 8.08960873, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.0425594885968952}
episode index:2644
target Thresh 14.441113233771505
target distance 12.0
model initialize at round 2644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.39984083, 8.        , 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.787185656699116}
done in step count: 8
reward sum = 0.8930828179682181
running average episode reward sum: 0.7482478971644199
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.73893435,  5.68910905,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.40596607801642326}
episode index:2645
target Thresh 14.443392107388737
target distance 2.0
model initialize at round 2645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.08115387, 10.51825267,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.037477084336382}
done in step count: 0
reward sum = 0.9989218903654703
running average episode reward sum: 0.7483426341233016
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.08115387, 10.51825267,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.037477084336382}
episode index:2646
target Thresh 14.445669841853974
target distance 13.0
model initialize at round 2646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.        , 11.16039288,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.760479110972256}
done in step count: 33
reward sum = 0.6307445426605752
running average episode reward sum: 0.7482982071903728
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.38827225, 6.8625681 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.4118772492448312}
episode index:2647
target Thresh 14.447946437736647
target distance 10.0
model initialize at round 2647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([12.        ,  8.46662533,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 8.145626916935962}
done in step count: 33
reward sum = 0.6421211804784234
running average episode reward sum: 0.7482581101259045
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.16573865, 10.79601192,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8130831923337996}
episode index:2648
target Thresh 14.450221895605907
target distance 4.0
model initialize at round 2648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.67313457, 7.        , 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 2.026534236662874}
done in step count: 1
reward sum = 0.9836847538865507
running average episode reward sum: 0.7483469839061085
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.58774918, 9.        , 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.41225081682204223}
episode index:2649
target Thresh 14.452496216030616
target distance 11.0
model initialize at round 2649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.2377975, 11.       ,  0.       ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.504137405186816}
done in step count: 5
reward sum = 0.925524769191135
running average episode reward sum: 0.7484138434477255
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.56631669, 6.50336618, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.6644235973535173}
episode index:2650
target Thresh 14.454769399579355
target distance 6.0
model initialize at round 2650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.34194255, 5.80017114, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 4.213726045478495}
done in step count: 4
reward sum = 0.9498034893193169
running average episode reward sum: 0.7484898108735542
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.75496116, 9.1189189 , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9145206015516958}
episode index:2651
target Thresh 14.45704144682042
target distance 11.0
model initialize at round 2651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([12.       , 10.7289567,  0.       ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 9.404637402209826}
done in step count: 5
reward sum = 0.9312542026693137
running average episode reward sum: 0.7485587265567352
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.        , 8.59572582, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.1639971013629227}
episode index:2652
target Thresh 14.459312358321824
target distance 3.0
model initialize at round 2652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14., 11.,  0.]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0000000000000195}
done in step count: 1
reward sum = 0.9834078321063165
running average episode reward sum: 0.7486472486470291
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.28779149, 11.53878736,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.6108320273316814}
episode index:2653
target Thresh 14.461582134651294
target distance 4.0
model initialize at round 2653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 2.46983528, 10.09362006,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 3.1290941626697646}
done in step count: 5
reward sum = 0.9420088939427094
running average episode reward sum: 0.7487201053332747
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.15172145, 7.92670136, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.2563247636542632}
episode index:2654
target Thresh 14.463850776376274
target distance 2.0
model initialize at round 2654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.        , 10.69869328,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0444068853938255}
done in step count: 0
reward sum = 0.9948358307051421
running average episode reward sum: 0.7488128042882171
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.        , 10.69869328,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0444068853938255}
episode index:2655
target Thresh 14.466118284063924
target distance 8.0
model initialize at round 2655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 9.        , 11.11110771,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.102012810262486}
done in step count: 6
reward sum = 0.9228028701758174
running average episode reward sum: 0.7488783125961568
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.98503423, 9.29420428, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.2117920786589076}
episode index:2656
target Thresh 14.468384658281124
target distance 11.0
model initialize at round 2656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.09519398,  9.21261817,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.014498541723539}
done in step count: 52
reward sum = 0.4976438391764084
running average episode reward sum: 0.7487837569042411
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.87167774, 3.2531061 , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9076809896320928}
episode index:2657
target Thresh 14.470649899594461
target distance 4.0
model initialize at round 2657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.        , 10.80922556,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.0090781187523303}
done in step count: 2
reward sum = 0.9733388086754674
running average episode reward sum: 0.7488682396174733
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.2624222, 11.3622928,  0.       ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.4473493975245263}
episode index:2658
target Thresh 14.47291400857025
target distance 12.0
model initialize at round 2658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([15.08794522,  4.        ,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 11.266877529512028}
done in step count: 25
reward sum = 0.7126549316631597
running average episode reward sum: 0.7488546204719471
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.85698395, 2.94714668, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.2773051046711168}
episode index:2659
target Thresh 14.475176985774517
target distance 8.0
model initialize at round 2659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([14.1623714,  8.       ,  0.       ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 6.478797823268679}
done in step count: 21
reward sum = 0.7776431720236242
running average episode reward sum: 0.7488654432356883
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.56643936, 9.12486675, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.9766437591331432}
episode index:2660
target Thresh 14.477438831773009
target distance 7.0
model initialize at round 2660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.70770138, 6.37202861, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 7.263277379736687}
done in step count: 13
reward sum = 0.8569932756751141
running average episode reward sum: 0.7489060775207087
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.24894918,  9.71037411,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.3819147101182089}
episode index:2661
target Thresh 14.479699547131187
target distance 9.0
model initialize at round 2661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 6.65272748, 10.28450024,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 8.505254595340629}
done in step count: 22
reward sum = 0.7274953023043278
running average episode reward sum: 0.7488980344045493
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.31702489,  6.25604921,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.4075119384600522}
episode index:2662
target Thresh 14.481959132414225
target distance 3.0
model initialize at round 2662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([11.00494245, 11.86740305,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 2.1206375157927146}
done in step count: 1
reward sum = 0.9825937460174303
running average episode reward sum: 0.7489857909616702
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.52370644, 10.55077535,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.7600144246760192}
episode index:2663
target Thresh 14.484217588187024
target distance 3.0
model initialize at round 2663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([6.11206365, 9.262676  , 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.1426654080471255}
done in step count: 1
reward sum = 0.9849270263880533
running average episode reward sum: 0.7490743574914849
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.94776714, 8.86744738, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.9569915082420445}
episode index:2664
target Thresh 14.486474915014197
target distance 8.0
model initialize at round 2664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.93233585, 8.        , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 6.072005446626735}
done in step count: 5
reward sum = 0.9353492675850311
running average episode reward sum: 0.7491442542682555
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.94017146, 1.15109428, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8510113800575455}
episode index:2665
target Thresh 14.488731113460073
target distance 11.0
model initialize at round 2665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 7.        , 10.31810629,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 9.025795201904462}
done in step count: 5
reward sum = 0.9268552155922886
running average episode reward sum: 0.7492109125433207
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.41003271, 10.47663378,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.7886530368751861}
episode index:2666
target Thresh 14.490986184088703
target distance 6.0
model initialize at round 2666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([4.00348532, 6.        , 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 7.208202846999166}
done in step count: 33
reward sum = 0.6559416228014652
running average episode reward sum: 0.749175940931119
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.60520563, 10.36511405,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.7068112366406106}
episode index:2667
target Thresh 14.493240127463858
target distance 7.0
model initialize at round 2667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([12.05521678,  8.045681  ,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 5.144505943758209}
done in step count: 5
reward sum = 0.9270521314251161
running average episode reward sum: 0.7492426111674362
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.60616796, 9.0974063 , 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.6139442806801765}
episode index:2668
target Thresh 14.495492944149023
target distance 9.0
model initialize at round 2668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([15.87445474,  7.        ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.734266632566868}
done in step count: 5
reward sum = 0.9324076151835982
running average episode reward sum: 0.7493112379954677
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.65107906, 10.52971995,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8031607978450201}
episode index:2669
target Thresh 14.497744634707399
target distance 1.0
model initialize at round 2669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.        ,  9.77184176,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.2632259138211321}
done in step count: 0
reward sum = 0.9965531953730348
running average episode reward sum: 0.7494038379795043
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.        ,  9.77184176,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.2632259138211321}
episode index:2670
target Thresh 14.499995199701909
target distance 12.0
model initialize at round 2670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([3.2444694, 6.       , 0.       ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.132291604981551}
done in step count: 22
reward sum = 0.7456297117684791
running average episode reward sum: 0.7494024249783021
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.68046691,  2.43884231,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.6457548647408472}
episode index:2671
target Thresh 14.502244639695196
target distance 1.0
model initialize at round 2671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.81388974,  7.57639712,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6056984969549032}
done in step count: 0
reward sum = 0.9986956815627103
running average episode reward sum: 0.7494957233527724
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.81388974,  7.57639712,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6056984969549032}
episode index:2672
target Thresh 14.50449295524962
target distance 6.0
model initialize at round 2672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([15.02966428,  5.        ,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 7.2358034319456745}
done in step count: 41
reward sum = 0.5874923955959609
running average episode reward sum: 0.7494351160472142
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.01546688, 9.72557816, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.7257429877507067}
episode index:2673
target Thresh 14.506740146927259
target distance 7.0
model initialize at round 2673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([12.        ,  8.73214227,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 5.00716963600487}
done in step count: 6
reward sum = 0.9308834955214538
running average episode reward sum: 0.7495029725840407
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.71943477, 9.95044416, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.1920278898693943}
episode index:2674
target Thresh 14.508986215289912
target distance 8.0
model initialize at round 2674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([8.02619884, 8.13224972, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 6.79161694877612}
done in step count: 11
reward sum = 0.860969509907323
running average episode reward sum: 0.7495446423176196
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.78612751, 4.35319918, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0180116700282915}
episode index:2675
target Thresh 14.511231160899095
target distance 10.0
model initialize at round 2675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 3.99505755, 11.86740305,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 8.503005580988798}
done in step count: 5
reward sum = 0.9241202595228504
running average episode reward sum: 0.7496098798427338
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.15323611,  8.21385267,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.1554378859733434}
episode index:2676
target Thresh 14.513474984316044
target distance 14.0
model initialize at round 2676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.64873934,  9.38333488,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 13.746668731595157}
done in step count: 6
reward sum = 0.9096131252322694
running average episode reward sum: 0.7496696494525169
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.69306055, 4.76597071, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.0329782464168242}
episode index:2677
target Thresh 14.515717686101718
target distance 6.0
model initialize at round 2677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([9.00001496, 8.18507235, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 4.0821840642010505}
done in step count: 7
reward sum = 0.9123002826084655
running average episode reward sum: 0.7497303778442853
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.33331663, 9.72482973, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.9848070131526301}
episode index:2678
target Thresh 14.517959266816788
target distance 3.0
model initialize at round 2678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 3.26486637, 11.87470869,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.9431428147393963}
done in step count: 1
reward sum = 0.9842502122842208
running average episode reward sum: 0.7498179179093991
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.2681095 , 11.64562818,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.6990840077716886}
episode index:2679
target Thresh 14.520199727021653
target distance 10.0
model initialize at round 2679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.06359041, 7.06211329, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 12.051142920427296}
done in step count: 42
reward sum = 0.5314792334693013
running average episode reward sum: 0.749736448251026
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.14173466,  2.84886307,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8606144496220584}
episode index:2680
target Thresh 14.522439067276427
target distance 9.0
model initialize at round 2680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 9.        , 11.16535652,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 7.327262029231191}
done in step count: 5
reward sum = 0.9323725986032677
running average episode reward sum: 0.7498045706495162
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.06323123,  8.61696768,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0120521174816426}
episode index:2681
target Thresh 14.524677288140943
target distance 3.0
model initialize at round 2681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([2.21856785, 9.        , 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 2.9557342260487056}
done in step count: 29
reward sum = 0.7043051402522316
running average episode reward sum: 0.7497876059103673
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.76997686, 9.3318888 , 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.7065997637348521}
episode index:2682
target Thresh 14.526914390174756
target distance 5.0
model initialize at round 2682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.74661154,  8.31943703,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.548188949152516}
done in step count: 5
reward sum = 0.9408702570271812
running average episode reward sum: 0.7498588256834261
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.49412376,  5.11467576,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5187111872735497}
episode index:2683
target Thresh 14.529150373937144
target distance 3.0
model initialize at round 2683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.        ,  2.78452682,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.215473175048819}
done in step count: 33
reward sum = 0.6730544390769746
running average episode reward sum: 0.7498302100401302
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.72218633,  4.85641665,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.31272450170117255}
episode index:2684
target Thresh 14.531385239987104
target distance 14.0
model initialize at round 2684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([15.25599372,  6.06695414,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.288790170357238}
done in step count: 24
reward sum = 0.7142415993591782
running average episode reward sum: 0.7498169554365246
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.71015759, 7.36382067, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.4651602968231394}
episode index:2685
target Thresh 14.533618988883353
target distance 5.0
model initialize at round 2685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.04523444, 3.66528738, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.335019406698174}
done in step count: 10
reward sum = 0.8936669585404822
running average episode reward sum: 0.7498705109105023
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.02145291, 6.33006298, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.6702804211065863}
episode index:2686
target Thresh 14.535851621184321
target distance 1.0
model initialize at round 2686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.35670614, 7.52540308, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.6088079614033197}
done in step count: 2
reward sum = 0.9775881333433281
running average episode reward sum: 0.7499552588161341
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.4796375, 8.4391349, 0.       ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.7650795989584044}
episode index:2687
target Thresh 14.53808313744817
target distance 11.0
model initialize at round 2687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.1590191 , 11.86599861,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.941507739857975}
done in step count: 4
reward sum = 0.9379800704584474
running average episode reward sum: 0.7500252085228463
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.98446342, 7.0848334 , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.3441347181485315}
episode index:2688
target Thresh 14.540313538232784
target distance 5.0
model initialize at round 2688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([5.        , 7.38272584, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 3.98122142909778}
done in step count: 32
reward sum = 0.6684997835744482
running average episode reward sum: 0.7499948904027465
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.84382006, 10.83154678,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.8460864163378384}
episode index:2689
target Thresh 14.54254282409576
target distance 6.0
model initialize at round 2689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.0000000000000275}
done in step count: 2
reward sum = 0.9678037957341482
running average episode reward sum: 0.7500758602560296
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.44868231, 10.08201528,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0217689402543657}
episode index:2690
target Thresh 14.544770995594417
target distance 7.0
model initialize at round 2690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([9.10015552, 8.13045333, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 8.21418608501439}
done in step count: 9
reward sum = 0.882215380951489
running average episode reward sum: 0.7501249645000636
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.13189149, 3.92469293, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8713687704778658}
episode index:2691
target Thresh 14.5469980532858
target distance 6.0
model initialize at round 2691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([2.63610709, 4.5249964 , 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 5.5983419643956305}
done in step count: 15
reward sum = 0.8296379393939293
running average episode reward sum: 0.7501545012663688
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.38570773, 8.10368074, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.975786174320486}
episode index:2692
target Thresh 14.549223997726674
target distance 8.0
model initialize at round 2692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([5.401042  , 7.42016574, 0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 7.08532226057424}
done in step count: 4
reward sum = 0.9467020488326408
running average episode reward sum: 0.7502274858737087
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.21927027, 10.01467563,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.7808676450289811}
episode index:2693
target Thresh 14.551448829473523
target distance 2.0
model initialize at round 2693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.17861933,  4.        ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.1786193251609447}
done in step count: 0
reward sum = 0.9964940001760982
running average episode reward sum: 0.7503188988337319
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.17861933,  4.        ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.1786193251609447}
episode index:2694
target Thresh 14.553672549082554
target distance 9.0
model initialize at round 2694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([2.0154295 , 1.98859406, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.227348329001314}
done in step count: 15
reward sum = 0.8347716466178778
running average episode reward sum: 0.750350235660368
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.34676585, 10.00813173,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.050737461796635}
episode index:2695
target Thresh 14.555895157109703
target distance 8.0
model initialize at round 2695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.12937103,  2.87497019,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.126059706211201}
done in step count: 6
reward sum = 0.9260137271329995
running average episode reward sum: 0.7504153927417747
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.79720658, 10.99635601,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.20282615609890323}
episode index:2696
target Thresh 14.55811665411062
target distance 1.0
model initialize at round 2696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.78674021, 10.38427424,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.43948427880915036}
done in step count: 0
reward sum = 0.9985287288016836
running average episode reward sum: 0.7505073887877739
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.78674021, 10.38427424,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.43948427880915036}
episode index:2697
target Thresh 14.56033704064067
target distance 12.0
model initialize at round 2697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.        ,  7.58806777,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.017276261645085}
done in step count: 18
reward sum = 0.8077016435286299
running average episode reward sum: 0.7505285875478707
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.92025254, 6.66637626, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9788613510317256}
episode index:2698
target Thresh 14.562556317254963
target distance 6.0
model initialize at round 2698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([14.34079587,  9.7273227 ,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 4.349351875409643}
done in step count: 4
reward sum = 0.9463459137193725
running average episode reward sum: 0.7506011393545293
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.88306403, 10.67248368,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.1099713452673377}
episode index:2699
target Thresh 14.564774484508309
target distance 3.0
model initialize at round 2699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([12.97611349, 11.8671188 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.3417297036668987}
done in step count: 1
reward sum = 0.9835466576811174
running average episode reward sum: 0.750687415472428
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.58055096, 11.88724174,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9813946184882978}
episode index:2700
target Thresh 14.566991542955256
target distance 3.0
model initialize at round 2700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.4142135623730963}
done in step count: 24
reward sum = 0.7304785789030425
running average episode reward sum: 0.7506799334892479
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.33320621, 11.43845873,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5507017672268154}
episode index:2701
target Thresh 14.569207493150063
target distance 8.0
model initialize at round 2701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 6.772923854276528}
done in step count: 3
reward sum = 0.9466566068092447
running average episode reward sum: 0.7507524637162354
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.06099255, 10.16139315,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.17253359677873917}
episode index:2702
target Thresh 14.571422335646721
target distance 12.0
model initialize at round 2702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([14.04786537, 11.86778303,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 12.079077256350796}
done in step count: 8
reward sum = 0.8838712278857711
running average episode reward sum: 0.7508017122416404
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.9665325 , 11.86730319,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.2986146057511312}
episode index:2703
target Thresh 14.573636070998939
target distance 11.0
model initialize at round 2703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 5.99505755, 11.86740305,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.324761065526559}
done in step count: 11
reward sum = 0.8560030932919588
running average episode reward sum: 0.7508406180778276
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.62679199,  4.83252734,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6487798463224327}
episode index:2704
target Thresh 14.575848699760154
target distance 9.0
model initialize at round 2704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 3.85912251, 11.        ,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 8.202065973293655}
done in step count: 17
reward sum = 0.8050395072028728
running average episode reward sum: 0.7508606546357297
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.35800247,  9.6335371 ,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.5123093045706855}
episode index:2705
target Thresh 14.578060222483522
target distance 7.0
model initialize at round 2705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([13.27413213, 11.23591293,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 6.660633653387226}
done in step count: 13
reward sum = 0.8542715608733712
running average episode reward sum: 0.7508988700482343
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.89134999, 9.81981853, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.2110356047068889}
episode index:2706
target Thresh 14.580270639721919
target distance 12.0
model initialize at round 2706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 5.97984201, 11.86192491,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.13740903132707}
done in step count: 5
reward sum = 0.9242902283667374
running average episode reward sum: 0.7509629230066084
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.95241289,  6.36735641,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.634430799502621}
episode index:2707
target Thresh 14.582479952027953
target distance 5.0
model initialize at round 2707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.6482911 ,  5.00000002,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 3.06924770678262}
done in step count: 2
reward sum = 0.9698134315778256
running average episode reward sum: 0.7510437392948548
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.85190597,  1.12791722,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.2191276215901878}
episode index:2708
target Thresh 14.584688159953954
target distance 7.0
model initialize at round 2708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.87177902, 3.15025508, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 6.850944900689428}
done in step count: 26
reward sum = 0.7183465129657474
running average episode reward sum: 0.7510316694438658
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.93629617, 10.74475697,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.7474765007570798}
episode index:2709
target Thresh 14.58689526405197
target distance 11.0
model initialize at round 2709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([5.84128201, 9.        , 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 9.158717989921575}
done in step count: 9
reward sum = 0.8845414706778403
running average episode reward sum: 0.7510809350531772
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.97468774,  9.56251175,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.1253602314960627}
episode index:2710
target Thresh 14.58910126487378
target distance 8.0
model initialize at round 2710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([15.85920095,  9.        ,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 7.859200954437243}
done in step count: 6
reward sum = 0.9235473408601235
running average episode reward sum: 0.7511445523183218
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.96784254, 8.1503698 , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.2878628229280897}
episode index:2711
target Thresh 14.591306162970884
target distance 11.0
model initialize at round 2711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.28495431,  9.0610261 ,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.339538748168424}
done in step count: 36
reward sum = 0.639613925099117
running average episode reward sum: 0.7511034274557779
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.2563754 , 7.05103843, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9829834161518103}
episode index:2712
target Thresh 14.593509958894506
target distance 2.0
model initialize at round 2712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.31189704,  8.33165205,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9592573549296899}
done in step count: 0
reward sum = 0.9985970673816206
running average episode reward sum: 0.7511946525349985
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.31189704,  8.33165205,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9592573549296899}
episode index:2713
target Thresh 14.595712653195594
target distance 9.0
model initialize at round 2713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.02619884, 11.86775028,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.079580542486184}
done in step count: 61
reward sum = 0.38703427907646604
running average episode reward sum: 0.7510604740628325
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.74417165, 11.86021172,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.1374338025887687}
episode index:2714
target Thresh 14.597914246424821
target distance 11.0
model initialize at round 2714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 5.59963261, 11.90412587,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.108998188987476}
done in step count: 9
reward sum = 0.8768064871205724
running average episode reward sum: 0.7511067893530932
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.13791721,  7.5766295 ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.44526816329237984}
episode index:2715
target Thresh 14.60011473913259
target distance 3.0
model initialize at round 2715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.86740305, 5.00494245, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.3275154913209866}
done in step count: 1
reward sum = 0.9802345508925021
running average episode reward sum: 0.7511911515627909
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.86751625, 4.02285302, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.867817206102574}
episode index:2716
target Thresh 14.602314131869019
target distance 7.0
model initialize at round 2716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([11.1578327 , 11.87293827,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 7.712868682657688}
done in step count: 10
reward sum = 0.8829704839789466
running average episode reward sum: 0.7512396533413761
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.54848601, 9.93029623, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.0340773443158686}
episode index:2717
target Thresh 14.604512425183957
target distance 6.0
model initialize at round 2717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.10655189,  9.90844929,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.989100518835777}
done in step count: 11
reward sum = 0.8835213600752916
running average episode reward sum: 0.7512883221076505
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.43483738,  5.93707946,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0330544267850401}
episode index:2718
target Thresh 14.60670961962698
target distance 9.0
model initialize at round 2718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([13.13646004,  3.98178586,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 10.120320321849407}
done in step count: 10
reward sum = 0.8763514094707597
running average episode reward sum: 0.7513343180941761
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.65609161, 10.2416137 ,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.4202977083060429}
episode index:2719
target Thresh 14.608905715747383
target distance 10.0
model initialize at round 2719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([11.42743015,  9.63626349,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 10.50577833545993}
done in step count: 24
reward sum = 0.7386427282198564
running average episode reward sum: 0.751329652068487
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.82972129, 5.15267347, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8436507637529633}
episode index:2720
target Thresh 14.611100714094196
target distance 12.0
model initialize at round 2720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.0640893 ,  9.81260753,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.096842298248225}
done in step count: 8
reward sum = 0.9008425066794609
running average episode reward sum: 0.7513845998283588
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.62838119, 9.78536554, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.0058140707666499}
episode index:2721
target Thresh 14.61329461521616
target distance 4.0
model initialize at round 2721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 2.236067977499798}
done in step count: 9
reward sum = 0.8942230800071985
running average episode reward sum: 0.7514370753905112
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.95457596, 9.94415329, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.3426245572162203}
episode index:2722
target Thresh 14.615487419661758
target distance 4.0
model initialize at round 2722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([14.22404003,  9.        ,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 2.438514724461825}
done in step count: 1
reward sum = 0.9795069214224004
running average episode reward sum: 0.7515208322197554
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.51940084, 11.        ,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.1268439232819043}
episode index:2723
target Thresh 14.617679127979187
target distance 1.0
model initialize at round 2723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.57446694, 9.3772558 , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.4922619746159962}
done in step count: 3
reward sum = 0.9637841666276112
running average episode reward sum: 0.7515987556171151
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.60373545, 7.60912269, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7192228851935938}
episode index:2724
target Thresh 14.619869740716375
target distance 3.0
model initialize at round 2724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([11.41614079,  9.        ,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.4161407947539715}
done in step count: 1
reward sum = 0.9791785426675721
running average episode reward sum: 0.7516822711352988
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.43977994, 8.08901204, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.0694604151049405}
episode index:2725
target Thresh 14.622059258420977
target distance 7.0
model initialize at round 2725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([7.        , 9.55429506, 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 5.204811501371788}
done in step count: 3
reward sum = 0.9565915089550987
running average episode reward sum: 0.7517574396011169
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.60339809, 10.48673006,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6486440497942466}
episode index:2726
target Thresh 14.62424768164037
target distance 8.0
model initialize at round 2726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.9098398 , 4.53024106, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 6.470387125388877}
done in step count: 6
reward sum = 0.9234499539143725
running average episode reward sum: 0.7518203998190535
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.44247272, 10.42880819,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7225248712614334}
episode index:2727
target Thresh 14.626435010921657
target distance 2.0
model initialize at round 2727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10., 11.,  0.]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 2.6407442894293542e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7519091753323915
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10., 11.,  0.]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 2.6407442894293542e-14}
episode index:2728
target Thresh 14.62862124681168
target distance 4.0
model initialize at round 2728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([12.41347405, 11.85625949,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 4.584908573696843}
done in step count: 19
reward sum = 0.7691501683292744
running average episode reward sum: 0.7519154930286162
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.84075255,  9.12369571,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8498031980408084}
episode index:2729
target Thresh 14.63080638985699
target distance 1.0
model initialize at round 2729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.71834672,  5.51490135,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8838243077606405}
done in step count: 0
reward sum = 0.9976089698305703
running average episode reward sum: 0.7520054906391663
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.71834672,  5.51490135,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8838243077606405}
episode index:2730
target Thresh 14.632990440603871
target distance 12.0
model initialize at round 2730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.00001316,  7.0937693 ,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.805517580619805}
done in step count: 27
reward sum = 0.7067773527066619
running average episode reward sum: 0.7519889296219812
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.28055318, 2.60780837, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8194010060411615}
episode index:2731
target Thresh 14.635173399598342
target distance 2.0
model initialize at round 2731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4., 7., 0.]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.701289205785704e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7520775134691933
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4., 7., 0.]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.701289205785704e-14}
episode index:2732
target Thresh 14.637355267386141
target distance 13.0
model initialize at round 2732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([3.65764225, 4.        , 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.328506271738412}
done in step count: 11
reward sum = 0.8573613028093362
running average episode reward sum: 0.7521160366266539
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.2498706 , 10.77128101,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7842234932279096}
episode index:2733
target Thresh 14.639536044512734
target distance 11.0
model initialize at round 2733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 5.2610352 , 11.41253805,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.74769834079004}
done in step count: 6
reward sum = 0.9137982711783532
running average episode reward sum: 0.7521751742398769
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.32316492, 11.2382235 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.4014797682900618}
episode index:2734
target Thresh 14.641715731523313
target distance 8.0
model initialize at round 2734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 9.80259561, 11.59342479,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 7.7141021764456195}
done in step count: 3
reward sum = 0.9497294795988511
running average episode reward sum: 0.7522474061613975
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.80259561,  6.14586266,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8766522076093934}
episode index:2735
target Thresh 14.643894328962805
target distance 4.0
model initialize at round 2735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.0000000000000178}
done in step count: 5
reward sum = 0.936400382885579
running average episode reward sum: 0.7523147135359312
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.85466194,  6.61935887,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0554868265082729}
episode index:2736
target Thresh 14.646071837375855
target distance 3.0
model initialize at round 2736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.        ,  9.39224684,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.1701982313231585}
done in step count: 1
reward sum = 0.9849393119994964
running average episode reward sum: 0.7523997060819536
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.33065462, 10.18698049,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.37986073840847145}
episode index:2737
target Thresh 14.648248257306843
target distance 9.0
model initialize at round 2737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 6.99505755, 11.86740305,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 8.00162640329068}
done in step count: 16
reward sum = 0.8055084638228139
running average episode reward sum: 0.7524191029985864
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.43789127,  7.56395049,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.711410854213617}
episode index:2738
target Thresh 14.650423589299871
target distance 12.0
model initialize at round 2738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.00000000000002}
done in step count: 12
reward sum = 0.8537630669157615
running average episode reward sum: 0.7524561033505094
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.77843106, 10.19871386,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.29762391883801603}
episode index:2739
target Thresh 14.652597833898776
target distance 11.0
model initialize at round 2739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([12.19931567,  8.56186485,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 9.209743272808167}
done in step count: 36
reward sum = 0.6190606677770052
running average episode reward sum: 0.7524074188849716
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.17126069, 9.02560173, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.17316371281478563}
episode index:2740
target Thresh 14.654770991647116
target distance 5.0
model initialize at round 2740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4.41513658, 6.73979366, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 5.625825984710013}
done in step count: 3
reward sum = 0.9583199144637619
running average episode reward sum: 0.752482542013603
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.5462302 , 9.98602729, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.45398488015111926}
episode index:2741
target Thresh 14.656943063088178
target distance 13.0
model initialize at round 2741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.        ,  7.96172345,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.511680400996259}
done in step count: 16
reward sum = 0.8054438129387061
running average episode reward sum: 0.7525018568461797
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.51672635, 1.0871717 , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0328644313372464}
episode index:2742
target Thresh 14.659114048764987
target distance 2.0
model initialize at round 2742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.3416376 ,  7.65091611,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 2.373796827098518}
done in step count: 4
reward sum = 0.9482173113167578
running average episode reward sum: 0.752573207722764
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.00068786,  9.39149903,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.6085013579841788}
episode index:2743
target Thresh 14.661283949220284
target distance 9.0
model initialize at round 2743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 5.        , 11.51154673,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.018666544119979}
done in step count: 8
reward sum = 0.8976716553214465
running average episode reward sum: 0.7526260861657664
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.79643643, 11.5032326 ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9421008608673437}
episode index:2744
target Thresh 14.663452764996547
target distance 6.0
model initialize at round 2744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([ 8.97380116, 11.86775028,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 4.438329435418137}
done in step count: 17
reward sum = 0.8087243638465849
running average episode reward sum: 0.7526465226967978
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.78498555,  9.96427073,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.21796283297556407}
episode index:2745
target Thresh 14.665620496635977
target distance 4.0
model initialize at round 2745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([11.54962277, 10.32461822,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.462200627668409}
done in step count: 2
reward sum = 0.9742395393962348
running average episode reward sum: 0.7527272193525514
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.1338737 ,  9.96058393,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8670227174664614}
episode index:2746
target Thresh 14.66778714468051
target distance 12.0
model initialize at round 2746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 5.36482322, 10.        ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.050202943652234}
done in step count: 7
reward sum = 0.9042977510086289
running average episode reward sum: 0.7527823961023351
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.18208327,  6.68383431,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.36484936910294286}
episode index:2747
target Thresh 14.669952709671804
target distance 2.0
model initialize at round 2747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([6.04654455, 9.49579483, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 3.0879858414715056}
done in step count: 6
reward sum = 0.9338192462193137
running average episode reward sum: 0.7528482755965552
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.74278253, 10.63221994,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9754116795744836}
episode index:2748
target Thresh 14.672117192151257
target distance 5.0
model initialize at round 2748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 6.49380183, 11.2721774 ,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.786835715584956}
done in step count: 7
reward sum = 0.9190196059592248
running average episode reward sum: 0.7529087235159306
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.65285175, 8.89531251, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9602585078717211}
episode index:2749
target Thresh 14.674280592659983
target distance 2.0
model initialize at round 2749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.82274139, 4.6047492 , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.2418377628623944}
done in step count: 22
reward sum = 0.7675717805525616
running average episode reward sum: 0.7529140555366712
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.79781167, 4.87620663, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.23707576807276495}
episode index:2750
target Thresh 14.676442911738834
target distance 1.0
model initialize at round 2750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.00003438,  8.02227855,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.3985243950597197}
done in step count: 0
reward sum = 0.9969996302013886
running average episode reward sum: 0.7530027816634124
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.00003438,  8.02227855,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.3985243950597197}
episode index:2751
target Thresh 14.678604149928393
target distance 2.0
model initialize at round 2751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.57555246,  3.        ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5755524562402066}
done in step count: 0
reward sum = 0.9951382254163882
running average episode reward sum: 0.753090766926404
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.57555246,  3.        ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5755524562402066}
episode index:2752
target Thresh 14.680764307768964
target distance 13.0
model initialize at round 2752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([3.05142927, 3.81261396, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 13.026024579083403}
done in step count: 29
reward sum = 0.6819126876584407
running average episode reward sum: 0.7530649121936513
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.06220447,  8.54877269,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.4554947609477668}
episode index:2753
target Thresh 14.68292338580059
target distance 9.0
model initialize at round 2753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([14.99981607, 11.86728348,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 8.498139304896373}
done in step count: 14
reward sum = 0.8309022896326401
running average episode reward sum: 0.7530931755841522
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.75629196, 9.45490737, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.8825634478391999}
episode index:2754
target Thresh 14.685081384563041
target distance 9.0
model initialize at round 2754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 6.8149426, 11.       ,  0.       ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 8.717520557384512}
done in step count: 20
reward sum = 0.7828254273610245
running average episode reward sum: 0.7531039676900603
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.21358488,  7.50420337,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5398451597231643}
episode index:2755
target Thresh 14.687238304595816
target distance 6.0
model initialize at round 2755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([13.1190773 ,  5.28502783,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 6.260811478499692}
done in step count: 49
reward sum = 0.482229605160456
running average episode reward sum: 0.7530056823625823
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.6394573 , 10.32595878,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.4860454302328005}
episode index:2756
target Thresh 14.689394146438145
target distance 3.0
model initialize at round 2756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.        ,  5.97153604,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0284639596938714}
done in step count: 5
reward sum = 0.9378210866219564
running average episode reward sum: 0.7530727173296695
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.87288677,  6.76187118,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9047854166354288}
episode index:2757
target Thresh 14.691548910628988
target distance 7.0
model initialize at round 2757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([10.97380131,  8.13224963,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 5.922301997568515}
done in step count: 4
reward sum = 0.9448013332775317
running average episode reward sum: 0.7531422345943352
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.179743  ,  5.70684764,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0827996728985323}
episode index:2758
target Thresh 14.693702597707034
target distance 13.0
model initialize at round 2758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.052885694591254}
done in step count: 11
reward sum = 0.858061829191002
running average episode reward sum: 0.7531802627185095
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.52903073,  7.98635244,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.119269690059579}
episode index:2759
target Thresh 14.695855208210709
target distance 9.0
model initialize at round 2759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 9.        , 11.47456288,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 7.153623955523059}
done in step count: 4
reward sum = 0.9437673923974709
running average episode reward sum: 0.7532493160263642
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.21662513, 9.4186777 , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.6203725214008002}
episode index:2760
target Thresh 14.698006742678164
target distance 3.0
model initialize at round 2760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.14414883, 10.        ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.5195645881002446}
done in step count: 7
reward sum = 0.9154851833352139
running average episode reward sum: 0.7533080758479175
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.43447071, 11.05481166,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.43791451021930905}
episode index:2761
target Thresh 14.700157201647283
target distance 4.0
model initialize at round 2761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.86772514,  9.02518436,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.203251826423465}
done in step count: 6
reward sum = 0.9311408703057447
running average episode reward sum: 0.7533724613636517
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.68949678,  7.05623706,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6917863964307481}
episode index:2762
target Thresh 14.702306585655677
target distance 12.0
model initialize at round 2762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.74164021,  7.56653702,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.098312643844942}
done in step count: 52
reward sum = 0.49975133888155
running average episode reward sum: 0.7532806694264522
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.89726231, 2.63120995, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.639516249430791}
episode index:2763
target Thresh 14.704454895240696
target distance 6.0
model initialize at round 2763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4., 7., 0.]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 4.0000000000000195}
done in step count: 2
reward sum = 0.968310311900553
running average episode reward sum: 0.7533584659685919
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.96726775, 3.46961701, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.4707563439554927}
episode index:2764
target Thresh 14.706602130939418
target distance 4.0
model initialize at round 2764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.65660661, 8.65571702, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 3.7142158179125295}
done in step count: 11
reward sum = 0.8828476065034422
running average episode reward sum: 0.753405297484156
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.66011093, 5.48887272, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8214273987847026}
episode index:2765
target Thresh 14.708748293288648
target distance 11.0
model initialize at round 2765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.61146367, 6.64557076, 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.531658702356504}
done in step count: 17
reward sum = 0.8027666490423362
running average episode reward sum: 0.7534231432367078
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.99946415,  5.54678405,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5467843133290945}
episode index:2766
target Thresh 14.71089338282493
target distance 1.0
model initialize at round 2766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.54498208, 8.9838903 , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.45530301342789703}
done in step count: 0
reward sum = 0.9990512157181338
running average episode reward sum: 0.75351191377248
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.54498208, 8.9838903 , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.45530301342789703}
episode index:2767
target Thresh 14.713037400084538
target distance 10.0
model initialize at round 2767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([ 4.92660534, 11.13219714,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 8.152396680854759}
done in step count: 4
reward sum = 0.9399885577342865
running average episode reward sum: 0.7535792825022348
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.50858044,  9.84150916,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.516345363226099}
episode index:2768
target Thresh 14.71518034560347
target distance 6.0
model initialize at round 2768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.42692843, 6.        , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 4.040842860521857}
done in step count: 2
reward sum = 0.969182796359607
running average episode reward sum: 0.7536571458152929
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.1033501 , 2.10505855, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.14737211820467552}
episode index:2769
target Thresh 14.717322219917467
target distance 11.0
model initialize at round 2769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.        , 11.09802222,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.34358886370188}
done in step count: 6
reward sum = 0.9198038151551672
running average episode reward sum: 0.753717126562347
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.19381987, 5.31667206, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0568176154316697}
episode index:2770
target Thresh 14.719463023561996
target distance 7.0
model initialize at round 2770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([7.02619869, 8.13224963, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 7.335854506752832}
done in step count: 3
reward sum = 0.9491504177152528
running average episode reward sum: 0.753787654635661
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.06003158, 2.91749021, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.3135177608980237}
episode index:2771
target Thresh 14.72160275707226
target distance 9.0
model initialize at round 2771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([12.        ,  9.92180037,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 8.170368485026195}
done in step count: 23
reward sum = 0.7342722206108041
running average episode reward sum: 0.7537806144357962
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.95705489,  1.13235075,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8687114087067357}
episode index:2772
target Thresh 14.723741420983188
target distance 8.0
model initialize at round 2772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.32957745, 5.        , 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 8.336338347807548}
done in step count: 7
reward sum = 0.9027102178106275
running average episode reward sum: 0.7538343214691086
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.47304013,  9.98290157,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.5272371969932979}
episode index:2773
target Thresh 14.725879015829454
target distance 1.0
model initialize at round 2773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.11416456, 8.31626052, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.3362354255331923}
done in step count: 0
reward sum = 0.9996396000600749
running average episode reward sum: 0.7539229318795595
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.11416456, 8.31626052, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.3362354255331923}
episode index:2774
target Thresh 14.72801554214545
target distance 10.0
model initialize at round 2774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([4., 4., 0.]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 10.000000000000025}
done in step count: 21
reward sum = 0.7328290228229292
running average episode reward sum: 0.7539153304708904
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.14864121,  9.97099928,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.8518525923298141}
episode index:2775
target Thresh 14.730151000465307
target distance 5.0
model initialize at round 2775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([11.63356578,  8.53883934,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.88428737993939}
done in step count: 3
reward sum = 0.9598146909776937
running average episode reward sum: 0.753989501710266
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.81978261,  4.98332894,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.18098682342530675}
episode index:2776
target Thresh 14.732285391322895
target distance 2.0
model initialize at round 2776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.08672273,  9.        ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.9132772684097183}
done in step count: 0
reward sum = 0.9963744437962094
running average episode reward sum: 0.7540767847286622
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.08672273,  9.        ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.9132772684097183}
episode index:2777
target Thresh 14.734418715251808
target distance 2.0
model initialize at round 2777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.03225619, 4.        , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0322561860084685}
done in step count: 16
reward sum = 0.8185057142572721
running average episode reward sum: 0.7540999772878878
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.1806403 , 4.73538412, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.1009723493208874}
episode index:2778
target Thresh 14.736550972785375
target distance 12.0
model initialize at round 2778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.55698383, 5.        , 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 12.043943989226143}
done in step count: 8
reward sum = 0.8843973251227906
running average episode reward sum: 0.7541468637030856
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.16238366, 11.86200968,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.201940856100188}
episode index:2779
target Thresh 14.738682164456664
target distance 2.0
model initialize at round 2779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 8.65407622, 11.        ,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.676756042280619}
done in step count: 1
reward sum = 0.9859273348871028
running average episode reward sum: 0.7542302379732957
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.54406971, 10.5304881 ,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.6994927160500926}
episode index:2780
target Thresh 14.740812290798473
target distance 13.0
model initialize at round 2780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.17077088,  8.        ,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.566595096786052}
done in step count: 15
reward sum = 0.8213454713789232
running average episode reward sum: 0.7542543714624743
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.58301916, 5.8876162 , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9806811581258472}
episode index:2781
target Thresh 14.742941352343333
target distance 5.0
model initialize at round 2781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.85398948, 9.        , 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.603194892243927}
done in step count: 3
reward sum = 0.9556607150349755
running average episode reward sum: 0.7543267677038734
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.46317951, 11.89947273,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0474862453386955}
episode index:2782
target Thresh 14.745069349623508
target distance 9.0
model initialize at round 2782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([3.64283538, 4.        , 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.763411253750851}
done in step count: 5
reward sum = 0.9261346423264563
running average episode reward sum: 0.7543885024773634
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.53500682, 11.91257175,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0578371811557339}
episode index:2783
target Thresh 14.747196283171
target distance 5.0
model initialize at round 2783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([13.        ,  9.38233006,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 4.521079144940321}
done in step count: 2
reward sum = 0.9633167585092368
running average episode reward sum: 0.7544635485463403
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.90485065,  5.39223225,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0900166732743664}
episode index:2784
target Thresh 14.749322153517538
target distance 4.0
model initialize at round 2784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.33233988,  4.        ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.027424424666235}
done in step count: 1
reward sum = 0.9839010849125562
running average episode reward sum: 0.7545459318628095
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.58145394,  2.        ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.5814539407698334}
episode index:2785
target Thresh 14.751446961194592
target distance 9.0
model initialize at round 2785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([13.0958577 ,  1.57787484,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 11.68225595040832}
done in step count: 15
reward sum = 0.8072933027188272
running average episode reward sum: 0.7545648648746027
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.58545961, 10.98031454,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.1418316687163215}
episode index:2786
target Thresh 14.753570706733367
target distance 7.0
model initialize at round 2786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 6.95006597, 10.78719693,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 8.002776420718508}
done in step count: 22
reward sum = 0.7774565852587709
running average episode reward sum: 0.754573078624292
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.2671446,  6.7788867,  0.       ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.34678138515735774}
episode index:2787
target Thresh 14.755693390664794
target distance 10.0
model initialize at round 2787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([10.00494245, 11.86740305,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 8.503005580988798}
done in step count: 18
reward sum = 0.801531323253034
running average episode reward sum: 0.7545899216101704
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.13096707, 8.09528313, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.2544842898949637}
episode index:2788
target Thresh 14.757815013519545
target distance 2.0
model initialize at round 2788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.        ,  9.54009366,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.45990633964543015}
done in step count: 0
reward sum = 0.9967812240693913
running average episode reward sum: 0.7546767596533612
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.        ,  9.54009366,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.45990633964543015}
episode index:2789
target Thresh 14.759935575828031
target distance 10.0
model initialize at round 2789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([10.       ,  8.7633028,  0.       ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 9.85980016022252}
done in step count: 6
reward sum = 0.9159681622662667
running average episode reward sum: 0.7547345701919321
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.12498744, 3.27817174, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9181647483149402}
episode index:2790
target Thresh 14.762055078120389
target distance 13.0
model initialize at round 2790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.53979754, 7.        , 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.816264870568249}
done in step count: 7
reward sum = 0.9028668974398877
running average episode reward sum: 0.7547876451927376
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.40659443,  3.85696837,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.43101865016588614}
episode index:2791
target Thresh 14.764173520926493
target distance 4.0
model initialize at round 2791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.15134093, 8.38108945, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 4.383702640133051}
done in step count: 8
reward sum = 0.9066646601525182
running average episode reward sum: 0.7548420424043994
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.90065377, 4.36504174, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9718192662137918}
episode index:2792
target Thresh 14.766290904775955
target distance 5.0
model initialize at round 2792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([9.45798326, 8.08963987, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 5.844328534878442}
done in step count: 6
reward sum = 0.9174844243096844
running average episode reward sum: 0.754900274549729
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.48804626, 5.87621719, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.5034990957485963}
episode index:2793
target Thresh 14.768407230198122
target distance 11.0
model initialize at round 2793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.55208445,  8.60872793,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.571461067036388}
done in step count: 20
reward sum = 0.7621218022768858
running average episode reward sum: 0.7549028592053221
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.28377079, 8.47144084, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.550256603767013}
episode index:2794
target Thresh 14.770522497722073
target distance 12.0
model initialize at round 2794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.99983213, 7.00007785, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.050050404146477}
done in step count: 5
reward sum = 0.9208889777530521
running average episode reward sum: 0.7549622460098113
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.97749235,  5.12480902,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8754803517827237}
episode index:2795
target Thresh 14.772636707876627
target distance 8.0
model initialize at round 2795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 8.27276063, 10.67683136,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 6.3091700415415435}
done in step count: 3
reward sum = 0.9556987753104412
running average episode reward sum: 0.755034040190534
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.27276063, 9.38007736, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.677275747475941}
episode index:2796
target Thresh 14.774749861190337
target distance 7.0
model initialize at round 2796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.09516316, 7.39573136, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.47107362880053}
done in step count: 11
reward sum = 0.8680841245844673
running average episode reward sum: 0.7550744585260342
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.8131699 , 1.67587435, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.3741161933462039}
episode index:2797
target Thresh 14.776861958191489
target distance 13.0
model initialize at round 2797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.045361017187282}
done in step count: 12
reward sum = 0.8577020485038851
running average episode reward sum: 0.755111137435962
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.80290917, 11.83915588,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8619903686644182}
episode index:2798
target Thresh 14.778972999408108
target distance 2.0
model initialize at round 2798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.82524192,  5.        ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.1747580766677093}
done in step count: 3
reward sum = 0.9606167607052134
running average episode reward sum: 0.7551845585232322
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.58323747,  5.31507495,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5224588323370145}
episode index:2799
target Thresh 14.781082985367958
target distance 7.0
model initialize at round 2799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([11.94790933,  7.92538854,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 7.178416829834802}
done in step count: 12
reward sum = 0.858812380118288
running average episode reward sum: 0.7552215684595162
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.86870257,  2.09958888,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8743924158292704}
episode index:2800
target Thresh 14.783191916598529
target distance 9.0
model initialize at round 2800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([7.        , 9.88148403, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.81604922961524}
done in step count: 5
reward sum = 0.9235750662088421
running average episode reward sum: 0.7552816732427183
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.53371458,  2.96364334,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.4677006488695361}
episode index:2801
target Thresh 14.785299793627058
target distance 2.0
model initialize at round 2801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.89753103, 5.        , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.3437120057557705}
done in step count: 0
reward sum = 0.9960884214281869
running average episode reward sum: 0.7553676142663391
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.89753103, 5.        , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.3437120057557705}
episode index:2802
target Thresh 14.787406616980515
target distance 6.0
model initialize at round 2802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([15.6445688 ,  4.65051842,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 5.967469928357749}
done in step count: 10
reward sum = 0.8862880042580841
running average episode reward sum: 0.755414321505009
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.63473217, 10.09920678,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.6424382523199939}
episode index:2803
target Thresh 14.789512387185603
target distance 11.0
model initialize at round 2803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([12.        , 10.26938546,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.004030682114783}
done in step count: 6
reward sum = 0.9201858975983445
running average episode reward sum: 0.7554730845492649
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.57734322, 9.3258588 , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8875762253168957}
episode index:2804
target Thresh 14.791617104768765
target distance 12.0
model initialize at round 2804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 10.198039027185583}
done in step count: 5
reward sum = 0.9230603054615472
running average episode reward sum: 0.7555328304390732
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.94525953,  8.86335546,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.14720139007930133}
episode index:2805
target Thresh 14.793720770256185
target distance 3.0
model initialize at round 2805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([14.86515343,  9.36527443,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.9005848397838516}
done in step count: 8
reward sum = 0.9109061095866905
running average episode reward sum: 0.7555882022420481
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.20026994,  8.21572737,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.8094390661655613}
episode index:2806
target Thresh 14.795823384173772
target distance 4.0
model initialize at round 2806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.75059938,  7.45814824,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.7574435032171962}
done in step count: 1
reward sum = 0.981892960123585
running average episode reward sum: 0.7556688238159284
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.9429493 ,  5.45814824,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.4616866834843982}
episode index:2807
target Thresh 14.797924947047186
target distance 11.0
model initialize at round 2807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.01823725, 11.86412216,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.059542495133481}
done in step count: 6
reward sum = 0.9210055139360129
running average episode reward sum: 0.7557277044035781
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.36185184, 10.81879192,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6633773034879008}
episode index:2808
target Thresh 14.80002545940181
target distance 1.0
model initialize at round 2808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.        , 10.14836949,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.010946836331835}
done in step count: 0
reward sum = 0.9964560439279324
running average episode reward sum: 0.755813403349653
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.        , 10.14836949,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.010946836331835}
episode index:2809
target Thresh 14.80212492176278
target distance 8.0
model initialize at round 2809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.        ,  8.18447769,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.627757229686799}
done in step count: 3
reward sum = 0.9542265645881729
running average episode reward sum: 0.7558840130155742
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.01633145, 11.84944764,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8496046186825064}
episode index:2810
target Thresh 14.80422333465496
target distance 13.0
model initialize at round 2810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.        ,  9.56702769,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.811161253192717}
done in step count: 8
reward sum = 0.8940290340220607
running average episode reward sum: 0.7559331574556333
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.61211655, 3.04838011, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.6140254921485615}
episode index:2811
target Thresh 14.80632069860295
target distance 10.0
model initialize at round 2811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.90007704, 1.60448778, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 13.079917727567752}
done in step count: 28
reward sum = 0.6858981082866142
running average episode reward sum: 0.7559082516771236
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.46778558, 10.52776128,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.6647050148013116}
episode index:2812
target Thresh 14.808417014131091
target distance 9.0
model initialize at round 2812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.11600056,  3.29746231,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.308943752114583}
done in step count: 5
reward sum = 0.9239736723458767
running average episode reward sum: 0.755967997649633
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.18676302, 11.00506145,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8132527340234194}
episode index:2813
target Thresh 14.810512281763469
target distance 1.0
model initialize at round 2813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.0384599 ,  5.15253729,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.1531788068304178}
done in step count: 1
reward sum = 0.9894498398307371
running average episode reward sum: 0.7560509691642673
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.80115909,  4.78820412,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.123886840615032}
episode index:2814
target Thresh 14.812606502023892
target distance 6.0
model initialize at round 2814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([10.58634573, 11.90184057,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.674171962360513}
done in step count: 12
reward sum = 0.862024602908855
running average episode reward sum: 0.7560886152153311
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.7315319 , 11.88145129,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.1454672842283424}
episode index:2815
target Thresh 14.814699675435918
target distance 13.0
model initialize at round 2815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 3.64210631, 11.89762038,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.390450308603086}
done in step count: 7
reward sum = 0.9043684446444121
running average episode reward sum: 0.7561412714047591
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.92506341, 11.762516  ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.7661893631088189}
episode index:2816
target Thresh 14.816791802522843
target distance 7.0
model initialize at round 2816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([11.74522835, 11.88322093,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 7.795424386900201}
done in step count: 14
reward sum = 0.8320757882472135
running average episode reward sum: 0.7561682272147847
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.21401743, 11.8746946 ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9004965903092143}
episode index:2817
target Thresh 14.818882883807696
target distance 11.0
model initialize at round 2817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.86009807, 3.97362968, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.795539430365347}
done in step count: 10
reward sum = 0.866055033104091
running average episode reward sum: 0.7562072218229783
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.36540875,  9.75502994,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.4399248640614047}
episode index:2818
target Thresh 14.820972919813249
target distance 12.0
model initialize at round 2818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 5.45695615, 10.50592053,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 10.554614544614346}
done in step count: 5
reward sum = 0.9233628926087126
running average episode reward sum: 0.7562665179105219
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.34020613, 11.87760519,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0979612149140567}
episode index:2819
target Thresh 14.823061911062009
target distance 3.0
model initialize at round 2819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.63254547, 9.        , 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.0653745025773773}
done in step count: 19
reward sum = 0.7764673540038863
running average episode reward sum: 0.7562736813275762
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.46759162, 9.76678268, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.5225248689761132}
episode index:2820
target Thresh 14.825149858076225
target distance 1.0
model initialize at round 2820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.68431294, 7.15027452, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0910167729985625}
done in step count: 0
reward sum = 0.9993837312667446
running average episode reward sum: 0.7563598600053285
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.68431294, 7.15027452, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0910167729985625}
episode index:2821
target Thresh 14.827236761377886
target distance 14.0
model initialize at round 2821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([16.36876297,  9.71404338,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 14.426192627257384}
done in step count: 9
reward sum = 0.8902457619821943
running average episode reward sum: 0.7564073036275739
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.0167975 , 11.40047486,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.4008269812843422}
episode index:2822
target Thresh 14.829322621488712
target distance 3.0
model initialize at round 2822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([9.15369284, 9.05064893, 0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 2.8467577617764337}
done in step count: 12
reward sum = 0.8705509961973973
running average episode reward sum: 0.7564477370999685
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.3559806 ,  9.05145462,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.3596800879973335}
episode index:2823
target Thresh 14.831407438930174
target distance 4.0
model initialize at round 2823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([11.98093107,  8.00024856,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.8421178112374306}
done in step count: 1
reward sum = 0.9794711038806527
running average episode reward sum: 0.7565267113799901
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.60027304,  6.17221383,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.43524618743656435}
episode index:2824
target Thresh 14.833491214223475
target distance 6.0
model initialize at round 2824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([11.,  9.,  0.]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 4.472135954999604}
done in step count: 2
reward sum = 0.9657545343189529
running average episode reward sum: 0.7566007743261631
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.00000008,  6.45670615,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.5432938482849162}
episode index:2825
target Thresh 14.835573947889557
target distance 6.0
model initialize at round 2825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.097798445291377}
done in step count: 2
reward sum = 0.9662939680906395
running average episode reward sum: 0.7566749757393848
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.96064037, 11.80404051,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.8050033007574777}
episode index:2826
target Thresh 14.837655640449103
target distance 1.0
model initialize at round 2826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([7.15676749, 9.34135914, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.9573742308239752}
done in step count: 1
reward sum = 0.9869710573691566
running average episode reward sum: 0.7567564388032794
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.92030084, 9.70660907, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.30402334371239886}
episode index:2827
target Thresh 14.839736292422536
target distance 6.0
model initialize at round 2827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([2.93867779, 9.36839807, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 5.074711782370588}
done in step count: 3
reward sum = 0.9619537239119745
running average episode reward sum: 0.7568289979564294
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.20327229, 8.29705356, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.7317467572861627}
episode index:2828
target Thresh 14.841815904330025
target distance 5.0
model initialize at round 2828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 5.88914812, 11.15496063,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 3.318332940756518}
done in step count: 2
reward sum = 0.9736798120262178
running average episode reward sum: 0.7569056507715831
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.40033042, 10.20863181,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.63492584891853}
episode index:2829
target Thresh 14.843894476691466
target distance 10.0
model initialize at round 2829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([12.72354887,  7.2780138 ,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.01607740221725}
done in step count: 34
reward sum = 0.6288183598103236
running average episode reward sum: 0.7568603902447417
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.95779273, 4.06410174, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9368495080887262}
episode index:2830
target Thresh 14.845972010026504
target distance 2.0
model initialize at round 2830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.0766232 ,  7.87825681,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.124357109141253}
done in step count: 5
reward sum = 0.9434783535105595
running average episode reward sum: 0.7569263096948533
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.22255426,  9.41027134,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.46674721892735155}
episode index:2831
target Thresh 14.848048504854523
target distance 4.0
model initialize at round 2831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([6.        , 9.49588388, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 2.0605583768660423}
done in step count: 1
reward sum = 0.9808694001262149
running average episode reward sum: 0.7570053856448644
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.95243138, 8.13169077, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.8696112309601116}
episode index:2832
target Thresh 14.850123961694647
target distance 12.0
model initialize at round 2832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([12.54049558,  7.46533434,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.11736967376564}
done in step count: 29
reward sum = 0.6867547608723369
running average episode reward sum: 0.7569805883893852
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.64894491, 10.86737392,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.6623587923159722}
episode index:2833
target Thresh 14.85219838106574
target distance 5.0
model initialize at round 2833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([4.71279001, 9.        , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.454511488827179}
done in step count: 2
reward sum = 0.966788582681033
running average episode reward sum: 0.7570546208503209
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.21909833, 5.        , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0237207031260505}
episode index:2834
target Thresh 14.854271763486405
target distance 6.0
model initialize at round 2834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 4.123105625617684}
done in step count: 58
reward sum = 0.46751611551379024
running average episode reward sum: 0.7569524908660752
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.98939116,  2.18001562,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.2850172190013704}
episode index:2835
target Thresh 14.856344109474993
target distance 6.0
model initialize at round 2835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([4.86988162, 4.80043264, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 5.618977224309331}
done in step count: 8
reward sum = 0.9094309433036639
running average episode reward sum: 0.7570062561878091
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.65109831, 9.22791411, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.8472597102042634}
episode index:2836
target Thresh 14.858415419549583
target distance 13.0
model initialize at round 2836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 5.        , 11.14752054,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.601270127097994}
done in step count: 5
reward sum = 0.9218584344322318
running average episode reward sum: 0.7570643641110535
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.14864602,  4.95175459,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8527198987498571}
episode index:2837
target Thresh 14.860485694228009
target distance 13.0
model initialize at round 2837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([15.93924904,  7.        ,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 13.282475886874588}
done in step count: 48
reward sum = 0.5326061262330869
running average episode reward sum: 0.7569852738228653
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.05788445, 3.10382743, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.3002718888849516}
episode index:2838
target Thresh 14.862554934027836
target distance 3.0
model initialize at round 2838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.        , 11.31467652,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0483421742249543}
done in step count: 1
reward sum = 0.9826016994245682
running average episode reward sum: 0.7570647442087766
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 8.       , 10.8930077,  0.       ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.005707389445216}
episode index:2839
target Thresh 14.864623139466376
target distance 2.0
model initialize at round 2839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.        , 8.45226681, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5477331876754423}
done in step count: 0
reward sum = 0.9954181906873261
running average episode reward sum: 0.7571486714786634
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.        , 8.45226681, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5477331876754423}
episode index:2840
target Thresh 14.866690311060678
target distance 5.0
model initialize at round 2840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.75033736,  7.66576076,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.8036933408706455}
done in step count: 13
reward sum = 0.8427490240464104
running average episode reward sum: 0.7571788018385958
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.5593282 , 11.90557817,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0071064821772113}
episode index:2841
target Thresh 14.868756449327535
target distance 10.0
model initialize at round 2841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 8.246211251235334}
done in step count: 23
reward sum = 0.7203686242505611
running average episode reward sum: 0.757165849629733
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.92846403, 9.97444082, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.9770630978321985}
episode index:2842
target Thresh 14.870821554783484
target distance 3.0
model initialize at round 2842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.51832533, 11.46867537,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.5019593581130155}
done in step count: 18
reward sum = 0.763634709347732
running average episode reward sum: 0.7571681249936858
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.34758525,  7.94793481,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.3514630716405992}
episode index:2843
target Thresh 14.8728856279448
target distance 2.0
model initialize at round 2843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.        , 9.20217574, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.20217573642728404}
done in step count: 0
reward sum = 0.9945758710332957
running average episode reward sum: 0.7572516016976378
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.        , 9.20217574, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.20217573642728404}
episode index:2844
target Thresh 14.874948669327502
target distance 11.0
model initialize at round 2844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.99774583, 6.86631253, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.003246769780407}
done in step count: 5
reward sum = 0.92428618568288
running average episode reward sum: 0.757310313326455
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.66543122,  6.52116279,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8198071613783815}
episode index:2845
target Thresh 14.877010679447352
target distance 9.0
model initialize at round 2845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([12.01665115,  9.69692665,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 9.411344824137176}
done in step count: 5
reward sum = 0.9340213742569143
running average episode reward sum: 0.7573724043527835
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.39712047, 6.53584309, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.7608583079133365}
episode index:2846
target Thresh 14.879071658819848
target distance 6.0
model initialize at round 2846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.85999428, 9.79005516, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.901216125031374}
done in step count: 9
reward sum = 0.8964043166052742
running average episode reward sum: 0.75742123888466
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.87246332, 3.76501787, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.26736156127281063}
episode index:2847
target Thresh 14.881131607960235
target distance 7.0
model initialize at round 2847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.58891654, 5.6781435 , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.3377098284799604}
done in step count: 3
reward sum = 0.9554404918063777
running average episode reward sum: 0.7574907681167251
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.79206692, 11.42562684,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8991819693189383}
episode index:2848
target Thresh 14.883190527383505
target distance 12.0
model initialize at round 2848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 4.        , 10.97323728,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 13.435735454852557}
done in step count: 19
reward sum = 0.7788164360430887
running average episode reward sum: 0.7574982534336526
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.80096559,  2.25662296,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8410714754841856}
episode index:2849
target Thresh 14.885248417604387
target distance 2.0
model initialize at round 2849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.67999935,  9.27793097,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.9918582534636758}
done in step count: 0
reward sum = 0.9964690573072339
running average episode reward sum: 0.7575821028385207
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.67999935,  9.27793097,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.9918582534636758}
episode index:2850
target Thresh 14.887305279137351
target distance 7.0
model initialize at round 2850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([10.08440912,  9.        ,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 6.166038786087456}
done in step count: 14
reward sum = 0.8290300176054594
running average episode reward sum: 0.7576071634890877
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.58740712, 7.43330509, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8162047877654707}
episode index:2851
target Thresh 14.88936111249661
target distance 4.0
model initialize at round 2851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([13.        ,  8.30370474,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 2.6224831007635623}
done in step count: 9
reward sum = 0.8946021357564852
running average episode reward sum: 0.7576551981918463
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.98628921,  9.55330539,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.4469049824110676}
episode index:2852
target Thresh 14.89141591819613
target distance 6.0
model initialize at round 2852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 4.182375703590784}
done in step count: 2
reward sum = 0.9652189916325885
running average episode reward sum: 0.7577279510111387
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.77994023,  8.00509914,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.0189475088319624}
episode index:2853
target Thresh 14.893469696749607
target distance 9.0
model initialize at round 2853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.68969226, 8.        , 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.458727927546303}
done in step count: 10
reward sum = 0.8783416540300505
running average episode reward sum: 0.7577702122946071
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.33043402,  2.7464538 ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7159638769423391}
episode index:2854
target Thresh 14.895522448670485
target distance 1.0
model initialize at round 2854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.30196452, 8.74110401, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8002610330609624}
done in step count: 0
reward sum = 0.9983167397835719
running average episode reward sum: 0.7578544667700848
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.30196452, 8.74110401, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8002610330609624}
episode index:2855
target Thresh 14.897574174471954
target distance 11.0
model initialize at round 2855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([12.73781857, 11.87854484,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 9.77736935948044}
done in step count: 14
reward sum = 0.834351643842137
running average episode reward sum: 0.7578812514959504
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.47917597, 10.40048341,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7941522583305851}
episode index:2856
target Thresh 14.899624874666948
target distance 7.0
model initialize at round 2856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([3.94993675, 4.13171744, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 9.1727930330326}
done in step count: 11
reward sum = 0.8723615785540917
running average episode reward sum: 0.7579213216139267
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.04718139, 10.35679787,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.0174320679611304}
episode index:2857
target Thresh 14.901674549768137
target distance 11.0
model initialize at round 2857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([6.69229484, 9.46304023, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.930228201229243}
done in step count: 6
reward sum = 0.9139873901544332
running average episode reward sum: 0.7579759283558933
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.91164203,  2.45353977,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.018228618012299}
episode index:2858
target Thresh 14.903723200287944
target distance 2.0
model initialize at round 2858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 6.43624055, 11.24295807,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 3.4448189745154987}
done in step count: 9
reward sum = 0.8911237552096265
running average episode reward sum: 0.758022499823838
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.11736582, 11.87126526,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.879134739663797}
episode index:2859
target Thresh 14.905770826738527
target distance 6.0
model initialize at round 2859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.46805239,  7.        ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 4.027291029612699}
done in step count: 2
reward sum = 0.9703115951146203
running average episode reward sum: 0.7580967267802334
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.75554192, 11.        ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.755541920661928}
episode index:2860
target Thresh 14.907817429631798
target distance 10.0
model initialize at round 2860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([14.7962029,  9.       ,  0.       ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 8.796202898025602}
done in step count: 9
reward sum = 0.8857935576841822
running average episode reward sum: 0.7581413604156421
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.06657186, 8.76598403, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.24330081789418423}
episode index:2861
target Thresh 14.909863009479405
target distance 7.0
model initialize at round 2861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([10.765704  , 11.88192395,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 6.516685365650606}
done in step count: 39
reward sum = 0.6312089940334175
running average episode reward sum: 0.7580970094839921
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.98855298,  7.76903769,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.23124580214308524}
episode index:2862
target Thresh 14.91190756679274
target distance 14.0
model initialize at round 2862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.29975128,  5.        ,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.6603270728928}
done in step count: 8
reward sum = 0.8897775167757687
running average episode reward sum: 0.7581430033740695
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.59766253, 7.32709502, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9000008957873766}
episode index:2863
target Thresh 14.913951102082947
target distance 10.0
model initialize at round 2863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.9434377 , 11.73711729,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.973762854749305}
done in step count: 9
reward sum = 0.8819924752554076
running average episode reward sum: 0.7581862469047543
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.65174143, 10.70864961,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.4540584524344488}
episode index:2864
target Thresh 14.91599361586091
target distance 10.0
model initialize at round 2864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 9.43398113205663}
done in step count: 11
reward sum = 0.85065019716422
running average episode reward sum: 0.7582185205348624
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.64132773, 8.02741433, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.036614043429893}
episode index:2865
target Thresh 14.918035108637255
target distance 8.0
model initialize at round 2865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([7.38220572, 9.        , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.887620457335282}
done in step count: 7
reward sum = 0.9070339059862791
running average episode reward sum: 0.7582704449540708
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.229273  , 2.63205711, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8540503921573562}
episode index:2866
target Thresh 14.920075580922354
target distance 8.0
model initialize at round 2866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 5.        , 11.11256921,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.001055892732227}
done in step count: 22
reward sum = 0.7622649863934764
running average episode reward sum: 0.7582718382367494
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.55360413, 11.73041734,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9165080597580624}
episode index:2867
target Thresh 14.922115033226328
target distance 3.0
model initialize at round 2867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.58720863,  6.89879107,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 2.957668620490257}
done in step count: 13
reward sum = 0.8529752945477905
running average episode reward sum: 0.7583048589676807
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.18415936,  4.32239352,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8772306011788931}
episode index:2868
target Thresh 14.924153466059039
target distance 10.0
model initialize at round 2868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.09621211,  5.41975898,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.412561198297494}
done in step count: 80
reward sum = 0.34148156280590525
running average episode reward sum: 0.7581595737476869
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.45147812, 3.98647874, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0848837708190064}
episode index:2869
target Thresh 14.926190879930097
target distance 3.0
model initialize at round 2869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.763273  ,  6.70292854,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.504985062368548}
done in step count: 29
reward sum = 0.7019296281549721
running average episode reward sum: 0.7581399814321494
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.19697869,  7.73312627,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8462061297983758}
episode index:2870
target Thresh 14.928227275348853
target distance 2.0
model initialize at round 2870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.        , 9.15891165, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.15891164541249658}
done in step count: 0
reward sum = 0.9964694274361835
running average episode reward sum: 0.7582229941266825
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.        , 9.15891165, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.15891164541249658}
episode index:2871
target Thresh 14.930262652824403
target distance 3.0
model initialize at round 2871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.61583388, 9.37009525, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.4229352097559076}
done in step count: 3
reward sum = 0.9639476017711566
running average episode reward sum: 0.7582946252574778
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.83529311, 7.43886   , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.584813182781506}
episode index:2872
target Thresh 14.9322970128656
target distance 2.0
model initialize at round 2872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.02944461, 11.85944827,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8599525030601693}
done in step count: 0
reward sum = 0.9957429648158864
running average episode reward sum: 0.7583772734786954
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.02944461, 11.85944827,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8599525030601693}
episode index:2873
target Thresh 14.934330355981027
target distance 11.0
model initialize at round 2873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([12.2913091 ,  7.76481068,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 9.373052724602289}
done in step count: 12
reward sum = 0.8555073330344284
running average episode reward sum: 0.75841106960241
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.48005227, 9.2144291 , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.525766128058685}
episode index:2874
target Thresh 14.936362682679022
target distance 10.0
model initialize at round 2874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 8.246211251235334}
done in step count: 24
reward sum = 0.7120203257201125
running average episode reward sum: 0.7583949336914945
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.21999418,  8.33457088,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.7008518772889324}
episode index:2875
target Thresh 14.938393993467667
target distance 10.0
model initialize at round 2875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.354328771680768}
done in step count: 12
reward sum = 0.8452946795512823
running average episode reward sum: 0.7584251491803191
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.80789743,  7.85905081,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8802679714199697}
episode index:2876
target Thresh 14.940424288854787
target distance 7.0
model initialize at round 2876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([8.50738382, 9.        , 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 7.427572447543405}
done in step count: 7
reward sum = 0.902732419357334
running average episode reward sum: 0.7584753081202485
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.72614468,  3.61632478,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8212750829227882}
episode index:2877
target Thresh 14.94245356934796
target distance 13.0
model initialize at round 2877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.        , 8.43498242, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.523849366376458}
done in step count: 27
reward sum = 0.7148349819463872
running average episode reward sum: 0.7584601446990623
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.9447341 ,  5.71731149,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.7194373473315165}
episode index:2878
target Thresh 14.944481835454505
target distance 1.0
model initialize at round 2878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.04565063, 9.92248708, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9236159337485581}
done in step count: 0
reward sum = 0.9999939308248841
running average episode reward sum: 0.7585440397272408
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.04565063, 9.92248708, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9236159337485581}
episode index:2879
target Thresh 14.946509087681484
target distance 3.0
model initialize at round 2879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.,  9.,  0.]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.4142135623730938}
done in step count: 12
reward sum = 0.8669202436298772
running average episode reward sum: 0.7585816703535959
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.12890681, 10.83974742,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.8495838412238754}
episode index:2880
target Thresh 14.948535326535717
target distance 6.0
model initialize at round 2880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.72333074, 4.83900726, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 5.21143487271643}
done in step count: 7
reward sum = 0.9247190833514124
running average episode reward sum: 0.7586393369322136
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.6920684 , 9.90582293, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.322011167011255}
episode index:2881
target Thresh 14.95056055252376
target distance 5.0
model initialize at round 2881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([13.74598193,  7.        ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 3.752412217798576}
done in step count: 2
reward sum = 0.9686857080457867
running average episode reward sum: 0.7587122190873538
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.50209284,  3.        ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.1170996113527747}
episode index:2882
target Thresh 14.952584766151922
target distance 9.0
model initialize at round 2882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([13.21781003,  7.74380481,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 7.326309305989984}
done in step count: 16
reward sum = 0.7946123727095278
running average episode reward sum: 0.7587246714472643
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.49416625, 8.56651621, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.657349587713179}
episode index:2883
target Thresh 14.954607967926252
target distance 11.0
model initialize at round 2883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.12772195,  5.84127652,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.90306585364026}
done in step count: 17
reward sum = 0.7930770382133866
running average episode reward sum: 0.7587365828088338
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.63324082, 2.45168449, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.5818343196464751}
episode index:2884
target Thresh 14.956630158352553
target distance 3.0
model initialize at round 2884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([4.59981275, 9.34780931, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.544628445965465}
done in step count: 1
reward sum = 0.9855347875941182
running average episode reward sum: 0.7588151957047733
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5.34966671, 9.67782897, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.7257599852201035}
episode index:2885
target Thresh 14.958651337936374
target distance 5.0
model initialize at round 2885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([4.        , 8.61440897, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 3.024678568461402}
done in step count: 2
reward sum = 0.9694098365582028
running average episode reward sum: 0.7588881668208001
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([8.        , 9.19403124, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.0186501467802649}
episode index:2886
target Thresh 14.960671507183005
target distance 1.0
model initialize at round 2886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.99505755, 11.86740305,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.3200483200778628}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7589696049341995
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.99505755, 11.86740305,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.3200483200778628}
episode index:2887
target Thresh 14.962690666597496
target distance 11.0
model initialize at round 2887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([12.47951913,  9.8162843 ,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.121620653433109}
done in step count: 24
reward sum = 0.7401632113735264
running average episode reward sum: 0.7589630930250718
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.53995309, 3.99920455, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.5399536734814857}
episode index:2888
target Thresh 14.96470881668463
target distance 13.0
model initialize at round 2888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4., 4., 0.]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.401754250991402}
done in step count: 10
reward sum = 0.864138126196879
running average episode reward sum: 0.7589994983671181
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.27964956,  7.24252179,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.3701630678554106}
episode index:2889
target Thresh 14.966725957948947
target distance 6.0
model initialize at round 2889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 3.83277071, 10.85404408,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.169784541903109}
done in step count: 9
reward sum = 0.8976208380472186
running average episode reward sum: 0.7590474642285991
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.81557041, 11.11571978,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8237391312762894}
episode index:2890
target Thresh 14.968742090894734
target distance 6.0
model initialize at round 2890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 3.99999998, 11.68342785,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.057964242938645}
done in step count: 2
reward sum = 0.9689473103354829
running average episode reward sum: 0.7591200688104417
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.98557254, 11.83243538,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8325603998721591}
episode index:2891
target Thresh 14.970757216026023
target distance 2.0
model initialize at round 2891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.43283129, 9.87178922, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.9733238732342239}
done in step count: 0
reward sum = 0.9975879744739787
running average episode reward sum: 0.759202526592483
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.43283129, 9.87178922, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.9733238732342239}
episode index:2892
target Thresh 14.972771333846593
target distance 5.0
model initialize at round 2892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.605551275463995}
done in step count: 52
reward sum = 0.47712591569507984
running average episode reward sum: 0.7591050234431924
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.0766171 , 9.87365897, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.877012075055196}
episode index:2893
target Thresh 14.974784444859976
target distance 10.0
model initialize at round 2893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([14.        ,  3.88188815,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 10.071310371287774}
done in step count: 10
reward sum = 0.8692519423313487
running average episode reward sum: 0.7591430838851027
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.20349919, 10.12738748,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.24008225690232796}
episode index:2894
target Thresh 14.976796549569451
target distance 2.0
model initialize at round 2894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.        , 10.84051657,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8405165672302441}
done in step count: 0
reward sum = 0.9944593737889335
running average episode reward sum: 0.7592243675776429
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.        , 10.84051657,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8405165672302441}
episode index:2895
target Thresh 14.978807648478039
target distance 4.0
model initialize at round 2895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4.37587985, 7.        , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 2.4275595507718553}
done in step count: 1
reward sum = 0.9832642300721083
running average episode reward sum: 0.7593017294086147
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.43320951, 9.        , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.4332095086574661}
episode index:2896
target Thresh 14.980817742088522
target distance 1.0
model initialize at round 2896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 5.48220158, 11.        ,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 2.0573085233191013}
done in step count: 24
reward sum = 0.7517497253866953
running average episode reward sum: 0.7592991225725698
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.67716159, 9.95764664, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.0105996894640807}
episode index:2897
target Thresh 14.982826830903416
target distance 13.0
model initialize at round 2897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.00016787,  7.00007785,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.083231010926347}
done in step count: 18
reward sum = 0.7731070116235168
running average episode reward sum: 0.7593038871995715
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.87010422, 2.30833667, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.923121261437121}
episode index:2898
target Thresh 14.984834915425
target distance 7.0
model initialize at round 2898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([12.6859082 , 10.40657303,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 5.857303109919347}
done in step count: 7
reward sum = 0.9146823850131559
running average episode reward sum: 0.7593574844737397
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.31796271, 9.54944345, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.8758213167848773}
episode index:2899
target Thresh 14.986841996155292
target distance 13.0
model initialize at round 2899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 4.        , 10.64257121,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.939575681234146}
done in step count: 12
reward sum = 0.8547551837058626
running average episode reward sum: 0.7593903802320957
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.64573708,  5.18091385,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.0430141404812683}
episode index:2900
target Thresh 14.98884807359606
target distance 6.0
model initialize at round 2900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([13.54289937,  6.47137439,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 5.199219378960811}
done in step count: 3
reward sum = 0.959063848511837
running average episode reward sum: 0.7594592094179902
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.13013413, 8.51643809, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.5007664305423869}
episode index:2901
target Thresh 14.990853148248826
target distance 4.0
model initialize at round 2901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([ 9.        , 11.28533018,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 3.0368954567044515}
done in step count: 23
reward sum = 0.7283304574233217
running average episode reward sum: 0.7594484827632711
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.25214518, 8.63294169, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.44531898062110997}
episode index:2902
target Thresh 14.992857220614859
target distance 13.0
model initialize at round 2902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.831072604542193}
done in step count: 45
reward sum = 0.5103047547424229
running average episode reward sum: 0.7593626599151757
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.05012368,  7.51169181,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.068040214866341}
episode index:2903
target Thresh 14.994860291195176
target distance 6.0
model initialize at round 2903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([11.48692286,  8.47198033,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 5.129733405600551}
done in step count: 4
reward sum = 0.9506700453318973
running average episode reward sum: 0.7594285371140107
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.84138956,  4.58676638,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6078256793192608}
episode index:2904
target Thresh 14.996862360490546
target distance 8.0
model initialize at round 2904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([12.60810328, 10.05562794,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 6.608337413068292}
done in step count: 3
reward sum = 0.9594647058753469
running average episode reward sum: 0.7594973963803657
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.60810328, 9.1001507 , 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.0860563283046138}
episode index:2905
target Thresh 14.998863429001482
target distance 2.0
model initialize at round 2905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 2.6407442894293542e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7595780923899407
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 2.6407442894293542e-14}
episode index:2906
target Thresh 15.000863497228258
target distance 2.0
model initialize at round 2906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.59419203,  6.94209921,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.4099178102589438}
done in step count: 0
reward sum = 0.9969063899609932
running average episode reward sum: 0.7596597326711829
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.59419203,  6.94209921,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.4099178102589438}
episode index:2907
target Thresh 15.002862565670885
target distance 5.0
model initialize at round 2907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([15.,  9.,  0.]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.1622776601683915}
done in step count: 16
reward sum = 0.8055307244253139
running average episode reward sum: 0.7596755067398742
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.11454913,  5.71824208,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9291989879828784}
episode index:2908
target Thresh 15.004860634829134
target distance 2.0
model initialize at round 2908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.        , 7.27571106, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.037312194264265}
done in step count: 0
reward sum = 0.9947701207230466
running average episode reward sum: 0.7597563230389403
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.        , 7.27571106, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.037312194264265}
episode index:2909
target Thresh 15.00685770520252
target distance 12.0
model initialize at round 2909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([1.13259695, 5.99505755, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 13.903768431532194}
done in step count: 10
reward sum = 0.8686662365268578
running average episode reward sum: 0.7597937491260495
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.40893759,  7.57707567,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8260575603708469}
episode index:2910
target Thresh 15.008853777290312
target distance 4.0
model initialize at round 2910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.91186379,  5.52521478,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 3.6412408389154063}
done in step count: 2
reward sum = 0.9714701194828683
running average episode reward sum: 0.7598664651584633
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.64027609,  2.32727797,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.4863251642492941}
episode index:2911
target Thresh 15.010848851591529
target distance 13.0
model initialize at round 2911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.648875396109553}
done in step count: 7
reward sum = 0.8883199436613667
running average episode reward sum: 0.7599105769299273
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.82735655, 11.86035728,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.1936220135971687}
episode index:2912
target Thresh 15.012842928604934
target distance 2.0
model initialize at round 2912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([ 9.93970788, 11.08734226,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 2.0882128388066787}
done in step count: 2
reward sum = 0.9750910723709094
running average episode reward sum: 0.759984445963721
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.6561591 ,  9.87694657,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.095253419225511}
episode index:2913
target Thresh 15.014836008829056
target distance 1.0
model initialize at round 2913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.86747647,  4.98925206,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.3157260582112342}
done in step count: 0
reward sum = 0.9940164292487637
running average episode reward sum: 0.7600647589298449
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.86747647,  4.98925206,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.3157260582112342}
episode index:2914
target Thresh 15.016828092762156
target distance 5.0
model initialize at round 2914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.80556554, 5.99999071, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.1062825942710814}
done in step count: 2
reward sum = 0.9692735887489768
running average episode reward sum: 0.7601365286827845
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.8262819 , 9.09817823, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.19954183312477786}
episode index:2915
target Thresh 15.018819180902257
target distance 9.0
model initialize at round 2915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.10558414, 4.        , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 7.000796241256387}
done in step count: 6
reward sum = 0.9175804832029754
running average episode reward sum: 0.7601905218084773
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.08718112, 10.51004002,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.0360014882534987}
episode index:2916
target Thresh 15.020809273747133
target distance 2.0
model initialize at round 2916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.        , 10.57803547,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5780354738234639}
done in step count: 0
reward sum = 0.9967494059133695
running average episode reward sum: 0.7602716184434121
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.        , 10.57803547,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5780354738234639}
episode index:2917
target Thresh 15.02279837179431
target distance 3.0
model initialize at round 2917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([11.82246697,  7.97232445,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 4.289195384140245}
done in step count: 7
reward sum = 0.9194424576704071
running average episode reward sum: 0.7603261663663823
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.8215283 ,  6.61861207,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9057403027335076}
episode index:2918
target Thresh 15.024786475541058
target distance 1.0
model initialize at round 2918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.69173646, 10.43314219,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8943305383970305}
done in step count: 0
reward sum = 0.9996877343450871
running average episode reward sum: 0.7604081675887114
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.69173646, 10.43314219,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8943305383970305}
episode index:2919
target Thresh 15.026773585484403
target distance 2.0
model initialize at round 2919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([16.,  7.,  0.]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9999999999999822}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7604881647916623
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([16.,  7.,  0.]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9999999999999822}
episode index:2920
target Thresh 15.028759702121125
target distance 12.0
model initialize at round 2920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([15.50046754,  2.94269037,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 14.096963686580587}
done in step count: 9
reward sum = 0.8803142167634788
running average episode reward sum: 0.7605291870621079
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.90564046, 6.16502104, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.2318175603491655}
episode index:2921
target Thresh 15.03074482594775
target distance 2.0
model initialize at round 2921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.50886935,  8.69909012,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5759826988564737}
done in step count: 0
reward sum = 0.9976539124812794
running average episode reward sum: 0.7606103385766252
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.50886935,  8.69909012,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5759826988564737}
episode index:2922
target Thresh 15.032728957460563
target distance 5.0
model initialize at round 2922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.44720155,  6.01397288,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.006041917832848}
done in step count: 11
reward sum = 0.8823272355859004
running average episode reward sum: 0.7606519796635253
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.65856501, 10.81301751,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.389281782098808}
episode index:2923
target Thresh 15.034712097155593
target distance 8.0
model initialize at round 2923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([2.82643412, 5.        , 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 6.002509901242512}
done in step count: 3
reward sum = 0.953933142591958
running average episode reward sum: 0.7607180812924339
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.74857794, 11.        ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.25142206251613297}
episode index:2924
target Thresh 15.036694245528627
target distance 10.0
model initialize at round 2924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 1.93509173, 11.        ,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 10.261694714049627}
done in step count: 24
reward sum = 0.7101126428000027
running average episode reward sum: 0.7607007802878212
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.63572806,  8.05332395,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.0143419489798402}
episode index:2925
target Thresh 15.038675403075203
target distance 1.0
model initialize at round 2925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([13.51109242,  9.        ,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.8120155394082715}
done in step count: 31
reward sum = 0.6746787655521458
running average episode reward sum: 0.7606713811030174
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.88184971, 10.51945802,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.5327251821541202}
episode index:2926
target Thresh 15.040655570290607
target distance 6.0
model initialize at round 2926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.11459514, 6.66321368, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 4.746525420853151}
done in step count: 2
reward sum = 0.9711729039960173
running average episode reward sum: 0.7607432982615049
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.1593706 , 2.84389967, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8588164174704992}
episode index:2927
target Thresh 15.042634747669883
target distance 11.0
model initialize at round 2927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([5.62117323, 7.67803784, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 9.527759574253258}
done in step count: 28
reward sum = 0.7042848174917923
running average episode reward sum: 0.7607240159934825
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.43900507,  6.55594649,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.7898049199617048}
episode index:2928
target Thresh 15.044612935707827
target distance 11.0
model initialize at round 2928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 6., 11.,  0.]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 9.48683298050515}
done in step count: 5
reward sum = 0.9213724221344112
running average episode reward sum: 0.7607788635203316
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.77308714,  8.8676528 ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.1621037440722}
episode index:2929
target Thresh 15.046590134898983
target distance 7.0
model initialize at round 2929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 7.38613118, 11.89866664,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 5.460587034696248}
done in step count: 3
reward sum = 0.9555185820247689
running average episode reward sum: 0.7608453275880807
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.40787329, 11.16446854,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.6145436823806373}
episode index:2930
target Thresh 15.048566345737655
target distance 7.0
model initialize at round 2930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([9.       , 9.1391629, 0.       ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 7.917658813624175}
done in step count: 5
reward sum = 0.9314949299923366
running average episode reward sum: 0.7609035499021046
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.37190908,  2.00089264,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0660824938580868}
episode index:2931
target Thresh 15.05054156871789
target distance 7.0
model initialize at round 2931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 9.65994419, 11.70340133,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.985363377309091}
done in step count: 9
reward sum = 0.900477268579973
running average episode reward sum: 0.7609511534896481
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.10833248,  9.35909813,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.37508318664246043}
episode index:2932
target Thresh 15.052515804333495
target distance 3.0
model initialize at round 2932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.        , 8.69817984, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.6981798410415028}
done in step count: 4
reward sum = 0.9549373940167736
running average episode reward sum: 0.761017292678372
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.62175222, 6.67371798, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.772636587084442}
episode index:2933
target Thresh 15.054489053078033
target distance 7.0
model initialize at round 2933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([1.74332631, 9.30407608, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.764182437684196}
done in step count: 7
reward sum = 0.9094367355039815
running average episode reward sum: 0.7610678787188716
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.13181113, 3.20551162, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.8053483522217012}
episode index:2934
target Thresh 15.056461315444814
target distance 11.0
model initialize at round 2934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([5.        , 8.98525286, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.21635659613136}
done in step count: 32
reward sum = 0.6639361255992761
running average episode reward sum: 0.7610347844247934
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.1068561 ,  6.34280308,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.1088795353182512}
episode index:2935
target Thresh 15.058432591926904
target distance 10.0
model initialize at round 2935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.25454724, 6.75430322, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.630135968240863}
done in step count: 9
reward sum = 0.875796518723365
running average episode reward sum: 0.7610738722089551
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.64702013, 11.86772428,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0823957136780389}
episode index:2936
target Thresh 15.06040288301712
target distance 8.0
model initialize at round 2936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([11.83804142, 11.87243578,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 6.893471884152815}
done in step count: 3
reward sum = 0.9541083385193828
running average episode reward sum: 0.7611395972570688
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.90033037, 11.2034607 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9230335991551083}
episode index:2937
target Thresh 15.062372189208036
target distance 3.0
model initialize at round 2937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([14.01775122, 10.62948954,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.196693202861137}
done in step count: 1
reward sum = 0.983865209876028
running average episode reward sum: 0.7612154058386273
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.05983043, 10.38529816,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.0160578230862276}
episode index:2938
target Thresh 15.064340510991979
target distance 4.0
model initialize at round 2938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([12.12397489,  7.84313009,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.204045868036734}
done in step count: 16
reward sum = 0.8128780877627678
running average episode reward sum: 0.7612329841584382
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.00507676,  8.75622769,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.24382517071402743}
episode index:2939
target Thresh 15.06630784886103
target distance 5.0
model initialize at round 2939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([14.35232544,  6.62542081,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.507315266865016}
done in step count: 21
reward sum = 0.7705900404559697
running average episode reward sum: 0.7612361668306482
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.1581753 , 10.14933111,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.21752978792723163}
episode index:2940
target Thresh 15.068274203307022
target distance 7.0
model initialize at round 2940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([10.97380131,  8.13224963,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 7.335854506752832}
done in step count: 6
reward sum = 0.9171503282287254
running average episode reward sum: 0.7612891808263633
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.99899114,  1.47155495,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.1301493134087905}
episode index:2941
target Thresh 15.070239574821546
target distance 1.0
model initialize at round 2941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.85653843,  1.12847354,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 2.636161267680752}
done in step count: 3
reward sum = 0.9615603275089502
running average episode reward sum: 0.7613572539557592
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.60951697,  3.2482627 ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.6581377553215603}
episode index:2942
target Thresh 15.072203963895943
target distance 2.0
model initialize at round 2942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.72339761,  7.        ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7233976125717057}
done in step count: 0
reward sum = 0.9947724256994002
running average episode reward sum: 0.7614365659407213
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.72339761,  7.        ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7233976125717057}
episode index:2943
target Thresh 15.074167371021307
target distance 12.0
model initialize at round 2943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([15.11426055, 10.        ,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 11.292775906267478}
done in step count: 41
reward sum = 0.592804699031968
running average episode reward sum: 0.7613792860946246
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.4593672 , 7.32119609, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8196297820161862}
episode index:2944
target Thresh 15.076129796688495
target distance 5.0
model initialize at round 2944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.06867671,  9.59608424,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.5967399587437017}
done in step count: 7
reward sum = 0.9223255695072972
running average episode reward sum: 0.7614339367850873
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.10642758,  5.52224033,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0132748746491012}
episode index:2945
target Thresh 15.078091241388114
target distance 1.0
model initialize at round 2945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.11628254, 5.24945824, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.1594263577221893}
done in step count: 0
reward sum = 0.9946600830117969
running average episode reward sum: 0.7615131038408329
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.11628254, 5.24945824, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.1594263577221893}
episode index:2946
target Thresh 15.08005170561052
target distance 1.0
model initialize at round 2946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([13.        ,  9.04443336,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.0004935198760165}
done in step count: 27
reward sum = 0.6818254735632847
running average episode reward sum: 0.7614860635862426
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.23323605,  9.91854979,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.196520236148876}
episode index:2947
target Thresh 15.082011189845835
target distance 14.0
model initialize at round 2947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 2.16183698, 10.        ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 15.082929281774634}
done in step count: 10
reward sum = 0.8743422846376567
running average episode reward sum: 0.7615243458864637
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.10338771,  4.89727673,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.268471175346206}
episode index:2948
target Thresh 15.083969694583924
target distance 8.0
model initialize at round 2948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.09042942,  3.5350697 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.752101626585663}
done in step count: 4
reward sum = 0.9399369110743488
running average episode reward sum: 0.7615848452303727
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.37085472, 10.15178838,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9257408841429213}
episode index:2949
target Thresh 15.085927220314419
target distance 1.0
model initialize at round 2949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.69876841, 11.20869946,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.36646410351388853}
done in step count: 0
reward sum = 0.9988362288702695
running average episode reward sum: 0.7616652694282168
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.69876841, 11.20869946,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.36646410351388853}
episode index:2950
target Thresh 15.087883767526698
target distance 3.0
model initialize at round 2950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.00567734, 6.        , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.4182337314584672}
done in step count: 1
reward sum = 0.9844333677039082
running average episode reward sum: 0.7617407584483034
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.36121488, 6.86744118, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.6523942641361719}
episode index:2951
target Thresh 15.089839336709897
target distance 8.0
model initialize at round 2951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([10.00002758,  8.1733224 ,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 8.608730477591997}
done in step count: 6
reward sum = 0.9127858434291493
running average episode reward sum: 0.761791925482511
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.56397492, 1.3042421 , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8956264629355707}
episode index:2952
target Thresh 15.09179392835291
target distance 7.0
model initialize at round 2952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([12.14612992,  7.82335934,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 6.111329500017496}
done in step count: 3
reward sum = 0.9548328261868952
running average episode reward sum: 0.7618572965968706
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.15392636,  2.50095586,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.5240707036651578}
episode index:2953
target Thresh 15.093747542944385
target distance 9.0
model initialize at round 2953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([9.02619884, 8.13224972, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 9.32587565885833}
done in step count: 6
reward sum = 0.9129846189635801
running average episode reward sum: 0.7619084568278681
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.45205882, 1.08916439, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.0168474222299428}
episode index:2954
target Thresh 15.095700180972726
target distance 4.0
model initialize at round 2954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([13.71608198, 11.39680845,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 2.3181325412209026}
done in step count: 1
reward sum = 0.9836894169990147
running average episode reward sum: 0.7619835096062678
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.71608198, 10.22831804,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8222545214364577}
episode index:2955
target Thresh 15.09765184292609
target distance 6.0
model initialize at round 2955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 6.00042438, 11.84182922,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.088040080258941}
done in step count: 2
reward sum = 0.9688759389086582
running average episode reward sum: 0.7620535002792389
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.05209145, 11.86042442,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8619998233003594}
episode index:2956
target Thresh 15.0996025292924
target distance 1.0
model initialize at round 2956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.08852573, 9.53018772, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0544592773119605}
done in step count: 0
reward sum = 0.9968405516950749
running average episode reward sum: 0.7621329007024434
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.08852573, 9.53018772, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0544592773119605}
episode index:2957
target Thresh 15.101552240559316
target distance 7.0
model initialize at round 2957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.53438812, 4.5220834 , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 5.497669022080537}
done in step count: 25
reward sum = 0.7082644769013936
running average episode reward sum: 0.7621146896058238
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.12123552, 9.67267377, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9377470235598875}
episode index:2958
target Thresh 15.103500977214274
target distance 11.0
model initialize at round 2958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.60722268, 4.        , 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.583469205273044}
done in step count: 23
reward sum = 0.7399517224240715
running average episode reward sum: 0.7621071995864991
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.05626268,  2.35478674,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.0082230713292801}
episode index:2959
target Thresh 15.105448739744457
target distance 5.0
model initialize at round 2959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([13.64178896,  8.        ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.815908713730087}
done in step count: 3
reward sum = 0.9600497524675796
running average episode reward sum: 0.7621740720705805
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.61213767,  4.45934224,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8167149643186052}
episode index:2960
target Thresh 15.107395528636802
target distance 9.0
model initialize at round 2960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([5.        , 9.78684556, 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.104346816107956}
done in step count: 10
reward sum = 0.8644203679326847
running average episode reward sum: 0.7622086030722226
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.35112651, 11.85040015,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0696808955863768}
episode index:2961
target Thresh 15.10934134437801
target distance 7.0
model initialize at round 2961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([10.54163476,  8.08960012,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 7.234543121924219}
done in step count: 5
reward sum = 0.9291463800656884
running average episode reward sum: 0.7622649628888982
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.45893208, 5.3735516 , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6574916622721435}
episode index:2962
target Thresh 15.111286187454537
target distance 2.0
model initialize at round 2962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.        ,  4.14627707,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8537229299544897}
done in step count: 0
reward sum = 0.9944227836778143
running average episode reward sum: 0.7623433151740109
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.        ,  4.14627707,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8537229299544897}
episode index:2963
target Thresh 15.113230058352585
target distance 5.0
model initialize at round 2963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([12.00007785,  8.00016787,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.6056477715799446}
done in step count: 2
reward sum = 0.9670950743157624
running average episode reward sum: 0.7624123947148819
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.3657491 ,  4.23825082,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9912295455904454}
episode index:2964
target Thresh 15.115172957558132
target distance 10.0
model initialize at round 2964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([12.7963877 ,  7.14244707,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.340846324213992}
done in step count: 12
reward sum = 0.831370961011322
running average episode reward sum: 0.7624356522414575
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.89313151, 3.31628926, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.1247863234827837}
episode index:2965
target Thresh 15.117114885556896
target distance 3.0
model initialize at round 2965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 2.236067977499782}
done in step count: 2
reward sum = 0.972912678845777
running average episode reward sum: 0.7625066155005958
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.65543252,  9.38963389,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7009090740494881}
episode index:2966
target Thresh 15.119055842834362
target distance 2.0
model initialize at round 2966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([5.33959031, 9.        , 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.6604096889495992}
done in step count: 1
reward sum = 0.9789922802680352
running average episode reward sum: 0.7625795799983266
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.1627668 , 8.12804552, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.8870161466144643}
episode index:2967
target Thresh 15.120995829875767
target distance 7.0
model initialize at round 2967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4., 6., 0.]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 13
reward sum = 0.8255144610764076
running average episode reward sum: 0.7626007844730833
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.36922528, 10.90885822,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.3803079460073733}
episode index:2968
target Thresh 15.12293484716611
target distance 2.0
model initialize at round 2968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.65297413, 7.32047963, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7273805104203332}
done in step count: 0
reward sum = 0.9973313911691168
running average episode reward sum: 0.7626798449670867
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.65297413, 7.32047963, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7273805104203332}
episode index:2969
target Thresh 15.124872895190144
target distance 9.0
model initialize at round 2969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 7.071067811865492}
done in step count: 15
reward sum = 0.8262087654932284
running average episode reward sum: 0.7627012351760182
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.14869953, 10.08586529,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.17171021371418443}
episode index:2970
target Thresh 15.126809974432383
target distance 7.0
model initialize at round 2970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.23190337, 8.11834994, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.3191747471893205}
done in step count: 18
reward sum = 0.8015581509626045
running average episode reward sum: 0.7627143139090328
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9505099 , 2.79158144, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.2142138294050066}
episode index:2971
target Thresh 15.128746085377097
target distance 11.0
model initialize at round 2971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.486832980505163}
done in step count: 5
reward sum = 0.9265371365844792
running average episode reward sum: 0.7627694359893409
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.748358  , 11.74361488,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7850393481580592}
episode index:2972
target Thresh 15.130681228508308
target distance 1.0
model initialize at round 2972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.89096911,  1.30411333,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1305238687226458}
done in step count: 0
reward sum = 0.9948013659305464
running average episode reward sum: 0.7628474823835356
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.89096911,  1.30411333,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1305238687226458}
episode index:2973
target Thresh 15.13261540430981
target distance 2.0
model initialize at round 2973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.,  8.,  0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.5121479338940402e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7629252068347198
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.,  8.,  0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.5121479338940402e-14}
episode index:2974
target Thresh 15.13454861326514
target distance 6.0
model initialize at round 2974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([16.        ,  4.50890422,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 5.400920413878244}
done in step count: 59
reward sum = 0.4855508084845379
running average episode reward sum: 0.7628319717428375
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.18564788,  9.89620551,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.9152319130918489}
episode index:2975
target Thresh 15.136480855857606
target distance 4.0
model initialize at round 2975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.18070918, 5.67013037, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.6119660523419594}
done in step count: 2
reward sum = 0.9753906876056465
running average episode reward sum: 0.7629033960425226
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.14427711, 7.62527233, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.40154291006054893}
episode index:2976
target Thresh 15.138412132570263
target distance 6.0
model initialize at round 2976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([10., 11.,  0.]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.0000000000000195}
done in step count: 29
reward sum = 0.7001801801493489
running average episode reward sum: 0.7628823267728239
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.86637513, 11.57517505,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5904929692313738}
episode index:2977
target Thresh 15.140342443885933
target distance 6.0
model initialize at round 2977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4.8674032 , 5.99505776, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 5.754816996627359}
done in step count: 3
reward sum = 0.9516686081868407
running average episode reward sum: 0.7629457204200415
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.12344863, 10.70754267,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.7182312916537721}
episode index:2978
target Thresh 15.142271790287195
target distance 11.0
model initialize at round 2978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 3.53736341, 10.        ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.5153292761986}
done in step count: 9
reward sum = 0.8761158533432089
running average episode reward sum: 0.7629837097228019
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.53034798, 11.8293764 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9844461325585233}
episode index:2979
target Thresh 15.144200172256385
target distance 8.0
model initialize at round 2979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([ 3.97380116, 11.86775028,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 6.6737593664686665}
done in step count: 3
reward sum = 0.9496194929955443
running average episode reward sum: 0.7630463391802761
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.92815701, 8.13080034, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.8721636650618246}
episode index:2980
target Thresh 15.146127590275597
target distance 6.0
model initialize at round 2980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.87030979, 4.86552162, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.225085865292418}
done in step count: 15
reward sum = 0.8293339459583566
running average episode reward sum: 0.7630685758816441
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.09938228, 9.76901072, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.1842676035356627}
episode index:2981
target Thresh 15.148054044826688
target distance 6.0
model initialize at round 2981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 4.123105625617684}
done in step count: 5
reward sum = 0.9318035775488194
running average episode reward sum: 0.7631251603892454
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.01572849, 9.45017211, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.0823332836954793}
episode index:2982
target Thresh 15.149979536391267
target distance 4.0
model initialize at round 2982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13666423, 3.9817063 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.195189771881736}
done in step count: 1
reward sum = 0.9810752207674094
running average episode reward sum: 0.7631982244389865
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.14193473, 5.95444674, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8592735925278141}
episode index:2983
target Thresh 15.151904065450713
target distance 10.0
model initialize at round 2983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([3.40354371, 4.        , 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 11.71686331280104}
done in step count: 12
reward sum = 0.8564736678634748
running average episode reward sum: 0.7632294829656033
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.42026952,  9.17882478,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.4567327141868333}
episode index:2984
target Thresh 15.153827632486156
target distance 6.0
model initialize at round 2984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.34727842, 6.        , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.220800768887588}
done in step count: 3
reward sum = 0.9606577176484568
running average episode reward sum: 0.7632956230777249
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.70603853, 9.63838935, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.46602103605337875}
episode index:2985
target Thresh 15.155750237978484
target distance 10.0
model initialize at round 2985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([12.69275765,  7.32083078,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 8.997237966189095}
done in step count: 48
reward sum = 0.524885032478959
running average episode reward sum: 0.7632157802811411
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.32454629, 5.03852325, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.32682462422089076}
episode index:2986
target Thresh 15.157671882408355
target distance 12.0
model initialize at round 2986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.16168332,  7.70215201,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.166047478699351}
done in step count: 6
reward sum = 0.9129793153387354
running average episode reward sum: 0.7632659187260885
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.85587189, 7.72292183, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.3123223135064116}
episode index:2987
target Thresh 15.159592566256174
target distance 5.0
model initialize at round 2987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([5.77995536, 7.74810208, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 3.454842384004621}
done in step count: 2
reward sum = 0.9680785128219593
running average episode reward sum: 0.7633344637709667
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.55026021, 8.08839056, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.0165124911596572}
episode index:2988
target Thresh 15.161512290002117
target distance 9.0
model initialize at round 2988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.54899311,  3.09394526,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.056368979421379}
done in step count: 8
reward sum = 0.893990619864321
running average episode reward sum: 0.7633781761015432
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.266061  , 11.21314192,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7642616941076958}
episode index:2989
target Thresh 15.163431054126109
target distance 8.0
model initialize at round 2989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([9.86445308, 8.61783433, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.87021863347053}
done in step count: 6
reward sum = 0.9185562858093003
running average episode reward sum: 0.7634300751348903
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.02694322, 3.048635  , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.05559946339006563}
episode index:2990
target Thresh 15.165348859107848
target distance 1.0
model initialize at round 2990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.00106939,  1.13347741,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.3223931919747052}
done in step count: 0
reward sum = 0.9942398237918549
running average episode reward sum: 0.7635072432220373
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.00106939,  1.13347741,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.3223931919747052}
episode index:2991
target Thresh 15.167265705426777
target distance 5.0
model initialize at round 2991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.23655608, 4.29215932, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.7153790246258542}
done in step count: 11
reward sum = 0.880821110186687
running average episode reward sum: 0.7635464524021727
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.10921101, 8.90506721, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.2699022270240954}
episode index:2992
target Thresh 15.169181593562117
target distance 3.0
model initialize at round 2992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 1.10354409, 11.52613566,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 4.643676348049298}
done in step count: 6
reward sum = 0.914297165233752
running average episode reward sum: 0.7635968201645621
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.13037636, 9.68340919, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.6957342261343575}
episode index:2993
target Thresh 15.171096523992833
target distance 4.0
model initialize at round 2993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([12.55594695,  8.9087339 ,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 3.557117960372545}
done in step count: 3
reward sum = 0.9653015512280826
running average episode reward sum: 0.7636641898142159
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.03202581, 8.89783089, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.10707091062819558}
episode index:2994
target Thresh 15.173010497197662
target distance 12.0
model initialize at round 2994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([12.        ,  9.97837663,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.650793410488495}
done in step count: 9
reward sum = 0.8836740006065361
running average episode reward sum: 0.7637042598679027
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.92859824, 3.04650324, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9561664539965951}
episode index:2995
target Thresh 15.174923513655093
target distance 2.0
model initialize at round 2995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.01925576, 8.13759418, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.8626207665511507}
done in step count: 0
reward sum = 0.9952439523377676
running average episode reward sum: 0.7637815428093145
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.01925576, 8.13759418, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.8626207665511507}
episode index:2996
target Thresh 15.176835573843384
target distance 4.0
model initialize at round 2996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([5.20319223, 9.        , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 2.796807765960666}
done in step count: 18
reward sum = 0.776480144973224
running average episode reward sum: 0.763785779913807
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.29635531, 9.69917772, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.9919502654074446}
episode index:2997
target Thresh 15.178746678240548
target distance 12.0
model initialize at round 2997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([12.        ,  9.67741251,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.039845460328692}
done in step count: 6
reward sum = 0.9142803500055203
running average episode reward sum: 0.7638359782360524
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.09949306, 5.60623596, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0855573605660012}
episode index:2998
target Thresh 15.180656827324364
target distance 7.0
model initialize at round 2998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([6.34054351, 8.03601074, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 8.657751232166108}
done in step count: 15
reward sum = 0.8337672859421356
running average episode reward sum: 0.7638592964446906
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.22898782,  4.64543223,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6848490251337382}
episode index:2999
target Thresh 15.182566021572365
target distance 5.0
model initialize at round 2999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([12.        ,  9.92182267,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.5627801453167707}
done in step count: 10
reward sum = 0.8874157148233998
running average episode reward sum: 0.7639004819174835
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.49045358,  8.12108401,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.505179227182912}
episode index:3000
target Thresh 15.18447426146185
target distance 11.0
model initialize at round 3000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 5.25136207, 11.86346073,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.894456927826983}
done in step count: 6
reward sum = 0.9191776893792727
running average episode reward sum: 0.7639522237393636
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.28895688,  7.08540719,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.30131456623415803}
episode index:3001
target Thresh 15.18638154746988
target distance 2.0
model initialize at round 3001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11., 11.,  0.]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9999999999999822}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7640288552438492
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11., 11.,  0.]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9999999999999822}
episode index:3002
target Thresh 15.18828788007328
target distance 9.0
model initialize at round 3002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([15.11324568, 11.87072332,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 8.606149420608226}
done in step count: 22
reward sum = 0.7630576339673336
running average episode reward sum: 0.7640285318268407
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.4852288 , 8.12323389, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.016714410428833}
episode index:3003
target Thresh 15.19019325974863
target distance 10.0
model initialize at round 3003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.08699531,  3.49311221,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.101121855132437}
done in step count: 51
reward sum = 0.4608426573797012
running average episode reward sum: 0.7639276044385427
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.27823419, 4.42872048, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.5110924736357109}
episode index:3004
target Thresh 15.192097686972275
target distance 2.0
model initialize at round 3004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.82432961, 3.77276599, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.28722008589734654}
done in step count: 0
reward sum = 0.9971333360460967
running average episode reward sum: 0.7640052103392442
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.82432961, 3.77276599, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.28722008589734654}
episode index:3005
target Thresh 15.19400116222032
target distance 8.0
model initialize at round 3005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([13.19175708,  2.85477081,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 8.832250096416132}
done in step count: 46
reward sum = 0.5469975636629203
running average episode reward sum: 0.7639330188400172
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.99195202, 10.65029971,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.1861106740108414}
episode index:3006
target Thresh 15.195903685968638
target distance 1.0
model initialize at round 3006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.3386687 , 3.69489205, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.7730274578735749}
done in step count: 0
reward sum = 0.9965014802403748
running average episode reward sum: 0.7640103611949891
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.3386687 , 3.69489205, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.7730274578735749}
episode index:3007
target Thresh 15.197805258692856
target distance 6.0
model initialize at round 3007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([9.73785923, 8.12644952, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 7.90103364671511}
done in step count: 12
reward sum = 0.8591908578081656
running average episode reward sum: 0.7640420036473206
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.2151173 , 3.49901715, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.5452148851322258}
episode index:3008
target Thresh 15.199705880868368
target distance 3.0
model initialize at round 3008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.        ,  8.73856866,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.0336084092408637}
done in step count: 1
reward sum = 0.9834533155555489
running average episode reward sum: 0.7641149219962432
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.77854896,  8.66007482,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.8495219879747934}
episode index:3009
target Thresh 15.201605552970332
target distance 12.0
model initialize at round 3009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.90192467, 5.61399533, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.671685327920132}
done in step count: 34
reward sum = 0.6165566626111324
running average episode reward sum: 0.7640658993187065
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.8927838 ,  1.65001667,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9589323442736392}
episode index:3010
target Thresh 15.203504275473666
target distance 10.0
model initialize at round 3010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([7.97091392, 8.14047535, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 9.033812011622937}
done in step count: 4
reward sum = 0.9375118624679359
running average episode reward sum: 0.764123503424701
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.13655451,  4.91183269,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9220010796348636}
episode index:3011
target Thresh 15.205402048853047
target distance 11.0
model initialize at round 3011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 3.97380116, 11.86775028,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.067814290739564}
done in step count: 5
reward sum = 0.9239495720380607
running average episode reward sum: 0.7641765665284903
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.87115725, 11.21303195,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8968263824025138}
episode index:3012
target Thresh 15.20729887358292
target distance 12.0
model initialize at round 3012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([16.43228173, 10.46658874,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 13.21030067827919}
done in step count: 10
reward sum = 0.8787235960287031
running average episode reward sum: 0.7642145841287226
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.86931083, 6.89507349, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9045640962249731}
episode index:3013
target Thresh 15.209194750137492
target distance 1.0
model initialize at round 3013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.74056661, 10.69002569,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.4042149895320862}
done in step count: 0
reward sum = 0.9984532690736685
running average episode reward sum: 0.7642923010115843
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.74056661, 10.69002569,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.4042149895320862}
episode index:3014
target Thresh 15.211089678990733
target distance 3.0
model initialize at round 3014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.94858241,  8.49892724,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.7756767819949326}
done in step count: 30
reward sum = 0.6802261819879034
running average episode reward sum: 0.7642644183850424
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.25393008,  9.97955873,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7463498976729547}
episode index:3015
target Thresh 15.212983660616374
target distance 2.0
model initialize at round 3015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.63941118, 5.12849462, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9431574221947219}
done in step count: 0
reward sum = 0.9989473567845224
running average episode reward sum: 0.7643422310304003
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.63941118, 5.12849462, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9431574221947219}
episode index:3016
target Thresh 15.21487669548791
target distance 3.0
model initialize at round 3016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.        , 9.56882954, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0889940168101675}
done in step count: 21
reward sum = 0.7562203243736197
running average episode reward sum: 0.764339538983116
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.43539955, 10.87322061,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9757494547735638}
episode index:3017
target Thresh 15.2167687840786
target distance 9.0
model initialize at round 3017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 3.67803347, 10.        ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.381833147230216}
done in step count: 10
reward sum = 0.8655967439941823
running average episode reward sum: 0.7643730900782157
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.23652042, 11.83054506,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.863566446944668}
episode index:3018
target Thresh 15.218659926861463
target distance 6.0
model initialize at round 3018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([11.        ,  9.86821175,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 5.564446253991678}
done in step count: 17
reward sum = 0.8176085782996577
running average episode reward sum: 0.7643907235622243
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.01183937,  6.72019175,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.2227581929841904}
episode index:3019
target Thresh 15.220550124309293
target distance 12.0
model initialize at round 3019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.03015494, 6.57962656, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.887408464225965}
done in step count: 26
reward sum = 0.6845241558721231
running average episode reward sum: 0.7643642776788832
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.13225467,  2.91990797,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.9293664343170608}
episode index:3020
target Thresh 15.222439376894634
target distance 6.0
model initialize at round 3020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([5.73467999, 7.79090139, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 4.433381802477642}
done in step count: 2
reward sum = 0.9689239566739806
running average episode reward sum: 0.7644319902505466
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.15773855, 8.70619381, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.8920349947935214}
episode index:3021
target Thresh 15.224327685089795
target distance 8.0
model initialize at round 3021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 7.306884440612533}
done in step count: 27
reward sum = 0.6913174028346928
running average episode reward sum: 0.7644077961448497
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.74479429, 10.83623028,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.1198212366084237}
episode index:3022
target Thresh 15.226215049366864
target distance 7.0
model initialize at round 3022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([4., 6., 0.]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 5.8309518948453265}
done in step count: 3
reward sum = 0.9489301108448667
running average episode reward sum: 0.7644688356138208
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.94477814, 9.1098115 , 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.9511384192288889}
episode index:3023
target Thresh 15.228101470197672
target distance 7.0
model initialize at round 3023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([16.,  5.,  0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 5.385164807134515}
done in step count: 16
reward sum = 0.7981897907729076
running average episode reward sum: 0.7644799867233312
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.92828158,  9.06483119,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.3176674078960315}
episode index:3024
target Thresh 15.229986948053828
target distance 2.0
model initialize at round 3024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.60047865,  9.        ,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.6004786491393279}
done in step count: 0
reward sum = 0.9967295690148312
running average episode reward sum: 0.7645567634447499
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.60047865,  9.        ,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.6004786491393279}
episode index:3025
target Thresh 15.231871483406701
target distance 10.0
model initialize at round 3025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 5.13710976, 10.66335475,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 8.887680405995274}
done in step count: 35
reward sum = 0.6467027443149511
running average episode reward sum: 0.7645178163135107
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.93504457, 10.21631713,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.22585904712966812}
episode index:3026
target Thresh 15.233755076727427
target distance 2.0
model initialize at round 3026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.18175387, 11.        ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.18175387382508257}
done in step count: 0
reward sum = 0.9945204860375525
running average episode reward sum: 0.7645938000167561
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.18175387, 11.        ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.18175387382508257}
episode index:3027
target Thresh 15.2356377284869
target distance 5.0
model initialize at round 3027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.85459549,  5.98695889,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.131892432253556}
done in step count: 21
reward sum = 0.7669989045825524
running average episode reward sum: 0.7645945943049218
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.85669182,  9.79665057,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.16986024806308}
episode index:3028
target Thresh 15.237519439155786
target distance 11.0
model initialize at round 3028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.00268177,  6.7756323 ,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 11.084236842978303}
done in step count: 56
reward sum = 0.44556969316350875
running average episode reward sum: 0.7644892707984374
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.39874861, 2.40884421, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.5710989761609072}
episode index:3029
target Thresh 15.239400209204515
target distance 8.0
model initialize at round 3029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.11810362, 9.        , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.001162259584231}
done in step count: 3
reward sum = 0.9556500011245666
running average episode reward sum: 0.76455236014838
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.89610051, 3.46600315, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0100272622964752}
episode index:3030
target Thresh 15.241280039103273
target distance 8.0
model initialize at round 3030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([16.88021579,  7.71142912,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 9.170375625649072}
done in step count: 21
reward sum = 0.7895227413165017
running average episode reward sum: 0.7645605984793493
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.86359572, 10.53231327,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.5495120998156191}
episode index:3031
target Thresh 15.243158929322025
target distance 3.0
model initialize at round 3031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.83824182, 5.        , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.5328672738032652}
done in step count: 1
reward sum = 0.984550292281144
running average episode reward sum: 0.7646331544469621
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.58393019, 3.82530713, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.45125567220255486}
episode index:3032
target Thresh 15.245036880330487
target distance 7.0
model initialize at round 3032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.51367235, 7.        , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 5.0235957820437545}
done in step count: 6
reward sum = 0.9275345037676961
running average episode reward sum: 0.764686864090655
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.001787  , 2.33836995, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.33837466663301735}
episode index:3033
target Thresh 15.246913892598151
target distance 4.0
model initialize at round 3033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.61282715,  8.        ,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 2.091783238828876}
done in step count: 1
reward sum = 0.9836053313455944
running average episode reward sum: 0.7647590191556699
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.96434668, 10.        ,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.03565332293499601}
episode index:3034
target Thresh 15.248789966594268
target distance 5.0
model initialize at round 3034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([5.81958067, 8.00340725, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.2207076765991354}
done in step count: 7
reward sum = 0.9159088924885155
running average episode reward sum: 0.7648088214203594
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.31682555, 10.66023291,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.7630000043333456}
episode index:3035
target Thresh 15.250665102787858
target distance 9.0
model initialize at round 3035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 5.76796937, 11.        ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.232030630111664}
done in step count: 9
reward sum = 0.8885653926506251
running average episode reward sum: 0.7648495844543615
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.57052554, 10.61776596,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.574935801085645}
episode index:3036
target Thresh 15.252539301647703
target distance 7.0
model initialize at round 3036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([13.09611538,  6.41627427,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 7.071471756082075}
done in step count: 4
reward sum = 0.943536828361609
running average episode reward sum: 0.7649084212156084
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.8086872 , 9.85420143, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.8217251457900858}
episode index:3037
target Thresh 15.254412563642353
target distance 12.0
model initialize at round 3037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.78128743,  9.        ,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.854421756162253}
done in step count: 45
reward sum = 0.5513672682773951
running average episode reward sum: 0.76483813117185
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.94074495, 2.16108563, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9544368142594829}
episode index:3038
target Thresh 15.256284889240126
target distance 3.0
model initialize at round 3038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([15.00481451,  9.63957906,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 2.1043627474397875}
done in step count: 2
reward sum = 0.9761114277103704
running average episode reward sum: 0.7649076518354033
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.51895261,  9.99275005,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.1031587611558429}
episode index:3039
target Thresh 15.258156278909102
target distance 2.0
model initialize at round 3039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.14815839, 9.        , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.14815838634964162}
done in step count: 0
reward sum = 0.9969835366064429
running average episode reward sum: 0.7649839925869727
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.14815839, 9.        , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.14815838634964162}
episode index:3040
target Thresh 15.260026733117128
target distance 7.0
model initialize at round 3040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 9.87031589, 11.87234242,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.2033297393963105}
done in step count: 3
reward sum = 0.9548366114373583
running average episode reward sum: 0.7650464235698238
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.61524463, 11.57320918,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.690366174148796}
episode index:3041
target Thresh 15.261896252331818
target distance 11.0
model initialize at round 3041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.01473025,  6.73313808,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.380985776843557}
done in step count: 10
reward sum = 0.8729779522866572
running average episode reward sum: 0.7650819040197637
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.54130251, 3.19934682, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9227398920285207}
episode index:3042
target Thresh 15.263764837020549
target distance 4.0
model initialize at round 3042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([7.00250483, 9.        , 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.6034674024610083}
done in step count: 33
reward sum = 0.6394116614356027
running average episode reward sum: 0.765040605878921
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.85900574, 10.61705901,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9404970342863029}
episode index:3043
target Thresh 15.265632487650473
target distance 10.0
model initialize at round 3043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([7., 9., 0.]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.630145812734677}
done in step count: 5
reward sum = 0.9184254423839897
running average episode reward sum: 0.7650909951156178
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.81592837,  2.96052469,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.2602963104520668}
episode index:3044
target Thresh 15.267499204688498
target distance 1.0
model initialize at round 3044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.41155803, 5.67931175, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.3833283996560404}
done in step count: 1
reward sum = 0.9870751764718682
running average episode reward sum: 0.7651638963246019
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.905289  , 6.47393182, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0470414962322763}
episode index:3045
target Thresh 15.269364988601307
target distance 10.0
model initialize at round 3045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([6.        , 8.88288212, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.553391220521652}
done in step count: 6
reward sum = 0.9123652592282161
running average episode reward sum: 0.7652122224450562
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.0349386 ,  2.70226932,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7031378931818575}
episode index:3046
target Thresh 15.27122983985534
target distance 6.0
model initialize at round 3046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([5.        , 7.63755393, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 4.645551789865586}
done in step count: 2
reward sum = 0.9684360722915533
running average episode reward sum: 0.765278918818488
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.29918657, 9.99861482, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.7008148021295508}
episode index:3047
target Thresh 15.273093758916815
target distance 6.0
model initialize at round 3047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([12.00530476,  7.90160517,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 5.291931813672721}
done in step count: 63
reward sum = 0.421020056498599
running average episode reward sum: 0.7651659729975169
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.09916244,  3.44065951,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0028405274070225}
episode index:3048
target Thresh 15.27495674625171
target distance 7.0
model initialize at round 3048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([11.00011672,  8.14858717,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 5.072087444610588}
done in step count: 3
reward sum = 0.9581599488261026
running average episode reward sum: 0.765229270464171
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.5170594 , 8.10901933, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.0301441589781295}
episode index:3049
target Thresh 15.276818802325772
target distance 3.0
model initialize at round 3049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([11.64400148,  8.75741887,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 4.03632210811143}
done in step count: 3
reward sum = 0.9653490703801588
running average episode reward sum: 0.7652948835133238
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.12557387, 10.5027262 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0059335393452455}
episode index:3050
target Thresh 15.278679927604516
target distance 2.0
model initialize at round 3050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([14.61914957,  9.        ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.6191495656967447}
done in step count: 12
reward sum = 0.8655468738470741
running average episode reward sum: 0.7653277422449966
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.41279343,  8.38708208,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.8488108955495887}
episode index:3051
target Thresh 15.280540122553225
target distance 12.0
model initialize at round 3051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.39360487,  6.73752642,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 11.422398121200787}
done in step count: 32
reward sum = 0.6452005207474141
running average episode reward sum: 0.7652883820806788
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.53682276, 1.11289627, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0007428173345985}
episode index:3052
target Thresh 15.282399387636941
target distance 3.0
model initialize at round 3052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([9., 9., 0.]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 2.236067977499781}
done in step count: 9
reward sum = 0.8792090094431907
running average episode reward sum: 0.765325696403431
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.753201  , 11.88364849,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9174662947334387}
episode index:3053
target Thresh 15.284257723320486
target distance 7.0
model initialize at round 3053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 9.00494245, 11.86740305,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 5.341969964170625}
done in step count: 15
reward sum = 0.8254204804613441
running average episode reward sum: 0.7653453738048906
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.26485078, 10.07954756,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.7394404542984556}
episode index:3054
target Thresh 15.286115130068445
target distance 11.0
model initialize at round 3054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([12.20080698, 10.75565708,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.231785667772641}
done in step count: 7
reward sum = 0.9038651456874275
running average episode reward sum: 0.7653907157924136
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.24579335, 10.20603782,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7818434997541185}
episode index:3055
target Thresh 15.287971608345165
target distance 7.0
model initialize at round 3055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([9.41391993, 8.35575092, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 5.904243510795758}
done in step count: 16
reward sum = 0.8347586735433348
running average episode reward sum: 0.7654134147314683
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.8092855 , 6.72558925, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0869327409054539}
episode index:3056
target Thresh 15.289827158614768
target distance 5.0
model initialize at round 3056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([14.62976897,  5.04073322,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 4.281581522914704}
done in step count: 3
reward sum = 0.9589242793402463
running average episode reward sum: 0.7654767156358219
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.20662618,  9.9626407 ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.2474450472171332}
episode index:3057
target Thresh 15.29168178134114
target distance 6.0
model initialize at round 3057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([11.00494245, 11.86740305,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 4.418909163121902}
done in step count: 2
reward sum = 0.9654502956332065
running average episode reward sum: 0.7655421092198629
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.02261248, 9.99011269, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.024679612467081447}
episode index:3058
target Thresh 15.29353547698794
target distance 7.0
model initialize at round 3058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.18047667,  9.        ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 5.32077674449903}
done in step count: 3
reward sum = 0.9544604312567732
running average episode reward sum: 0.7656038674160175
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.46143583,  3.        ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.135804280218908}
episode index:3059
target Thresh 15.295388246018588
target distance 3.0
model initialize at round 3059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([10.02012527,  9.        ,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 2.254086534729185}
done in step count: 8
reward sum = 0.911041410722209
running average episode reward sum: 0.7656513960249411
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.43204575, 10.08529844,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.4403854662090568}
episode index:3060
target Thresh 15.29724008889628
target distance 14.0
model initialize at round 3060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([15.24476445,  9.        ,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 14.15711076675376}
done in step count: 81
reward sum = 0.3371262636398391
running average episode reward sum: 0.7655114008820515
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.15106011, 4.77961877, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.1526076382321846}
episode index:3061
target Thresh 15.299091006083973
target distance 5.0
model initialize at round 3061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([8.8060354 , 9.38043851, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 5.713471427741805}
done in step count: 26
reward sum = 0.7082827308685959
running average episode reward sum: 0.7654927109179713
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.81178745,  7.68128833,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7068081427323257}
episode index:3062
target Thresh 15.300940998044398
target distance 2.0
model initialize at round 3062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.51765299, 10.19446611,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.957522562584104}
done in step count: 0
reward sum = 0.9972024848195022
running average episode reward sum: 0.7655683589016152
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.51765299, 10.19446611,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.957522562584104}
episode index:3063
target Thresh 15.302790065240057
target distance 2.0
model initialize at round 3063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.64402914, 8.        , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.3559708595275701}
done in step count: 0
reward sum = 0.9969049632859885
running average episode reward sum: 0.7656438604043516
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.64402914, 8.        , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.3559708595275701}
episode index:3064
target Thresh 15.304638208133209
target distance 13.0
model initialize at round 3064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.67050752, 5.89059827, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.524190887039046}
done in step count: 35
reward sum = 0.6345369548089497
running average episode reward sum: 0.765601084904973
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.04876531,  8.65952587,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.6613262624628486}
episode index:3065
target Thresh 15.306485427185896
target distance 1.0
model initialize at round 3065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([16.15002656,  3.79142392,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 2.1601199481625346}
done in step count: 17
reward sum = 0.8097303911985158
running average episode reward sum: 0.7656154780250949
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.27229085,  3.76767855,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.35793234149209485}
episode index:3066
target Thresh 15.30833172285992
target distance 7.0
model initialize at round 3066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([15.50411344,  3.25970507,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 6.725310170695409}
done in step count: 15
reward sum = 0.8369433800000781
running average episode reward sum: 0.7656387345956769
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.64191471,  8.03869555,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.0258320121709628}
episode index:3067
target Thresh 15.310177095616856
target distance 9.0
model initialize at round 3067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.29512501,  4.        ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 10.11033376684178}
done in step count: 39
reward sum = 0.589136868826631
running average episode reward sum: 0.7655812046524666
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.00382246, 10.08459931,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.352896200079419}
episode index:3068
target Thresh 15.312021545918046
target distance 13.0
model initialize at round 3068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.92223668, 7.09119666, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.273417565545103}
done in step count: 25
reward sum = 0.6985143795106612
running average episode reward sum: 0.7655593516628472
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.54891888,  5.10559972,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.46327689571604996}
episode index:3069
target Thresh 15.313865074224603
target distance 9.0
model initialize at round 3069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([14.18951535,  8.65840912,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 8.298676267793493}
done in step count: 11
reward sum = 0.8767688348957102
running average episode reward sum: 0.7655955762502195
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5.8884871 , 9.30314046, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.7057254021270957}
episode index:3070
target Thresh 15.31570768099741
target distance 1.0
model initialize at round 3070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.95873809, 4.27921081, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7219692567369403}
done in step count: 0
reward sum = 0.9969659938999347
running average episode reward sum: 0.7656709166662564
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.95873809, 4.27921081, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7219692567369403}
episode index:3071
target Thresh 15.317549366697119
target distance 2.0
model initialize at round 3071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.59079659, 10.24419153,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8594730235334205}
done in step count: 0
reward sum = 0.9969410154219135
running average episode reward sum: 0.7657461999015284
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.59079659, 10.24419153,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8594730235334205}
episode index:3072
target Thresh 15.31939013178415
target distance 12.0
model initialize at round 3072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.93076229,  5.55016315,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.801805465879148}
done in step count: 12
reward sum = 0.8529741187396005
running average episode reward sum: 0.7657745851663634
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.0723669, 9.9519   , 0.       ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9288793120782322}
episode index:3073
target Thresh 15.321229976718694
target distance 5.0
model initialize at round 3073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.84826171, 9.        , 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.7327542409799968}
done in step count: 2
reward sum = 0.9641038393952481
running average episode reward sum: 0.7658391034663727
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.83751322, 11.86914106,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.2069940249888356}
episode index:3074
target Thresh 15.323068901960712
target distance 7.0
model initialize at round 3074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 9.39708412, 10.20989585,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 6.457251563597573}
done in step count: 11
reward sum = 0.8765012795646089
running average episode reward sum: 0.765875091165917
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.33222502,  6.75765101,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.710391764204111}
episode index:3075
target Thresh 15.32490690796994
target distance 6.0
model initialize at round 3075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([7.        , 9.57032642, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.297199676309944}
done in step count: 2
reward sum = 0.9680582854830623
running average episode reward sum: 0.7659408204228471
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.        , 7.30403867, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6959613263606661}
episode index:3076
target Thresh 15.326743995205872
target distance 4.0
model initialize at round 3076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([4., 9., 0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.828427124746218}
done in step count: 1
reward sum = 0.979124955244381
running average episode reward sum: 0.7660101035345863
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.        , 7.39821458, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.39821457862859155}
episode index:3077
target Thresh 15.328580164127784
target distance 4.0
model initialize at round 3077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([13.09397612,  6.41991206,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 3.781659574339806}
done in step count: 26
reward sum = 0.7292683189023272
running average episode reward sum: 0.7659981666324966
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.65334928,  4.19331716,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.39691087727780294}
episode index:3078
target Thresh 15.330415415194718
target distance 1.0
model initialize at round 3078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.27616319, 11.75478385,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0457716725776505}
done in step count: 0
reward sum = 0.9973439427330342
running average episode reward sum: 0.7660733032924839
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.27616319, 11.75478385,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0457716725776505}
episode index:3079
target Thresh 15.33224974886549
target distance 11.0
model initialize at round 3079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([5.08757162, 9.80410838, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.984307343712745}
done in step count: 39
reward sum = 0.6002039364040742
running average episode reward sum: 0.7660194496019356
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.52898123, 10.03579702,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0997765848492889}
episode index:3080
target Thresh 15.334083165598678
target distance 12.0
model initialize at round 3080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.049875621120906}
done in step count: 17
reward sum = 0.7976492086382334
running average episode reward sum: 0.7660297156710808
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.8547469 , 8.30629896, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.100823963449607}
episode index:3081
target Thresh 15.335915665852637
target distance 1.0
model initialize at round 3081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([6.05670439, 8.00016984, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.4547455147494328}
done in step count: 21
reward sum = 0.7693623824447083
running average episode reward sum: 0.7660307970035836
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.54104857, 9.02642313, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.5416933928347512}
episode index:3082
target Thresh 15.337747250085494
target distance 1.0
model initialize at round 3082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 3.67698795, 11.8834029 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.5908367475154492}
done in step count: 1
reward sum = 0.9856165901249476
running average episode reward sum: 0.766102021717538
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.01474797, 11.84267267,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.2964639608972772}
episode index:3083
target Thresh 15.339577918755147
target distance 4.0
model initialize at round 3083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 9.02619884, 11.86775028,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.2041942457664434}
done in step count: 1
reward sum = 0.9807096547853595
running average episode reward sum: 0.7661716091471968
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.04579816, 11.84777931,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8490154514703243}
episode index:3084
target Thresh 15.341407672319257
target distance 3.0
model initialize at round 3084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 3.        , 10.22353673,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.438055698466715}
done in step count: 1
reward sum = 0.9827354880100952
running average episode reward sum: 0.7662418081354829
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.13172438, 8.92077654, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.26559550757087}
episode index:3085
target Thresh 15.343236511235268
target distance 3.0
model initialize at round 3085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.13259695, 9.00494245, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.3275154913209848}
done in step count: 1
reward sum = 0.9833429689530022
running average episode reward sum: 0.7663121584792345
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.34868954, 8.07809118, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6559752619603512}
episode index:3086
target Thresh 15.345064435960387
target distance 1.0
model initialize at round 3086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.87331462, 8.36211324, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.945412301886347}
done in step count: 0
reward sum = 0.9959765050526658
running average episode reward sum: 0.7663865557408391
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.87331462, 8.36211324, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.945412301886347}
episode index:3087
target Thresh 15.346891446951595
target distance 3.0
model initialize at round 3087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 9.01884404, 11.86269883,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.335025260708786}
done in step count: 1
reward sum = 0.9821565684085773
running average episode reward sum: 0.7664564294496047
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.01840356, 11.53669031,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.1187350292784928}
episode index:3088
target Thresh 15.348717544665648
target distance 8.0
model initialize at round 3088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.        ,  9.47044992,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 6.5472683025106475}
done in step count: 3
reward sum = 0.9570884716567061
running average episode reward sum: 0.7665181426390534
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.89797774,  3.92746758,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9330619819310956}
episode index:3089
target Thresh 15.350542729559066
target distance 11.0
model initialize at round 3089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([12.33607447, 11.        ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 9.33607447147365}
done in step count: 12
reward sum = 0.8602346105840818
running average episode reward sum: 0.7665484715930809
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.32074011, 11.87759392,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9343689345481193}
episode index:3090
target Thresh 15.35236700208815
target distance 5.0
model initialize at round 3090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.63403785,  7.60296261,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 4.617487748896414}
done in step count: 16
reward sum = 0.8281714479087059
running average episode reward sum: 0.7665684078519989
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.49658114,  2.22988194,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9163376334588347}
episode index:3091
target Thresh 15.354190362708964
target distance 5.0
model initialize at round 3091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([16.50496477,  2.63010752,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 4.621783121780599}
done in step count: 27
reward sum = 0.7205416397847484
running average episode reward sum: 0.7665535220925981
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.31860424,  7.21531895,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.3845398685025394}
episode index:3092
target Thresh 15.35601281187735
target distance 6.0
model initialize at round 3092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([4.        , 8.59557807, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 4.23938685955306}
done in step count: 40
reward sum = 0.5999136970973
running average episode reward sum: 0.7664996456538669
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.09309428, 9.08712109, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.9176134508332807}
episode index:3093
target Thresh 15.357834350048918
target distance 12.0
model initialize at round 3093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 10.00000000000002}
done in step count: 9
reward sum = 0.886701196061166
running average episode reward sum: 0.7665384955408764
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.24854319, 11.85895178,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.1412648646412134}
episode index:3094
target Thresh 15.359654977679057
target distance 13.0
model initialize at round 3094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.045361017187277}
done in step count: 7
reward sum = 0.9008613014296557
running average episode reward sum: 0.7665818954781587
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.18541246, 8.2255073 , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8452256526749959}
episode index:3095
target Thresh 15.361474695222922
target distance 2.0
model initialize at round 3095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([3.41586661, 4.        , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.4158666133880424}
done in step count: 1
reward sum = 0.985222686449563
running average episode reward sum: 0.7666525158886792
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.04712272, 3.41217625, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.5897095131105867}
episode index:3096
target Thresh 15.36329350313544
target distance 7.0
model initialize at round 3096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.12151434,  4.22511792,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.93006353537298}
done in step count: 12
reward sum = 0.8580282561900131
running average episode reward sum: 0.7666820204867746
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.16211636, 11.22954876,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.2810237533540908}
episode index:3097
target Thresh 15.365111401871314
target distance 2.0
model initialize at round 3097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.        , 10.35484654,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.3548465371131808}
done in step count: 0
reward sum = 0.9966878325768533
running average episode reward sum: 0.766756263808947
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.        , 10.35484654,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.3548465371131808}
episode index:3098
target Thresh 15.366928391885022
target distance 5.0
model initialize at round 3098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([ 5.91157389, 11.23620033,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 5.574218209625897}
done in step count: 6
reward sum = 0.9311898113478296
running average episode reward sum: 0.7668093240049905
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.59405287, 6.70613542, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8145061750451957}
episode index:3099
target Thresh 15.368744473630809
target distance 4.0
model initialize at round 3099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([13.49859238,  4.        ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.202661408556711}
done in step count: 5
reward sum = 0.9435959334400225
running average episode reward sum: 0.7668663519435179
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.84133775,  6.15147801,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.21936111056135937}
episode index:3100
target Thresh 15.370559647562693
target distance 13.0
model initialize at round 3100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.54597497,  4.        ,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.717915258471745}
done in step count: 17
reward sum = 0.7904304122946318
running average episode reward sum: 0.7668739508020639
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.12736997, 6.82729572, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.202456387749484}
episode index:3101
target Thresh 15.372373914134473
target distance 12.0
model initialize at round 3101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([2.12734509, 7.        , 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 13.908650343821165}
done in step count: 9
reward sum = 0.8709665675229747
running average episode reward sum: 0.7669075074160938
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.24720097,  5.6398532 ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8345130858293069}
episode index:3102
target Thresh 15.37418727379971
target distance 4.0
model initialize at round 3102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([16.83317016,  6.22520892,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 2.8830656541966477}
done in step count: 19
reward sum = 0.7774811592750364
running average episode reward sum: 0.7669109149738955
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.74307317,  4.83149573,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.1151425465164182}
episode index:3103
target Thresh 15.375999727011749
target distance 13.0
model initialize at round 3103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([15.4339838,  4.       ,  0.       ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 12.433983802795472}
done in step count: 29
reward sum = 0.6875475486599718
running average episode reward sum: 0.7668853468790778
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.90129601, 4.56907546, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.5775719530742637}
episode index:3104
target Thresh 15.3778112742237
target distance 11.0
model initialize at round 3104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.00000053,  8.18684008,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 9.54756300377218}
done in step count: 6
reward sum = 0.9199196943739043
running average episode reward sum: 0.766934633303392
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.5966445 , 4.56558434, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5928006660129433}
episode index:3105
target Thresh 15.37962191588845
target distance 11.0
model initialize at round 3105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.       ,  7.7009443,  0.       ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.585602145000296}
done in step count: 5
reward sum = 0.9287607327287656
running average episode reward sum: 0.7669867344300582
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.00209644, 11.86679204,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.3217942201939887}
episode index:3106
target Thresh 15.381431652458659
target distance 6.0
model initialize at round 3106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.40132606, 8.85802019, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.072244961300978}
done in step count: 27
reward sum = 0.7285061028931842
running average episode reward sum: 0.7669743492895571
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.87923021, 3.64524603, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0905907628301363}
episode index:3107
target Thresh 15.383240484386763
target distance 1.0
model initialize at round 3107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.43408442,  8.32251519,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.4699027716653716}
done in step count: 8
reward sum = 0.9143981011826238
running average episode reward sum: 0.7670217829291623
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.28163526,  7.91115684,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7238376937195891}
episode index:3108
target Thresh 15.385048412124972
target distance 1.0
model initialize at round 3108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.        , 6.40208447, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.8850289232979058}
done in step count: 2
reward sum = 0.9731667765214007
running average episode reward sum: 0.7670880888132383
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.43077722, 7.49881902, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7584173957964007}
episode index:3109
target Thresh 15.386855436125261
target distance 8.0
model initialize at round 3109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([5.        , 8.03688443, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 6.3129884169605335}
done in step count: 9
reward sum = 0.8882376859648707
running average episode reward sum: 0.7671270436676279
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.40358296, 10.78778438,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9880878088392654}
episode index:3110
target Thresh 15.388661556839393
target distance 2.0
model initialize at round 3110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.09668612,  7.79904807,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.9411980609410833}
done in step count: 36
reward sum = 0.6384891573959054
running average episode reward sum: 0.7670856942988487
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.17445752,  4.84263183,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8404077106503618}
episode index:3111
target Thresh 15.390466774718895
target distance 6.0
model initialize at round 3111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 2.7774303, 10.       ,  0.       ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.317446221046466}
done in step count: 5
reward sum = 0.9299871612558267
running average episode reward sum: 0.7671380405285906
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.39923487, 11.90158653,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0834099024107218}
episode index:3112
target Thresh 15.392271090215072
target distance 11.0
model initialize at round 3112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 5.93037534, 10.        ,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 9.912521955425971}
done in step count: 4
reward sum = 0.9337430380387618
running average episode reward sum: 0.7671915596411862
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.09567943,  6.30284516,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9536827974660613}
episode index:3113
target Thresh 15.394074503779002
target distance 5.0
model initialize at round 3113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.61048883,  6.68275428,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 3.372953571116328}
done in step count: 8
reward sum = 0.9086081628720476
running average episode reward sum: 0.7672369728085693
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.40109558,  9.41419335,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8377684266015729}
episode index:3114
target Thresh 15.395877015861538
target distance 7.0
model initialize at round 3114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.33950663, 3.67036211, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.370408874857973}
done in step count: 14
reward sum = 0.8357670817809401
running average episode reward sum: 0.7672589728435524
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.01380201, 8.28549327, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.2178285349741111}
episode index:3115
target Thresh 15.397678626913311
target distance 3.0
model initialize at round 3115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([5.50070965, 9.12636638, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 2.6475815278391344}
done in step count: 2
reward sum = 0.9755135346694189
running average episode reward sum: 0.7673258067850883
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.45359772, 9.05225289, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.0939744209789384}
episode index:3116
target Thresh 15.399479337384722
target distance 3.0
model initialize at round 3116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.13134749,  4.05831301,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.3691543405142232}
done in step count: 1
reward sum = 0.9817593047356793
running average episode reward sum: 0.7673946016192078
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.11326115,  2.58646732,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9784247829551966}
episode index:3117
target Thresh 15.40127914772595
target distance 8.0
model initialize at round 3117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 8.08981311, 10.08139414,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 8.901058065021566}
done in step count: 5
reward sum = 0.935869767105882
running average episode reward sum: 0.7674486347062786
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.5018149 ,  5.54875441,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.6721688590999234}
episode index:3118
target Thresh 15.403078058386944
target distance 9.0
model initialize at round 3118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([9.00494245, 8.13259695, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 7.6734856774484435}
done in step count: 3
reward sum = 0.9466566068092447
running average episode reward sum: 0.7675060915745385
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.83860685, 4.93900745, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8408219385472306}
episode index:3119
target Thresh 15.404876069817435
target distance 2.0
model initialize at round 3119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.54520059,  3.88176978,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 3.1512223397285997}
done in step count: 29
reward sum = 0.6746801780455876
running average episode reward sum: 0.7674763396791766
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.06678247,  6.29658618,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.168625672365517}
episode index:3120
target Thresh 15.406673182466923
target distance 8.0
model initialize at round 3120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([9.99505755, 8.13259695, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 8.583011083224644}
done in step count: 8
reward sum = 0.8974894082820871
running average episode reward sum: 0.7675179971827342
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.32291178,  1.84539956,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.3580130082311368}
episode index:3121
target Thresh 15.40846939678469
target distance 2.0
model initialize at round 3121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.43569028, 6.69974959, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.9369712755536688}
done in step count: 9
reward sum = 0.9075112059376064
running average episode reward sum: 0.7675628380567748
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.51646668, 7.16239572, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9840318926243425}
episode index:3122
target Thresh 15.410264713219785
target distance 4.0
model initialize at round 3122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([7.02319813, 9.85974464, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.025642152702555}
done in step count: 2
reward sum = 0.9731185800579688
running average episode reward sum: 0.7676286580189909
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.02319813, 10.62491634,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6253467767811098}
episode index:3123
target Thresh 15.41205913222104
target distance 14.0
model initialize at round 3123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 3.97380116, 11.86775028,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.170371793518292}
done in step count: 8
reward sum = 0.8913788854700605
running average episode reward sum: 0.7676682707678549
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.10983517,  9.18190835,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8254318370927302}
episode index:3124
target Thresh 15.413852654237061
target distance 7.0
model initialize at round 3124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([9.00494245, 8.13259695, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 5.440350974279132}
done in step count: 2
reward sum = 0.9622794006099732
running average episode reward sum: 0.7677305463294044
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.83860745, 6.93900719, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.2589666265368804}
episode index:3125
target Thresh 15.415645279716228
target distance 10.0
model initialize at round 3125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12.05577481, 11.        ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.05577480792995}
done in step count: 4
reward sum = 0.9396067243097266
running average episode reward sum: 0.7677855291118677
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.4014785 , 11.33569133,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.523329390372145}
episode index:3126
target Thresh 15.417437009106694
target distance 4.0
model initialize at round 3126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([11.38389182,  9.55025232,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 4.64878826523312}
done in step count: 22
reward sum = 0.7664141426933423
running average episode reward sum: 0.7677850905488942
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.86201397,  8.21499205,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.165892605867771}
episode index:3127
target Thresh 15.419227842856394
target distance 6.0
model initialize at round 3127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.73098302,  6.31979156,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 5.41872857174507}
done in step count: 19
reward sum = 0.7900432960017825
running average episode reward sum: 0.7677922063434763
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.55054309, 11.90009301,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0551137965960398}
episode index:3128
target Thresh 15.421017781413036
target distance 4.0
model initialize at round 3128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.        , 9.18259513, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 4.18259513378149}
done in step count: 8
reward sum = 0.9104690753848855
running average episode reward sum: 0.7678378045758322
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.1625284, 5.1790307, 0.       ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8563939911215964}
episode index:3129
target Thresh 15.422806825224104
target distance 6.0
model initialize at round 3129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([10.        ,  8.86089683,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 4.002417980805519}
done in step count: 7
reward sum = 0.9168841253137388
running average episode reward sum: 0.7678854232086557
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.84257781, 9.00805197, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.1576279753133221}
episode index:3130
target Thresh 15.424594974736861
target distance 3.0
model initialize at round 3130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.        , 9.22019684, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.0239563710100321}
done in step count: 1
reward sum = 0.9814547483736443
running average episode reward sum: 0.767953634427169
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([8.        , 8.70187616, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.0434930866564391}
episode index:3131
target Thresh 15.426382230398344
target distance 6.0
model initialize at round 3131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([10.        ,  9.58885765,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.241617937748262}
done in step count: 7
reward sum = 0.9167957878729887
running average episode reward sum: 0.7680011574646677
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.55309579, 11.89296698,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.99855565442694}
episode index:3132
target Thresh 15.428168592655364
target distance 6.0
model initialize at round 3132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([12.,  9.,  0.]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 4.0000000000000195}
done in step count: 2
reward sum = 0.9665855188656436
running average episode reward sum: 0.7680645421954052
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.        , 9.51547062, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.5154706239700175}
episode index:3133
target Thresh 15.429954061954515
target distance 9.0
model initialize at round 3133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 9.560133908571554}
done in step count: 5
reward sum = 0.9200965287630669
running average episode reward sum: 0.7681130527207939
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.93900745, 9.91789349, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.10228182066500857}
episode index:3134
target Thresh 15.43173863874216
target distance 4.0
model initialize at round 3134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.51889002, 4.66685975, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.367683293114032}
done in step count: 6
reward sum = 0.9353358356271415
running average episode reward sum: 0.7681663933214019
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.17920641, 7.08376378, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.2301182590388675}
episode index:3135
target Thresh 15.43352232346445
target distance 5.0
model initialize at round 3135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([11.87063313,  9.39349294,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.1540091515363375}
done in step count: 4
reward sum = 0.9489077095816684
running average episode reward sum: 0.7682240276696992
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.15607147,  8.07312293,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.2535216228249595}
episode index:3136
target Thresh 15.4353051165673
target distance 14.0
model initialize at round 3136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([2.72763121, 7.        , 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.607195642885673}
done in step count: 7
reward sum = 0.9015786232183961
running average episode reward sum: 0.7682665379009866
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.7450645 ,  4.23945605,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7825984369042144}
episode index:3137
target Thresh 15.437087018496413
target distance 2.0
model initialize at round 3137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.51979506, 10.98135531,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.4805667578320217}
done in step count: 0
reward sum = 0.9954124185575153
running average episode reward sum: 0.7683389234588759
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.51979506, 10.98135531,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.4805667578320217}
episode index:3138
target Thresh 15.438868029697257
target distance 5.0
model initialize at round 3138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.42246425,  7.        ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 3.029599980824356}
done in step count: 2
reward sum = 0.9700665596640228
running average episode reward sum: 0.7684031883955453
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.82943602,  3.78419631,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.857050379023971}
episode index:3139
target Thresh 15.440648150615093
target distance 8.0
model initialize at round 3139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([4.86627595, 3.97860713, 0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 9.335265976308802}
done in step count: 8
reward sum = 0.9014818715005468
running average episode reward sum: 0.7684455701417571
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.51672404, 10.75629008,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.8975134243768212}
episode index:3140
target Thresh 15.44242738169495
target distance 8.0
model initialize at round 3140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.41571714,  4.75379586,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 6.273472133860685}
done in step count: 4
reward sum = 0.94766776591702
running average episode reward sum: 0.7685026291025261
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.7914703 , 11.07919123,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.22306027272060772}
episode index:3141
target Thresh 15.444205723381632
target distance 2.0
model initialize at round 3141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.44826138, 10.21595645,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.5924969980244735}
done in step count: 0
reward sum = 0.9979658608264009
running average episode reward sum: 0.7685756600483326
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.44826138, 10.21595645,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.5924969980244735}
episode index:3142
target Thresh 15.445983176119725
target distance 8.0
model initialize at round 3142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([11.80289543,  8.43964195,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 6.9795489412748735}
done in step count: 6
reward sum = 0.9247864666338237
running average episode reward sum: 0.7686253612276471
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.72387312, 10.1718977 ,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.7440034407162709}
episode index:3143
target Thresh 15.447759740353597
target distance 8.0
model initialize at round 3143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4., 4., 0.]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 7.810249675906681}
done in step count: 4
reward sum = 0.9368200521906106
running average episode reward sum: 0.7686788582667575
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.22103017, 10.0211637 ,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.779257276665982}
episode index:3144
target Thresh 15.449535416527382
target distance 12.0
model initialize at round 3144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([5.17664066, 7.14612992, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.271346008148733}
done in step count: 10
reward sum = 0.8744818801303438
running average episode reward sum: 0.7687124999271274
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.2495738,  4.2219909,  0.       ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.3340165334805208}
episode index:3145
target Thresh 15.451310205085006
target distance 8.0
model initialize at round 3145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([13.5516603 ,  5.92068636,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 8.583028196125076}
done in step count: 4
reward sum = 0.9415130834431488
running average episode reward sum: 0.7687674270038967
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.28916441, 10.9411103 ,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.9845327128372818}
episode index:3146
target Thresh 15.45308410647016
target distance 6.0
model initialize at round 3146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([13.5962956 ,  8.63020587,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.216953254666258}
done in step count: 2
reward sum = 0.9690469324959337
running average episode reward sum: 0.7688310684101541
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.81868207,  4.63020587,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6557710181633997}
episode index:3147
target Thresh 15.454857121126322
target distance 5.0
model initialize at round 3147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.21874678, 4.        , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.23810804591958}
done in step count: 34
reward sum = 0.643181923067884
running average episode reward sum: 0.7687911544503885
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.25194373, 6.88961566, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.27506425756126396}
episode index:3148
target Thresh 15.456629249496748
target distance 10.0
model initialize at round 3148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.06225774829857}
done in step count: 4
reward sum = 0.941931874323265
running average episode reward sum: 0.76884613721313
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.02514195, 11.00053757,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.025147700854711217}
episode index:3149
target Thresh 15.458400492024467
target distance 4.0
model initialize at round 3149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([6.        , 9.53992248, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 2.0715975187980225}
done in step count: 1
reward sum = 0.9811710271809181
running average episode reward sum: 0.768913541940104
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.99999797, 8.23730547, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.7626945334880945}
episode index:3150
target Thresh 15.46017084915229
target distance 9.0
model initialize at round 3150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([4.        , 5.55038333, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 7.803835926910606}
done in step count: 5
reward sum = 0.9322077151349155
running average episode reward sum: 0.7689653649084298
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.99390525,  8.43418182,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.1436773422232207}
episode index:3151
target Thresh 15.461940321322807
target distance 5.0
model initialize at round 3151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 9.       , 10.8888467,  0.       ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 3.1289053114411858}
done in step count: 39
reward sum = 0.5905742740870872
running average episode reward sum: 0.7689087687501743
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.00990067, 10.28009252,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.2802674470865327}
episode index:3152
target Thresh 15.463708908978386
target distance 9.0
model initialize at round 3152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([3.91081655, 4.        , 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 9.962756746209411}
done in step count: 10
reward sum = 0.8729463350413127
running average episode reward sum: 0.7689417651238791
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.76118511, 10.75797963,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7987343945517236}
episode index:3153
target Thresh 15.465476612561174
target distance 6.0
model initialize at round 3153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([5.07683337, 8.65261602, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 5.577969114056436}
done in step count: 8
reward sum = 0.9076719446197407
running average episode reward sum: 0.7689857505961353
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.95569557, 4.95952829, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9605505840866417}
episode index:3154
target Thresh 15.467243432513097
target distance 7.0
model initialize at round 3154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.72438788,  4.        ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 5.052201283271374}
done in step count: 4
reward sum = 0.9457172276744694
running average episode reward sum: 0.7690417669121664
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.20882726,  8.07374871,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.9495000156329022}
episode index:3155
target Thresh 15.469009369275858
target distance 11.0
model initialize at round 3155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.45113909,  5.99201918,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.451142139650717}
done in step count: 7
reward sum = 0.9043870311412653
running average episode reward sum: 0.7690846519768777
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.68592776, 6.58344162, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.6626050866082788}
episode index:3156
target Thresh 15.470774423290946
target distance 8.0
model initialize at round 3156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([12.        , 11.29953873,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 6.007472301368838}
done in step count: 24
reward sum = 0.7348330963177117
running average episode reward sum: 0.7690738025769224
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.0649203 , 11.05048784,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.08224151855588732}
episode index:3157
target Thresh 15.47253859499962
target distance 4.0
model initialize at round 3157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([12.82335934,  7.14612992,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 2.6002833598947155}
done in step count: 1
reward sum = 0.9792980763064271
running average episode reward sum: 0.7691403713779766
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.95943606,  8.7921936 ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.2117284418797354}
episode index:3158
target Thresh 15.474301884842923
target distance 12.0
model initialize at round 3158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 3.99505755, 11.86740305,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.042472879490532}
done in step count: 6
reward sum = 0.9131100071057487
running average episode reward sum: 0.7691859458115721
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.65457598, 10.95703679,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.6559844135808239}
episode index:3159
target Thresh 15.47606429326168
target distance 5.0
model initialize at round 3159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([6.44028351, 8.06334904, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 4.739808968328031}
done in step count: 2
reward sum = 0.9689010829964219
running average episode reward sum: 0.7692491468043521
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.47722608, 4.94181636, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0558232755210744}
episode index:3160
target Thresh 15.477825820696493
target distance 11.0
model initialize at round 3160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([14.01822155, 11.86407418,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 9.208859460175649}
done in step count: 12
reward sum = 0.8634040968558613
running average episode reward sum: 0.7692789332485317
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.48301482, 9.09458065, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.0262005229523783}
episode index:3161
target Thresh 15.479586467587742
target distance 1.0
model initialize at round 3161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.34692597,  4.11368525,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6628951800310123}
done in step count: 0
reward sum = 0.9993205664361828
running average episode reward sum: 0.7693516851881863
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.34692597,  4.11368525,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6628951800310123}
episode index:3162
target Thresh 15.48134623437559
target distance 3.0
model initialize at round 3162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.        , 9.29390007, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0422942255713556}
done in step count: 1
reward sum = 0.9859341789769803
running average episode reward sum: 0.769420158945312
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.60566282, 8.41280204, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8435809969290905}
episode index:3163
target Thresh 15.483105121499975
target distance 8.0
model initialize at round 3163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([12.00879003,  7.93947495,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 6.35226897333533}
done in step count: 31
reward sum = 0.6886045346236036
running average episode reward sum: 0.7693946167125935
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5.37893275, 9.90585836, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.6281617470636276}
episode index:3164
target Thresh 15.484863129400626
target distance 7.0
model initialize at round 3164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([9.00494245, 8.13259695, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 5.90445701395561}
done in step count: 27
reward sum = 0.7102458704084957
running average episode reward sum: 0.7693759283251356
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.66812217, 4.45687884, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6364931208626209}
episode index:3165
target Thresh 15.48662025851704
target distance 1.0
model initialize at round 3165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.12920126, 6.11458119, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8783047874179162}
done in step count: 0
reward sum = 0.9964004220366928
running average episode reward sum: 0.7694476353667375
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.12920126, 6.11458119, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8783047874179162}
episode index:3166
target Thresh 15.4883765092885
target distance 4.0
model initialize at round 3166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.23059326, 6.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.142892140366341}
done in step count: 31
reward sum = 0.6755014836862263
running average episode reward sum: 0.7694179712834788
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.22455496, 7.75513914, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.33223752095076325}
episode index:3167
target Thresh 15.490131882154069
target distance 2.0
model initialize at round 3167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.97810119,  8.        ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.978101193904882}
done in step count: 0
reward sum = 0.9962824881982404
running average episode reward sum: 0.7694895825577573
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.97810119,  8.        ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.978101193904882}
episode index:3168
target Thresh 15.491886377552591
target distance 10.0
model initialize at round 3168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.10069897,  3.52636279,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.227811496230826}
done in step count: 35
reward sum = 0.6194020949872291
running average episode reward sum: 0.7694422214067412
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.52374355, 2.52107072, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7059283984161191}
episode index:3169
target Thresh 15.493639995922688
target distance 13.0
model initialize at round 3169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.37863183, 9.99742419, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 14.10726733768522}
done in step count: 21
reward sum = 0.760112113325358
running average episode reward sum: 0.7694392781549805
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.80709552,  1.85947345,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8192379951221925}
episode index:3170
target Thresh 15.495392737702765
target distance 1.0
model initialize at round 3170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.4928194, 8.       , 0.       ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.114841226218971}
done in step count: 0
reward sum = 0.9968178467946739
running average episode reward sum: 0.7695109837899977
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.4928194, 8.       , 0.       ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.114841226218971}
episode index:3171
target Thresh 15.497144603331009
target distance 13.0
model initialize at round 3171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.96653697, 11.86168825,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.110486790188142}
done in step count: 82
reward sum = 0.3139243380452939
running average episode reward sum: 0.7693673562219824
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.16744632, 9.41734352, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6062399237776569}
episode index:3172
target Thresh 15.498895593245388
target distance 5.0
model initialize at round 3172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 3.17117834, 10.83607399,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 5.894633577538658}
done in step count: 6
reward sum = 0.926542455053678
running average episode reward sum: 0.7694168913933759
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.78021974, 5.99480231, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.2642683530648582}
episode index:3173
target Thresh 15.500645707883645
target distance 14.0
model initialize at round 3173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([15.90942419,  4.        ,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.945324706840015}
done in step count: 9
reward sum = 0.8800829808211338
running average episode reward sum: 0.7694517578361697
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.90448208, 3.30547236, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9546733412960846}
episode index:3174
target Thresh 15.50239494768331
target distance 3.0
model initialize at round 3174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.40584624, 11.41645122,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.441623995755554}
done in step count: 19
reward sum = 0.7479124385477647
running average episode reward sum: 0.7694449737985986
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.87723102, 10.97339787,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.12561805120915218}
episode index:3175
target Thresh 15.504143313081695
target distance 5.0
model initialize at round 3175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([8.        , 9.87174904, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 3.0027401332536807}
done in step count: 2
reward sum = 0.9728960978173895
running average episode reward sum: 0.7695090327167404
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.68797374, 10.32994163,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.45411657952713036}
episode index:3176
target Thresh 15.50589080451589
target distance 9.0
model initialize at round 3176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([3.39729929, 5.        , 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 9.685094635074298}
done in step count: 8
reward sum = 0.8862526434181055
running average episode reward sum: 0.7695457792105086
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.33995993, 11.88178635,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.1014536134079929}
episode index:3177
target Thresh 15.507637422422766
target distance 4.0
model initialize at round 3177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.9070844 , 5.93281555, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 2.9342870292002075}
done in step count: 2
reward sum = 0.9754342622011148
running average episode reward sum: 0.7696105647621103
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.24354863, 2.89057899, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.26699980236720605}
episode index:3178
target Thresh 15.509383167238981
target distance 8.0
model initialize at round 3178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4., 8., 0.]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.708203932499394}
done in step count: 3
reward sum = 0.9536000272230051
running average episode reward sum: 0.769668441283803
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.10773284, 11.86071732,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.239747953837396}
episode index:3179
target Thresh 15.511128039400969
target distance 5.0
model initialize at round 3179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.5651058,  4.       ,  0.       ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 3.031358271269556}
done in step count: 2
reward sum = 0.973789097429154
running average episode reward sum: 0.7697326301693833
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.48416805,  6.2065137 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.946415924219808}
episode index:3180
target Thresh 15.51287203934495
target distance 1.0
model initialize at round 3180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.18313003,  6.60421878,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.4360956084281478}
done in step count: 0
reward sum = 0.9986763423538351
running average episode reward sum: 0.7698046024146472
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.18313003,  6.60421878,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.4360956084281478}
episode index:3181
target Thresh 15.514615167506923
target distance 6.0
model initialize at round 3181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 7.2867893 , 10.78113413,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.7182896679391435}
done in step count: 2
reward sum = 0.9679575195621989
running average episode reward sum: 0.7698668754872895
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.27837857, 10.82493953,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.742552121802983}
episode index:3182
target Thresh 15.516357424322667
target distance 3.0
model initialize at round 3182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([8.44704747, 8.10142553, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.239703744843956}
done in step count: 13
reward sum = 0.8509800477049201
running average episode reward sum: 0.7698923587333523
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.84013231, 11.25852394,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8790090534078027}
episode index:3183
target Thresh 15.518098810227752
target distance 10.0
model initialize at round 3183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.1271905,  5.6698597,  0.       ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.837351093824775}
done in step count: 15
reward sum = 0.833946723274917
running average episode reward sum: 0.7699124763101555
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.74154403, 2.34936074, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8197197516088368}
episode index:3184
target Thresh 15.51983932565752
target distance 8.0
model initialize at round 3184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([6., 9., 0.]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 6.324555320336782}
done in step count: 4
reward sum = 0.9413372721113753
running average episode reward sum: 0.7699662988520083
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.36461505, 10.96907645,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.636137011703704}
episode index:3185
target Thresh 15.5215789710471
target distance 11.0
model initialize at round 3185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 5.        , 10.17115009,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.258179772826306}
done in step count: 18
reward sum = 0.8012138151584803
running average episode reward sum: 0.7699761066097944
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.44184361,  8.97838701,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.0735301205793961}
episode index:3186
target Thresh 15.523317746831404
target distance 8.0
model initialize at round 3186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([13.55740058,  3.8199048 ,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 7.67876791639387}
done in step count: 7
reward sum = 0.9134325854933574
running average episode reward sum: 0.7700211196248191
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.70416148, 9.24655235, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.8094465944319109}
episode index:3187
target Thresh 15.525055653445131
target distance 5.0
model initialize at round 3187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([14.97725892,  8.87461913,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 3.9792347110379658}
done in step count: 3
reward sum = 0.9629071575305249
running average episode reward sum: 0.7700816234008246
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.52905766,  8.09998921,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.0439930196534037}
episode index:3188
target Thresh 15.52679269132275
target distance 8.0
model initialize at round 3188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([8.83360457, 9.        , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.368449218831937}
done in step count: 22
reward sum = 0.7574044560552579
running average episode reward sum: 0.7700776481210048
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.07256129, 2.56589593, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.4401266708025818}
episode index:3189
target Thresh 15.528528860898525
target distance 5.0
model initialize at round 3189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.77971077,  6.18169188,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 3.82465739722179}
done in step count: 36
reward sum = 0.6402618701303963
running average episode reward sum: 0.7700369535197538
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.22144602,  9.54901784,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5024174015872049}
episode index:3190
target Thresh 15.530264162606496
target distance 10.0
model initialize at round 3190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([5.67220894, 7.69321475, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.109991171526234}
done in step count: 7
reward sum = 0.9030265729736253
running average episode reward sum: 0.7700786299909081
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.59065881,  4.62778761,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.7494514660598152}
episode index:3191
target Thresh 15.53199859688049
target distance 8.0
model initialize at round 3191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([4.10297048, 8.        , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 6.357867947953667}
done in step count: 17
reward sum = 0.801730829126563
running average episode reward sum: 0.7700885460933942
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.1327346 , 2.97567016, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.3054047382364315}
episode index:3192
target Thresh 15.533732164154117
target distance 11.0
model initialize at round 3192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([3.18533278, 3.99831045, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.814667345691086}
done in step count: 20
reward sum = 0.7704934557597807
running average episode reward sum: 0.7700886729050654
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.22621848,  3.75124844,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.336232271024559}
episode index:3193
target Thresh 15.535464864860765
target distance 5.0
model initialize at round 3193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([12.27467942, 11.27637017,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 3.937909878492256}
done in step count: 2
reward sum = 0.971183982042125
running average episode reward sum: 0.7701516332397983
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.36297798,  9.32028139,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9315655884990954}
episode index:3194
target Thresh 15.537196699433611
target distance 14.0
model initialize at round 3194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([2.55117714, 8.70082545, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.511428152300821}
done in step count: 26
reward sum = 0.7313867927037135
running average episode reward sum: 0.7701395002693645
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.20839428,  9.83098722,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.809447302840787}
episode index:3195
target Thresh 15.538927668305613
target distance 4.0
model initialize at round 3195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2., 6., 0.]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 2.0000000000000187}
done in step count: 2
reward sum = 0.9693527135890551
running average episode reward sum: 0.7702018323135822
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.29600912, 3.13125557, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9177898947101121}
episode index:3196
target Thresh 15.540657771909517
target distance 6.0
model initialize at round 3196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.9073381 ,  7.41314541,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 4.505453898244105}
done in step count: 3
reward sum = 0.9579527769642345
running average episode reward sum: 0.7702605595405608
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.27822785,  2.14534199,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8988053442883672}
episode index:3197
target Thresh 15.542387010677844
target distance 3.0
model initialize at round 3197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.        ,  8.94739735,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.283238409276259}
done in step count: 6
reward sum = 0.9338504681394434
running average episode reward sum: 0.770311713358134
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.96781191, 10.88023508,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9751941009969713}
episode index:3198
target Thresh 15.544115385042904
target distance 1.0
model initialize at round 3198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.87863195,  5.49897277,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0114456901841649}
done in step count: 0
reward sum = 0.9998006811167396
running average episode reward sum: 0.7703834510785962
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.87863195,  5.49897277,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0114456901841649}
episode index:3199
target Thresh 15.545842895436795
target distance 8.0
model initialize at round 3199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([15.0874157 ,  6.01885557,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.498320506380601}
done in step count: 6
reward sum = 0.9153938859469593
running average episode reward sum: 0.7704287668394925
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.39221288, 11.90484715,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0900245663633636}
episode index:3200
target Thresh 15.547569542291393
target distance 6.0
model initialize at round 3200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([2., 5., 0.]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.472135954999588}
done in step count: 14
reward sum = 0.8307043365359947
running average episode reward sum: 0.7704475970705754
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.06349215, 8.25722254, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.1953097107725925}
episode index:3201
target Thresh 15.549295326038356
target distance 9.0
model initialize at round 3201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 6.97266948, 11.85205879,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 8.518936240568324}
done in step count: 10
reward sum = 0.8666106135678107
running average episode reward sum: 0.770477629243123
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.79674517,  9.63995569,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.6714579746190104}
episode index:3202
target Thresh 15.551020247109133
target distance 4.0
model initialize at round 3202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.1414358,  9.       ,  0.       ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.302797361916221}
done in step count: 1
reward sum = 0.9839430355310499
running average episode reward sum: 0.7705442747024699
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.83011955, 11.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8301195502281082}
episode index:3203
target Thresh 15.552744305934954
target distance 2.0
model initialize at round 3203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.95276546, 10.        ,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.04723453521728782}
done in step count: 0
reward sum = 0.9963191780806573
running average episode reward sum: 0.7706147412765579
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.95276546, 10.        ,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.04723453521728782}
episode index:3204
target Thresh 15.554467502946833
target distance 1.0
model initialize at round 3204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.33680406, 8.92807124, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.038942753814979}
done in step count: 1
reward sum = 0.9834822776073056
running average episode reward sum: 0.7706811586045863
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.44862457, 7.9316961 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.0340800858299553}
episode index:3205
target Thresh 15.556189838575571
target distance 9.0
model initialize at round 3205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([8.2989521 , 9.48070121, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 9.167321525472108}
done in step count: 8
reward sum = 0.9029335129356352
running average episode reward sum: 0.7707224101187258
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.68731925, 1.89518656, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6952651379795578}
episode index:3206
target Thresh 15.55791131325175
target distance 12.0
model initialize at round 3206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([5.63005364, 9.20273185, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 10.439461260960647}
done in step count: 6
reward sum = 0.9223432666201998
running average episode reward sum: 0.7707696882155457
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.72577578,  8.62649363,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.6838809780812538}
episode index:3207
target Thresh 15.559631927405741
target distance 9.0
model initialize at round 3207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4., 5., 0.]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 9.219544457292914}
done in step count: 4
reward sum = 0.9339559016848943
running average episode reward sum: 0.7708205567359538
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.10402701, 10.22915961,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7778280848808238}
episode index:3208
target Thresh 15.561351681467693
target distance 8.0
model initialize at round 3208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([10.18731014,  8.12474304,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 10.224701689582869}
done in step count: 75
reward sum = 0.3137410654320208
running average episode reward sum: 0.7706781199982462
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.72410513, 2.18213522, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7466602173312563}
episode index:3209
target Thresh 15.563070575867549
target distance 2.0
model initialize at round 3209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([16.55625438,  7.        ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.556254377613154}
done in step count: 4
reward sum = 0.9500755685758651
running average episode reward sum: 0.7707340070538778
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.00490432,  6.3113209 ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6886965624593857}
episode index:3210
target Thresh 15.56478861103503
target distance 11.0
model initialize at round 3210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.184084079649052}
done in step count: 15
reward sum = 0.8138425946522505
running average episode reward sum: 0.770747432338088
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.10171713,  6.60623889,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.6147129156114767}
episode index:3211
target Thresh 15.566505787399649
target distance 14.0
model initialize at round 3211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([16.6705562 ,  9.00000007,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 16.255006005854508}
done in step count: 33
reward sum = 0.6477915469358826
running average episode reward sum: 0.7707091521745131
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13211827, 2.04816432, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8692171758019331}
episode index:3212
target Thresh 15.568222105390692
target distance 6.0
model initialize at round 3212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([ 8.52062643, 11.42108291,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 4.699389775902821}
done in step count: 75
reward sum = 0.36150140588501045
running average episode reward sum: 0.7705817921538815
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.37832062, 10.05957346,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.6245272165714959}
episode index:3213
target Thresh 15.569937565437248
target distance 13.0
model initialize at round 3213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.38191593, 10.        ,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.770641875776302}
done in step count: 7
reward sum = 0.9023065197739074
running average episode reward sum: 0.7706227768233339
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.12709263, 6.92111074, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8764649402173671}
episode index:3214
target Thresh 15.571652167968177
target distance 9.0
model initialize at round 3214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([5.43014443, 8.85968351, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.866617317783218}
done in step count: 6
reward sum = 0.9213796911404383
running average episode reward sum: 0.7706696685540702
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.72543849, 11.48930068,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8750292337476608}
episode index:3215
target Thresh 15.573365913412127
target distance 2.0
model initialize at round 3215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([3.90435754, 1.19280749, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 2.068365873745942}
done in step count: 2
reward sum = 0.9773254460757503
running average episode reward sum: 0.7707339271913591
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.46068685, 1.09936737, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.0116182586174989}
episode index:3216
target Thresh 15.575078802197542
target distance 10.0
model initialize at round 3216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([11.558792  , 10.75348508,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 8.59189502878637}
done in step count: 5
reward sum = 0.9274361209072134
running average episode reward sum: 0.7707826378515134
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.17849968, 10.4715806 ,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9472333610019094}
episode index:3217
target Thresh 15.576790834752634
target distance 7.0
model initialize at round 3217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([11.00494245, 11.86740305,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 5.079550866406478}
done in step count: 3
reward sum = 0.9534962344694562
running average episode reward sum: 0.7708394164707234
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.17121431, 11.82159745,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.167008175610477}
episode index:3218
target Thresh 15.578502011505421
target distance 6.0
model initialize at round 3218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([7.25766943, 8.11634552, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 8.4638060566901}
done in step count: 17
reward sum = 0.7941173873396759
running average episode reward sum: 0.7708466479000086
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.88751173,  2.419345  ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0605834757286694}
episode index:3219
target Thresh 15.580212332883692
target distance 2.0
model initialize at round 3219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([9., 9., 0.]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.9999999999999822}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7709159501833331
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([9., 9., 0.]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.9999999999999822}
episode index:3220
target Thresh 15.58192179931503
target distance 12.0
model initialize at round 3220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([16.43158293,  9.        ,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 12.591435743726011}
done in step count: 12
reward sum = 0.8549012731268898
running average episode reward sum: 0.7709420244841539
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.53982835, 6.26175904, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9145569233835513}
episode index:3221
target Thresh 15.583630411226798
target distance 14.0
model initialize at round 3221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.        , 4.55797076, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.01296513653184}
done in step count: 8
reward sum = 0.882241425707184
running average episode reward sum: 0.7709765680599524
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.81928555,  3.92322036,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.19634873733607303}
episode index:3222
target Thresh 15.58533816904615
target distance 1.0
model initialize at round 3222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.14182106, 11.87090834,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.2226824697577083}
done in step count: 0
reward sum = 0.9961428558500875
running average episode reward sum: 0.7710464303893939
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.14182106, 11.87090834,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.2226824697577083}
episode index:3223
target Thresh 15.587045073200029
target distance 4.0
model initialize at round 3223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.13009584, 9.02484918, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.1474506808627556}
done in step count: 6
reward sum = 0.9290795271812092
running average episode reward sum: 0.771095448099317
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.32830711, 5.63306327, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7653848068729069}
episode index:3224
target Thresh 15.588751124115158
target distance 2.0
model initialize at round 3224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 3., 11.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.9999999999999813}
done in step count: 10
reward sum = 0.8822329583208246
running average episode reward sum: 0.7711299093427966
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.13921505, 11.62808985,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0655738313951144}
episode index:3225
target Thresh 15.59045632221805
target distance 5.0
model initialize at round 3225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([13.37228608, 11.        ,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 3.517429944465874}
done in step count: 7
reward sum = 0.9119005839330483
running average episode reward sum: 0.771173545633742
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.4608504 ,  9.85940202,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.4818203860296779}
episode index:3226
target Thresh 15.592160667935003
target distance 2.0
model initialize at round 3226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.96892053, 8.        , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.031079471111284285}
done in step count: 0
reward sum = 0.9962958945942506
running average episode reward sum: 0.7712433077499368
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.96892053, 8.        , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.031079471111284285}
episode index:3227
target Thresh 15.593864161692107
target distance 2.0
model initialize at round 3227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.       , 8.4913398, 0.       ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.5086601972579405}
done in step count: 0
reward sum = 0.995292958558842
running average episode reward sum: 0.7713127159441155
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.       , 8.4913398, 0.       ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.5086601972579405}
episode index:3228
target Thresh 15.595566803915235
target distance 3.0
model initialize at round 3228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.21985686,  2.78317761,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.227698033453285}
done in step count: 4
reward sum = 0.9477647393725321
running average episode reward sum: 0.7713673619718109
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.15556296,  5.10195199,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8505692941291485}
episode index:3229
target Thresh 15.597268595030043
target distance 10.0
model initialize at round 3229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.00000000000002}
done in step count: 4
reward sum = 0.9369875608354523
running average episode reward sum: 0.7714186375751743
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.09995622, 11.84270526,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8486126292662921}
episode index:3230
target Thresh 15.598969535461984
target distance 3.0
model initialize at round 3230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([12.11007572, 11.86671715,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 3.017094790444951}
done in step count: 2
reward sum = 0.9706307490750448
running average episode reward sum: 0.7714802940627942
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.33358204, 11.89149926,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.1130515867562505}
episode index:3231
target Thresh 15.600669625636291
target distance 5.0
model initialize at round 3231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.2145623 , 10.73426735,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 6.779916605741416}
done in step count: 18
reward sum = 0.8000002483301976
running average episode reward sum: 0.7714891183060698
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.99408553,  4.94799504,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9480134888853983}
episode index:3232
target Thresh 15.602368865977986
target distance 4.0
model initialize at round 3232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.21857429, 10.62374806,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 3.707044129614856}
done in step count: 6
reward sum = 0.9285346988722945
running average episode reward sum: 0.7715376941119981
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.91264891,  7.46696367,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0251746692753334}
episode index:3233
target Thresh 15.60406725691188
target distance 12.0
model initialize at round 3233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.99999997, 7.31731393, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.325803636887608}
done in step count: 28
reward sum = 0.6619808623610588
running average episode reward sum: 0.7715038175406466
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.82460418,  1.74682558,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.862594541328696}
episode index:3234
target Thresh 15.605764798862571
target distance 12.0
model initialize at round 3234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([15.69446206,  3.82155883,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 11.835477154598347}
done in step count: 24
reward sum = 0.7333641903020577
running average episode reward sum: 0.7714920278568015
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.60258789, 1.84008492, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.4283797585884012}
episode index:3235
target Thresh 15.607461492254446
target distance 1.0
model initialize at round 3235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.        ,  7.00753534,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.4088953490310097}
done in step count: 0
reward sum = 0.9940225632573486
running average episode reward sum: 0.7715607950185445
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.        ,  7.00753534,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.4088953490310097}
episode index:3236
target Thresh 15.609157337511673
target distance 5.0
model initialize at round 3236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.605551275463996}
done in step count: 2
reward sum = 0.9685521524697235
running average episode reward sum: 0.771621651168514
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.96899105,  6.31869442,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.32019944736178807}
episode index:3237
target Thresh 15.610852335058219
target distance 2.0
model initialize at round 3237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.20899224, 11.31569898,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.5351733065958606}
done in step count: 4
reward sum = 0.9514150066260999
running average episode reward sum: 0.7716771772202303
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.48414153, 10.63958386,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.8021599237891948}
episode index:3238
target Thresh 15.61254648531783
target distance 4.0
model initialize at round 3238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([7.        , 8.89745671, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 2.0026270564517494}
done in step count: 1
reward sum = 0.981052113623972
running average episode reward sum: 0.771741819065369
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.96429903, 8.13202718, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.8687067306206071}
episode index:3239
target Thresh 15.614239788714045
target distance 13.0
model initialize at round 3239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.99999649, 7.13059131, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.157722264081427}
done in step count: 6
reward sum = 0.921115466066248
running average episode reward sum: 0.7717879220428383
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.82341009,  8.88877719,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.20869717221680054}
episode index:3240
target Thresh 15.615932245670189
target distance 5.0
model initialize at round 3240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.,  6.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.1622776601683915}
done in step count: 6
reward sum = 0.9273362055286941
running average episode reward sum: 0.7718359159593721
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.85843679,  8.96027158,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8593556109734466}
episode index:3241
target Thresh 15.617623856609379
target distance 9.0
model initialize at round 3241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.35751402,  3.70403743,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 11.094102510394649}
done in step count: 6
reward sum = 0.918311859593225
running average episode reward sum: 0.7718810966946077
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.58250384, 10.24749321,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9516181949847073}
episode index:3242
target Thresh 15.619314621954512
target distance 13.0
model initialize at round 3242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 3.97380116, 11.86775028,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.06029164918956}
done in step count: 19
reward sum = 0.7656647589710462
running average episode reward sum: 0.7718791798467127
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.24654042, 11.8698681 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.150813560162826}
episode index:3243
target Thresh 15.621004542128285
target distance 7.0
model initialize at round 3243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([13.63985733, 11.89466109,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 5.949599259103748}
done in step count: 12
reward sum = 0.8508451893837172
running average episode reward sum: 0.7719035220198128
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.80754706, 10.23368107,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.8406777629048888}
episode index:3244
target Thresh 15.622693617553177
target distance 6.0
model initialize at round 3244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([5.41882885, 8.37618423, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 4.8604430684922635}
done in step count: 2
reward sum = 0.970913691170695
running average episode reward sum: 0.771964850269166
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([9.33740413, 9.54359853, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.8045716772371738}
episode index:3245
target Thresh 15.624381848651451
target distance 8.0
model initialize at round 3245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.12488115,  7.75633752,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.8284914025409895}
done in step count: 32
reward sum = 0.6652000244078413
running average episode reward sum: 0.771931959072043
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.15304452, 11.57760266,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.5975344864751924}
episode index:3246
target Thresh 15.626069235845174
target distance 6.0
model initialize at round 3246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 7.19349456, 10.23157823,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 5.957486964706739}
done in step count: 9
reward sum = 0.892420552845577
running average episode reward sum: 0.7719690667387425
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.03583664, 6.60980215, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.6108542628703696}
episode index:3247
target Thresh 15.627755779556187
target distance 2.0
model initialize at round 3247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.64450607,  9.        ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.35549393296249576}
done in step count: 0
reward sum = 0.996905217554599
running average episode reward sum: 0.7720383204797574
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.64450607,  9.        ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.35549393296249576}
episode index:3248
target Thresh 15.629441480206127
target distance 7.0
model initialize at round 3248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.55642396,  7.07301009,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.09236595742318}
done in step count: 3
reward sum = 0.9560820836974245
running average episode reward sum: 0.7720949667596028
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.12673944,  1.87810004,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.17584790896133107}
episode index:3249
target Thresh 15.631126338216422
target distance 3.0
model initialize at round 3249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.       , 9.0834564, 0.       ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.161744522829557}
done in step count: 1
reward sum = 0.9854206909530346
running average episode reward sum: 0.7721606054439699
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.42411816, 10.47633255,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7783749067066433}
episode index:3250
target Thresh 15.632810354008281
target distance 12.0
model initialize at round 3250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.13854373,  4.80843948,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.487144499089483}
done in step count: 33
reward sum = 0.6316933565717432
running average episode reward sum: 0.7721173980465931
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.36637278, 1.10727571, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.0947328982865654}
episode index:3251
target Thresh 15.634493528002713
target distance 11.0
model initialize at round 3251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.12306571,  5.81359733,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.356486661671514}
done in step count: 12
reward sum = 0.8460242612952922
running average episode reward sum: 0.772140124634308
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.53404189, 7.10589052, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0414569135590594}
episode index:3252
target Thresh 15.636175860620508
target distance 7.0
model initialize at round 3252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([8.97380116, 8.13224972, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 7.183499286638246}
done in step count: 6
reward sum = 0.9101056287775933
running average episode reward sum: 0.772182536409329
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.64773404,  2.2768722 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9708105844660911}
episode index:3253
target Thresh 15.637857352282252
target distance 4.0
model initialize at round 3253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.26713526, 4.        , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.1300447703010037}
done in step count: 5
reward sum = 0.9367130044573923
running average episode reward sum: 0.772233098937924
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.7778321 , 6.22547011, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8098515632081139}
episode index:3254
target Thresh 15.639538003408315
target distance 4.0
model initialize at round 3254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.46339369,  7.        ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.052981663402128}
done in step count: 1
reward sum = 0.9843700158969786
running average episode reward sum: 0.7722982715698623
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.09983373,  8.17979467,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.2177996978199246}
episode index:3255
target Thresh 15.641217814418862
target distance 12.0
model initialize at round 3255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.        ,  8.26847684,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.52059603248214}
done in step count: 7
reward sum = 0.8955307875550772
running average episode reward sum: 0.7723361193941821
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.12518939, 4.19854957, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.186430099248326}
episode index:3256
target Thresh 15.642896785733845
target distance 3.0
model initialize at round 3256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.92882305, 4.        , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.364812172182965}
done in step count: 17
reward sum = 0.8088701105489479
running average episode reward sum: 0.7723473364623905
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.12294539, 4.80366164, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8987622307214137}
episode index:3257
target Thresh 15.644574917773008
target distance 2.0
model initialize at round 3257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.91962636, 10.2807982 ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.167460434816687}
done in step count: 0
reward sum = 0.9987366560535711
running average episode reward sum: 0.7724168236691403
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.91962636, 10.2807982 ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.167460434816687}
episode index:3258
target Thresh 15.646252210955883
target distance 7.0
model initialize at round 3258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([11.        , 10.26956713,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 5.490986702464057}
done in step count: 4
reward sum = 0.9455851925427611
running average episode reward sum: 0.7724699590999086
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.19974154,  7.06667638,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9544577867315319}
episode index:3259
target Thresh 15.64792866570179
target distance 5.0
model initialize at round 3259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.87213016,  7.        ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 3.0027238795993845}
done in step count: 2
reward sum = 0.970383969391075
running average episode reward sum: 0.7725306689190163
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.65632677,  4.31266338,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.46461777342219407}
episode index:3260
target Thresh 15.64960428242985
target distance 7.0
model initialize at round 3260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.16951442,  4.61789894,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.876605323903322}
done in step count: 6
reward sum = 0.9199374402007743
running average episode reward sum: 0.7725758718540919
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.58210196, 11.1157916 ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.4336432520277046}
episode index:3261
target Thresh 15.651279061558961
target distance 7.0
model initialize at round 3261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([9.23627114, 9.09893262, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 6.544001355800897}
done in step count: 7
reward sum = 0.9128749780778483
running average episode reward sum: 0.7726188820031489
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.03284579,  5.77292083,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.2294423550588983}
episode index:3262
target Thresh 15.652953003507822
target distance 5.0
model initialize at round 3262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([3.3397522 , 7.56988955, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 4.393500972889425}
done in step count: 4
reward sum = 0.952214591256907
running average episode reward sum: 0.7726739220611488
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.4699412, 9.3873541, 0.       ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.8101217981712281}
episode index:3263
target Thresh 15.654626108694915
target distance 3.0
model initialize at round 3263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.52241254,  9.56325626,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.1510543583084996}
done in step count: 5
reward sum = 0.9373836666924656
running average episode reward sum: 0.7727243846054599
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.33300224,  7.79467397,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.39121511753247595}
episode index:3264
target Thresh 15.656298377538517
target distance 8.0
model initialize at round 3264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([13.        , 10.31644899,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 6.142722355286532}
done in step count: 3
reward sum = 0.9571022587884288
running average episode reward sum: 0.7727808556235863
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.00000088, 8.28067215, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.7193278462329937}
episode index:3265
target Thresh 15.657969810456697
target distance 7.0
model initialize at round 3265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([12.06329613,  8.00002627,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 5.798662973773134}
done in step count: 3
reward sum = 0.9553026025828523
running average episode reward sum: 0.7728367410329431
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.73620312,  2.66517086,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.42626206240880093}
episode index:3266
target Thresh 15.659640407867311
target distance 8.0
model initialize at round 3266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 6.        , 10.44205594,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 6.170860988217187}
done in step count: 61
reward sum = 0.4293711135295466
running average episode reward sum: 0.7727316092216473
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.85317186,  9.16495367,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.8689717732738597}
episode index:3267
target Thresh 15.66131017018801
target distance 8.0
model initialize at round 3267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 7.2111025509279845}
done in step count: 23
reward sum = 0.75081269669718
running average episode reward sum: 0.772724902088072
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.71679214,  7.71595677,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.0131066362947414}
episode index:3268
target Thresh 15.662979097836235
target distance 8.0
model initialize at round 3268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 7.9808637, 11.8624837,  0.       ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 7.151837676390902}
done in step count: 5
reward sum = 0.9292483377173728
running average episode reward sum: 0.7727727832246977
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.73175952,  8.42035621,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.843902448965298}
episode index:3269
target Thresh 15.664647191229218
target distance 5.0
model initialize at round 3269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([11.        , 10.07518458,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 4.296133171074269}
done in step count: 2
reward sum = 0.9672645817171434
running average episode reward sum: 0.7728322608389155
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([15.        ,  7.39516139,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.075245332185797}
episode index:3270
target Thresh 15.666314450783979
target distance 5.0
model initialize at round 3270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.86740305, 5.00494245, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 3.1276296443239597}
done in step count: 2
reward sum = 0.9672585380720486
running average episode reward sum: 0.7728917002388647
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.37405587, 1.10016767, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9744824304387211}
episode index:3271
target Thresh 15.667980876917335
target distance 7.0
model initialize at round 3271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([4.98876879, 7.00043436, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 5.381407347983755}
done in step count: 3
reward sum = 0.9559793843320828
running average episode reward sum: 0.7729476561325362
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.50572134, 1.08713393, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0380924086910186}
episode index:3272
target Thresh 15.669646470045898
target distance 11.0
model initialize at round 3272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([12.        ,  9.44299829,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 9.325676417571309}
done in step count: 5
reward sum = 0.9314306345631717
running average episode reward sum: 0.7729960774519468
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.00377152, 7.27007552, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0321879525220468}
episode index:3273
target Thresh 15.671311230586054
target distance 1.0
model initialize at round 3273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.11957948, 4.74766326, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9158679588083943}
done in step count: 0
reward sum = 0.9958840521012182
running average episode reward sum: 0.7730641556360179
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.11957948, 4.74766326, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9158679588083943}
episode index:3274
target Thresh 15.672975158954005
target distance 9.0
model initialize at round 3274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 3.65988817, 11.89631302,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 7.890872622844334}
done in step count: 4
reward sum = 0.9365729368768079
running average episode reward sum: 0.773114081981435
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.57186368,  8.07890801,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.0841764250171955}
episode index:3275
target Thresh 15.674638255565727
target distance 13.0
model initialize at round 3275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([15.11398792, 11.01466441,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.156407662570956}
done in step count: 7
reward sum = 0.903775104052314
running average episode reward sum: 0.7731539662983065
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.05790912, 9.8995419 , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9474318239829649}
episode index:3276
target Thresh 15.676300520836996
target distance 10.0
model initialize at round 3276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 1.73492403, 11.88163979,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 10.302867240594413}
done in step count: 9
reward sum = 0.8948960181160163
running average episode reward sum: 0.7731911167565969
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.23140887, 10.06282579,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.2120345811929694}
episode index:3277
target Thresh 15.677961955183378
target distance 4.0
model initialize at round 3277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 6.        , 10.00932997,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 2.2402560116490298}
done in step count: 1
reward sum = 0.9810599351146924
running average episode reward sum: 0.7732545300629906
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.99567144, 8.13260389, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.8674069108473336}
episode index:3278
target Thresh 15.679622559020231
target distance 13.0
model initialize at round 3278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.88205874, 3.74543142, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.344236304655919}
done in step count: 7
reward sum = 0.8941100057084155
running average episode reward sum: 0.7732913874816078
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.65212125,  6.60605124,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8902585165115863}
episode index:3279
target Thresh 15.681282332762708
target distance 3.0
model initialize at round 3279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([6., 9., 0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.236067977499781}
done in step count: 32
reward sum = 0.6390549374857754
running average episode reward sum: 0.7732504617346578
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.29521714, 10.7216022 ,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.77965563190105}
episode index:3280
target Thresh 15.682941276825748
target distance 13.0
model initialize at round 3280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.18457091, 11.        ,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.170564461153432}
done in step count: 7
reward sum = 0.9040813534007075
running average episode reward sum: 0.7732903370445225
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.93089176, 6.8082704 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.2328262323476098}
episode index:3281
target Thresh 15.684599391624094
target distance 11.0
model initialize at round 3281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([2.16694093, 9.        , 0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 10.879116180469588}
done in step count: 24
reward sum = 0.7075431964243832
running average episode reward sum: 0.7732703043996048
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.92138626, 10.75329944,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.1901313729032283}
episode index:3282
target Thresh 15.686256677572267
target distance 2.0
model initialize at round 3282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.       ,  4.6079483,  0.       ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.607948303222626}
done in step count: 0
reward sum = 0.9967227987768409
running average episode reward sum: 0.7733383679068777
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.       ,  4.6079483,  0.       ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.607948303222626}
episode index:3283
target Thresh 15.687913135084592
target distance 10.0
model initialize at round 3283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4.85398256, 4.99044053, 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.42810246939607}
done in step count: 19
reward sum = 0.7781130275066436
running average episode reward sum: 0.773339821822712
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.12164606,  9.49264299,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.5217364290852132}
episode index:3284
target Thresh 15.689568764575185
target distance 7.0
model initialize at round 3284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([ 3.99505755, 11.86740305,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 5.768140878289246}
done in step count: 3
reward sum = 0.9536952222308538
running average episode reward sum: 0.7733947245321208
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.98295184, 9.77865112, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.2539903867695936}
episode index:3285
target Thresh 15.691223566457952
target distance 2.0
model initialize at round 3285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.        , 10.72378227,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.2762177288533003}
done in step count: 0
reward sum = 0.9969427774654652
running average episode reward sum: 0.7734627549803659
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.        , 10.72378227,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.2762177288533003}
episode index:3286
target Thresh 15.692877541146594
target distance 3.0
model initialize at round 3286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.94478322, 8.045681  , 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.955915083918433}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7735298487574346
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.94478322, 8.045681  , 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.955915083918433}
episode index:3287
target Thresh 15.694530689054602
target distance 5.0
model initialize at round 3287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([13.        , 10.55580091,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.9410808563021194}
done in step count: 2
reward sum = 0.9663632770532175
running average episode reward sum: 0.7735884963937775
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.86788822,  7.03229283,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.2998796584348087}
episode index:3288
target Thresh 15.696183010595266
target distance 6.0
model initialize at round 3288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 9.84866929, 10.70189697,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.85626142597069}
done in step count: 16
reward sum = 0.8137406685134998
running average episode reward sum: 0.7736007044120566
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.29032046, 10.10480687,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.1423728795476924}
episode index:3289
target Thresh 15.697834506181668
target distance 10.0
model initialize at round 3289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([10.87639513,  8.1574742 ,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 9.801784649125638}
done in step count: 23
reward sum = 0.7485862839585165
running average episode reward sum: 0.7735931012447455
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.75727976, 4.43916489, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8754075814740377}
episode index:3290
target Thresh 15.699485176226673
target distance 10.0
model initialize at round 3290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([10.,  9.,  0.]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 10.000000000000027}
done in step count: 33
reward sum = 0.6726163474064574
running average episode reward sum: 0.7735624185483497
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.36756084, 2.09903468, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9730567663062142}
episode index:3291
target Thresh 15.70113502114296
target distance 6.0
model initialize at round 3291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.98679805,  7.        ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 4.126327446470706}
done in step count: 4
reward sum = 0.9517267931962046
running average episode reward sum: 0.7736165389537712
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.03231239,  3.57086504,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.1235240062533138}
episode index:3292
target Thresh 15.702784041342985
target distance 13.0
model initialize at round 3292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.        ,  9.27075624,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.197576455616762}
done in step count: 10
reward sum = 0.8708079178937193
running average episode reward sum: 0.7736460534933826
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.21490772, 4.22115748, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8156473042600555}
episode index:3293
target Thresh 15.704432237239002
target distance 9.0
model initialize at round 3293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2., 9., 0.]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 7.071067811865492}
done in step count: 4
reward sum = 0.9449111111017384
running average episode reward sum: 0.7736980465284792
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.39917378, 2.27853906, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.48674810370984006}
episode index:3294
target Thresh 15.706079609243062
target distance 7.0
model initialize at round 3294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.78324127, 9.50829995, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.425840089351241}
done in step count: 4
reward sum = 0.9468726965737139
running average episode reward sum: 0.773750603326672
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.43300116, 10.85581666,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.4563757661912493}
episode index:3295
target Thresh 15.707726157767004
target distance 10.0
model initialize at round 3295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.63122141,  9.88035214,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.797141423695644}
done in step count: 63
reward sum = 0.44315189937126187
running average episode reward sum: 0.773650300321831
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.87524875, 5.21888021, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9022022586195095}
episode index:3296
target Thresh 15.70937188322247
target distance 12.0
model initialize at round 3296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.        , 2.47937274, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.114954634821077}
done in step count: 11
reward sum = 0.8430930317723302
running average episode reward sum: 0.7736713627214217
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.57207833,  3.51800602,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6445425925863356}
episode index:3297
target Thresh 15.711016786020894
target distance 13.0
model initialize at round 3297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.22338164, 6.75372219, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.360387142719976}
done in step count: 15
reward sum = 0.8266673957198399
running average episode reward sum: 0.7736874318642352
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.56966357,  2.7127918 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.517376064194517}
episode index:3298
target Thresh 15.71266086657349
target distance 4.0
model initialize at round 3298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.40472746,  9.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 2.5582991381415487}
done in step count: 5
reward sum = 0.9385449351705794
running average episode reward sum: 0.7737374038264377
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.64233476, 11.81404491,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8891532682067643}
episode index:3299
target Thresh 15.71430412529129
target distance 1.0
model initialize at round 3299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.09446925, 10.29695058,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.581792256941493}
done in step count: 6
reward sum = 0.9281084998291603
running average episode reward sum: 0.7737841829464386
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.42958777,  9.56477663,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.7095902279889409}
episode index:3300
target Thresh 15.715946562585104
target distance 8.0
model initialize at round 3300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([10.35154831,  9.42357028,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 9.135665117858526}
done in step count: 11
reward sum = 0.8636581671298649
running average episode reward sum: 0.7738114092367093
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.9723203, 3.7021746, 0.       ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0169103825534882}
episode index:3301
target Thresh 15.717588178865542
target distance 10.0
model initialize at round 3301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([5.        , 8.64102709, 0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 8.008049796798712}
done in step count: 15
reward sum = 0.8115564609336168
running average episode reward sum: 0.7738228402033044
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.15568434,  8.65630422,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.3773120749092342}
episode index:3302
target Thresh 15.719228974543007
target distance 3.0
model initialize at round 3302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([7.       , 9.2732147, 0.       ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.0366514704155245}
done in step count: 3
reward sum = 0.960099781764883
running average episode reward sum: 0.773879236492
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.50852597, 8.70897836, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.5711745072283596}
episode index:3303
target Thresh 15.720868950027699
target distance 6.0
model initialize at round 3303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.30754459,  7.        ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 4.059494364593232}
done in step count: 2
reward sum = 0.9691150828905298
running average episode reward sum: 0.7739383272445418
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.51520008,  3.        ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.4847999215125949}
episode index:3304
target Thresh 15.722508105729608
target distance 4.0
model initialize at round 3304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 4.03172243, 10.30100083,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.0494707063033895}
done in step count: 10
reward sum = 0.8823724222258529
running average episode reward sum: 0.7739711363504362
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.67828646, 10.22635245,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0288843755421153}
episode index:3305
target Thresh 15.72414644205853
target distance 11.0
model initialize at round 3305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 5.        , 10.31862354,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.096085314440979}
done in step count: 5
reward sum = 0.9263863250317975
running average episode reward sum: 0.7740172389483435
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.97736093,  8.07387014,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.3464586488431605}
episode index:3306
target Thresh 15.725783959424044
target distance 1.0
model initialize at round 3306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.00895101, 9.71628797, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.2837432356678042}
done in step count: 9
reward sum = 0.905012370972457
running average episode reward sum: 0.7740568504185655
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.14338042, 10.64752114,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.38052502205290717}
episode index:3307
target Thresh 15.727420658235532
target distance 12.0
model initialize at round 3307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.11658593,  3.26705708,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.15227679279482}
done in step count: 32
reward sum = 0.6722856191986895
running average episode reward sum: 0.7740260852337952
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.69650464, 10.65436153,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.721317164755417}
episode index:3308
target Thresh 15.729056538902167
target distance 8.0
model initialize at round 3308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 7.899549692763975}
done in step count: 61
reward sum = 0.4152394635931016
running average episode reward sum: 0.7739176577264998
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.67104569, 10.07948187,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.6757364019967693}
episode index:3309
target Thresh 15.73069160183292
target distance 13.0
model initialize at round 3309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 23
reward sum = 0.7429594248924429
running average episode reward sum: 0.7739083047860665
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.97275756,  9.57305343,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5737006059204969}
episode index:3310
target Thresh 15.732325847436554
target distance 10.0
model initialize at round 3310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.17711612,  5.4698407 ,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.81118009022728}
done in step count: 36
reward sum = 0.636534995507646
running average episode reward sum: 0.7738668148104464
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.11593321, 2.69593802, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7055283407142325}
episode index:3311
target Thresh 15.733959276121634
target distance 9.0
model initialize at round 3311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([5.        , 9.55776072, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 8.131404985149974}
done in step count: 6
reward sum = 0.9211544152082728
running average episode reward sum: 0.7739112857042862
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.36499171, 2.66922358, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7622854782091977}
episode index:3312
target Thresh 15.735591888296518
target distance 10.0
model initialize at round 3312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.343119182645127}
done in step count: 10
reward sum = 0.8564856447481136
running average episode reward sum: 0.7739362100505114
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.02768905,  1.13914377,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8613014167079965}
episode index:3313
target Thresh 15.737223684369356
target distance 10.0
model initialize at round 3313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([12.50317247,  7.50317247,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 8.51767454249565}
done in step count: 5
reward sum = 0.9282813200363145
running average episode reward sum: 0.7739827837107364
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.16676699, 8.12297239, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.20720385580070108}
episode index:3314
target Thresh 15.738854664748098
target distance 2.0
model initialize at round 3314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.        , 10.58736694,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.41263306140897704}
done in step count: 0
reward sum = 0.9967412504365474
running average episode reward sum: 0.7740499808349373
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.        , 10.58736694,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.41263306140897704}
episode index:3315
target Thresh 15.740484829840486
target distance 9.0
model initialize at round 3315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([10.        ,  9.32544661,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 7.749748068630691}
done in step count: 4
reward sum = 0.9473633763997393
running average episode reward sum: 0.774102246635771
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.93279224, 6.84824388, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.2608009542295435}
episode index:3316
target Thresh 15.74211418005407
target distance 6.0
model initialize at round 3316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 9.12892568, 11.        ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 7.156232409281649}
done in step count: 39
reward sum = 0.5636449671854029
running average episode reward sum: 0.7740387985563467
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.89830275,  8.35003518,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.1087840592399838}
episode index:3317
target Thresh 15.74374271579618
target distance 11.0
model initialize at round 3317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([15.59148312, 10.63908076,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 10.610746383791618}
done in step count: 33
reward sum = 0.6412928890423188
running average episode reward sum: 0.7739987907475722
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.20838842, 10.59334565,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.6288758213330734}
episode index:3318
target Thresh 15.745370437473952
target distance 9.0
model initialize at round 3318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([13.83236814,  7.        ,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 8.387251671987045}
done in step count: 18
reward sum = 0.8063495133564789
running average episode reward sum: 0.7740085378770114
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.99058249, 10.24895609,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.021387683667193}
episode index:3319
target Thresh 15.746997345494314
target distance 6.0
model initialize at round 3319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 9.11998343, 11.        ,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 6.438493398359844}
done in step count: 5
reward sum = 0.9353183415777409
running average episode reward sum: 0.7740571251672828
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.26635673, 8.32999576, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.7210073419118538}
episode index:3320
target Thresh 15.748623440263998
target distance 8.0
model initialize at round 3320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.40180039,  3.80706823,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 7.217763520288994}
done in step count: 4
reward sum = 0.947753276883386
running average episode reward sum: 0.7741094275315453
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.84202227, 10.9753108 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.15989533971411143}
episode index:3321
target Thresh 15.750248722189525
target distance 7.0
model initialize at round 3321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([6.06841993, 8.40206456, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 7.099410647455612}
done in step count: 99
reward sum = -0.1573594716639121
running average episode reward sum: 0.7738290335221548
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.82906343, 4.52253695, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 2.6552850793358473}
episode index:3322
target Thresh 15.751873191677214
target distance 4.0
model initialize at round 3322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.28015158,  6.        ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.37461324367425}
done in step count: 1
reward sum = 0.9810011362402964
running average episode reward sum: 0.7738913784221603
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.28015158,  8.        ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7198484241961349}
episode index:3323
target Thresh 15.753496849133187
target distance 5.0
model initialize at round 3323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([11.        , 10.97625661,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 4.981025659472425}
done in step count: 2
reward sum = 0.965527785800578
running average episode reward sum: 0.7739490307709503
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.91350973,  6.97625661,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9138182363427587}
episode index:3324
target Thresh 15.755119694963353
target distance 1.0
model initialize at round 3324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 4.        , 10.63943589,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0630176288945417}
done in step count: 0
reward sum = 0.996902495611639
running average episode reward sum: 0.7740160844445867
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 4.        , 10.63943589,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0630176288945417}
episode index:3325
target Thresh 15.756741729573427
target distance 2.0
model initialize at round 3325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([15.33186579,  9.15013874,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 2.336694180016038}
done in step count: 21
reward sum = 0.7632441561569394
running average episode reward sum: 0.7740128457409524
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.57444192,  9.96892917,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.1264134433325572}
episode index:3326
target Thresh 15.758362953368916
target distance 11.0
model initialize at round 3326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.18772204862523}
done in step count: 7
reward sum = 0.8928835741294583
running average episode reward sum: 0.7740485748447662
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.3561218 , 4.58718211, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.5451984471668128}
episode index:3327
target Thresh 15.759983366755126
target distance 1.0
model initialize at round 3327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.29033035, 11.44894326,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8397505938483028}
done in step count: 0
reward sum = 0.999785618655217
running average episode reward sum: 0.7741164044853341
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.29033035, 11.44894326,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8397505938483028}
episode index:3328
target Thresh 15.761602970137162
target distance 7.0
model initialize at round 3328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 9., 11.,  0.]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.830951894845307}
done in step count: 3
reward sum = 0.9530538922494964
running average episode reward sum: 0.7741701556081231
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.        ,  8.34446239,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.0576645694038318}
episode index:3329
target Thresh 15.763221763919923
target distance 10.0
model initialize at round 3329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.90975997, 5.56755606, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.224407611673605}
done in step count: 20
reward sum = 0.7584179345671849
running average episode reward sum: 0.7741654252114141
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.04108398,  4.55369663,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.5552187434771114}
episode index:3330
target Thresh 15.764839748508107
target distance 11.0
model initialize at round 3330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.13661216,  3.98173097,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.335584939474616}
done in step count: 16
reward sum = 0.8000180448400405
running average episode reward sum: 0.7741731864301556
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.39664522, 6.77542716, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.870984905610593}
episode index:3331
target Thresh 15.766456924306212
target distance 12.0
model initialize at round 3331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([12.53562164,  9.91212225,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 10.575031461138517}
done in step count: 6
reward sum = 0.9149361854006995
running average episode reward sum: 0.774215432228166
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.62802979, 8.02032071, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0479185808656364}
episode index:3332
target Thresh 15.768073291718531
target distance 13.0
model initialize at round 3332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.04360199, 8.        , 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 13.377423271303165}
done in step count: 8
reward sum = 0.8845697594288129
running average episode reward sum: 0.7742485418372871
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.03750043,  1.82438272,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.17957647933466092}
episode index:3333
target Thresh 15.769688851149157
target distance 3.0
model initialize at round 3333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.96460187, 8.18727326, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.997245563912396}
done in step count: 9
reward sum = 0.9065215693743358
running average episode reward sum: 0.7742882158107536
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.70417204, 10.32572764,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7363133791328231}
episode index:3334
target Thresh 15.771303603001979
target distance 13.0
model initialize at round 3334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.045361017187282}
done in step count: 15
reward sum = 0.8159853662734085
running average episode reward sum: 0.7743007187044455
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.11804467, 8.6532519 , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9476705364229581}
episode index:3335
target Thresh 15.772917547680684
target distance 3.0
model initialize at round 3335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([2.03657877, 7.        , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.203411652172965}
done in step count: 3
reward sum = 0.9660088258289958
running average episode reward sum: 0.7743581851634156
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.08588362, 6.77636445, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1993125170119703}
episode index:3336
target Thresh 15.774530685588761
target distance 11.0
model initialize at round 3336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([12.        ,  9.37304366,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 11.634507843412674}
done in step count: 8
reward sum = 0.8897284548645332
running average episode reward sum: 0.7743927582139704
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.39130654, 1.59631747, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7303884651044106}
episode index:3337
target Thresh 15.77614301712949
target distance 6.0
model initialize at round 3337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([4.86775037, 5.97380131, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 4.555959210248825}
done in step count: 2
reward sum = 0.9640728417499815
running average episode reward sum: 0.7744495826847719
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.92337858, 9.58024475, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.0143088695132898}
episode index:3338
target Thresh 15.777754542705958
target distance 3.0
model initialize at round 3338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.88046753,  4.9105866 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.91432211225056}
done in step count: 2
reward sum = 0.9749292706272253
running average episode reward sum: 0.7745096245200348
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.11513223,  3.45552304,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9952347567216375}
episode index:3339
target Thresh 15.779365262721045
target distance 5.0
model initialize at round 3339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.30708241, 7.77487755, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 3.7873473716325856}
done in step count: 2
reward sum = 0.9718905961327348
running average episode reward sum: 0.77456872061932
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.65365404, 4.14765151, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6701228034997523}
episode index:3340
target Thresh 15.78097517757743
target distance 3.0
model initialize at round 3340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 9.        , 10.67955863,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.209049187615675}
done in step count: 2
reward sum = 0.9715500447854853
running average episode reward sum: 0.7746276794113482
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.0493598 ,  9.51369381,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.48880476737734824}
episode index:3341
target Thresh 15.782584287677594
target distance 1.0
model initialize at round 3341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.75613189, 5.        , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.253688729787974}
done in step count: 0
reward sum = 0.9965711985392147
running average episode reward sum: 0.7746940898000758
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.75613189, 5.        , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.253688729787974}
episode index:3342
target Thresh 15.784192593423812
target distance 9.0
model initialize at round 3342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.        ,  3.30469036,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 10.402778015498246}
done in step count: 6
reward sum = 0.9099773077138392
running average episode reward sum: 0.7747345574093829
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.31416434, 11.8759984 ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.112539323271624}
episode index:3343
target Thresh 15.78580009521816
target distance 3.0
model initialize at round 3343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 4.47871184, 11.        ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.5212881565093985}
done in step count: 2
reward sum = 0.9737466527625716
running average episode reward sum: 0.7747940705957924
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.08154678, 10.50783455,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0420091830254803}
episode index:3344
target Thresh 15.787406793462518
target distance 2.0
model initialize at round 3344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.84920201,  3.91595423,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.249046117751488}
done in step count: 0
reward sum = 0.9985360412817893
running average episode reward sum: 0.7748609590773128
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.84920201,  3.91595423,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.249046117751488}
episode index:3345
target Thresh 15.789012688558559
target distance 13.0
model initialize at round 3345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.58480978,  5.        ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 13.046371819076102}
done in step count: 15
reward sum = 0.8165212869736017
running average episode reward sum: 0.7748734098626973
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.91006171, 11.86570823,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.2560505795889378}
episode index:3346
target Thresh 15.790617780907752
target distance 7.0
model initialize at round 3346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 5.099019513592806}
done in step count: 15
reward sum = 0.8237202011186403
running average episode reward sum: 0.7748880040638493
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.60539186, 8.72644456, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.6643281427190773}
episode index:3347
target Thresh 15.792222070911373
target distance 8.0
model initialize at round 3347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([14.        ,  3.49033904,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 6.809969589087967}
done in step count: 9
reward sum = 0.8828009131715374
running average episode reward sum: 0.7749202361155542
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.63170895, 10.92154312,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.992411210975122}
episode index:3348
target Thresh 15.793825558970497
target distance 9.0
model initialize at round 3348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 7.270210533951997}
done in step count: 4
reward sum = 0.9377836369800574
running average episode reward sum: 0.7749688665726651
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.79310869, 10.74197179,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.0860679191981681}
episode index:3349
target Thresh 15.795428245485995
target distance 10.0
model initialize at round 3349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.05112229,  6.60150619,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.417571295197188}
done in step count: 9
reward sum = 0.8875832351822803
running average episode reward sum: 0.7750024828021007
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.79742014, 3.23592539, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.1043952592118604}
episode index:3350
target Thresh 15.797030130858534
target distance 10.0
model initialize at round 3350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 4.        , 11.18603921,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.002162869338624}
done in step count: 8
reward sum = 0.9021548003925456
running average episode reward sum: 0.7750404273910564
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.63030264, 10.53737629,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7818581171373797}
episode index:3351
target Thresh 15.798631215488593
target distance 5.0
model initialize at round 3351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.49943525, 9.32769394, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 3.364963969790186}
done in step count: 2
reward sum = 0.9691047499283674
running average episode reward sum: 0.7750983224753455
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.28787525, 5.54426029, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8454704829692676}
episode index:3352
target Thresh 15.800231499776437
target distance 11.0
model initialize at round 3352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.25136662, 1.78486776, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.750602890378374}
done in step count: 41
reward sum = 0.5712114931641807
running average episode reward sum: 0.7750375151895385
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.61214455,  1.43223419,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8349125456412159}
episode index:3353
target Thresh 15.80183098412214
target distance 5.0
model initialize at round 3353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.        , 6.13211346, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 5.262729272743142}
done in step count: 15
reward sum = 0.8349459086032296
running average episode reward sum: 0.7750553769645574
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.78358226, 11.17748952,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.2798913564454382}
episode index:3354
target Thresh 15.803429668925572
target distance 6.0
model initialize at round 3354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.70949674,  8.27369928,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 4.464292131982188}
done in step count: 2
reward sum = 0.9685914378195264
running average episode reward sum: 0.7751130628247229
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.88181948,  4.73278183,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.146549082790785}
episode index:3355
target Thresh 15.805027554586404
target distance 1.0
model initialize at round 3355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.3200483200778617}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7751782853924764
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.3200483200778617}
episode index:3356
target Thresh 15.806624641504108
target distance 11.0
model initialize at round 3356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.85192275,  5.        ,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.298562126883963}
done in step count: 9
reward sum = 0.884624886993811
running average episode reward sum: 0.7752108878951874
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.01596698, 8.40035357, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.4006718434964964}
episode index:3357
target Thresh 15.808220930077958
target distance 2.0
model initialize at round 3357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 3.        , 10.03561127,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.267975584109283}
done in step count: 10
reward sum = 0.8877354447810866
running average episode reward sum: 0.7752443972927114
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.38468561, 8.27340425, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.4719458698558108}
episode index:3358
target Thresh 15.809816420707023
target distance 2.0
model initialize at round 3358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.64409733, 10.        ,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.644097328186021}
done in step count: 0
reward sum = 0.9949727074524943
running average episode reward sum: 0.7753098120918064
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.64409733, 10.        ,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.644097328186021}
episode index:3359
target Thresh 15.811411113790175
target distance 2.0
model initialize at round 3359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.71401828, 11.87572743,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9212405098005303}
done in step count: 0
reward sum = 0.9970834687632546
running average episode reward sum: 0.7753758161562919
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.71401828, 11.87572743,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9212405098005303}
episode index:3360
target Thresh 15.81300500972609
target distance 3.0
model initialize at round 3360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.84937245, 5.        , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0112807028032726}
done in step count: 17
reward sum = 0.8132265032887962
running average episode reward sum: 0.7753870778900416
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.86929465, 6.21312807, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.25001492728539165}
episode index:3361
target Thresh 15.81459810891324
target distance 12.0
model initialize at round 3361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 5.97380116, 11.86775028,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 10.198683946405735}
done in step count: 18
reward sum = 0.7924364486346392
running average episode reward sum: 0.7753921490889544
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.72065156,  9.38250652,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.677741652305562}
episode index:3362
target Thresh 15.816190411749902
target distance 7.0
model initialize at round 3362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.15669609,  3.64435147,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 6.357579880920589}
done in step count: 14
reward sum = 0.8502393381214685
running average episode reward sum: 0.7754144051665732
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.22087463,  9.20590177,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.1124874585032802}
episode index:3363
target Thresh 15.81778191863415
target distance 10.0
model initialize at round 3363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 6.70682406, 11.        ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 9.765404188906583}
done in step count: 50
reward sum = 0.5391899240007133
running average episode reward sum: 0.77534418385826
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.87394114,  8.38056012,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9532046638350866}
episode index:3364
target Thresh 15.81937262996386
target distance 11.0
model initialize at round 3364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.22906113,  8.41495287,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.485809121341703}
done in step count: 55
reward sum = 0.48936619637345086
running average episode reward sum: 0.7752591978292898
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.89693913, 5.84511075, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9102145251407844}
episode index:3365
target Thresh 15.82096254613671
target distance 13.0
model initialize at round 3365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.       , 4.7931447, 0.       ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.145194835693973}
done in step count: 9
reward sum = 0.8785042326917908
running average episode reward sum: 0.7752898707451729
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.60702742,  3.81184581,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9019540268930285}
episode index:3366
target Thresh 15.822551667550181
target distance 8.0
model initialize at round 3366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.59327972,  5.        ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 6.029260387924499}
done in step count: 14
reward sum = 0.8274903764852807
running average episode reward sum: 0.7753053743108812
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.59583445, 11.90035716,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0796581383752866}
episode index:3367
target Thresh 15.824139994601554
target distance 4.0
model initialize at round 3367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([ 6., 11.,  0.]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 5.385164807134495}
done in step count: 7
reward sum = 0.9092792257293656
running average episode reward sum: 0.7753451527703286
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.74729552, 6.37401375, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.451382137725603}
episode index:3368
target Thresh 15.825727527687908
target distance 9.0
model initialize at round 3368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([9.02354677, 8.13298367, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 7.690630401286217}
done in step count: 3
reward sum = 0.9503893764703584
running average episode reward sum: 0.7753971100940744
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.85117912, 5.01719661, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8513528145342698}
episode index:3369
target Thresh 15.82731426720613
target distance 12.0
model initialize at round 3369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.17552757, 6.13322127, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.526039527370289}
done in step count: 38
reward sum = 0.5538478218032423
running average episode reward sum: 0.7753313684655015
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.54281088,  2.05489076,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5455791832614664}
episode index:3370
target Thresh 15.828900213552897
target distance 5.0
model initialize at round 3370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.86687208, 4.99779304, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.5353158254894357}
done in step count: 6
reward sum = 0.9199042645709091
running average episode reward sum: 0.7753742557084874
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.75264076, 7.80446831, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.3153081609845492}
episode index:3371
target Thresh 15.830485367124705
target distance 12.0
model initialize at round 3371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.2338531 ,  5.47352809,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.528536961273714}
done in step count: 20
reward sum = 0.7521583285294081
running average episode reward sum: 0.7753673707953262
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.71177003, 2.8060854 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3473893882729124}
episode index:3372
target Thresh 15.832069728317835
target distance 7.0
model initialize at round 3372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([12.01913375, 11.86248542,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 5.092699156345075}
done in step count: 26
reward sum = 0.6982358675898059
running average episode reward sum: 0.7753445034655886
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.52802038, 10.25117046,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.885161249377034}
episode index:3373
target Thresh 15.833653297528386
target distance 3.0
model initialize at round 3373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 7.25995982, 10.68134785,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.2996299222879966}
done in step count: 1
reward sum = 0.9839823300059866
running average episode reward sum: 0.7754063404029152
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.25995982, 11.44118917,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8615726064452786}
episode index:3374
target Thresh 15.835236075152238
target distance 3.0
model initialize at round 3374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.        , 8.97307909, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0003623021913752}
done in step count: 12
reward sum = 0.8637240490424639
running average episode reward sum: 0.7754325086128826
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.11738414, 8.17624029, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8320811838858447}
episode index:3375
target Thresh 15.836818061585097
target distance 11.0
model initialize at round 3375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.1859839 ,  5.46756566,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.496070295254512}
done in step count: 11
reward sum = 0.862721007241504
running average episode reward sum: 0.7754583642108175
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.83463873, 8.00963289, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8346943133097285}
episode index:3376
target Thresh 15.838399257222452
target distance 12.0
model initialize at round 3376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.06507206,  3.85119081,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.233893831953695}
done in step count: 9
reward sum = 0.8662945356350055
running average episode reward sum: 0.7754852626921395
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.27839294, 1.21157288, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8361339309257608}
episode index:3377
target Thresh 15.839979662459607
target distance 8.0
model initialize at round 3377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 8.17200541, 10.19705606,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.224015570436079}
done in step count: 17
reward sum = 0.812310291620877
running average episode reward sum: 0.7754961641216626
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.15233937, 10.15923532,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.1939069472556483}
episode index:3378
target Thresh 15.84155927769166
target distance 1.0
model initialize at round 3378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([9.61310427, 8.09948793, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.6536026617274515}
done in step count: 2
reward sum = 0.974763003676769
running average episode reward sum: 0.7755551362552983
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.11955684,  8.52835091,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.48656623719238723}
episode index:3379
target Thresh 15.843138103313517
target distance 5.0
model initialize at round 3379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([8.        , 9.38165033, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 3.0630632243027187}
done in step count: 3
reward sum = 0.9563748426993555
running average episode reward sum: 0.7756086332098675
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.81271322, 10.13453381,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.23059853230336375}
episode index:3380
target Thresh 15.844716139719884
target distance 12.0
model initialize at round 3380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([5.38084447, 9.        , 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 11.347531189153012}
done in step count: 15
reward sum = 0.8324802369947468
running average episode reward sum: 0.7756254541515372
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.09343529,  4.68337503,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9602660763814808}
episode index:3381
target Thresh 15.846293387305268
target distance 11.0
model initialize at round 3381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.09253264,  2.55494726,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.974887055292497}
done in step count: 13
reward sum = 0.8405675946389328
running average episode reward sum: 0.7756446564402678
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.34067207, 8.29146501, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9678507875571901}
episode index:3382
target Thresh 15.847869846463983
target distance 10.0
model initialize at round 3382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([13.08927333,  3.47273582,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 9.7972951546884}
done in step count: 23
reward sum = 0.7449939557188037
running average episode reward sum: 0.7756355962272257
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.91738928, 9.5014188 , 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.5081784587173793}
episode index:3383
target Thresh 15.849445517590143
target distance 5.0
model initialize at round 3383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([14.37035298,  7.        ,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 4.512125799799634}
done in step count: 2
reward sum = 0.9691172218249541
running average episode reward sum: 0.7756927716485016
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.59430301,  9.4259156 ,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.8262983538458354}
episode index:3384
target Thresh 15.851020401077664
target distance 8.0
model initialize at round 3384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([11.95385493,  8.00018996,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.963236814300503}
done in step count: 18
reward sum = 0.7814763788144373
running average episode reward sum: 0.775694480247369
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.8792333, 1.6656023, 0.       ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9406768891836659}
episode index:3385
target Thresh 15.852594497320272
target distance 5.0
model initialize at round 3385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.46397185,  7.21199322,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.061060079786148}
done in step count: 9
reward sum = 0.8908279173443622
running average episode reward sum: 0.7757284830344621
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.8784258 , 10.12022137,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.2432386422017547}
episode index:3386
target Thresh 15.854167806711489
target distance 11.0
model initialize at round 3386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([ 4.        , 10.62087333,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 9.021390341539293}
done in step count: 8
reward sum = 0.8927207014517371
running average episode reward sum: 0.7757630245810865
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.69862397,  9.71137181,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.41729335655920424}
episode index:3387
target Thresh 15.855740329644638
target distance 12.0
model initialize at round 3387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.        ,  7.46016955,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.949571336536406}
done in step count: 6
reward sum = 0.9102276692623872
running average episode reward sum: 0.7758027130830586
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.14630932, 3.33907251, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3692920006608022}
episode index:3388
target Thresh 15.857312066512854
target distance 12.0
model initialize at round 3388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([5.92751789, 9.9158417 , 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.218173441160115}
done in step count: 21
reward sum = 0.7456334256784091
running average episode reward sum: 0.7757938109622546
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.54610076,  2.9724083 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.4547370940790773}
episode index:3389
target Thresh 15.858883017709068
target distance 7.0
model initialize at round 3389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 5.3851648071345295}
done in step count: 18
reward sum = 0.7970544199027063
running average episode reward sum: 0.7758000825283139
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.28690585, 8.57063454, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.5164006855572856}
episode index:3390
target Thresh 15.860453183626024
target distance 5.0
model initialize at round 3390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([11.42908835,  9.13443732,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.4317226481769407}
done in step count: 2
reward sum = 0.9686120564710198
running average episode reward sum: 0.7758569424439561
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.44160429, 8.10002294, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.0591338325222603}
episode index:3391
target Thresh 15.862022564656257
target distance 2.0
model initialize at round 3391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.        ,  9.18276465,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.8172353506088044}
done in step count: 27
reward sum = 0.7190592817319259
running average episode reward sum: 0.7758401978505858
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.50419079, 11.24803886,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.5543916041072774}
episode index:3392
target Thresh 15.863591161192117
target distance 9.0
model initialize at round 3392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([13.12504062,  4.83577516,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 8.252694853757646}
done in step count: 9
reward sum = 0.8805437204329861
running average episode reward sum: 0.7758710565368758
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.16330829, 9.33855823, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9025933162161606}
episode index:3393
target Thresh 15.86515897362575
target distance 12.0
model initialize at round 3393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.049875621120906}
done in step count: 28
reward sum = 0.7137453833573385
running average episode reward sum: 0.7758527519778955
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.01174032, 7.79763213, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.2699898451165113}
episode index:3394
target Thresh 15.86672600234911
target distance 3.0
model initialize at round 3394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 6.0094611 , 11.85396399,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 3.027230118264793}
done in step count: 3
reward sum = 0.9649337298153227
running average episode reward sum: 0.7759084459330758
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.82488545, 9.87834952, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.2049621906772403}
episode index:3395
target Thresh 15.868292247753955
target distance 9.0
model initialize at round 3395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.86774123, 3.9738369 , 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.700979713581484}
done in step count: 12
reward sum = 0.8505330066987422
running average episode reward sum: 0.7759304201853625
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.56819671, 10.07644536,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.084343431164989}
episode index:3396
target Thresh 15.869857710231843
target distance 9.0
model initialize at round 3396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.60471891, 11.90010056,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.657801949400786}
done in step count: 4
reward sum = 0.942377544984325
running average episode reward sum: 0.7759794184558362
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.66216761, 11.85632047,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0824743382850002}
episode index:3397
target Thresh 15.871422390174143
target distance 11.0
model initialize at round 3397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([11.        ,  9.71624255,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.85497360259672}
done in step count: 24
reward sum = 0.7321946450432544
running average episode reward sum: 0.7759665330016242
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.73339874, 1.64655705, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.44271678418494936}
episode index:3398
target Thresh 15.872986287972028
target distance 10.0
model initialize at round 3398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([12.        , 10.04859233,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 8.561186552610105}
done in step count: 10
reward sum = 0.8755494751408741
running average episode reward sum: 0.7759958307192292
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.14619048, 6.01856863, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9922596436194092}
episode index:3399
target Thresh 15.874549404016465
target distance 9.0
model initialize at round 3399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([13.72170579,  5.07201254,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 9.160229295379144}
done in step count: 8
reward sum = 0.8988001782498578
running average episode reward sum: 0.7760319496449735
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.93582051, 10.20305259,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.21295389675809798}
episode index:3400
target Thresh 15.876111738698237
target distance 3.0
model initialize at round 3400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.6539278, 9.2958591, 0.       ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.1011254081235418}
done in step count: 2
reward sum = 0.973223488975271
running average episode reward sum: 0.7760899301034652
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.45197964, 8.32725686, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6382972420107008}
episode index:3401
target Thresh 15.877673292407929
target distance 9.0
model initialize at round 3401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 8.062257748298576}
done in step count: 4
reward sum = 0.9364828465276013
running average episode reward sum: 0.776137076757323
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.89590841, 8.18352153, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.8230869618918779}
episode index:3402
target Thresh 15.879234065535927
target distance 7.0
model initialize at round 3402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 9.62917757, 10.78769827,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.6331795767119175}
done in step count: 12
reward sum = 0.8591321593569012
running average episode reward sum: 0.7761614655562061
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.15324858, 10.92850691,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8497642213797557}
episode index:3403
target Thresh 15.880794058472423
target distance 7.0
model initialize at round 3403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([ 5.28638411, 11.43184388,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 5.890295703565226}
done in step count: 11
reward sum = 0.866820507248772
running average episode reward sum: 0.7761880986471851
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.07480289,  9.02593021,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9769377855865826}
episode index:3404
target Thresh 15.882353271607421
target distance 1.0
model initialize at round 3404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.10711145, 5.32670652, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.8966183107440326}
done in step count: 12
reward sum = 0.8621990771629282
running average episode reward sum: 0.7762133588464555
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.28727594, 6.46260037, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.609365099361361}
episode index:3405
target Thresh 15.88391170533072
target distance 6.0
model initialize at round 3405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([13.90933681,  9.7506026 ,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 4.915667495869972}
done in step count: 4
reward sum = 0.9518817768183758
running average episode reward sum: 0.7762649350114502
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.9333867 , 9.46974514, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.07349008338767}
episode index:3406
target Thresh 15.885469360031928
target distance 5.0
model initialize at round 3406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([5.        , 9.71912694, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 4.778274290384509}
done in step count: 12
reward sum = 0.8474400688410911
running average episode reward sum: 0.776285825863763
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.14042444, 6.93439147, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.2696289051927032}
episode index:3407
target Thresh 15.887026236100459
target distance 11.0
model initialize at round 3407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([11., 11.,  0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 9.848857801796116}
done in step count: 8
reward sum = 0.8949094326438515
running average episode reward sum: 0.7763206332601187
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.56834676, 7.24682301, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.6196286298939536}
episode index:3408
target Thresh 15.888582333925534
target distance 10.0
model initialize at round 3408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 6.44748306, 10.74615687,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.53899875366703}
done in step count: 6
reward sum = 0.9145837833500776
running average episode reward sum: 0.7763611915323657
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.29626971,  3.77284542,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8276870071557076}
episode index:3409
target Thresh 15.890137653896176
target distance 3.0
model initialize at round 3409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.16270757, 9.        , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.533586934887937}
done in step count: 1
reward sum = 0.9855780986031796
running average episode reward sum: 0.7764225454640581
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.95336246, 8.34749556, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.350611210651568}
episode index:3410
target Thresh 15.891692196401216
target distance 2.0
model initialize at round 3410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.4758532 ,  3.23119688,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9041540296029007}
done in step count: 0
reward sum = 0.9986932884075471
running average episode reward sum: 0.7764877083907492
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.4758532 ,  3.23119688,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9041540296029007}
episode index:3411
target Thresh 15.893245961829287
target distance 6.0
model initialize at round 3411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.00016787,  7.00007785,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.00003844520609}
done in step count: 3
reward sum = 0.9553181803465078
running average episode reward sum: 0.7765401206041009
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.26933903, 11.18931752,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.32921821563520154}
episode index:3412
target Thresh 15.894798950568834
target distance 1.0
model initialize at round 3412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([ 9.        , 10.27673519,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.6217437333449403}
done in step count: 21
reward sum = 0.7792262392340794
running average episode reward sum: 0.7765409076297762
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.74916149, 8.6600855 , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.4224474204351494}
episode index:3413
target Thresh 15.896351163008102
target distance 12.0
model initialize at round 3413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.11572751,  3.74225441,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.34269867778367}
done in step count: 24
reward sum = 0.7255913527286583
running average episode reward sum: 0.7765259839171514
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13150853, 6.133084  , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8786289253158511}
episode index:3414
target Thresh 15.897902599535144
target distance 12.0
model initialize at round 3414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([3.28783107, 7.90858686, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.712497603419047}
done in step count: 41
reward sum = 0.5809005971901753
running average episode reward sum: 0.7764686997629122
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.22082573,  8.62496002,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9988431112187176}
episode index:3415
target Thresh 15.899453260537822
target distance 12.0
model initialize at round 3415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.440306508910576}
done in step count: 7
reward sum = 0.8965568323796972
running average episode reward sum: 0.7765038543684791
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.4157723 , 11.89804844,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.989624979164618}
episode index:3416
target Thresh 15.901003146403795
target distance 13.0
model initialize at round 3416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([3.72610199, 8.        , 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.314567481201333}
done in step count: 13
reward sum = 0.837253923426259
running average episode reward sum: 0.7765216331419815
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.96400427,  8.32774947,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.6732135425318566}
episode index:3417
target Thresh 15.902552257520542
target distance 11.0
model initialize at round 3417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.1346277 ,  2.30989063,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 13.350231440478709}
done in step count: 17
reward sum = 0.8018853129472143
running average episode reward sum: 0.7765290537621704
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.68628869, 11.85790228,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.0986302790972777}
episode index:3418
target Thresh 15.904100594275336
target distance 7.0
model initialize at round 3418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([4., 8., 0.]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 5.099019513592807}
done in step count: 3
reward sum = 0.9514308212571465
running average episode reward sum: 0.7765802095877027
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([10.        ,  9.30769002,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.04626629072154}
episode index:3419
target Thresh 15.905648157055264
target distance 5.0
model initialize at round 3419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 9.07583904, 11.37815404,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.230178860082756}
done in step count: 21
reward sum = 0.7127055773750893
running average episode reward sum: 0.776561532794658
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.35088187, 9.95738314, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6505155957522699}
episode index:3420
target Thresh 15.907194946247213
target distance 8.0
model initialize at round 3420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.32799065,  4.63216805,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 6.376273327919045}
done in step count: 5
reward sum = 0.9369641688716992
running average episode reward sum: 0.7766084204403981
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.27083581, 11.197798  ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.755516028554144}
episode index:3421
target Thresh 15.908740962237882
target distance 12.0
model initialize at round 3421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.198039027185592}
done in step count: 9
reward sum = 0.8762298957416977
running average episode reward sum: 0.7766375325021461
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.27682316, 8.67228583, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9873970670613131}
episode index:3422
target Thresh 15.910286205413778
target distance 10.0
model initialize at round 3422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([12.       , 10.8593545,  0.       ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 11.936840540353817}
done in step count: 16
reward sum = 0.7976109089188025
running average episode reward sum: 0.7766436596936203
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.43505648, 1.12461698, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.041852492013335}
episode index:3423
target Thresh 15.911830676161207
target distance 4.0
model initialize at round 3423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.14711773,  9.        ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.7263845458450704}
done in step count: 6
reward sum = 0.924096326768827
running average episode reward sum: 0.7766867241407801
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.46329857,  7.71363209,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8508327241061792}
episode index:3424
target Thresh 15.913374374866288
target distance 5.0
model initialize at round 3424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.49168041,  8.08543885,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.127030809562861}
done in step count: 4
reward sum = 0.9512626410178231
running average episode reward sum: 0.7767376952114012
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.81431288,  5.28069557,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.336555653936952}
episode index:3425
target Thresh 15.914917301914949
target distance 3.0
model initialize at round 3425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 5.        , 10.32295477,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.6583755104898217}
done in step count: 1
reward sum = 0.982529170813972
running average episode reward sum: 0.7767977627757918
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.        , 9.81953675, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.2929193622587722}
episode index:3426
target Thresh 15.916459457692918
target distance 2.0
model initialize at round 3426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.        , 10.05384111,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.0538411140441486}
done in step count: 0
reward sum = 0.9963285873945142
running average episode reward sum: 0.7768618219600985
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.        , 10.05384111,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.0538411140441486}
episode index:3427
target Thresh 15.918000842585739
target distance 5.0
model initialize at round 3427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 9.        , 10.53554916,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.035739544036991}
done in step count: 2
reward sum = 0.9679136981946438
running average episode reward sum: 0.7769175547127923
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.        , 11.15063584,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0112819368717465}
episode index:3428
target Thresh 15.91954145697875
target distance 14.0
model initialize at round 3428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.        , 8.66492832, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.874841987512006}
done in step count: 6
reward sum = 0.9129572929355054
running average episode reward sum: 0.7769572280106117
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.07676094,  4.80617852,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8098247020050608}
episode index:3429
target Thresh 15.921081301257113
target distance 4.0
model initialize at round 3429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([15.49023604,  6.74910271,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 3.5761902418426272}
done in step count: 53
reward sum = 0.4958456024333189
running average episode reward sum: 0.7768752712684609
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.22792058,  9.71529922,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.36469483786866186}
episode index:3430
target Thresh 15.922620375805781
target distance 11.0
model initialize at round 3430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([15.2255578 , 10.89747524,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 11.261376906243308}
done in step count: 19
reward sum = 0.7874981330912466
running average episode reward sum: 0.7768783674100589
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.39478801, 9.70480033, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.49295072629922343}
episode index:3431
target Thresh 15.92415868100953
target distance 1.0
model initialize at round 3431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.91125389, 2.52163193, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.029183976737624}
done in step count: 0
reward sum = 0.9953847455029654
running average episode reward sum: 0.7769420347696431
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.91125389, 2.52163193, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.029183976737624}
episode index:3432
target Thresh 15.92569621725293
target distance 7.0
model initialize at round 3432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.86873519, 4.17214966, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 5.829328439505009}
done in step count: 11
reward sum = 0.8749599573886353
running average episode reward sum: 0.7769705864511516
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.15022071, 9.17988844, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.180977482890327}
episode index:3433
target Thresh 15.927232984920373
target distance 4.0
model initialize at round 3433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([2.77768397, 7.84954023, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.506519423621479}
done in step count: 35
reward sum = 0.6549156458176232
running average episode reward sum: 0.776935043370012
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.12776019, 10.15392097,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.215175711613862}
episode index:3434
target Thresh 15.928768984396044
target distance 12.0
model initialize at round 3434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.69134474,  9.37389341,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.447143025952855}
done in step count: 6
reward sum = 0.9125511690967182
running average episode reward sum: 0.7769745240470795
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.66697475, 3.7346943 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9922857589657071}
episode index:3435
target Thresh 15.930304216063945
target distance 9.0
model initialize at round 3435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 1.93359184, 11.27225721,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 9.070495072673538}
done in step count: 28
reward sum = 0.6863718517976143
running average episode reward sum: 0.7769481553997426
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.34531178, 10.0830289 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.126699900755646}
episode index:3436
target Thresh 15.931838680307882
target distance 10.0
model initialize at round 3436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([12.,  9.,  0.]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.630145812734677}
done in step count: 9
reward sum = 0.8749456139073507
running average episode reward sum: 0.7769766678985809
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.14817165, 1.65806434, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.3726591402747968}
episode index:3437
target Thresh 15.933372377511477
target distance 10.0
model initialize at round 3437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([12.94253266,  8.37900627,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.253570184864225}
done in step count: 47
reward sum = 0.5325442180984508
running average episode reward sum: 0.7769055706182434
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.30615653, 6.35113163, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7776324243951774}
episode index:3438
target Thresh 15.934905308058148
target distance 1.0
model initialize at round 3438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([15.04704988,  7.22803712,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.613811828154451}
done in step count: 1
reward sum = 0.9865973413536399
running average episode reward sum: 0.7769665452535256
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.5151459 ,  6.19070303,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.5210097385716416}
episode index:3439
target Thresh 15.936437472331132
target distance 5.0
model initialize at round 3439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([11.17800331, 10.27751374,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 3.1900970083954268}
done in step count: 2
reward sum = 0.9722364840129516
running average episode reward sum: 0.7770233097706069
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.84361899, 9.60186988, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.932845426702185}
episode index:3440
target Thresh 15.937968870713465
target distance 10.0
model initialize at round 3440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12.        ,  9.68149459,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.107925536575475}
done in step count: 4
reward sum = 0.9411817894605529
running average episode reward sum: 0.7770710163906853
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.86423613, 11.8345021 ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.2013733160439757}
episode index:3441
target Thresh 15.939499503588001
target distance 7.0
model initialize at round 3441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.1325968 ,  5.99505776,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.904456753197368}
done in step count: 2
reward sum = 0.9622794006099732
running average episode reward sum: 0.7771248247533289
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.89597299, 10.20442761,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8023447142900676}
episode index:3442
target Thresh 15.941029371337399
target distance 5.0
model initialize at round 3442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 3.6055512754640158}
done in step count: 3
reward sum = 0.9548247409140975
running average episode reward sum: 0.7771764366952868
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.68998539, 10.55682802,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.8866438353029973}
episode index:3443
target Thresh 15.942558474344123
target distance 3.0
model initialize at round 3443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([9.        , 9.43478632, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.857388991271323}
done in step count: 4
reward sum = 0.9523426061598911
running average episode reward sum: 0.7772272979523904
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.16457999, 10.42625564,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.596882868795294}
episode index:3444
target Thresh 15.944086812990449
target distance 7.0
model initialize at round 3444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([7.        , 9.09179214, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 5.878705522042649}
done in step count: 13
reward sum = 0.8534930075524019
running average episode reward sum: 0.777249436039357
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.08023345, 6.21044264, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.225218805656878}
episode index:3445
target Thresh 15.94561438765846
target distance 5.0
model initialize at round 3445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.99999997, 7.58858494, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 3.725310971888796}
done in step count: 7
reward sum = 0.9136352327023824
running average episode reward sum: 0.777289014041871
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.11374575, 3.57111961, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.984573506644942}
episode index:3446
target Thresh 15.947141198730055
target distance 7.0
model initialize at round 3446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([4.08103549, 8.74192429, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 6.0511895786037115}
done in step count: 3
reward sum = 0.9547970918564127
running average episode reward sum: 0.7773405104381038
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.08103549,  9.60268748,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.40549228291555284}
episode index:3447
target Thresh 15.948667246586933
target distance 11.0
model initialize at round 3447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.8742847, 5.8354111, 0.       ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.160119197952747}
done in step count: 29
reward sum = 0.6960947851383371
running average episode reward sum: 0.7773169472927153
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.41922366,  4.71617052,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5062683579618394}
episode index:3448
target Thresh 15.950192531610607
target distance 12.0
model initialize at round 3448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([5.        , 8.81303442, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.100307349126048}
done in step count: 8
reward sum = 0.8857554172679227
running average episode reward sum: 0.7773483878464917
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.59582561,  2.21531154,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.6335354934500353}
episode index:3449
target Thresh 15.9517170541824
target distance 9.0
model initialize at round 3449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([5.17664066, 7.14612992, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.721081695109406}
done in step count: 4
reward sum = 0.9360140221385923
running average episode reward sum: 0.7773943778854171
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.47561478, 11.89513807,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0374256654635474}
episode index:3450
target Thresh 15.953240814683436
target distance 5.0
model initialize at round 3450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.02690852, 5.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.0001206756197325}
done in step count: 60
reward sum = 0.432272755416825
running average episode reward sum: 0.7772943716198509
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.00548698, 7.44907865, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.1369126965992464}
episode index:3451
target Thresh 15.954763813494663
target distance 1.0
model initialize at round 3451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.        ,  4.68163764,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.6547142652716103}
done in step count: 3
reward sum = 0.965257885957814
running average episode reward sum: 0.7773488222323475
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.69446999,  5.27115059,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.7902974452393065}
episode index:3452
target Thresh 15.956286050996829
target distance 2.0
model initialize at round 3452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.08723317,  2.47077173,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0270196261535716}
done in step count: 0
reward sum = 0.9952525022306528
running average episode reward sum: 0.7774119278448579
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.08723317,  2.47077173,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0270196261535716}
episode index:3453
target Thresh 15.957807527570488
target distance 3.0
model initialize at round 3453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 4.41773769, 11.8988213 ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.734219880536148}
done in step count: 2
reward sum = 0.9760010208742599
running average episode reward sum: 0.7774694232394813
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.60483775, 11.63472564,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.747682984618976}
episode index:3454
target Thresh 15.959328243596014
target distance 13.0
model initialize at round 3454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4., 7., 0.]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.704699910719649}
done in step count: 17
reward sum = 0.7898827137983068
running average episode reward sum: 0.7774730160876894
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.67797447, 10.765277  ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7174568005682166}
episode index:3455
target Thresh 15.960848199453586
target distance 7.0
model initialize at round 3455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.,  9.,  0.]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.099019513592808}
done in step count: 16
reward sum = 0.8102438407925471
running average episode reward sum: 0.7774824983865045
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.10277435,  4.38072427,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9746613924489329}
episode index:3456
target Thresh 15.962367395523192
target distance 9.0
model initialize at round 3456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([13.        ,  9.84086764,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 7.0503232823484625}
done in step count: 4
reward sum = 0.9437119630774761
running average episode reward sum: 0.7775305832764933
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.        , 8.99152533, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.0000359093739384}
episode index:3457
target Thresh 15.963885832184632
target distance 7.0
model initialize at round 3457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([14.        ,  3.71823525,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 7.273035049348322}
done in step count: 30
reward sum = 0.6555406192330377
running average episode reward sum: 0.7774953056697715
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.01617057, 8.13273617, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.3115132068475772}
episode index:3458
target Thresh 15.965403509817511
target distance 8.0
model initialize at round 3458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.81791535,  8.        ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 6.115335160013384}
done in step count: 5
reward sum = 0.9354830262278702
running average episode reward sum: 0.7775409800613755
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.26632141,  1.20714844,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.0802304668678016}
episode index:3459
target Thresh 15.966920428801256
target distance 10.0
model initialize at round 3459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([10.5186553 , 10.21863723,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 9.106432546730666}
done in step count: 6
reward sum = 0.9176343196333615
running average episode reward sum: 0.777581469465876
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.12939844, 6.12875745, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.2316698630785885}
episode index:3460
target Thresh 15.96843658951509
target distance 5.0
model initialize at round 3460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([14.        ,  6.98863804,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 3.611866127148573}
done in step count: 2
reward sum = 0.9661732736572844
running average episode reward sum: 0.7776359600189506
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.02554412,  8.14004552,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.2996484014138934}
episode index:3461
target Thresh 15.969951992338054
target distance 9.0
model initialize at round 3461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([9.       , 8.4299896, 0.       ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 7.795179833033751}
done in step count: 4
reward sum = 0.9406846430648133
running average episode reward sum: 0.7776830566922741
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13192213, 5.04407764, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8691961946306795}
episode index:3462
target Thresh 15.971466637649005
target distance 7.0
model initialize at round 3462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 7.168891158489506}
done in step count: 4
reward sum = 0.9368710509172762
running average episode reward sum: 0.7777290249262403
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.89742229, 8.13951522, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.243302386170961}
episode index:3463
target Thresh 15.972980525826594
target distance 5.0
model initialize at round 3463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.52247494,  9.        ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.037767301579063}
done in step count: 2
reward sum = 0.9671343411529396
running average episode reward sum: 0.7777837031353126
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.74450979,  5.0457247 ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9878849072512303}
episode index:3464
target Thresh 15.974493657249303
target distance 2.0
model initialize at round 3464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.        ,  7.40154636,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.784234421732642}
done in step count: 5
reward sum = 0.9445425084828886
running average episode reward sum: 0.7778318297746626
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.72758356,  9.89247597,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7354857232371111}
episode index:3465
target Thresh 15.976006032295407
target distance 12.0
model initialize at round 3465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.206555615733706}
done in step count: 30
reward sum = 0.6924641283751445
running average episode reward sum: 0.7778071997396367
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.66334538,  2.22847007,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8417807060386702}
episode index:3466
target Thresh 15.977517651343007
target distance 7.0
model initialize at round 3466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([8.02619884, 8.13224972, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 7.33585467234724}
done in step count: 5
reward sum = 0.9229321722605611
running average episode reward sum: 0.7778490586875805
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.70441494, 1.7971423 , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.733042733443685}
episode index:3467
target Thresh 15.979028514770002
target distance 12.0
model initialize at round 3467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([5., 8., 0.]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.440306508910572}
done in step count: 5
reward sum = 0.9202517982277989
running average episode reward sum: 0.777890120607863
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.90950457, 11.84299615,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8478395716187207}
episode index:3468
target Thresh 15.98053862295411
target distance 5.0
model initialize at round 3468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.44035661, 7.9301213 , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 4.185747319596707}
done in step count: 2
reward sum = 0.9727025974719462
running average episode reward sum: 0.7779462787159241
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.63253391, 3.9301213 , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.6363820998273235}
episode index:3469
target Thresh 15.98204797627286
target distance 7.0
model initialize at round 3469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([4.90157412, 5.60732826, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 6.978611590126582}
done in step count: 18
reward sum = 0.7883326442694997
running average episode reward sum: 0.7779492719048444
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.9232988 ,  9.45181644,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.0279196314017403}
episode index:3470
target Thresh 15.983556575103586
target distance 12.0
model initialize at round 3470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.08970951,  4.56284464,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.210030431085052}
done in step count: 7
reward sum = 0.8942944403344776
running average episode reward sum: 0.7779827911121131
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.91697742, 3.8623853 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.866372413116293}
episode index:3471
target Thresh 15.98506441982344
target distance 1.0
model initialize at round 3471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.55100429,  6.85350239,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.0159094756427116}
done in step count: 0
reward sum = 0.9998327059769739
running average episode reward sum: 0.7780466879769936
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.55100429,  6.85350239,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.0159094756427116}
episode index:3472
target Thresh 15.986571510809384
target distance 3.0
model initialize at round 3472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.87297976, 7.4404614 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.4460508905520735}
done in step count: 3
reward sum = 0.961897320112183
running average episode reward sum: 0.7780996251011327
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.1578095, 6.6513437, 0.       ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.6701883741112582}
episode index:3473
target Thresh 15.988077848438191
target distance 7.0
model initialize at round 3473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 7.928881475384448}
done in step count: 5
reward sum = 0.9234764929919324
running average episode reward sum: 0.7781414722133638
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.77856593,  8.25649816,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.7757757670584501}
episode index:3474
target Thresh 15.989583433086443
target distance 3.0
model initialize at round 3474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.58182478,  4.        ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.1569442830372407}
done in step count: 8
reward sum = 0.9104048327767802
running average episode reward sum: 0.7781795336120871
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.67093147,  3.146819  ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.3603358353699352}
episode index:3475
target Thresh 15.991088265130537
target distance 4.0
model initialize at round 3475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.        , 9.66703582, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.4034961015629936}
done in step count: 2
reward sum = 0.9723957591623642
running average episode reward sum: 0.7782354070946965
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.54621744, 11.01009524,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.5463107241370906}
episode index:3476
target Thresh 15.992592344946685
target distance 9.0
model initialize at round 3476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 6.27959061, 10.60313958,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 8.988526864108472}
done in step count: 7
reward sum = 0.908378744091763
running average episode reward sum: 0.7782728368723776
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.76032498,  6.10836172,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7680080290171442}
episode index:3477
target Thresh 15.994095672910902
target distance 3.0
model initialize at round 3477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4.12839687, 9.97954464, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.1284951614899725}
done in step count: 10
reward sum = 0.8868993862194016
running average episode reward sum: 0.7783040693477504
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.3148765 , 10.48360906,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5770831229270168}
episode index:3478
target Thresh 15.99559824939902
target distance 6.0
model initialize at round 3478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([13.65395999,  5.        ,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 6.136720914650647}
done in step count: 3
reward sum = 0.9535161238315226
running average episode reward sum: 0.7783544321113273
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.51017261, 8.18042004, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.9653949427103669}
episode index:3479
target Thresh 15.997100074786685
target distance 8.0
model initialize at round 3479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 8.44665902, 11.90923331,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 7.607868639694939}
done in step count: 64
reward sum = 0.38400080934370506
running average episode reward sum: 0.7782411121047849
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.20577603, 11.84801033,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8726197873743728}
episode index:3480
target Thresh 15.998601149449353
target distance 6.0
model initialize at round 3480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.47725731,  4.        ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 4.034012880314493}
done in step count: 2
reward sum = 0.9680271905687032
running average episode reward sum: 0.7782956326674001
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.64721336,  7.94927824,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.6491978318036317}
episode index:3481
target Thresh 16.000101473762292
target distance 11.0
model initialize at round 3481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 7., 11.,  0.]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.727922061357857}
done in step count: 13
reward sum = 0.8489533757452186
running average episode reward sum: 0.7783159249543266
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.8740752 ,  1.15033558,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8589451004196662}
episode index:3482
target Thresh 16.001601048100586
target distance 4.0
model initialize at round 3482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([5.05038179, 7.12310048, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.3378238124713775}
done in step count: 1
reward sum = 0.9784715679166887
running average episode reward sum: 0.778373391403641
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.05023235, 5.12318221, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.878255502101778}
episode index:3483
target Thresh 16.003099872839126
target distance 11.0
model initialize at round 3483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.09278655,  9.60059452,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.091880376973233}
done in step count: 5
reward sum = 0.9321502286424135
running average episode reward sum: 0.7784175294166258
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.27474592, 5.66451819, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7190756169130708}
episode index:3484
target Thresh 16.004597948352618
target distance 2.0
model initialize at round 3484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.89139356,  8.38554744,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.082651568675191}
done in step count: 0
reward sum = 0.996987074310556
running average episode reward sum: 0.7784802466461506
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.89139356,  8.38554744,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.082651568675191}
episode index:3485
target Thresh 16.006095275015582
target distance 3.0
model initialize at round 3485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 8.97934377, 11.29574114,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 3.248912805145187}
done in step count: 2
reward sum = 0.9773886949928485
running average episode reward sum: 0.7785373058682811
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.64808846, 10.58410993,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.8724695173126612}
episode index:3486
target Thresh 16.00759185320235
target distance 13.0
model initialize at round 3486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.        , 4.18077457, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.149420665081065}
done in step count: 15
reward sum = 0.8185280046130321
running average episode reward sum: 0.7785487743795357
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.98604186,  5.1808038 ,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.2819364090139498}
episode index:3487
target Thresh 16.009087683287063
target distance 7.0
model initialize at round 3487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.079550866406481}
done in step count: 3
reward sum = 0.9517375262699688
running average episode reward sum: 0.7785984271180364
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.74044235, 11.87110167,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.1432729257298078}
episode index:3488
target Thresh 16.010582765643683
target distance 12.0
model initialize at round 3488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 5.51662686, 11.91300343,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.134562636537584}
done in step count: 17
reward sum = 0.7913643352162038
running average episode reward sum: 0.7786020860197556
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.84143595,  4.12685667,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8509448156514343}
episode index:3489
target Thresh 16.01207710064598
target distance 11.0
model initialize at round 3489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([6., 9., 0.]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.295630140987008}
done in step count: 23
reward sum = 0.7366593857665215
running average episode reward sum: 0.7785900680540669
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.75543594,  4.0894813 ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.2604198164238897}
episode index:3490
target Thresh 16.013570688667535
target distance 4.0
model initialize at round 3490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([5.44384098, 8.09515929, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.6780090764324935}
done in step count: 9
reward sum = 0.9004244092949191
running average episode reward sum: 0.778624967607559
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.82873756, 6.77089618, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.859822364913521}
episode index:3491
target Thresh 16.015063530081747
target distance 2.0
model initialize at round 3491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.45551336,  7.87652278,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.558312033885198}
done in step count: 0
reward sum = 0.9955699176760525
running average episode reward sum: 0.77868709388192
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.45551336,  7.87652278,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.558312033885198}
episode index:3492
target Thresh 16.016555625261823
target distance 6.0
model initialize at round 3492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 7.29017055, 10.53565609,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 4.953860422494398}
done in step count: 2
reward sum = 0.9707560941865786
running average episode reward sum: 0.7787420807128117
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.29017055,  9.54858816,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.8971102570292573}
episode index:3493
target Thresh 16.018046974580795
target distance 1.0
model initialize at round 3493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.,  5.,  0.]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 2.236067977499765}
done in step count: 30
reward sum = 0.6964268649746577
running average episode reward sum: 0.7787185216928522
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.59404216,  3.05978348,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.5970428418357356}
episode index:3494
target Thresh 16.01953757841149
target distance 5.0
model initialize at round 3494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([8.        , 9.13780582, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 3.003163406125499}
done in step count: 33
reward sum = 0.632746840893711
running average episode reward sum: 0.7786767558328238
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.42299452,  8.82294217,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.4585562592226186}
episode index:3495
target Thresh 16.02102743712657
target distance 13.0
model initialize at round 3495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.311697890307714}
done in step count: 9
reward sum = 0.8736555771529009
running average episode reward sum: 0.7787039236878924
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.28839932,  6.97384138,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7120813160934063}
episode index:3496
target Thresh 16.02251655109849
target distance 6.0
model initialize at round 3496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.2981661 ,  2.74814367,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 5.298543734702552}
done in step count: 25
reward sum = 0.7388103421918771
running average episode reward sum: 0.7786925157435126
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.85762835,  8.07056139,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.860526174914473}
episode index:3497
target Thresh 16.024004920699532
target distance 9.0
model initialize at round 3497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 7.8867985 , 10.47450221,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 8.403499601672651}
done in step count: 8
reward sum = 0.8985207335153536
running average episode reward sum: 0.7787267719521381
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.38496128,  6.83941422,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9234778904859141}
episode index:3498
target Thresh 16.02549254630179
target distance 10.0
model initialize at round 3498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([15.66730821, 11.        ,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 9.87202350345583}
done in step count: 7
reward sum = 0.9096803881583537
running average episode reward sum: 0.7787641979642005
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.81449555, 8.89244077, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.8215667851174919}
episode index:3499
target Thresh 16.02697942827717
target distance 7.0
model initialize at round 3499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.48001528,  6.        ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.418259057596098}
done in step count: 7
reward sum = 0.9108407914278093
running average episode reward sum: 0.7788019341337615
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.63597378, 11.87291489,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9457777150905419}
episode index:3500
target Thresh 16.028465566997387
target distance 6.0
model initialize at round 3500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.57033789,  6.        ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 4.040456077050033}
done in step count: 2
reward sum = 0.9697978874717629
running average episode reward sum: 0.778856488819091
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.88600373, 10.        ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.11399626731876822}
episode index:3501
target Thresh 16.029950962833986
target distance 2.0
model initialize at round 3501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.82161122, 11.87369296,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 2.0459155200853245}
done in step count: 37
reward sum = 0.5868770305883374
running average episode reward sum: 0.7788016688709953
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.35137104,  9.37915352,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.8978696314533092}
episode index:3502
target Thresh 16.031435616158305
target distance 11.0
model initialize at round 3502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.00000002,  7.34727074,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.71300321542753}
done in step count: 5
reward sum = 0.9316670925316581
running average episode reward sum: 0.7788453073019576
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.27284232, 10.34312584,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7112851766058579}
episode index:3503
target Thresh 16.032919527341516
target distance 12.0
model initialize at round 3503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.91247953, 5.49573275, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.676058105163929}
done in step count: 16
reward sum = 0.8122401534503214
running average episode reward sum: 0.7788548377945798
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.32533245,  2.30112159,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.738816972333951}
episode index:3504
target Thresh 16.034402696754594
target distance 11.0
model initialize at round 3504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([11.        , 10.21639204,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 10.938168493770632}
done in step count: 5
reward sum = 0.9242841292795825
running average episode reward sum: 0.7788963297465012
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.10394419, 3.33556411, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.1155227771502514}
episode index:3505
target Thresh 16.035885124768328
target distance 9.0
model initialize at round 3505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([10.        ,  9.62136197,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 7.134468642812638}
done in step count: 9
reward sum = 0.8905311811854402
running average episode reward sum: 0.7789281708336202
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.37111407, 11.11957211,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.6401523295400079}
episode index:3506
target Thresh 16.03736681175333
target distance 7.0
model initialize at round 3506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([13.58198988,  4.57562745,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 7.783471505181732}
done in step count: 10
reward sum = 0.8795801011746595
running average episode reward sum: 0.7789568711274157
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.20380435, 10.95717584,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.2450353802932892}
episode index:3507
target Thresh 16.038847758080017
target distance 13.0
model initialize at round 3507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([1.37756038, 5.99999999, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 13.948865947091884}
done in step count: 33
reward sum = 0.6375752779250317
running average episode reward sum: 0.7789165685067766
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.5813387 ,  9.13856665,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5976248036412197}
episode index:3508
target Thresh 16.04032796411863
target distance 8.0
model initialize at round 3508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 7.97380116, 11.86775028,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 6.309006540739263}
done in step count: 3
reward sum = 0.9504781561951212
running average episode reward sum: 0.7789654603812959
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.62196217,  9.45524695,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6630750280125188}
episode index:3509
target Thresh 16.041807430239217
target distance 6.0
model initialize at round 3509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.76484676, 11.66767971,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.811399054431499}
done in step count: 9
reward sum = 0.8916997362186792
running average episode reward sum: 0.7789975784085998
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.13773122, 11.08148471,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.1600301452167748}
episode index:3510
target Thresh 16.04328615681165
target distance 2.0
model initialize at round 3510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([9.71418273, 9.79815483, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.2353636386402669}
done in step count: 1
reward sum = 0.9870926576235456
running average episode reward sum: 0.7790568478700681
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.68223301, 10.34861135,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.7247641167656258}
episode index:3511
target Thresh 16.044764144205605
target distance 4.0
model initialize at round 3511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([9.1266917 , 9.08192521, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 2.1282690909035225}
done in step count: 9
reward sum = 0.892882665609619
running average episode reward sum: 0.7790892584104268
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.2234127 , 8.28418346, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.7498710289704899}
episode index:3512
target Thresh 16.04624139279058
target distance 8.0
model initialize at round 3512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.50416708,  8.        ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 6.183649094570201}
done in step count: 3
reward sum = 0.9539080513020074
running average episode reward sum: 0.7791390218015146
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.82514466,  2.19656693,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8482347961954727}
episode index:3513
target Thresh 16.047717902935887
target distance 1.0
model initialize at round 3513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.61123455,  7.40407389,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.642595115719984}
done in step count: 43
reward sum = 0.569007766533094
running average episode reward sum: 0.7790792234932424
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.3236011 ,  9.06728422,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.3305220723477578}
episode index:3514
target Thresh 16.049193675010653
target distance 8.0
model initialize at round 3514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 8.        , 10.32490188,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 6.859662710649271}
done in step count: 4
reward sum = 0.9450592262709783
running average episode reward sum: 0.7791264439776743
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.36393316,  7.80890327,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8870016061872045}
episode index:3515
target Thresh 16.050668709383825
target distance 1.0
model initialize at round 3515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.62126184,  7.18305892,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.2422040894645534}
done in step count: 2
reward sum = 0.9761285664925632
running average episode reward sum: 0.7791824741604145
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.09756136,  6.97215366,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9770368286653809}
episode index:3516
target Thresh 16.05214300642416
target distance 8.0
model initialize at round 3516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 8.        , 10.56658053,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 8.184547564850522}
done in step count: 8
reward sum = 0.8904905454435035
running average episode reward sum: 0.7792141227447998
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.11192446, 5.54033466, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.039538222510481}
episode index:3517
target Thresh 16.05361656650023
target distance 5.0
model initialize at round 3517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([8.        , 9.32275987, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 3.075492513099758}
done in step count: 12
reward sum = 0.8598666595740161
running average episode reward sum: 0.7792370484232618
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.3511997 , 9.99656359, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.6488094051210913}
episode index:3518
target Thresh 16.055089389980424
target distance 9.0
model initialize at round 3518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([13.34881261,  4.76270078,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 8.482909374405528}
done in step count: 5
reward sum = 0.9255625673771691
running average episode reward sum: 0.7792786299859086
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.3164989 , 8.56202158, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.5403671438815217}
episode index:3519
target Thresh 16.056561477232954
target distance 11.0
model initialize at round 3519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.28945434,  8.2180419 ,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.458823216119411}
done in step count: 13
reward sum = 0.8344270701268598
running average episode reward sum: 0.7792942971564031
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.17233189, 9.80348636, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.26137309206509335}
episode index:3520
target Thresh 16.058032828625834
target distance 14.0
model initialize at round 3520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.16258517, 11.8748229 ,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 13.971101167726696}
done in step count: 10
reward sum = 0.8609746114346686
running average episode reward sum: 0.7793174952007877
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13917923, 4.22451331, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.1586164154465142}
episode index:3521
target Thresh 16.05950344452691
target distance 7.0
model initialize at round 3521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([7.13026538, 8.12761802, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.007580868420293}
done in step count: 3
reward sum = 0.9526163109828103
running average episode reward sum: 0.7793666998617139
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.15561044, 3.02645954, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.15784395974437418}
episode index:3522
target Thresh 16.06097332530383
target distance 8.0
model initialize at round 3522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([8.15061855, 9.4655031 , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.339147366412942}
done in step count: 29
reward sum = 0.7001225340786199
running average episode reward sum: 0.7793442064851078
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.14825848, 10.83811026,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8669901381255112}
episode index:3523
target Thresh 16.062442471324065
target distance 12.0
model initialize at round 3523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([1.92553902, 5.79048526, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.392716249433068}
done in step count: 54
reward sum = 0.5007959701350932
running average episode reward sum: 0.7792651632852357
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.09681961,  3.45207141,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0100016708924064}
episode index:3524
target Thresh 16.063910882954904
target distance 12.0
model initialize at round 3524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.        , 10.29238212,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.990904558271078}
done in step count: 17
reward sum = 0.7872256321428153
running average episode reward sum: 0.7792674215742732
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.48361568, 1.58464868, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6626986344504893}
episode index:3525
target Thresh 16.06537856056345
target distance 2.0
model initialize at round 3525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([12.96296322,  8.71540773,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 3.4880103295856175}
done in step count: 5
reward sum = 0.9424909614182495
running average episode reward sum: 0.7793137129922664
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.27830635,  7.48235975,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5568889917926804}
episode index:3526
target Thresh 16.066845504516618
target distance 5.0
model initialize at round 3526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([4.63290644, 7.        , 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 3.2968082762293633}
done in step count: 1
reward sum = 0.9801516412640265
running average episode reward sum: 0.779370655982987
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.22352325, 9.00000001, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.0246768392438683}
episode index:3527
target Thresh 16.068311715181146
target distance 1.0
model initialize at round 3527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.94932526, 9.41698036, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.4200482665014457}
done in step count: 0
reward sum = 0.9991936815664662
running average episode reward sum: 0.7794329640968145
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.94932526, 9.41698036, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.4200482665014457}
episode index:3528
target Thresh 16.06977719292359
target distance 6.0
model initialize at round 3528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([13.26201558,  7.66888177,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 5.412487015880965}
done in step count: 11
reward sum = 0.8774954121859088
running average episode reward sum: 0.7794607516989933
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.65595963,  2.9738251 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.34503464075976115}
episode index:3529
target Thresh 16.07124193811031
target distance 7.0
model initialize at round 3529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([10., 11.,  0.]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 5.0990195135928005}
done in step count: 3
reward sum = 0.9506266766959605
running average episode reward sum: 0.7795092406295875
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.01752352, 9.7813773 , 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.0065067922552733}
episode index:3530
target Thresh 16.07270595110751
target distance 2.0
model initialize at round 3530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.36502194, 8.23790044, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.050743946160367}
done in step count: 13
reward sum = 0.8575203194332369
running average episode reward sum: 0.7795313338266432
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.69728426, 7.54145968, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.620334912342247}
episode index:3531
target Thresh 16.07416923228118
target distance 11.0
model initialize at round 3531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4.1647085, 6.       , 0.       ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.617577830445597}
done in step count: 14
reward sum = 0.8296831007626865
running average episode reward sum: 0.7795455330811552
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.4931343 ,  9.85561909,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.5270281663248001}
episode index:3532
target Thresh 16.07563178199714
target distance 6.0
model initialize at round 3532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.15918851,  5.80291915,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.280473295081056}
done in step count: 2
reward sum = 0.9710179324124718
running average episode reward sum: 0.7795997284956275
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.97242254,  9.55427384,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.44657846190950223}
episode index:3533
target Thresh 16.07709360062103
target distance 5.0
model initialize at round 3533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.8320365, 8.       , 0.       ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.515161122875712}
done in step count: 9
reward sum = 0.8976609620973893
running average episode reward sum: 0.7796331357490519
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.18875052, 5.14127218, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.23576383725509653}
episode index:3534
target Thresh 16.078554688518306
target distance 11.0
model initialize at round 3534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 4.50536418, 11.        ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.302820456642438}
done in step count: 6
reward sum = 0.9204724178498084
running average episode reward sum: 0.7796729771301271
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.58604614,  7.64378118,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7653835737643913}
episode index:3535
target Thresh 16.080015046054235
target distance 10.0
model initialize at round 3535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.04437983, 10.29658145,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.481153636019954}
done in step count: 11
reward sum = 0.8682887053918518
running average episode reward sum: 0.7796980381392509
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.89958724, 5.60073447, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0817296832403842}
episode index:3536
target Thresh 16.081474673593917
target distance 11.0
model initialize at round 3536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.22758007,  9.        ,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.495152879182294}
done in step count: 6
reward sum = 0.9080066406450217
running average episode reward sum: 0.7797343142496569
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.12449208, 4.6772681 , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6886148071285002}
episode index:3537
target Thresh 16.08293357150225
target distance 3.0
model initialize at round 3537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.        , 10.49768388,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.117000110614413}
done in step count: 1
reward sum = 0.9832625468571103
running average episode reward sum: 0.7797918406014396
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.99750948,  9.60959965,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0711851357684234}
episode index:3538
target Thresh 16.08439174014396
target distance 4.0
model initialize at round 3538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([15.06097257,  5.68341899,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.885550254059993}
done in step count: 5
reward sum = 0.9373095320799513
running average episode reward sum: 0.7798363496976471
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.44796097,  2.9267681 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.45390741160695636}
episode index:3539
target Thresh 16.08584917988359
target distance 2.0
model initialize at round 3539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([12.38580179,  9.31410956,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.420954406446402}
done in step count: 28
reward sum = 0.7148413306323769
running average episode reward sum: 0.7798179895227699
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.11717518,  8.63786164,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.3806234585727597}
episode index:3540
target Thresh 16.0873058910855
target distance 4.0
model initialize at round 3540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([6.96938217, 8.98053384, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 2.030711135466672}
done in step count: 1
reward sum = 0.9829741367051581
running average episode reward sum: 0.7798753620579809
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.96938096, 8.27700995, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.7236381238856185}
episode index:3541
target Thresh 16.088761874113867
target distance 9.0
model initialize at round 3541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([15.19295204,  4.        ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 10.036860017183463}
done in step count: 47
reward sum = 0.5051531409813658
running average episode reward sum: 0.779797800730743
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.42143722, 11.8785832 ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0519711576681519}
episode index:3542
target Thresh 16.090217129332686
target distance 10.0
model initialize at round 3542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([14.16981328, 11.8754337 ,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 8.382308782732254}
done in step count: 33
reward sum = 0.6491661285321877
running average episode reward sum: 0.7797609303744917
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5.13802275, 9.54103934, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.9765498823834392}
episode index:3543
target Thresh 16.091671657105774
target distance 6.0
model initialize at round 3543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([7.        , 8.78582287, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.6483714933771045}
done in step count: 5
reward sum = 0.9348107304085821
running average episode reward sum: 0.7798046803180679
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.45307076, 4.34873486, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6486505942042271}
episode index:3544
target Thresh 16.09312545779676
target distance 9.0
model initialize at round 3544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.10490999,  2.64707867,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.91132766873844}
done in step count: 12
reward sum = 0.8603341289124183
running average episode reward sum: 0.7798273966646388
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.09888531, 10.1669178 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.2272056207936894}
episode index:3545
target Thresh 16.0945785317691
target distance 6.0
model initialize at round 3545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 7.78089976, 11.        ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 6.29898466076237}
done in step count: 9
reward sum = 0.8968740293029759
running average episode reward sum: 0.779860404739269
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.80830385, 10.8247002 ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8466863814744001}
episode index:3546
target Thresh 16.096030879386053
target distance 8.0
model initialize at round 3546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([11.99234343,  9.        ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 6.326980711218125}
done in step count: 3
reward sum = 0.9531136565854871
running average episode reward sum: 0.779909249749657
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.61556542,  3.        ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.6155654191971802}
episode index:3547
target Thresh 16.09748250101071
target distance 4.0
model initialize at round 3547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([5.        , 9.54667985, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.4722741445129373}
done in step count: 27
reward sum = 0.6949173145121283
running average episode reward sum: 0.7798852948637389
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.10743226, 11.52167841,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0338401833990043}
episode index:3548
target Thresh 16.09893339700598
target distance 5.0
model initialize at round 3548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4., 9., 0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.605551275464015}
done in step count: 2
reward sum = 0.9652148192704454
running average episode reward sum: 0.7799375150734901
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 8.       , 10.9185132,  0.       ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0033145560502439}
episode index:3549
target Thresh 16.100383567734582
target distance 11.0
model initialize at round 3549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.7823947 , 11.87906543,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.129049534531863}
done in step count: 14
reward sum = 0.834853776512826
running average episode reward sum: 0.7799529844429095
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.72566042, 9.61507788, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9512643403178296}
episode index:3550
target Thresh 16.10183301355906
target distance 13.0
model initialize at round 3550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.99983213, 7.00007785, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.083231010926346}
done in step count: 14
reward sum = 0.8294181954446841
running average episode reward sum: 0.7799669143812373
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.84967581,  2.94795306,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.2730137377028656}
episode index:3551
target Thresh 16.10328173484178
target distance 4.0
model initialize at round 3551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([15.11141241,  9.        ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 4.1114124059676715}
done in step count: 11
reward sum = 0.8774419859816973
running average episode reward sum: 0.7799943566874311
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.39907958,  8.10442621,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.0784979211426309}
episode index:3552
target Thresh 16.104729731944914
target distance 2.0
model initialize at round 3552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3., 8., 0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9999999999999813}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.780054589066693
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3., 8., 0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9999999999999813}
episode index:3553
target Thresh 16.10617700523047
target distance 11.0
model initialize at round 3553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.58447313, 6.75828075, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.36751751096179}
done in step count: 37
reward sum = 0.6122012694333989
running average episode reward sum: 0.7800073596576796
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.14010562,  2.94343121,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.2765111804077591}
episode index:3554
target Thresh 16.10762355506026
target distance 11.0
model initialize at round 3554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.219544457292901}
done in step count: 19
reward sum = 0.7804947112108691
running average episode reward sum: 0.7800074967467242
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.63048042, 7.66454041, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.7603674587937397}
episode index:3555
target Thresh 16.109069381795926
target distance 4.0
model initialize at round 3555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([12.        ,  9.26592147,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.8296531267320892}
done in step count: 2
reward sum = 0.9705839502211822
running average episode reward sum: 0.7800610896751478
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.84390187,  6.7995646 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1625291050959219}
episode index:3556
target Thresh 16.11051448579892
target distance 14.0
model initialize at round 3556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.02619884, 11.86775028,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.170371793518289}
done in step count: 6
reward sum = 0.9093511976789075
running average episode reward sum: 0.7800974377516178
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.10973015, 9.81375636, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.21616521137554215}
episode index:3557
target Thresh 16.111958867430523
target distance 3.0
model initialize at round 3557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([13.87393451,  4.22215617,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.45230915171036}
done in step count: 1
reward sum = 0.9862568179786373
running average episode reward sum: 0.7801553802418446
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.15920937,  3.79708838,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.158567553219946}
episode index:3558
target Thresh 16.113402527051825
target distance 12.0
model initialize at round 3558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.658675142976174}
done in step count: 7
reward sum = 0.8860660314157203
running average episode reward sum: 0.7801851387838997
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.86691401, 11.8712681 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.2290842165476392}
episode index:3559
target Thresh 16.114845465023745
target distance 3.0
model initialize at round 3559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 7.858922, 11.      ,  0.      ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.858922004699653}
done in step count: 1
reward sum = 0.9810450728900594
running average episode reward sum: 0.7802415601137048
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.87785785, 11.86966476,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.2356986708269075}
episode index:3560
target Thresh 16.116287681707014
target distance 8.0
model initialize at round 3560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 6.392304852660002}
done in step count: 3
reward sum = 0.9513418957737314
running average episode reward sum: 0.7802896085090039
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.16079629,  9.94495606,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.1699567061447389}
episode index:3561
target Thresh 16.117729177462195
target distance 12.0
model initialize at round 3561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([5.17664066, 7.14612992, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.193287354540534}
done in step count: 17
reward sum = 0.7825350078588786
running average episode reward sum: 0.7802902388850145
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.44238936, 10.94000726,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0389042304555085}
episode index:3562
target Thresh 16.11916995264965
target distance 1.0
model initialize at round 3562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.19113356, 10.69932956,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.35627903917597215}
done in step count: 0
reward sum = 0.999605804623764
running average episode reward sum: 0.7803517925099761
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.19113356, 10.69932956,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.35627903917597215}
episode index:3563
target Thresh 16.12061000762958
target distance 6.0
model initialize at round 3563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([14.22322807,  6.        ,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 4.182856309304352}
done in step count: 3
reward sum = 0.9605862440425333
running average episode reward sum: 0.7804023633437395
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.12654193, 10.17726549,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.21779787518795315}
episode index:3564
target Thresh 16.122049342761994
target distance 7.0
model initialize at round 3564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 5.099019513592808}
done in step count: 8
reward sum = 0.8966240554200177
running average episode reward sum: 0.7804349640988801
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.62392414, 9.28768803, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.4734949356877622}
episode index:3565
target Thresh 16.12348795840673
target distance 5.0
model initialize at round 3565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.24815655,  7.        ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 3.010246114870052}
done in step count: 15
reward sum = 0.8300263362801872
running average episode reward sum: 0.780448870821309
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.2550088 ,  4.79202898,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8320693491333033}
episode index:3566
target Thresh 16.124925854923443
target distance 12.0
model initialize at round 3566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.10686327,  4.33754473,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.187110336751404}
done in step count: 28
reward sum = 0.6973440482488947
running average episode reward sum: 0.780425572581171
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.09174564, 2.54189062, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0172463777965566}
episode index:3567
target Thresh 16.126363032671602
target distance 10.0
model initialize at round 3567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([11.13834751, 10.49752665,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 8.512951279240136}
done in step count: 6
reward sum = 0.9154963321087151
running average episode reward sum: 0.7804634287357471
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.05282566, 7.83860771, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9608260543784206}
episode index:3568
target Thresh 16.127799492010503
target distance 9.0
model initialize at round 3568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.08753602,  3.49543157,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.582959968501919}
done in step count: 17
reward sum = 0.8027252283349412
running average episode reward sum: 0.7804696662811658
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.39236641, 11.47977479,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6197864571617439}
episode index:3569
target Thresh 16.129235233299266
target distance 10.0
model initialize at round 3569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([13.0030676 , 11.85229305,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 8.214626009430424}
done in step count: 4
reward sum = 0.9408540139163031
running average episode reward sum: 0.7805145918687386
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.00280902, 9.06490221, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.9351020061827101}
episode index:3570
target Thresh 16.13067025689682
target distance 1.0
model initialize at round 3570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.       ,  3.8053441,  0.       ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0187693151783555}
done in step count: 0
reward sum = 0.9959295975749682
running average episode reward sum: 0.7805749153091492
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.       ,  3.8053441,  0.       ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0187693151783555}
episode index:3571
target Thresh 16.132104563161924
target distance 2.0
model initialize at round 3571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.        ,  5.40206003,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0777997352217337}
done in step count: 0
reward sum = 0.9950849402944091
running average episode reward sum: 0.7806349685076334
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.        ,  5.40206003,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0777997352217337}
episode index:3572
target Thresh 16.133538152453156
target distance 11.0
model initialize at round 3572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 2.67225718, 10.        ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 12.68752311971176}
done in step count: 32
reward sum = 0.6245190481428278
running average episode reward sum: 0.7805912752749535
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.94126181,  7.04000065,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.07106494964706274}
episode index:3573
target Thresh 16.13497102512891
target distance 3.0
model initialize at round 3573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.        ,  8.73698699,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.2422358191667018}
done in step count: 1
reward sum = 0.9824919533477563
running average episode reward sum: 0.7806477667909224
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.        ,  8.05865932,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0017189801780806}
episode index:3574
target Thresh 16.1364031815474
target distance 5.0
model initialize at round 3574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4., 5., 0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.162277660168391}
done in step count: 2
reward sum = 0.9669448043468825
running average episode reward sum: 0.7806998778503785
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.44381523, 9.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.1442646096545728}
episode index:3575
target Thresh 16.137834622066677
target distance 12.0
model initialize at round 3575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 4.29934847, 10.        ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 13.634707409528149}
done in step count: 14
reward sum = 0.833451692003006
running average episode reward sum: 0.7807146294762602
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.55616625,  3.98621161,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.1322253437841225}
episode index:3576
target Thresh 16.139265347044592
target distance 13.0
model initialize at round 3576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4., 4., 0.]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.704699910719649}
done in step count: 14
reward sum = 0.8251977512050099
running average episode reward sum: 0.7807270653503806
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.46299102,  8.24148265,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5888060071613967}
episode index:3577
target Thresh 16.14069535683883
target distance 6.0
model initialize at round 3577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([4.        , 4.82840049, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 5.978749155486881}
done in step count: 28
reward sum = 0.6800380470316826
running average episode reward sum: 0.780698924204959
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.08367127, 10.22190478,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.9428149735272959}
episode index:3578
target Thresh 16.142124651806892
target distance 8.0
model initialize at round 3578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([11.        , 10.24160886,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 6.127119433839503}
done in step count: 3
reward sum = 0.9520808511413098
running average episode reward sum: 0.7807468096274055
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.41416711, 9.12036109, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.43130173615162765}
episode index:3579
target Thresh 16.1435532323061
target distance 11.0
model initialize at round 3579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([12.85736418,  9.86162698,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.468491592884654}
done in step count: 10
reward sum = 0.869834694767199
running average episode reward sum: 0.7807716945115228
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.31402548, 4.54365441, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8752834695206235}
episode index:3580
target Thresh 16.144981098693602
target distance 9.0
model initialize at round 3580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.08808772,  3.48663791,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.798122870075267}
done in step count: 6
reward sum = 0.9179108570780581
running average episode reward sum: 0.7808099908428734
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.03890526, 10.7216512 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.28105457299698916}
episode index:3581
target Thresh 16.146408251326367
target distance 13.0
model initialize at round 3581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.33493137,  9.2704978 ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.358382046115375}
done in step count: 16
reward sum = 0.8220746349304549
running average episode reward sum: 0.7808215108440144
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.98692585, 9.30197378, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.2088272175524832}
episode index:3582
target Thresh 16.147834690561176
target distance 1.0
model initialize at round 3582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.86698035, 5.96197827, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8678136747254102}
done in step count: 0
reward sum = 0.9971762954714334
running average episode reward sum: 0.7808818945405334
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.86698035, 5.96197827, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8678136747254102}
episode index:3583
target Thresh 16.149260416754647
target distance 6.0
model initialize at round 3583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 8.59827662, 11.        ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 4.598276615142849}
done in step count: 4
reward sum = 0.9490438947486449
running average episode reward sum: 0.7809288147414843
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.97639563, 11.86077756,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.301647583683419}
episode index:3584
target Thresh 16.150685430263206
target distance 9.0
model initialize at round 3584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([11.39169923, 11.81756721,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.812022878635842}
done in step count: 5
reward sum = 0.9304968777386163
running average episode reward sum: 0.7809705352611489
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.89762746, 4.58899366, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.42356387077366064}
episode index:3585
target Thresh 16.152109731443105
target distance 9.0
model initialize at round 3585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([10.03436685, 10.25008535,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 8.66683941381071}
done in step count: 6
reward sum = 0.9204529269090514
running average episode reward sum: 0.7810094316336106
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.13588152, 7.94277871, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.2788793677510244}
episode index:3586
target Thresh 16.153533320650425
target distance 4.0
model initialize at round 3586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.00002692,  7.04329118,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.16511101244017}
done in step count: 14
reward sum = 0.843578670334789
running average episode reward sum: 0.7810268749675112
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.23236474,  3.85785566,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8887686443454234}
episode index:3587
target Thresh 16.154956198241056
target distance 6.0
model initialize at round 3587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 6.11318004, 10.62178111,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 4.421359974429444}
done in step count: 6
reward sum = 0.9229887822498178
running average episode reward sum: 0.781066440716475
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.22095785, 8.31552696, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7192535813695686}
episode index:3588
target Thresh 16.156378364570724
target distance 5.0
model initialize at round 3588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.61188954,  8.04239941,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.0670545930148534}
done in step count: 2
reward sum = 0.9703288256666438
running average episode reward sum: 0.7811191747329003
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.14536071,  4.04239941,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.2835136212076954}
episode index:3589
target Thresh 16.15779981999497
target distance 12.0
model initialize at round 3589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.60751712, 10.13589621,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.785450749006463}
done in step count: 20
reward sum = 0.7646234617200971
running average episode reward sum: 0.7811145798267685
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.8819833 , 4.31399631, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.11736100632372}
episode index:3590
target Thresh 16.159220564869155
target distance 7.0
model initialize at round 3590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([10.99505776,  8.1325968 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 5.904456753197366}
done in step count: 2
reward sum = 0.9622794006099732
running average episode reward sum: 0.781165029512311
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.20442761,  4.89597299,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8023447142900665}
episode index:3591
target Thresh 16.160640599548465
target distance 6.0
model initialize at round 3591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([9.97380116, 8.13224972, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 5.769381671780252}
done in step count: 23
reward sum = 0.7469705870919847
running average episode reward sum: 0.7811555099013923
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.14860335,  3.62846198,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.928933124476831}
episode index:3592
target Thresh 16.162059924387915
target distance 3.0
model initialize at round 3592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([12.06460977,  8.96592903,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.163042822388154}
done in step count: 21
reward sum = 0.7880541848863167
running average episode reward sum: 0.7811574299333948
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.48788477,  8.73647877,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8834209210094169}
episode index:3593
target Thresh 16.16347853974233
target distance 12.0
model initialize at round 3593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 3.99505755, 11.86740305,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 13.368983143262065}
done in step count: 16
reward sum = 0.8108875154145846
running average episode reward sum: 0.7811657020773795
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.17763314,  2.77688411,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8520962098218663}
episode index:3594
target Thresh 16.16489644596636
target distance 9.0
model initialize at round 3594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([11.,  9.,  0.]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.899494936611694}
done in step count: 29
reward sum = 0.661733255693794
running average episode reward sum: 0.7811324802564105
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.43072591, 1.10247064, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.0628414465174265}
episode index:3595
target Thresh 16.16631364341449
target distance 2.0
model initialize at round 3595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.21446902, 4.        , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7855309844017118}
done in step count: 0
reward sum = 0.9965372055806643
running average episode reward sum: 0.7811923814592259
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.21446902, 4.        , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7855309844017118}
episode index:3596
target Thresh 16.167730132441015
target distance 7.0
model initialize at round 3596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([14.        ,  8.68848944,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 5.169144992800049}
done in step count: 3
reward sum = 0.9604215909940862
running average episode reward sum: 0.7812422088736086
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.91294742, 10.59768581,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.0911926122737519}
episode index:3597
target Thresh 16.169145913400058
target distance 10.0
model initialize at round 3597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 7.        , 10.73059362,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 9.84071660284059}
done in step count: 4
reward sum = 0.9357384266875614
running average episode reward sum: 0.78128514834493
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.67593439,  5.42446622,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5340319181086025}
episode index:3598
target Thresh 16.17056098664557
target distance 1.0
model initialize at round 3598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.7563839 ,  8.34290561,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8304823031561029}
done in step count: 0
reward sum = 0.9998673002023755
running average episode reward sum: 0.7813458824799279
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.7563839 ,  8.34290561,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8304823031561029}
episode index:3599
target Thresh 16.171975352531305
target distance 3.0
model initialize at round 3599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4., 8., 0.]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.4142135623731213}
done in step count: 1
reward sum = 0.980994050010924
running average episode reward sum: 0.781401340304242
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.99999999, 8.22004117, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.268201785222739}
episode index:3600
target Thresh 16.17338901141087
target distance 7.0
model initialize at round 3600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([15.19608533,  4.93690991,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 5.9874738145455115}
done in step count: 35
reward sum = 0.6680328626096539
running average episode reward sum: 0.7813698578055766
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.05839631,  9.03355475,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.9682079101582737}
episode index:3601
target Thresh 16.17480196363767
target distance 3.0
model initialize at round 3601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.89887616, 5.36614986, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 3.6352569151976524}
done in step count: 7
reward sum = 0.9163882916800388
running average episode reward sum: 0.7814073421014884
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.35634933, 9.29798131, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.7092806553459843}
episode index:3602
target Thresh 16.17621420956495
target distance 13.0
model initialize at round 3602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.        , 9.50400901, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.011176461384185}
done in step count: 6
reward sum = 0.9098783528977957
running average episode reward sum: 0.781442998779478
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.52557356,  9.85166507,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.4970751386074321}
episode index:3603
target Thresh 16.177625749545765
target distance 7.0
model initialize at round 3603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([4.91276409, 2.51919589, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 6.808625093086864}
done in step count: 27
reward sum = 0.7254539387213181
running average episode reward sum: 0.7814274635241899
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.42049149, 9.70655395, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.8222113997083159}
episode index:3604
target Thresh 16.179036583933
target distance 3.0
model initialize at round 3604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.        ,  9.18232226,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.0164848291278825}
done in step count: 1
reward sum = 0.9839726133799153
running average episode reward sum: 0.7814836480317783
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([14.        ,  8.97325034,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.000357708059909}
episode index:3605
target Thresh 16.18044671307937
target distance 4.0
model initialize at round 3605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.88115886, 4.77250507, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.9155603750454113}
done in step count: 30
reward sum = 0.6644252534647592
running average episode reward sum: 0.7814511859145938
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.04084437, 6.7891805 , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.21473966159496882}
episode index:3606
target Thresh 16.181856137337405
target distance 5.0
model initialize at round 3606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 5.09871399, 11.52490765,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.453602720405387}
done in step count: 2
reward sum = 0.9730120629226883
running average episode reward sum: 0.7815042940035898
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.64355659, 9.51649469, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8049487422263704}
episode index:3607
target Thresh 16.183264857059456
target distance 14.0
model initialize at round 3607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([15.7970545 , 11.86961787,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 15.883626736966704}
done in step count: 11
reward sum = 0.859914129551008
running average episode reward sum: 0.7815260262196505
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.57900447, 3.47834694, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6703425687602426}
episode index:3608
target Thresh 16.18467287259771
target distance 11.0
model initialize at round 3608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.82214129, 9.49173224, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.29829853150874}
done in step count: 5
reward sum = 0.932959910263665
running average episode reward sum: 0.7815679862872715
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.66627183,  7.16814565,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.0657860059282958}
episode index:3609
target Thresh 16.186080184304164
target distance 2.0
model initialize at round 3609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([15.09660399,  6.        ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.096603989601169}
done in step count: 1
reward sum = 0.9858867761324734
running average episode reward sum: 0.7816245842899987
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.45588493,  5.48579901,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6871926411024939}
episode index:3610
target Thresh 16.187486792530652
target distance 6.0
model initialize at round 3610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([13.68017459,  9.21298099,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 5.684166100981844}
done in step count: 4
reward sum = 0.9495761621062652
running average episode reward sum: 0.7816710953888123
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.84786153, 9.35048255, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.3820787021933734}
episode index:3611
target Thresh 16.18889269762882
target distance 7.0
model initialize at round 3611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([1.66839266, 9.        , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.516918777377367}
done in step count: 3
reward sum = 0.9537278777907064
running average episode reward sum: 0.7817187301569192
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.23687361, 4.5569611 , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6052394314396833}
episode index:3612
target Thresh 16.190297899950153
target distance 3.0
model initialize at round 3612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.31843853, 5.65939641, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.8802920365881381}
done in step count: 9
reward sum = 0.9005731522343599
running average episode reward sum: 0.7817516264818782
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.6584373 , 6.90048919, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.6659144650258749}
episode index:3613
target Thresh 16.19170239984594
target distance 5.0
model initialize at round 3613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([5., 8., 0.]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.162277660168403}
done in step count: 2
reward sum = 0.9653092209601226
running average episode reward sum: 0.7818024171831728
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.95942042, 8.13720995, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.290307792621662}
episode index:3614
target Thresh 16.193106197667316
target distance 6.0
model initialize at round 3614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([4.        , 5.72985125, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 5.85099738355025}
done in step count: 15
reward sum = 0.837305055018228
running average episode reward sum: 0.7818177706099597
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.7692575 , 9.92915928, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.2413721370338157}
episode index:3615
target Thresh 16.19450929376523
target distance 5.0
model initialize at round 3615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.        , 5.08632398, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 5.014400469024593}
done in step count: 4
reward sum = 0.9445177618103789
running average episode reward sum: 0.7818627650765527
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.49243942, 10.24356817,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.5493833227373166}
episode index:3616
target Thresh 16.19591168849045
target distance 5.0
model initialize at round 3616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([11.        , 10.38609385,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.0621692890802894}
done in step count: 2
reward sum = 0.9722151637851388
running average episode reward sum: 0.7819153922257672
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.71983618, 10.50496018,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.568820000211924}
episode index:3617
target Thresh 16.197313382193578
target distance 12.0
model initialize at round 3617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.        , 7.68175274, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.501404880000054}
done in step count: 9
reward sum = 0.8796706563863526
running average episode reward sum: 0.7819424113700901
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.34764129,  2.22917298,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6914420733345579}
episode index:3618
target Thresh 16.198714375225034
target distance 5.0
model initialize at round 3618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.43738472, 3.181952  , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.8592779740466705}
done in step count: 5
reward sum = 0.942161077964728
running average episode reward sum: 0.7819866828999588
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.25731236, 6.08619516, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9493413175522911}
episode index:3619
target Thresh 16.200114667935072
target distance 5.0
model initialize at round 3619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.82219362, 7.        , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 3.0052645653666}
done in step count: 2
reward sum = 0.9688072694901265
running average episode reward sum: 0.7820382907968069
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.45399166, 4.19225805, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.49302290170913415}
episode index:3620
target Thresh 16.20151426067376
target distance 6.0
model initialize at round 3620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 6.        , 10.74217796,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.008300413221359}
done in step count: 20
reward sum = 0.7699111880442171
running average episode reward sum: 0.7820349416935889
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.98379434, 11.8187938 ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.2799510112847607}
episode index:3621
target Thresh 16.202913153791002
target distance 6.0
model initialize at round 3621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([16.16416228,  7.        ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 5.774101443426325}
done in step count: 2
reward sum = 0.9652591883954221
running average episode reward sum: 0.7820855281780456
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.16416228, 11.        ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.16416227817530427}
episode index:3622
target Thresh 16.204311347636516
target distance 4.0
model initialize at round 3622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.08839517,  5.45106733,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.6151012281972714}
done in step count: 1
reward sum = 0.9792575933713893
running average episode reward sum: 0.7821399504979996
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.08725455,  3.49215322,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0369759153994704}
episode index:3623
target Thresh 16.20570884255985
target distance 2.0
model initialize at round 3623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.        , 3.09363651, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9063634872436559}
done in step count: 0
reward sum = 0.9961029692967153
running average episode reward sum: 0.7821989910661008
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.        , 3.09363651, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9063634872436559}
episode index:3624
target Thresh 16.207105638910384
target distance 5.0
model initialize at round 3624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([3.89429069, 8.64912641, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.8951299052919315}
done in step count: 2
reward sum = 0.9721520923321686
running average episode reward sum: 0.7822513919216224
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.10126996, 10.89464021,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9048847274614662}
episode index:3625
target Thresh 16.20850173703731
target distance 8.0
model initialize at round 3625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([4.        , 8.52129269, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 6.597519102095007}
done in step count: 4
reward sum = 0.9364038410297701
running average episode reward sum: 0.7822939050074218
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.76026845, 1.21351673, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.822208698435567}
episode index:3626
target Thresh 16.20989713728966
target distance 7.0
model initialize at round 3626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 5.099019513592799}
done in step count: 3
reward sum = 0.957553278279923
running average episode reward sum: 0.7823422257610122
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.16372738,  3.35323867,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9078157612009725}
episode index:3627
target Thresh 16.211291840016276
target distance 1.0
model initialize at round 3627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([1.62840581, 5.9400363 , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.37235213460653}
done in step count: 6
reward sum = 0.9324049241950375
running average episode reward sum: 0.7823835881365453
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.38112357, 6.97235808, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1526006561173863}
episode index:3628
target Thresh 16.21268584556584
target distance 12.0
model initialize at round 3628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.51634371,  4.4402349 ,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.795750620069324}
done in step count: 12
reward sum = 0.8595888978948334
running average episode reward sum: 0.7824048626776746
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.70585102, 2.95844108, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.1903087747964114}
episode index:3629
target Thresh 16.21407915428685
target distance 7.0
model initialize at round 3629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([11.5767611,  9.       ,  0.       ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 5.556265542825466}
done in step count: 4
reward sum = 0.951996170083194
running average episode reward sum: 0.7824515820461059
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.19698049,  4.68292109,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0541449395817974}
episode index:3630
target Thresh 16.21547176652763
target distance 9.0
model initialize at round 3630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 8.        , 10.74581373,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 7.039619174070915}
done in step count: 4
reward sum = 0.9410708616008847
running average episode reward sum: 0.7824952667829703
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.35215737, 10.86030374,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.076950600463814}
episode index:3631
target Thresh 16.216863682636344
target distance 7.0
model initialize at round 3631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 7.        , 10.10266519,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 5.001053902969043}
done in step count: 3
reward sum = 0.9551958728142518
running average episode reward sum: 0.7825428165093006
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.11413647, 10.70504831,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.1321869597747283}
episode index:3632
target Thresh 16.21825490296096
target distance 7.0
model initialize at round 3632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.12228262,  3.82146466,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.25239146316116}
done in step count: 3
reward sum = 0.9588243903788005
running average episode reward sum: 0.7825913388252569
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.90133327,  9.64041448,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.647970543080318}
episode index:3633
target Thresh 16.219645427849287
target distance 9.0
model initialize at round 3633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([10.65029325,  8.22319158,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 10.656251066457521}
done in step count: 35
reward sum = 0.6431946220235839
running average episode reward sum: 0.7825529797947666
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.14936682, 2.2547952 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8879737570519473}
episode index:3634
target Thresh 16.221035257648957
target distance 5.0
model initialize at round 3634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.17931449,  9.16199648,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.242138544499298}
done in step count: 3
reward sum = 0.9652413170511928
running average episode reward sum: 0.7826032379343144
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.36923721,  5.59827685,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.7030443186253245}
episode index:3635
target Thresh 16.22242439270743
target distance 8.0
model initialize at round 3635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 5.       , 10.3815608,  0.       ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.031788046761463}
done in step count: 3
reward sum = 0.9550340090104346
running average episode reward sum: 0.7826506611386808
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.97958931, 11.47527752,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.4757155843466012}
episode index:3636
target Thresh 16.223812833371984
target distance 6.0
model initialize at round 3636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 9.99505755, 11.86740305,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 4.418909163121902}
done in step count: 12
reward sum = 0.8588916145265076
running average episode reward sum: 0.7826716237324085
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.74019808, 10.59677764,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6508767812764013}
episode index:3637
target Thresh 16.225200579989732
target distance 2.0
model initialize at round 3637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.28392476,  8.        ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.2839247584343525}
done in step count: 0
reward sum = 0.9966154270348461
running average episode reward sum: 0.7827304318146797
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.28392476,  8.        ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.2839247584343525}
episode index:3638
target Thresh 16.22658763290761
target distance 13.0
model initialize at round 3638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.63021436, 11.88350806,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.54145360676774}
done in step count: 21
reward sum = 0.7718160987372216
running average episode reward sum: 0.7827274325475521
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.54575079, 7.6014111 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8121202102743194}
episode index:3639
target Thresh 16.227973992472382
target distance 13.0
model initialize at round 3639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.02004712, 11.86567881,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 13.539214962064689}
done in step count: 14
reward sum = 0.8385524194687572
running average episode reward sum: 0.7827427690824206
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.79402863, 4.64660779, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6786205390401813}
episode index:3640
target Thresh 16.22935965903064
target distance 6.0
model initialize at round 3640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([11.20413552,  8.12424849,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 5.837360767321594}
done in step count: 21
reward sum = 0.7693563972547288
running average episode reward sum: 0.7827390925177878
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.0980203 ,  3.80478225,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8107295758005574}
episode index:3641
target Thresh 16.230744632928797
target distance 3.0
model initialize at round 3641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4., 7., 0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.4142135623730956}
done in step count: 1
reward sum = 0.9806747925556266
running average episode reward sum: 0.7827934405957774
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.30833918, 9.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0464573804203863}
episode index:3642
target Thresh 16.232128914513098
target distance 9.0
model initialize at round 3642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([7.        , 8.82900262, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.779329055075083}
done in step count: 6
reward sum = 0.9167418675448366
running average episode reward sum: 0.7828302093102844
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.71968535,  1.71840245,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.3973329644913665}
episode index:3643
target Thresh 16.23351250412961
target distance 1.0
model initialize at round 3643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.0842403,  9.       ,  0.       ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.0035419416027154}
done in step count: 0
reward sum = 0.9969946775692285
running average episode reward sum: 0.7828889811182588
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.0842403,  9.       ,  0.       ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.0035419416027154}
episode index:3644
target Thresh 16.234895402124238
target distance 3.0
model initialize at round 3644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([10.        ,  9.33103824,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.9456190191378133}
done in step count: 1
reward sum = 0.9812342389739395
running average episode reward sum: 0.7829433968268612
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.        , 10.30662262,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.216869830753329}
episode index:3645
target Thresh 16.2362776088427
target distance 4.0
model initialize at round 3645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.13259695, 5.00494245, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 2.184532509686137}
done in step count: 2
reward sum = 0.9674623775784743
running average episode reward sum: 0.7829940054337596
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.59941499, 3.80396049, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0028214190182525}
episode index:3646
target Thresh 16.23765912463055
target distance 6.0
model initialize at round 3646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.86775037, 5.97380131, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 4.02837011904985}
done in step count: 62
reward sum = 0.40366750874531215
running average episode reward sum: 0.7828899948780458
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.71653682, 10.11172647,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.725195016916039}
episode index:3647
target Thresh 16.239039949833167
target distance 11.0
model initialize at round 3647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([12.01221359, 10.42627609,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 10.028637995980251}
done in step count: 32
reward sum = 0.6609770057675427
running average episode reward sum: 0.782856575747259
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.13225891, 10.7476881 ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9036790830370437}
episode index:3648
target Thresh 16.240420084795755
target distance 4.0
model initialize at round 3648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 2.49215829, 10.55395842,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.536086329681356}
done in step count: 4
reward sum = 0.9519335799204309
running average episode reward sum: 0.7829029109087204
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.01437976, 10.68111156,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0359233018976668}
episode index:3649
target Thresh 16.241799529863354
target distance 1.0
model initialize at round 3649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.        , 9.81761062, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0164968693542331}
done in step count: 0
reward sum = 0.9969750502883674
running average episode reward sum: 0.7829615608099204
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.        , 9.81761062, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0164968693542331}
episode index:3650
target Thresh 16.243178285380814
target distance 2.0
model initialize at round 3650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([1.09530411, 4.60532232, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.9451572910064847}
done in step count: 12
reward sum = 0.8686818111326775
running average episode reward sum: 0.7829850393775245
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.37481056, 5.9014714 , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9762856327778487}
episode index:3651
target Thresh 16.24455635169284
target distance 9.0
model initialize at round 3651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([4.86550196, 5.98028357, 0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 9.073487677033405}
done in step count: 4
reward sum = 0.9363836148722475
running average episode reward sum: 0.7830270433686238
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.08911424, 10.92216981,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.2961905807466358}
episode index:3652
target Thresh 16.245933729143935
target distance 1.0
model initialize at round 3652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.88989623,  1.34402693,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1055387635414538}
done in step count: 0
reward sum = 0.9966155531469311
running average episode reward sum: 0.7830855127115689
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.88989623,  1.34402693,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1055387635414538}
episode index:3653
target Thresh 16.24731041807845
target distance 1.0
model initialize at round 3653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.55449378,  3.07195544,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.4766523258801025}
done in step count: 4
reward sum = 0.9573149972472266
running average episode reward sum: 0.7831331945628376
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.93960756,  4.27505845,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1867614003504179}
episode index:3654
target Thresh 16.248686418840553
target distance 6.0
model initialize at round 3654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.5255512,  8.       ,  0.       ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.034377779005095}
done in step count: 2
reward sum = 0.967229168270134
running average episode reward sum: 0.7831835628182978
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.99092042,  4.        ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.00907957553871519}
episode index:3655
target Thresh 16.250061731774245
target distance 14.0
model initialize at round 3655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.04577245,  1.16469544,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 14.938649221211802}
done in step count: 11
reward sum = 0.8508227748135359
running average episode reward sum: 0.7832020636968523
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.41602012, 10.71888411,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9261894337200406}
episode index:3656
target Thresh 16.25143635722336
target distance 10.0
model initialize at round 3656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.30472851, 8.48653519, 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.303116855859285}
done in step count: 37
reward sum = 0.6163634523428729
running average episode reward sum: 0.7831564419819619
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.7061422 ,  5.71754064,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0067280598202546}
episode index:3657
target Thresh 16.252810295531546
target distance 2.0
model initialize at round 3657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.74542761,  6.        ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7454276084900737}
done in step count: 0
reward sum = 0.9965832535926165
running average episode reward sum: 0.7832147872011009
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.74542761,  6.        ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7454276084900737}
episode index:3658
target Thresh 16.254183547042295
target distance 10.0
model initialize at round 3658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([12.35332349, 11.89833528,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 8.566311353833237}
done in step count: 25
reward sum = 0.7275497473559855
running average episode reward sum: 0.7831995740172133
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.71577137, 10.87721486,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.1321813279874307}
episode index:3659
target Thresh 16.255556112098912
target distance 4.0
model initialize at round 3659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([6.00233817, 9.34030044, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 3.02156232487034}
done in step count: 2
reward sum = 0.973618361257873
running average episode reward sum: 0.7832516010082626
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.93616247, 8.51986706, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.48435820176486394}
episode index:3660
target Thresh 16.256927991044545
target distance 6.0
model initialize at round 3660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([1.71484983, 7.        , 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.628183184168918}
done in step count: 3
reward sum = 0.953965799092344
running average episode reward sum: 0.7832982314912137
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.2859978 , 11.88384487,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.136213398907241}
episode index:3661
target Thresh 16.258299184222164
target distance 6.0
model initialize at round 3661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([4.8674032 , 5.99505776, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 4.5373485063888666}
done in step count: 2
reward sum = 0.9640322445386997
running average episode reward sum: 0.7833475853997466
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 7.37501918, 10.20442759,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.4271182806224155}
episode index:3662
target Thresh 16.259669691974562
target distance 1.0
model initialize at round 3662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.66453183, 7.        , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 2.1075109843272335}
done in step count: 1
reward sum = 0.9840355255238697
running average episode reward sum: 0.7834023732621883
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.04669344, 5.87684321, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.2952402909234193}
episode index:3663
target Thresh 16.26103951464437
target distance 5.0
model initialize at round 3663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.        , 9.98555547, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.1668750692863887}
done in step count: 2
reward sum = 0.9708863685445105
running average episode reward sum: 0.7834535424748745
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.56795067, 10.69511628,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.5287917457500844}
episode index:3664
target Thresh 16.262408652574038
target distance 5.0
model initialize at round 3664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.86541545, 7.40330195, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.40535822612858}
done in step count: 17
reward sum = 0.8173846613017245
running average episode reward sum: 0.7834628006246227
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.50695306, 2.06363149, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.058244425753486}
episode index:3665
target Thresh 16.26377710610586
target distance 8.0
model initialize at round 3665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([8.48222686, 8.28064978, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 7.265117207778686}
done in step count: 5
reward sum = 0.9325172617636859
running average episode reward sum: 0.7835034592337714
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.89852495, 4.20702224, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.7994441295342242}
episode index:3666
target Thresh 16.265144875581942
target distance 7.0
model initialize at round 3666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.27695704, 8.        , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 5.007664645248521}
done in step count: 4
reward sum = 0.9496119399250926
running average episode reward sum: 0.7835487574286695
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.8366462 , 2.44817793, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0022397195997474}
episode index:3667
target Thresh 16.266511961344232
target distance 4.0
model initialize at round 3667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.35505807,  8.12906837,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 3.17465443496353}
done in step count: 4
reward sum = 0.9538305751748603
running average episode reward sum: 0.7835951810431041
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.89135838, 10.92120111,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8948346320660913}
episode index:3668
target Thresh 16.267878363734493
target distance 8.0
model initialize at round 3668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([10.06157291, 11.3204633 ,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 8.278418633932162}
done in step count: 5
reward sum = 0.937545302213784
running average episode reward sum: 0.7836371407381629
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.45720148, 7.88208407, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.035713542350757}
episode index:3669
target Thresh 16.269244083094335
target distance 14.0
model initialize at round 3669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([2.89002204, 7.59098268, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 13.185478828910933}
done in step count: 25
reward sum = 0.7038949705367606
running average episode reward sum: 0.7836154126263913
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.87184848,  8.83903936,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8865822600377109}
episode index:3670
target Thresh 16.27060911976518
target distance 3.0
model initialize at round 3670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([5., 9., 0.]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.162277660168368}
done in step count: 30
reward sum = 0.6770668266715452
running average episode reward sum: 0.783586388222699
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.58298555, 7.46745806, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6763889239900236}
episode index:3671
target Thresh 16.271973474088295
target distance 2.0
model initialize at round 3671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([7., 9., 0.]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9999999999999805}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7836436904046115
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([7., 9., 0.]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9999999999999805}
episode index:3672
target Thresh 16.27333714640476
target distance 2.0
model initialize at round 3672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.99110627, 4.35211998, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 2.093860717141167}
done in step count: 1
reward sum = 0.9880072123624646
running average episode reward sum: 0.7836993298061791
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.56210446, 4.8187834 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.5905936701351384}
episode index:3673
target Thresh 16.2747001370555
target distance 13.0
model initialize at round 3673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4., 7., 0.]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.04536101718728}
done in step count: 7
reward sum = 0.9023718116454099
running average episode reward sum: 0.7837316304272567
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.32208229,  7.86990227,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6902882265129644}
episode index:3674
target Thresh 16.27606244638126
target distance 1.0
model initialize at round 3674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.12541631, 7.82075012, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8927637730225142}
done in step count: 0
reward sum = 0.996978466632146
running average episode reward sum: 0.7837896567772444
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.12541631, 7.82075012, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8927637730225142}
episode index:3675
target Thresh 16.277424074722617
target distance 3.0
model initialize at round 3675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.75800484,  4.18435383,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.7161718905680337}
done in step count: 2
reward sum = 0.974998455291423
running average episode reward sum: 0.783841672228418
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.96373479,  3.0069258 ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.03692061656389866}
episode index:3676
target Thresh 16.278785022419978
target distance 2.0
model initialize at round 3676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.49745929, 7.64472151, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.7197837078386318}
done in step count: 1
reward sum = 0.9860598599666782
running average episode reward sum: 0.7838966676561413
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.00616217, 6.01995182, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.020881748810148735}
episode index:3677
target Thresh 16.280145289813582
target distance 12.0
model initialize at round 3677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([5.60239816, 8.56673369, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.297646742922256}
done in step count: 7
reward sum = 0.8976474084152791
running average episode reward sum: 0.783927594991856
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.79777999,  2.51231383,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.948113059922236}
episode index:3678
target Thresh 16.281504877243492
target distance 9.0
model initialize at round 3678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([4., 9., 0.]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 7.000000000000019}
done in step count: 5
reward sum = 0.9239126313300225
running average episode reward sum: 0.7839656447435109
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.67242998,  8.83488186,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.6924060072718974}
episode index:3679
target Thresh 16.28286378504961
target distance 1.0
model initialize at round 3679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.       ,  4.8110134,  0.       ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.2875335863111521}
done in step count: 0
reward sum = 0.9969732127928846
running average episode reward sum: 0.7840235272348287
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.       ,  4.8110134,  0.       ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.2875335863111521}
episode index:3680
target Thresh 16.284222013571657
target distance 3.0
model initialize at round 3680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.36587834, 10.62948024,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 3.647875243845882}
done in step count: 3
reward sum = 0.9625367408162722
running average episode reward sum: 0.784072023082039
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.61900271,  7.36782482,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5295791097150183}
episode index:3681
target Thresh 16.28557956314919
target distance 7.0
model initialize at round 3681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 8.        , 11.23369694,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 7.99118123366985}
done in step count: 10
reward sum = 0.875866675275808
running average episode reward sum: 0.7840969537317387
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.67136806, 4.18699174, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0543801540556326}
episode index:3682
target Thresh 16.286936434121607
target distance 11.0
model initialize at round 3682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.37777174,  6.86387885,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.543822387584658}
done in step count: 8
reward sum = 0.8889095955894594
running average episode reward sum: 0.7841254122280346
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.50596217, 5.60636277, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.7821439686158942}
episode index:3683
target Thresh 16.288292626828117
target distance 1.0
model initialize at round 3683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([11.14137399, 10.33610851,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.93470519302639}
done in step count: 16
reward sum = 0.8141328604374327
running average episode reward sum: 0.7841335575722825
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.48006832, 10.92960383,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.4852022377764758}
episode index:3684
target Thresh 16.289648141607767
target distance 4.0
model initialize at round 3684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5., 8., 0.]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 2.2360679774998147}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.7841861834726452
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 7., 10.,  0.]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.9999999999999618}
episode index:3685
target Thresh 16.29100297879944
target distance 9.0
model initialize at round 3685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4., 4., 0.]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.062257748298576}
done in step count: 3
reward sum = 0.9466566068092447
running average episode reward sum: 0.7842302611783795
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.08250979, 10.1829104 ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8212449618480665}
episode index:3686
target Thresh 16.292357138741846
target distance 6.0
model initialize at round 3686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([4.99983213, 7.00007785, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 4.4722512889936965}
done in step count: 26
reward sum = 0.6991941371436433
running average episode reward sum: 0.7842071974072824
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.61312685, 9.60893126, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.8641306701751504}
episode index:3687
target Thresh 16.293710621773517
target distance 1.0
model initialize at round 3687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.        , 10.21267503,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.022365233423042}
done in step count: 0
reward sum = 0.9965350897221923
running average episode reward sum: 0.7842647700461964
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.        , 10.21267503,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.022365233423042}
episode index:3688
target Thresh 16.295063428232833
target distance 9.0
model initialize at round 3688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.997157499371768}
done in step count: 5
reward sum = 0.922998361333828
running average episode reward sum: 0.7843023774171065
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.19205675, 10.34249549,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.6849802777059893}
episode index:3689
target Thresh 16.29641555845799
target distance 13.0
model initialize at round 3689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.11674382,  3.26510242,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.448216359948162}
done in step count: 15
reward sum = 0.8066968853142192
running average episode reward sum: 0.7843084463894363
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.23394835, 5.3568979 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.0002077026445515}
episode index:3690
target Thresh 16.297767012787023
target distance 1.0
model initialize at round 3690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.10844118, 8.39313526, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.6539960645214205}
done in step count: 2
reward sum = 0.9735519268895506
running average episode reward sum: 0.784359717990764
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.12562908, 6.23476112, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.1619445139081457}
episode index:3691
target Thresh 16.299117791557794
target distance 1.0
model initialize at round 3691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([4.02291071, 8.25477815, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 3.450921606326211}
done in step count: 23
reward sum = 0.726032807514257
running average episode reward sum: 0.7843439198026609
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.28161145, 9.62669661, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.8095909657713696}
episode index:3692
target Thresh 16.300467895108003
target distance 4.0
model initialize at round 3692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.5797286, 8.       , 0.       ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.043680025549805}
done in step count: 1
reward sum = 0.9831794490603473
running average episode reward sum: 0.7843977609966111
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.10455783, 9.50687873, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0222452111799323}
episode index:3693
target Thresh 16.30181732377517
target distance 12.0
model initialize at round 3693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.        , 11.59300838,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.977802783589427}
done in step count: 7
reward sum = 0.9055175975804134
running average episode reward sum: 0.7844305492577329
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.31875133, 5.49995984, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8450204634277578}
episode index:3694
target Thresh 16.303166077896652
target distance 11.0
model initialize at round 3694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.45496643, 7.92267406, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.942566277579092}
done in step count: 10
reward sum = 0.8878921703410613
running average episode reward sum: 0.7844585496964565
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.57332131,  5.81881503,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9995776006106746}
episode index:3695
target Thresh 16.30451415780964
target distance 13.0
model initialize at round 3695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.13407918,  4.99937833,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.31239171151635}
done in step count: 14
reward sum = 0.8236295799467893
running average episode reward sum: 0.7844691479189269
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.13888994, 7.87389938, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.2268702730694176}
episode index:3696
target Thresh 16.305861563851153
target distance 8.0
model initialize at round 3696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([12.        , 10.18512964,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 7.786275616707806}
done in step count: 7
reward sum = 0.9110268592994356
running average episode reward sum: 0.7845033804619024
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.26853911,  2.59377932,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.836690071903419}
episode index:3697
target Thresh 16.307208296358045
target distance 1.0
model initialize at round 3697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.83898288,  8.5663192 ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.4626073404363356}
done in step count: 0
reward sum = 0.999331021511441
running average episode reward sum: 0.784561473388092
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.83898288,  8.5663192 ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.4626073404363356}
episode index:3698
target Thresh 16.308554355666995
target distance 3.0
model initialize at round 3698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.47890651,  7.        ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.1087612220336531}
done in step count: 1
reward sum = 0.9843569110547484
running average episode reward sum: 0.7846154867532358
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.64391881,  7.80824184,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.40443170322824185}
episode index:3699
target Thresh 16.30989974211452
target distance 5.0
model initialize at round 3699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.31628579, 7.2403183 , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 3.9834425784935767}
done in step count: 3
reward sum = 0.9644965282680121
running average episode reward sum: 0.7846641032509426
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.10151066, 10.08459461,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9210165290314252}
episode index:3700
target Thresh 16.311244456036967
target distance 9.0
model initialize at round 3700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.89679071, 3.63293393, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 7.421448379607509}
done in step count: 4
reward sum = 0.9409045256062947
running average episode reward sum: 0.7847063189824625
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.85222235, 11.68464257,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7004096574719804}
episode index:3701
target Thresh 16.312588497770516
target distance 13.0
model initialize at round 3701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.052885694591254}
done in step count: 6
reward sum = 0.9083523848798989
running average episode reward sum: 0.7847397187841636
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.60553358,  6.51640149,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7749441525468596}
episode index:3702
target Thresh 16.313931867651174
target distance 6.0
model initialize at round 3702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([12.00007785,  8.00016787,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.656917905780965}
done in step count: 2
reward sum = 0.9643482991149499
running average episode reward sum: 0.784788222316524
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.00105821,  4.90012549,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.900126114756403}
episode index:3703
target Thresh 16.315274566014782
target distance 2.0
model initialize at round 3703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.        ,  8.61710131,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0707994235683704}
done in step count: 0
reward sum = 0.9950387369915605
running average episode reward sum: 0.7848449854144385
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.        ,  8.61710131,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0707994235683704}
episode index:3704
target Thresh 16.316616593197022
target distance 7.0
model initialize at round 3704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([10.97380131,  8.13224963,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 7.335854506752832}
done in step count: 4
reward sum = 0.9378875970195626
running average episode reward sum: 0.7848862924621053
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.31932126,  2.56040735,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.881691527569308}
episode index:3705
target Thresh 16.317957949533394
target distance 1.0
model initialize at round 3705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.        , 8.42239953, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.1548256595502495}
done in step count: 0
reward sum = 0.995133382879963
running average episode reward sum: 0.7849430240029627
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.        , 8.42239953, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.1548256595502495}
episode index:3706
target Thresh 16.319298635359242
target distance 5.0
model initialize at round 3706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 5.30158138, 10.7335645 ,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 3.770466435106038}
done in step count: 19
reward sum = 0.7741973587953798
running average episode reward sum: 0.784940125253244
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.48130481, 9.1225956 , 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.0192561880734023}
episode index:3707
target Thresh 16.32063865100973
target distance 5.0
model initialize at round 3707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([4.954319  , 7.05521678, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.954834961713479}
done in step count: 1
reward sum = 0.980956902373875
running average episode reward sum: 0.7849929884617446
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.95431222, 6.74145608, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9887147046765968}
episode index:3708
target Thresh 16.32197799681987
target distance 6.0
model initialize at round 3708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 6.22212064, 11.04231846,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 7.777994481204105}
done in step count: 18
reward sum = 0.7853778779954206
running average episode reward sum: 0.784993092233525
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.23080934, 10.83479627,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.786731560326977}
episode index:3709
target Thresh 16.323316673124495
target distance 8.0
model initialize at round 3709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.        , 9.10845935, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 7.178453481050522}
done in step count: 4
reward sum = 0.9461170359653616
running average episode reward sum: 0.7850365218679541
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.24832812, 1.91125572, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.2637089354923415}
episode index:3710
target Thresh 16.324654680258273
target distance 8.0
model initialize at round 3710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([13.09422848,  3.59788428,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 7.599328425078539}
done in step count: 35
reward sum = 0.6362578303032602
running average episode reward sum: 0.7849964306010275
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.62310754, 9.2635322 , 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.8273045080621628}
episode index:3711
target Thresh 16.32599201855571
target distance 6.0
model initialize at round 3711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([4.83647532, 5.62022725, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 5.402804617999278}
done in step count: 2
reward sum = 0.9654595608570375
running average episode reward sum: 0.7850450467460319
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.08305954, 9.18343641, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.2278256799063683}
episode index:3712
target Thresh 16.327328688351137
target distance 6.0
model initialize at round 3712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4., 6., 0.]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 4.000000000000019}
done in step count: 2
reward sum = 0.9663375349182609
running average episode reward sum: 0.7850938731635304
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.8743449 , 2.56739677, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.0423138179800446}
episode index:3713
target Thresh 16.32866468997872
target distance 3.0
model initialize at round 3713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 4.86877774, 11.86994545,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 2.1854884497649816}
done in step count: 1
reward sum = 0.9813910755050482
running average episode reward sum: 0.7851467264759542
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.86720701, 10.88416613,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.2384658842923095}
episode index:3714
target Thresh 16.330000023772463
target distance 1.0
model initialize at round 3714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([11.65682554,  9.27621651,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.6796923637472012}
done in step count: 2
reward sum = 0.9754832944972672
running average episode reward sum: 0.785197961083766
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.68953767,  8.13423809,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.106799837944441}
episode index:3715
target Thresh 16.331334690066196
target distance 11.0
model initialize at round 3715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([4.        , 6.00396562, 0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 9.847247879296173}
done in step count: 5
reward sum = 0.9320223573235318
running average episode reward sum: 0.7852374724928726
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.26945652,  9.49386195,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.8887460296707443}
episode index:3716
target Thresh 16.332668689193593
target distance 12.0
model initialize at round 3716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([5.        , 9.65602219, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.346712225597955}
done in step count: 17
reward sum = 0.7998949344563527
running average episode reward sum: 0.7852414158509473
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.12586564,  6.89550433,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.16358943409218754}
episode index:3717
target Thresh 16.334002021488143
target distance 7.0
model initialize at round 3717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4.87806139, 5.79315889, 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 8.0367485134767}
done in step count: 34
reward sum = 0.6290897648624159
running average episode reward sum: 0.7851994170206652
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.68101374, 10.59527241,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5153218968697494}
episode index:3718
target Thresh 16.335334687283183
target distance 9.0
model initialize at round 3718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.13094424,  3.93122788,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.357137192767604}
done in step count: 5
reward sum = 0.9210357987842643
running average episode reward sum: 0.785235941995595
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.17981925, 11.79077254,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.13930578058144}
episode index:3719
target Thresh 16.336666686911887
target distance 8.0
model initialize at round 3719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 6.66008818, 10.39264578,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.508094459097416}
done in step count: 18
reward sum = 0.7903366545730313
running average episode reward sum: 0.78523731315489
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.08720045,  3.94350024,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.1039044845508042}
episode index:3720
target Thresh 16.337998020707246
target distance 2.0
model initialize at round 3720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.        , 5.64378473, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.3562152683734503}
done in step count: 0
reward sum = 0.9969048332417195
running average episode reward sum: 0.7852941977343275
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.        , 5.64378473, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.3562152683734503}
episode index:3721
target Thresh 16.339328689002095
target distance 13.0
model initialize at round 3721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([5.        , 8.56221771, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.908561225242885}
done in step count: 5
reward sum = 0.9247617453807427
running average episode reward sum: 0.7853316688648074
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.19646051,  4.90394009,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.2094558267856135}
episode index:3722
target Thresh 16.340658692129104
target distance 7.0
model initialize at round 3722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.87648287,  2.20116236,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 6.855101421506709}
done in step count: 24
reward sum = 0.7374872777822791
running average episode reward sum: 0.7853188178330904
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.0277527,  8.7599588,  0.       ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0014412593891637}
episode index:3723
target Thresh 16.341988030420772
target distance 6.0
model initialize at round 3723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([14.        ,  6.84626913,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 4.542967824819638}
done in step count: 2
reward sum = 0.966688189464869
running average episode reward sum: 0.7853675206718745
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.        ,  8.81681836,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.18318164348606736}
episode index:3724
target Thresh 16.343316704209435
target distance 4.0
model initialize at round 3724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.40372956, 4.63757408, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.3865772759520563}
done in step count: 2
reward sum = 0.9729199814540793
running average episode reward sum: 0.7854178703257758
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.22090027, 8.31226406, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.839348105432544}
episode index:3725
target Thresh 16.344644713827257
target distance 6.0
model initialize at round 3725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([7.00074155, 8.15297883, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.003665252498751}
done in step count: 46
reward sum = 0.5584197602750564
running average episode reward sum: 0.7853569475909259
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.14371558, 8.42588327, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9563469955725463}
episode index:3726
target Thresh 16.345972059606247
target distance 12.0
model initialize at round 3726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.85463393, 9.        , 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.657771719125119}
done in step count: 10
reward sum = 0.8683131435611562
running average episode reward sum: 0.7853792057599547
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.5024751,  2.7504563,  0.       ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5610287775436383}
episode index:3727
target Thresh 16.347298741878237
target distance 8.0
model initialize at round 3727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 9.29722166, 11.33089828,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 7.09650084770998}
done in step count: 4
reward sum = 0.9458827367338742
running average episode reward sum: 0.7854222592822117
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.52593846,  9.61438733,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.776019419142181}
episode index:3728
target Thresh 16.3486247609749
target distance 8.0
model initialize at round 3728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.56540787,  5.        ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.894075585936195}
done in step count: 6
reward sum = 0.9243617473565854
running average episode reward sum: 0.7854595184637816
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.28960788, 10.02342039,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.018616931678598}
episode index:3729
target Thresh 16.34995011722774
target distance 9.0
model initialize at round 3729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([7.34085679, 9.        , 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.661479611532762}
done in step count: 38
reward sum = 0.5803505143005336
running average episode reward sum: 0.7854045294546225
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.46391963,  1.08731468,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0584784656983988}
episode index:3730
target Thresh 16.35127481096809
target distance 10.0
model initialize at round 3730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([8.       , 8.7174592, 0.       ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 9.287325843361412}
done in step count: 4
reward sum = 0.9344113074517334
running average episode reward sum: 0.7854444669453748
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.16517027,  3.93523033,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.17741569745682073}
episode index:3731
target Thresh 16.35259884252713
target distance 3.0
model initialize at round 3731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.        , 7.02315974, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.451433301176517}
done in step count: 19
reward sum = 0.7469330886833848
running average episode reward sum: 0.7854341477121857
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.71446008, 11.86829799,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9140429174439925}
episode index:3732
target Thresh 16.353922212235872
target distance 3.0
model initialize at round 3732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.12189564, 11.87128794,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.293814242924864}
done in step count: 1
reward sum = 0.9835760221677239
running average episode reward sum: 0.7854872261677055
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.22395203, 11.82360387,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8535091390165592}
episode index:3733
target Thresh 16.35524492042515
target distance 6.0
model initialize at round 3733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([13.1325968 ,  5.99505776,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.084557509415171}
done in step count: 4
reward sum = 0.9456194066034959
running average episode reward sum: 0.7855301110580203
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.56293544, 10.77003635,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.9538617782014487}
episode index:3734
target Thresh 16.356566967425643
target distance 4.0
model initialize at round 3734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.6140658 , 11.68891761,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.6791410774155344}
done in step count: 21
reward sum = 0.7603326567229814
running average episode reward sum: 0.7855233647516388
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.34389816, 11.87532602,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9404581797248797}
episode index:3735
target Thresh 16.357888353567866
target distance 8.0
model initialize at round 3735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([15.4920994,  9.       ,  0.       ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 7.558541756478917}
done in step count: 10
reward sum = 0.8741177597617822
running average episode reward sum: 0.7855470784547999
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.73314404, 10.82317328,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.1023222910978425}
episode index:3736
target Thresh 16.359209079182165
target distance 6.0
model initialize at round 3736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([5.53061232, 7.55882221, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 4.696000390398735}
done in step count: 2
reward sum = 0.9678589214978623
running average episode reward sum: 0.7855958640697431
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.51568435, 8.09623927, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.0253512077478606}
episode index:3737
target Thresh 16.360529144598715
target distance 11.0
model initialize at round 3737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([5.41580749, 8.        , 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.810029886797508}
done in step count: 7
reward sum = 0.8964211046313454
running average episode reward sum: 0.7856255123416965
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.48271103,  2.03632023,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0778166004540677}
episode index:3738
target Thresh 16.361848550147542
target distance 5.0
model initialize at round 3738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([15.02421463,  5.18315172,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 5.687521554591603}
done in step count: 14
reward sum = 0.8543711744110584
running average episode reward sum: 0.7856438984508352
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.43865898,  9.42453352,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.8039063452443306}
episode index:3739
target Thresh 16.363167296158494
target distance 11.0
model initialize at round 3739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.10968976,  5.69159861,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.114392642355382}
done in step count: 38
reward sum = 0.5959874467742017
running average episode reward sum: 0.7855931881696382
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.89256501, 6.31277316, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9457797551912773}
episode index:3740
target Thresh 16.364485382961252
target distance 13.0
model initialize at round 3740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 2.1121891 , 11.86306233,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 15.074707014988327}
done in step count: 17
reward sum = 0.7768107467512217
running average episode reward sum: 0.7855908405509752
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.80093818,  6.81332701,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.1414914728668961}
episode index:3741
target Thresh 16.365802810885345
target distance 8.0
model initialize at round 3741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([9.23684502, 9.        , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 7.963952895806625}
done in step count: 42
reward sum = 0.5405419506442759
running average episode reward sum: 0.7855253544767083
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.00302107, 2.1818958 , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.2896749445166351}
episode index:3742
target Thresh 16.367119580260123
target distance 12.0
model initialize at round 3742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([12.82335934,  7.14612992,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.590319265422584}
done in step count: 9
reward sum = 0.8751696744475881
running average episode reward sum: 0.7855493043351028
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.65951045, 2.5209092 , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8151576712540989}
episode index:3743
target Thresh 16.36843569141479
target distance 5.0
model initialize at round 3743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.86253452,  6.        ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 3.0031478081113696}
done in step count: 2
reward sum = 0.9692660263791744
running average episode reward sum: 0.785598373972401
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.15889351,  2.19414203,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8213733705848005}
episode index:3744
target Thresh 16.36975114467836
target distance 11.0
model initialize at round 3744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([5.17664066, 7.14612992, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.314868984088005}
done in step count: 21
reward sum = 0.7619538013994779
running average episode reward sum: 0.7855920603348648
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.41513755,  4.79350485,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8955384578076376}
episode index:3745
target Thresh 16.3710659403797
target distance 11.0
model initialize at round 3745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.49518943,  9.        ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.66787812728507}
done in step count: 8
reward sum = 0.8879447670178717
running average episode reward sum: 0.7856193835347268
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.32942164, 11.17030414,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.37083974610818227}
episode index:3746
target Thresh 16.37238007884752
target distance 7.0
model initialize at round 3746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([2.25375903, 4.45502245, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 8.981150823272621}
done in step count: 15
reward sum = 0.822806003600687
running average episode reward sum: 0.7856293079062416
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.4746116 ,  9.82295497,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.9500058164172241}
episode index:3747
target Thresh 16.373693560410338
target distance 12.0
model initialize at round 3747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.44271564,  9.09726739,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 12.092434810884935}
done in step count: 41
reward sum = 0.5985775449543975
running average episode reward sum: 0.7855794008190079
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.07125148, 3.13574814, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9386167333761366}
episode index:3748
target Thresh 16.37500638539654
target distance 9.0
model initialize at round 3748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 3.97263098, 10.82111835,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.06093260405603}
done in step count: 12
reward sum = 0.8390457523700995
running average episode reward sum: 0.7855936623158207
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.36701249,  9.05455377,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.1377793077071714}
episode index:3749
target Thresh 16.37631855413432
target distance 10.0
model initialize at round 3749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([11.05818534, 10.84376431,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 8.102239761932514}
done in step count: 4
reward sum = 0.9410648497387749
running average episode reward sum: 0.7856351212991335
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.19803851, 9.60534905, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.44155252139342394}
episode index:3750
target Thresh 16.377630066951724
target distance 10.0
model initialize at round 3750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([10.68318677, 10.88553357,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 9.51289144303809}
done in step count: 24
reward sum = 0.7484879378210673
running average episode reward sum: 0.7856252180244127
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.84901997, 7.98907649, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.3034980688595235}
episode index:3751
target Thresh 16.378940924176632
target distance 12.0
model initialize at round 3751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4., 5., 0.]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.440306508910561}
done in step count: 12
reward sum = 0.8458979126388257
running average episode reward sum: 0.7856412821754293
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.52392624,  2.93307055,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0701025033887153}
episode index:3752
target Thresh 16.380251126136756
target distance 7.0
model initialize at round 3752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([14.57905173,  4.        ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 5.243415335022038}
done in step count: 4
reward sum = 0.9499437637289431
running average episode reward sum: 0.7856850611473328
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.94956809,  8.91518266,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.9533485954727126}
episode index:3753
target Thresh 16.38156067315965
target distance 13.0
model initialize at round 3753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 4.       , 10.6937021,  0.       ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.386211835060287}
done in step count: 6
reward sum = 0.912672863594552
running average episode reward sum: 0.7857188884788318
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.49012125,  4.61086798,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6414047640574975}
episode index:3754
target Thresh 16.3828695655727
target distance 3.0
model initialize at round 3754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.18569899, 10.        ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.2896069716446237}
done in step count: 9
reward sum = 0.8935033630970741
running average episode reward sum: 0.7857475927330576
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.6079114, 10.9213349,  0.       ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.3999020704184465}
episode index:3755
target Thresh 16.384177803703125
target distance 6.0
model initialize at round 3755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.84610772,  7.00442934,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 4.167363882928157}
done in step count: 11
reward sum = 0.8625944190567976
running average episode reward sum: 0.785768052484475
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.83424159,  2.57263139,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9373382272884133}
episode index:3756
target Thresh 16.38548538787799
target distance 14.0
model initialize at round 3756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 4.        , 10.88065004,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.832691170590964}
done in step count: 7
reward sum = 0.9049431127532931
running average episode reward sum: 0.7857997732883794
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.84610364,  4.31656943,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.35199473258965647}
episode index:3757
target Thresh 16.386792318424185
target distance 7.0
model initialize at round 3757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([12.01836611, 11.86320955,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 5.777712982754906}
done in step count: 14
reward sum = 0.8221946817882451
running average episode reward sum: 0.7858094579367296
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.33617192, 8.03684221, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.1697609330761989}
episode index:3758
target Thresh 16.38809859566845
target distance 5.0
model initialize at round 3758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([3.60413039, 7.        , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 3.4019456638046575}
done in step count: 11
reward sum = 0.8718657957484311
running average episode reward sum: 0.7858323513492892
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.65148259, 4.28311736, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.44902096048656864}
episode index:3759
target Thresh 16.389404219937344
target distance 11.0
model initialize at round 3759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.88759668, 5.38530551, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.444966639395789}
done in step count: 10
reward sum = 0.8782014566497367
running average episode reward sum: 0.7858569176006989
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.28622582,  8.0374882 ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.28867037475636637}
episode index:3760
target Thresh 16.390709191557285
target distance 1.0
model initialize at round 3760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.5544477, 11.9089491,  0.       ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0647068707381944}
done in step count: 0
reward sum = 0.9968589450132568
running average episode reward sum: 0.7859130202402661
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.5544477, 11.9089491,  0.       ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0647068707381944}
episode index:3761
target Thresh 16.39201351085451
target distance 9.0
model initialize at round 3761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4., 6., 0.]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 8.602325267042653}
done in step count: 8
reward sum = 0.9015758182122088
running average episode reward sum: 0.7859437652689668
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.30264813, 10.61996329,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7941835590261904}
episode index:3762
target Thresh 16.393317178155097
target distance 8.0
model initialize at round 3762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 9.57671154, 11.16228628,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 6.777471258685362}
done in step count: 7
reward sum = 0.9149982600738139
running average episode reward sum: 0.7859780609093614
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.7800165 ,  8.64423998,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.857316123037421}
episode index:3763
target Thresh 16.394620193784966
target distance 2.0
model initialize at round 3763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.93634963, 4.        , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.06365036964416504}
done in step count: 0
reward sum = 0.996342437123251
running average episode reward sum: 0.78603394942589
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.93634963, 4.        , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.06365036964416504}
episode index:3764
target Thresh 16.39592255806987
target distance 1.0
model initialize at round 3764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.28820878,  1.11385869,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.931831917337447}
done in step count: 0
reward sum = 0.9969436994473738
running average episode reward sum: 0.7860899679517922
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.28820878,  1.11385869,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.931831917337447}
episode index:3765
target Thresh 16.3972242713354
target distance 1.0
model initialize at round 3765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.79284157, 4.36790323, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.66517737415997}
done in step count: 0
reward sum = 0.9998662994454339
running average episode reward sum: 0.7861467327769365
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.79284157, 4.36790323, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.66517737415997}
episode index:3766
target Thresh 16.398525333906985
target distance 5.0
model initialize at round 3766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([11.70951366,  8.74990201,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.299977171194808}
done in step count: 18
reward sum = 0.8148752628632505
running average episode reward sum: 0.7861543591454223
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.19223007,  8.99655003,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8077772988827198}
episode index:3767
target Thresh 16.399825746109887
target distance 3.0
model initialize at round 3767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.,  4.,  0.]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0000000000000187}
done in step count: 1
reward sum = 0.9813892931408437
running average episode reward sum: 0.7862061730875655
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.89665467,  5.57334215,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.064288881313406}
episode index:3768
target Thresh 16.401125508269214
target distance 4.0
model initialize at round 3768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.92603964, 8.68449521, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.8397295546730237}
done in step count: 13
reward sum = 0.8574295416215093
running average episode reward sum: 0.7862250702402674
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.7415127 , 5.24287339, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8000352399014272}
episode index:3769
target Thresh 16.402424620709905
target distance 7.0
model initialize at round 3769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 7.183499286638246}
done in step count: 5
reward sum = 0.927106715309706
running average episode reward sum: 0.7862624393768907
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.3816688 , 8.72862098, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.6752629499359986}
episode index:3770
target Thresh 16.403723083756738
target distance 2.0
model initialize at round 3770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([4.87377037, 5.158027  , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 3.098319326351806}
done in step count: 19
reward sum = 0.7967277514272848
running average episode reward sum: 0.786265214585602
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.33589385, 3.3613315 , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9213763831091519}
episode index:3771
target Thresh 16.405020897734325
target distance 10.0
model initialize at round 3771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([13.22756485,  2.93907908,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 10.84202138055252}
done in step count: 6
reward sum = 0.9108119024984591
running average episode reward sum: 0.7862982333257698
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.900677  , 10.90899954,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.2796480864621433}
episode index:3772
target Thresh 16.406318062967127
target distance 10.0
model initialize at round 3772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 6.97313317, 11.85999082,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 8.072804679640383}
done in step count: 9
reward sum = 0.8845360490852341
running average episode reward sum: 0.7863242703826898
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.56576622, 11.0300583 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.43527287267785625}
episode index:3773
target Thresh 16.40761457977943
target distance 4.0
model initialize at round 3773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.56705451,  7.        ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.0788340043023177}
done in step count: 1
reward sum = 0.9819712793398073
running average episode reward sum: 0.7863761111375804
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.02647841,  5.        ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.026478409767060285}
episode index:3774
target Thresh 16.408910448495362
target distance 8.0
model initialize at round 3774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.        , 6.60555959, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.43714370838704}
done in step count: 76
reward sum = 0.28289190202728415
running average episode reward sum: 0.7862427378371538
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.05749588, 11.86789596,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.2812327660189304}
episode index:3775
target Thresh 16.410205669438895
target distance 11.0
model initialize at round 3775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 4.91939731, 11.86913271,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.252424478742775}
done in step count: 15
reward sum = 0.8131988196673413
running average episode reward sum: 0.7862498766300114
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.48690716, 10.75110048,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9096241993186759}
episode index:3776
target Thresh 16.411500242933833
target distance 7.0
model initialize at round 3776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([12.05521678,  8.045681  ,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 5.419832046015292}
done in step count: 3
reward sum = 0.9517522746579079
running average episode reward sum: 0.7862936951097645
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.05522186, 10.0864044 ,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.9487209531823255}
episode index:3777
target Thresh 16.412794169303815
target distance 5.0
model initialize at round 3777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([10.54110892,  8.11133901,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 4.652349723628749}
done in step count: 4
reward sum = 0.9490620224969217
running average episode reward sum: 0.7863367783091789
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.1013122 ,  5.59869183,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0798479805336425}
episode index:3778
target Thresh 16.41408744887233
target distance 13.0
model initialize at round 3778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.12496392,  5.83486007,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.333695471077714}
done in step count: 11
reward sum = 0.8578201625074685
running average episode reward sum: 0.7863556942615997
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.16354778, 7.57802871, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9368628998948445}
episode index:3779
target Thresh 16.41538008196269
target distance 4.0
model initialize at round 3779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 5.       , 10.9920857,  0.       ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 2.2325398152670832}
done in step count: 1
reward sum = 0.9825899798438591
running average episode reward sum: 0.7864076080937643
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.        , 9.59947705, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.40052294731149196}
episode index:3780
target Thresh 16.41667206889806
target distance 6.0
model initialize at round 3780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([13.28636456,  7.        ,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 6.078293367219845}
done in step count: 7
reward sum = 0.9078583575372081
running average episode reward sum: 0.786439729423953
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.59282099, 9.69200161, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.6680566826137508}
episode index:3781
target Thresh 16.41796341000143
target distance 9.0
model initialize at round 3781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 8.602325267042653}
done in step count: 11
reward sum = 0.8686576138473908
running average episode reward sum: 0.7864614686847736
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.03944648, 9.93534432, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.9627270714411106}
episode index:3782
target Thresh 16.41925410559564
target distance 6.0
model initialize at round 3782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([4.52476811, 4.47185957, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 5.708002485593544}
done in step count: 21
reward sum = 0.7664874587637215
running average episode reward sum: 0.7864561887455928
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.45688426, 8.1423312 , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.0151701726510187}
episode index:3783
target Thresh 16.420544156003366
target distance 6.0
model initialize at round 3783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.58622408, 5.59683311, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.64873729213662}
done in step count: 7
reward sum = 0.913733466941207
running average episode reward sum: 0.7864898243899362
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.18449313, 11.8631429 ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.8826400051794412}
episode index:3784
target Thresh 16.421833561547118
target distance 5.0
model initialize at round 3784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([12.86301136, 11.        ,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 3.990345444794292}
done in step count: 7
reward sum = 0.918859740390404
running average episode reward sum: 0.7865247966266603
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.32640552, 10.06385696,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.6766145391348558}
episode index:3785
target Thresh 16.423122322549244
target distance 8.0
model initialize at round 3785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 9.49847852, 11.91261125,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 7.5880371875790145}
done in step count: 18
reward sum = 0.8012815358760511
running average episode reward sum: 0.7865286943390875
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.33115015,  7.63059936,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.4961020617037763}
episode index:3786
target Thresh 16.424410439331936
target distance 8.0
model initialize at round 3786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.65876615,  9.98186791,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 7.109527955065691}
done in step count: 3
reward sum = 0.9591645263924435
running average episode reward sum: 0.7865742807748027
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.34148246,  3.98186791,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.1822478341710188}
episode index:3787
target Thresh 16.425697912217228
target distance 13.0
model initialize at round 3787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 2.70023298, 11.1260128 ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 12.97336696176975}
done in step count: 26
reward sum = 0.6990078113892855
running average episode reward sum: 0.7865511639666228
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.37125727,  6.634414  ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.5210422999489271}
episode index:3788
target Thresh 16.42698474152698
target distance 1.0
model initialize at round 3788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.        , 4.48706937, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.12387625336248}
done in step count: 0
reward sum = 0.9968220722894132
running average episode reward sum: 0.7866066590598725
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.        , 4.48706937, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.12387625336248}
episode index:3789
target Thresh 16.428270927582908
target distance 7.0
model initialize at round 3789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([11.73004365, 10.37754178,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 8.08741489870191}
done in step count: 8
reward sum = 0.9026478988991626
running average episode reward sum: 0.7866372768012548
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.2222745 , 7.56032922, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8934021216054532}
episode index:3790
target Thresh 16.429556470706554
target distance 1.0
model initialize at round 3790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.54208541, 10.74441826,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5993151215299402}
done in step count: 0
reward sum = 0.9978008603175297
running average episode reward sum: 0.7866929780894416
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.54208541, 10.74441826,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5993151215299402}
episode index:3791
target Thresh 16.430841371219305
target distance 6.0
model initialize at round 3791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([7.20049157, 8.12170222, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.039451956240371}
done in step count: 3
reward sum = 0.9530180190366367
running average episode reward sum: 0.786736840178299
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.89657755, 3.52167481, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0373022267646583}
episode index:3792
target Thresh 16.432125629442382
target distance 1.0
model initialize at round 3792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.90310851,  3.3938317 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.842656120499785}
done in step count: 14
reward sum = 0.8559651732771054
running average episode reward sum: 0.7867550917820688
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.33040561,  4.92603411,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.3385835505106749}
episode index:3793
target Thresh 16.433409245696858
target distance 11.0
model initialize at round 3793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.1221105 ,  2.21834997,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.277713641596833}
done in step count: 19
reward sum = 0.7561844906386421
running average episode reward sum: 0.786747034164477
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.20598142, 3.39181969, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0001743851939882}
episode index:3794
target Thresh 16.434692220303628
target distance 6.0
model initialize at round 3794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.01240218, 3.42560673, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.574407068861039}
done in step count: 8
reward sum = 0.9119604824413744
running average episode reward sum: 0.7867800284854986
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.25255299, 9.33638293, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8196526771112806}
episode index:3795
target Thresh 16.435974553583442
target distance 5.0
model initialize at round 3795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([4.26735866, 9.07691932, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.733433801372695}
done in step count: 3
reward sum = 0.9589291635799719
running average episode reward sum: 0.7868253786264613
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.8312252 , 8.12801398, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.2046970365337615}
episode index:3796
target Thresh 16.43725624585688
target distance 6.0
model initialize at round 3796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([11.0530082 ,  9.14773619,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 5.055167439251431}
done in step count: 35
reward sum = 0.6401927884403419
running average episode reward sum: 0.7867867606148241
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.19346561, 9.45570242, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9263705584557865}
episode index:3797
target Thresh 16.438537297444366
target distance 3.0
model initialize at round 3797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.48667562,  5.        ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.1240560124038506}
done in step count: 42
reward sum = 0.5471960554225137
running average episode reward sum: 0.7867236772274644
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.90836256,  5.48337165,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0450011427742265}
episode index:3798
target Thresh 16.439817708666162
target distance 8.0
model initialize at round 3798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 6.0827625302982415}
done in step count: 3
reward sum = 0.9546062669532362
running average episode reward sum: 0.7867678684856181
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.1578325,  3.       ,  0.       ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.15783250331891807}
episode index:3799
target Thresh 16.441097479842373
target distance 4.0
model initialize at round 3799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([6.       , 9.7220521, 0.       ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.373425971452031}
done in step count: 9
reward sum = 0.8957204713662053
running average episode reward sum: 0.7867965402232182
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.98074937, 10.46050262,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.1193421024425991}
episode index:3800
target Thresh 16.44237661129294
target distance 7.0
model initialize at round 3800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([10.        ,  8.76392406,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 5.713079399229127}
done in step count: 5
reward sum = 0.9303895596538813
running average episode reward sum: 0.7868343179184117
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.85725534,  6.79226304,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8050197252357816}
episode index:3801
target Thresh 16.443655103337647
target distance 13.0
model initialize at round 3801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 3.99505755, 11.86740305,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 12.033302573119421}
done in step count: 11
reward sum = 0.8531209738290643
running average episode reward sum: 0.7868517525990826
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.90343509,  7.49365029,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.503006352186444}
episode index:3802
target Thresh 16.444932956296118
target distance 4.0
model initialize at round 3802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 5.36797667, 11.55511999,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.8329687034767415}
done in step count: 3
reward sum = 0.9610128338846524
running average episode reward sum: 0.7868975483080717
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.62989593, 9.69795847, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6985685110882058}
episode index:3803
target Thresh 16.446210170487813
target distance 11.0
model initialize at round 3803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.42523289,  8.43290007,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.9758955740403}
done in step count: 5
reward sum = 0.9251935273609265
running average episode reward sum: 0.7869339037179173
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.32756982, 5.42797042, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5389440276716856}
episode index:3804
target Thresh 16.447486746232038
target distance 4.0
model initialize at round 3804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3., 7., 0.]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 2.0000000000000195}
done in step count: 41
reward sum = 0.6053892206459983
running average episode reward sum: 0.7868861915804477
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.61435748, 4.56156622, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7547577707661773}
episode index:3805
target Thresh 16.448762683847935
target distance 2.0
model initialize at round 3805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.74311101, 10.32229924,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.8099943092878268}
done in step count: 0
reward sum = 0.9984707139386718
running average episode reward sum: 0.7869417839404999
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.74311101, 10.32229924,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.8099943092878268}
episode index:3806
target Thresh 16.45003798365449
target distance 6.0
model initialize at round 3806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.2798121,  7.       ,  0.       ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.199752255174582}
done in step count: 2
reward sum = 0.9697381388465374
running average episode reward sum: 0.7869897997941656
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.32911143, 10.56353057,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.5466442175790688}
episode index:3807
target Thresh 16.45131264597053
target distance 7.0
model initialize at round 3807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.        , 4.38911581, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.726822747312021}
done in step count: 3
reward sum = 0.9501368171568159
running average episode reward sum: 0.787032643023515
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.25707186, 10.35747246,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6920459369946719}
episode index:3808
target Thresh 16.452586671114716
target distance 5.0
model initialize at round 3808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4., 7., 0.]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.6055512754639953}
done in step count: 16
reward sum = 0.7913091036309127
running average episode reward sum: 0.7870337657487992
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.65198446, 10.01071619,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.6520725250168337}
episode index:3809
target Thresh 16.453860059405557
target distance 2.0
model initialize at round 3809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.16894722,  9.46699333,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.9532740925069265}
done in step count: 0
reward sum = 0.9982160106435192
running average episode reward sum: 0.7870891941595327
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.16894722,  9.46699333,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.9532740925069265}
episode index:3810
target Thresh 16.4551328111614
target distance 6.0
model initialize at round 3810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([4.34748566, 6.        , 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 6.399290456517269}
done in step count: 4
reward sum = 0.9423051709409954
running average episode reward sum: 0.7871299225711783
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.78430535,  8.15749016,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.1510680756036364}
episode index:3811
target Thresh 16.456404926700433
target distance 9.0
model initialize at round 3811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([9.75101936, 9.        , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 9.801953942859273}
done in step count: 4
reward sum = 0.9331537073381535
running average episode reward sum: 0.7871682289155557
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.58903171, 3.73276754, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.84014475541158}
episode index:3812
target Thresh 16.45767640634068
target distance 10.0
model initialize at round 3812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.06411327,  6.54322028,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.73203777710276}
done in step count: 22
reward sum = 0.7450869774283055
running average episode reward sum: 0.7871571926576257
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.14002192, 3.63371313, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0682483942253116}
episode index:3813
target Thresh 16.45894725040002
target distance 2.0
model initialize at round 3813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([1.68601286, 5.78057528, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.2063035684605743}
done in step count: 7
reward sum = 0.9194719544847446
running average episode reward sum: 0.7871918845196675
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.14311672, 8.1399787 , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8682413209591688}
episode index:3814
target Thresh 16.460217459196162
target distance 11.0
model initialize at round 3814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 6.        , 10.84908438,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.733675421431569}
done in step count: 9
reward sum = 0.8772960334463646
running average episode reward sum: 0.7872155029073284
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.67914785,  4.43065611,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8862247322262969}
episode index:3815
target Thresh 16.461487033046648
target distance 10.0
model initialize at round 3815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.95606496, 6.63063532, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.34915236614067}
done in step count: 10
reward sum = 0.8734421886217278
running average episode reward sum: 0.7872380989989727
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.86504739,  8.72547391,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.90756353195423}
episode index:3816
target Thresh 16.462755972268884
target distance 11.0
model initialize at round 3816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 6.       , 10.1176815,  0.       ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 9.524701462827789}
done in step count: 9
reward sum = 0.886466519094905
running average episode reward sum: 0.7872640954412299
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.55287102,  6.16372883,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.948300476116246}
episode index:3817
target Thresh 16.464024277180098
target distance 1.0
model initialize at round 3817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.31386998,  8.60250437,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.5568456663706922}
done in step count: 5
reward sum = 0.9433303539086018
running average episode reward sum: 0.7873049718839924
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.97683593, 10.02984753,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9772918284754074}
episode index:3818
target Thresh 16.46529194809737
target distance 3.0
model initialize at round 3818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.        ,  9.34728456,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0585870594541946}
done in step count: 1
reward sum = 0.9862121666840189
running average episode reward sum: 0.7873570554647203
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.76959449,  8.75825614,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.3339562754519862}
episode index:3819
target Thresh 16.466558985337613
target distance 11.0
model initialize at round 3819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([11.42226171, 11.        ,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 10.23616216287804}
done in step count: 26
reward sum = 0.7162685668825693
running average episode reward sum: 0.7873384459127355
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.12841538, 6.15075632, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.2169119002751416}
episode index:3820
target Thresh 16.46782538921759
target distance 11.0
model initialize at round 3820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.055385138137437}
done in step count: 19
reward sum = 0.7695179651409793
running average episode reward sum: 0.78733378208631
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.28263354, 9.96161261, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.2852285197148417}
episode index:3821
target Thresh 16.469091160053903
target distance 12.0
model initialize at round 3821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([5.96471609, 7.90821414, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 10.076297732831888}
done in step count: 25
reward sum = 0.7228097009078149
running average episode reward sum: 0.7873168998044737
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.15583425,  6.5346338 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.4907647256095584}
episode index:3822
target Thresh 16.470356298162994
target distance 3.0
model initialize at round 3822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.72277975,  9.        ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0377143472811767}
done in step count: 1
reward sum = 0.9815673676777663
running average episode reward sum: 0.7873677108083641
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.87756346,  7.20481816,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.1842431275886227}
episode index:3823
target Thresh 16.471620803861143
target distance 6.0
model initialize at round 3823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([15.03158391,  6.09655857,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 5.809200351830239}
done in step count: 5
reward sum = 0.9344337723136673
running average episode reward sum: 0.7874061695064565
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.55816395, 8.09531453, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.0068142287275825}
episode index:3824
target Thresh 16.47288467746448
target distance 2.0
model initialize at round 3824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.9999999999999805}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7874601809654628
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.9999999999999805}
episode index:3825
target Thresh 16.474147919288974
target distance 2.0
model initialize at round 3825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([14.28340948,  9.        ,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 2.283409476280273}
done in step count: 8
reward sum = 0.9051893976582311
running average episode reward sum: 0.787490951800981
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.30469495,  8.76152093,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.7350655624679943}
episode index:3826
target Thresh 16.475410529650436
target distance 8.0
model initialize at round 3826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([13.09197723,  2.57207597,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 9.606572853642863}
done in step count: 8
reward sum = 0.8861545204552039
running average episode reward sum: 0.787516732717797
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.97718277, 9.37330051, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.6271147266386832}
episode index:3827
target Thresh 16.476672508864517
target distance 11.0
model initialize at round 3827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.        ,  5.99943399,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 10.295905029546008}
done in step count: 5
reward sum = 0.925779387785736
running average episode reward sum: 0.7875528514887133
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.21314222, 11.84241385,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8689595527334695}
episode index:3828
target Thresh 16.47793385724671
target distance 1.0
model initialize at round 3828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.85074885, 11.63628885,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 2.4703668749813477}
done in step count: 2
reward sum = 0.9738962167921857
running average episode reward sum: 0.7876015178155098
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.45285925,  9.59080191,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6103479160708386}
episode index:3829
target Thresh 16.479194575112356
target distance 7.0
model initialize at round 3829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 9.82362576, 11.87517809,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 6.812800292754711}
done in step count: 26
reward sum = 0.721797944035663
running average episode reward sum: 0.78758433672575
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.31225794,  8.82446648,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.709789517441973}
episode index:3830
target Thresh 16.48045466277663
target distance 3.0
model initialize at round 3830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.25962728,  8.02901375,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 3.4432043906276903}
done in step count: 26
reward sum = 0.7427321397753556
running average episode reward sum: 0.7875726290262067
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.70573751, 10.43501224,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9040335189681831}
episode index:3831
target Thresh 16.48171412055456
target distance 12.0
model initialize at round 3831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([16.76991947,  5.99999216,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 12.925587490956852}
done in step count: 35
reward sum = 0.6193347081801804
running average episode reward sum: 0.7875287256021865
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.27825215, 3.94161844, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7241052217502434}
episode index:3832
target Thresh 16.482972948761006
target distance 5.0
model initialize at round 3832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([8.41746247, 9.16120368, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 3.5188959893091734}
done in step count: 11
reward sum = 0.8747397949974969
running average episode reward sum: 0.7875514782944366
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.58606856, 9.47746003, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.6666237800953585}
episode index:3833
target Thresh 16.484231147710673
target distance 4.0
model initialize at round 3833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.09729207,  9.70280957,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.8495721079620737}
done in step count: 2
reward sum = 0.9739630360705935
running average episode reward sum: 0.7876000989407006
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.43228412,  7.48586828,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6503364809100739}
episode index:3834
target Thresh 16.48548871771812
target distance 5.0
model initialize at round 3834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([12.        , 10.82576317,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 4.121278620773471}
done in step count: 13
reward sum = 0.857226088931537
running average episode reward sum: 0.7876182543487816
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.79470516,  8.93132825,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9536866759825855}
episode index:3835
target Thresh 16.48674565909773
target distance 7.0
model initialize at round 3835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 5.52189049, 11.9129552 ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 6.204433237180729}
done in step count: 33
reward sum = 0.6387282080040483
running average episode reward sum: 0.7875794404680869
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.07111462,  8.13344862,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8694645367057334}
episode index:3836
target Thresh 16.488001972163737
target distance 9.0
model initialize at round 3836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([13.5433296, 11.9018835,  0.       ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 7.779394725250429}
done in step count: 18
reward sum = 0.7838162892948812
running average episode reward sum: 0.7875784597145886
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.97308042, 10.540806  ,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.5414755699519603}
episode index:3837
target Thresh 16.48925765723023
target distance 1.0
model initialize at round 3837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.7685864 , 11.19172555,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.2139860160066958}
done in step count: 23
reward sum = 0.7354419193844657
running average episode reward sum: 0.7875648754153884
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.74452482, 9.64426224, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.4379690905077896}
episode index:3838
target Thresh 16.490512714611125
target distance 1.0
model initialize at round 3838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([2.91542971, 8.03301316, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.246181185949716}
done in step count: 22
reward sum = 0.7769239796940074
running average episode reward sum: 0.7875621036269744
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.27150554, 10.73851808,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.7868444631368137}
episode index:3839
target Thresh 16.49176714462018
target distance 8.0
model initialize at round 3839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([14.78359057, 11.87692414,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 7.368432233838086}
done in step count: 4
reward sum = 0.945344812871758
running average episode reward sum: 0.7876031928741736
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.8670381 , 8.77803457, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.8949992862746479}
episode index:3840
target Thresh 16.49302094757101
target distance 6.0
model initialize at round 3840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.90994859, 7.2734791 , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 4.410310163101609}
done in step count: 3
reward sum = 0.9609006191183364
running average episode reward sum: 0.7876483106628339
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.87919962, 3.86619973, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8745825897213598}
episode index:3841
target Thresh 16.494274123777068
target distance 7.0
model initialize at round 3841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 7.26363258, 11.88548604,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 5.5911434902224775}
done in step count: 8
reward sum = 0.9064251796277449
running average episode reward sum: 0.7876792260373692
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.53452121, 10.61253357,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8129638998689684}
episode index:3842
target Thresh 16.49552667355164
target distance 4.0
model initialize at round 3842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([13.71992409,  7.31395841,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 3.1895075365232555}
done in step count: 11
reward sum = 0.8712393439017942
running average episode reward sum: 0.7877009694976514
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.9524994 ,  9.77963908,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.22542236810046754}
episode index:3843
target Thresh 16.496778597207868
target distance 10.0
model initialize at round 3843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 7.99505755, 11.86740305,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 8.503005580988797}
done in step count: 29
reward sum = 0.656185842905172
running average episode reward sum: 0.7876667564054056
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.23734011,  8.64479888,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8413191703183172}
episode index:3844
target Thresh 16.498029895058735
target distance 12.0
model initialize at round 3844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.35727876,  4.99999998,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.401218389405837}
done in step count: 16
reward sum = 0.803611402599579
running average episode reward sum: 0.7876709032574717
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.7046498 , 6.49584992, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5771471936531875}
episode index:3845
target Thresh 16.49928056741706
target distance 5.0
model initialize at round 3845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 5.99999999, 11.65937425,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 3.428341135250337}
done in step count: 10
reward sum = 0.8864328476857036
running average episode reward sum: 0.7876965823901884
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.40489636, 9.61787383, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.5567418370446267}
episode index:3846
target Thresh 16.500530614595515
target distance 12.0
model initialize at round 3846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.1973101 ,  5.99999197,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.24622609907897}
done in step count: 31
reward sum = 0.6750989829097558
running average episode reward sum: 0.7876673134534895
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.29834694, 7.02006725, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.7019399609088234}
episode index:3847
target Thresh 16.50178003690661
target distance 3.0
model initialize at round 3847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([14.        , 11.29086804,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.632893231892674}
done in step count: 15
reward sum = 0.8338048408249519
running average episode reward sum: 0.7876793034554052
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.37899956, 10.94397088,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.0172127037898766}
episode index:3848
target Thresh 16.503028834662697
target distance 3.0
model initialize at round 3848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.01118994, 10.71864557,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.0086128990224643}
done in step count: 10
reward sum = 0.8924079825992872
running average episode reward sum: 0.7877065127770846
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.9879126, 10.5136117,  0.       ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.1011561516308068}
episode index:3849
target Thresh 16.504277008175986
target distance 1.0
model initialize at round 3849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.51491606,  6.71948373,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5603532836643842}
done in step count: 0
reward sum = 0.9994352774836097
running average episode reward sum: 0.7877615072614239
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.51491606,  6.71948373,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5603532836643842}
episode index:3850
target Thresh 16.505524557758513
target distance 3.0
model initialize at round 3850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.74477607,  6.        ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.032055838791931}
done in step count: 1
reward sum = 0.9846748719742506
running average episode reward sum: 0.7878126403086099
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.41895455,  4.54512024,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6184161323258351}
episode index:3851
target Thresh 16.506771483722165
target distance 3.0
model initialize at round 3851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.       ,  9.7755239,  0.       ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.024885124796294}
done in step count: 8
reward sum = 0.9051602666702843
running average episode reward sum: 0.7878431043860662
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.45868319, 10.04381049,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5430867772515541}
episode index:3852
target Thresh 16.508017786378673
target distance 1.0
model initialize at round 3852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.        ,  9.05653581,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.0015968737058596}
done in step count: 0
reward sum = 0.9969976026992946
running average episode reward sum: 0.7878973879309178
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.        ,  9.05653581,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.0015968737058596}
episode index:3853
target Thresh 16.509263466039616
target distance 1.0
model initialize at round 3853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.79835737,  8.        ,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 2.0101392367781914}
done in step count: 12
reward sum = 0.8609728073853448
running average episode reward sum: 0.7879163488596812
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.53468151, 10.10532562,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.5449566986492994}
episode index:3854
target Thresh 16.510508523016416
target distance 1.0
model initialize at round 3854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.85032934, 11.05038607,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8518208390225547}
done in step count: 0
reward sum = 0.9986302217980642
running average episode reward sum: 0.7879710087488999
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.85032934, 11.05038607,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8518208390225547}
episode index:3855
target Thresh 16.511752957620327
target distance 12.0
model initialize at round 3855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.180339887498956}
done in step count: 5
reward sum = 0.9245534825573622
running average episode reward sum: 0.788006429514929
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.43239398, 6.27286988, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.5112949505159126}
episode index:3856
target Thresh 16.512996770162474
target distance 6.0
model initialize at round 3856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.42493892,  3.64682806,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 4.373863173754825}
done in step count: 19
reward sum = 0.7942082610006889
running average episode reward sum: 0.7880080374567195
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.90196578,  7.55969847,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0036970154345228}
episode index:3857
target Thresh 16.514239960953795
target distance 13.0
model initialize at round 3857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([15.92688632,  7.3920871 ,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 13.026502747127125}
done in step count: 8
reward sum = 0.9016166081410864
running average episode reward sum: 0.7880374849867051
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.11086619, 9.62629968, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0875707894772566}
episode index:3858
target Thresh 16.515482530305093
target distance 8.0
model initialize at round 3858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 6.93598938, 10.39245749,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.939684408237657}
done in step count: 9
reward sum = 0.8938353990790258
running average episode reward sum: 0.7880649008753011
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.37878265,  3.9629417 ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.1459353005274973}
episode index:3859
target Thresh 16.51672447852701
target distance 9.0
model initialize at round 3859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 7.96812605, 11.85779691,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 7.590405418408335}
done in step count: 4
reward sum = 0.9414947644421552
running average episode reward sum: 0.788104649544619
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.69773353,  8.94079942,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7002405175826}
episode index:3860
target Thresh 16.517965805930032
target distance 3.0
model initialize at round 3860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 6.99592662, 10.6806891 ,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 2.609298546672881}
done in step count: 12
reward sum = 0.8684264510335483
running average episode reward sum: 0.788125452912008
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.9959741 , 8.49434106, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.1169849444662616}
episode index:3861
target Thresh 16.5192065128245
target distance 12.0
model initialize at round 3861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([15.03080106,  8.19466192,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 13.531962545677922}
done in step count: 17
reward sum = 0.7992033354460224
running average episode reward sum: 0.7881283213435291
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.38116677, 2.03713726, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.38297164429530567}
episode index:3862
target Thresh 16.520446599520575
target distance 9.0
model initialize at round 3862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([12.        , 10.13730145,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.052960286063348}
done in step count: 17
reward sum = 0.8166355524851427
running average episode reward sum: 0.7881357009011634
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.26257216, 10.02624042,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0085394673309387}
episode index:3863
target Thresh 16.52168606632829
target distance 9.0
model initialize at round 3863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([8.37084842, 9.        , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 9.465078423129244}
done in step count: 4
reward sum = 0.9334220196688193
running average episode reward sum: 0.7881733008801405
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.1239996 , 2.78512818, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.1763515424674946}
episode index:3864
target Thresh 16.52292491355751
target distance 7.0
model initialize at round 3864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.08882132,  4.46639868,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.859381950463987}
done in step count: 9
reward sum = 0.8846373898868705
running average episode reward sum: 0.7881982592472833
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.59378765, 10.14716061,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9446393446467041}
episode index:3865
target Thresh 16.524163141517946
target distance 9.0
model initialize at round 3865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([6.97876835, 9.        , 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 8.619611006379774}
done in step count: 17
reward sum = 0.8006181328308933
running average episode reward sum: 0.7882014718374497
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.84316946,  3.76721741,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8747127945470213}
episode index:3866
target Thresh 16.52540075051915
target distance 4.0
model initialize at round 3866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.17296505, 8.30550587, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 3.507451507326062}
done in step count: 18
reward sum = 0.7937345509938527
running average episode reward sum: 0.7882029026828484
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.17848187, 5.61035536, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.023438172728111}
episode index:3867
target Thresh 16.526637740870534
target distance 6.0
model initialize at round 3867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([7.        , 9.22410393, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 5.137591472280137}
done in step count: 6
reward sum = 0.9263560081388167
running average episode reward sum: 0.788238619618075
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.4848936 , 6.91505966, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0500803749739707}
episode index:3868
target Thresh 16.52787411288134
target distance 5.0
model initialize at round 3868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.64130825, 7.        , 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 3.0213672025958807}
done in step count: 2
reward sum = 0.9714467208071564
running average episode reward sum: 0.788285972448571
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.97574598, 10.52065182,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.1059649743570177}
episode index:3869
target Thresh 16.529109866860658
target distance 5.0
model initialize at round 3869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 5.71034026, 10.        ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 3.438293355049213}
done in step count: 2
reward sum = 0.9683054962993181
running average episode reward sum: 0.7883324891214006
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.37354667, 11.85964888,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.937301076989164}
episode index:3870
target Thresh 16.530345003117432
target distance 2.0
model initialize at round 3870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 3.36268747, 11.59846305,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.488312858440242}
done in step count: 9
reward sum = 0.9007671557337326
running average episode reward sum: 0.788361534501564
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.65296144, 10.72027383,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7103558030019356}
episode index:3871
target Thresh 16.531579521960445
target distance 6.0
model initialize at round 3871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([16.87508389,  6.83429192,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.061544350122587}
done in step count: 28
reward sum = 0.6933598456771951
running average episode reward sum: 0.7883369989414337
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.15136824, 10.17211237,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8416117081696077}
episode index:3872
target Thresh 16.53281342369832
target distance 12.0
model initialize at round 3872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([15.75567245,  8.50915492,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 12.268251835061545}
done in step count: 15
reward sum = 0.8189899571620305
running average episode reward sum: 0.7883449134671814
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.86769457, 4.66822116, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9289623555083598}
episode index:3873
target Thresh 16.534046708639544
target distance 7.0
model initialize at round 3873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([10.16543325,  8.13376109,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 7.422974333842042}
done in step count: 11
reward sum = 0.8635113403971423
running average episode reward sum: 0.788364316261949
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.30464094, 3.60588648, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.4981280680161565}
episode index:3874
target Thresh 16.535279377092433
target distance 5.0
model initialize at round 3874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([5.50540853, 9.        , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 4.61388002991153}
done in step count: 6
reward sum = 0.926454674185867
running average episode reward sum: 0.7883999524833488
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.94327729, 5.1137661 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.2942884460741269}
episode index:3875
target Thresh 16.53651142936515
target distance 10.0
model initialize at round 3875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([5.26102901, 9.59593141, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 8.759266424588978}
done in step count: 34
reward sum = 0.6662487328837254
running average episode reward sum: 0.7883684377208101
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.39843033,  8.5488963 ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6018648338972771}
episode index:3876
target Thresh 16.53774286576571
target distance 2.0
model initialize at round 3876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.39891827,  6.54137051,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.5121956794705418}
done in step count: 1
reward sum = 0.98525477357619
running average episode reward sum: 0.7884192208871387
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.7425193 ,  8.54137391,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9189236157921836}
episode index:3877
target Thresh 16.538973686601977
target distance 8.0
model initialize at round 3877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([8.        , 9.44664884, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 6.171935908213364}
done in step count: 7
reward sum = 0.9207467390666986
running average episode reward sum: 0.7884533435065764
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.14573479,  7.8179226 ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.23321837173224508}
episode index:3878
target Thresh 16.54020389218165
target distance 2.0
model initialize at round 3878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.        ,  6.45490766,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.45490765571598146}
done in step count: 0
reward sum = 0.9954124327093343
running average episode reward sum: 0.7885066972289797
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.        ,  6.45490766,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.45490765571598146}
episode index:3879
target Thresh 16.541433482812288
target distance 7.0
model initialize at round 3879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([7.46806741, 9.72305536, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.072366429299217}
done in step count: 9
reward sum = 0.8882304837503049
running average episode reward sum: 0.7885323992358151
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.82096008, 2.5819297 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.45479453421184773}
episode index:3880
target Thresh 16.54266245880128
target distance 10.0
model initialize at round 3880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.14134987,  3.96769169,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.952306653235462}
done in step count: 11
reward sum = 0.853563297741606
running average episode reward sum: 0.788549155458053
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.39797418, 10.33192186,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6874643297109914}
episode index:3881
target Thresh 16.543890820455875
target distance 2.0
model initialize at round 3881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.        , 6.93676817, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.06323182582852827}
done in step count: 0
reward sum = 0.9941866966963022
running average episode reward sum: 0.7886021275191655
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.        , 6.93676817, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.06323182582852827}
episode index:3882
target Thresh 16.54511856808316
target distance 4.0
model initialize at round 3882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.81472313, 6.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.008563545686253}
done in step count: 5
reward sum = 0.9382376018349928
running average episode reward sum: 0.7886406635671478
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.44257687, 7.81161787, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5883947401675348}
episode index:3883
target Thresh 16.54634570199008
target distance 13.0
model initialize at round 3883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([14.        , 10.92219639,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 12.996799715215534}
done in step count: 34
reward sum = 0.6331774640477574
running average episode reward sum: 0.7886006369967257
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.54808967, 3.93509052, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.5519198517148782}
episode index:3884
target Thresh 16.547572222483407
target distance 11.0
model initialize at round 3884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.95575252, 6.61552587, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.053957961016417}
done in step count: 14
reward sum = 0.8300494567818721
running average episode reward sum: 0.7886113059336074
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.51921918,  2.57296304,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7479551070735314}
episode index:3885
target Thresh 16.54879812986978
target distance 2.0
model initialize at round 3885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([6.30123925, 8.38490868, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 2.3439940404909}
done in step count: 11
reward sum = 0.8776135530747502
running average episode reward sum: 0.7886342092396138
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.27490134, 9.79745117, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.3414627014791869}
episode index:3886
target Thresh 16.550023424455674
target distance 13.0
model initialize at round 3886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.1325968 ,  5.99505776,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.132597896098035}
done in step count: 45
reward sum = 0.5502469323700208
running average episode reward sum: 0.78857287986558
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.56784458, 5.05342987, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.1038308147628746}
episode index:3887
target Thresh 16.551248106547405
target distance 13.0
model initialize at round 3887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.21812725,  3.39757419,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 13.016850834104918}
done in step count: 32
reward sum = 0.6555271531516202
running average episode reward sum: 0.7885386602856639
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.23516201, 9.30496931, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0334625342630301}
episode index:3888
target Thresh 16.552472176451154
target distance 4.0
model initialize at round 3888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.23892272,  7.00000008,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.139915640040136}
done in step count: 1
reward sum = 0.981906496164716
running average episode reward sum: 0.7885883820228403
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.38325837,  5.00000009,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.3832583651474507}
episode index:3889
target Thresh 16.553695634472934
target distance 3.0
model initialize at round 3889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([16.,  6.,  0.]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.4142135623730956}
done in step count: 33
reward sum = 0.6734280565116524
running average episode reward sum: 0.7885587778260508
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.11231768,  4.44305098,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0479370732094004}
episode index:3890
target Thresh 16.554918480918612
target distance 8.0
model initialize at round 3890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.63126183,  3.2938081 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 7.732003947997297}
done in step count: 60
reward sum = 0.3933875885197135
running average episode reward sum: 0.7884572175101149
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.51036728, 11.82642546,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9713154988170896}
episode index:3891
target Thresh 16.556140716093896
target distance 4.0
model initialize at round 3891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.82180285,  4.        ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.007922862724282}
done in step count: 1
reward sum = 0.9800188890110637
running average episode reward sum: 0.7885064368501716
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.13173919,  5.95400969,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8694779747672269}
episode index:3892
target Thresh 16.557362340304344
target distance 6.0
model initialize at round 3892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.22210842, 5.        , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 4.006161773054481}
done in step count: 4
reward sum = 0.9445759760492726
running average episode reward sum: 0.7885465266367627
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.32379074, 8.67509499, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7502147911916655}
episode index:3893
target Thresh 16.55858335385537
target distance 2.0
model initialize at round 3893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.89423386,  9.67361671,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9519350063681435}
done in step count: 0
reward sum = 0.994859065326777
running average episode reward sum: 0.7885995087987273
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.89423386,  9.67361671,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9519350063681435}
episode index:3894
target Thresh 16.559803757052215
target distance 7.0
model initialize at round 3894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([7.75160051, 8.13349605, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 6.13455082301554}
done in step count: 7
reward sum = 0.9199526143296477
running average episode reward sum: 0.7886332323174772
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.14993215, 5.53323532, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9697858612685495}
episode index:3895
target Thresh 16.56102355019999
target distance 3.0
model initialize at round 3895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.87122325,  7.71818438,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.2681551375547198}
done in step count: 1
reward sum = 0.984682676240658
running average episode reward sum: 0.7886835530166361
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.37851152,  8.79606681,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.42995316207153983}
episode index:3896
target Thresh 16.56224273360364
target distance 6.0
model initialize at round 3896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.7877115 , 5.91951776, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.1558181813705914}
done in step count: 6
reward sum = 0.918811422328475
running average episode reward sum: 0.7887169448229774
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.14855507, 9.66570832, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.3658135293779358}
episode index:3897
target Thresh 16.56346130756796
target distance 7.0
model initialize at round 3897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 9.87862504, 11.87179378,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.452714320563563}
done in step count: 3
reward sum = 0.9545076671135245
running average episode reward sum: 0.788759477076002
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.41202954, 10.34038798,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6793918174098448}
episode index:3898
target Thresh 16.564679272397594
target distance 5.0
model initialize at round 3898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 8.49668351, 11.90252358,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 3.98658029089407}
done in step count: 3
reward sum = 0.958330563471505
running average episode reward sum: 0.7888029679932618
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.06443874,  9.07231013,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.929925186753042}
episode index:3899
target Thresh 16.565896628397034
target distance 14.0
model initialize at round 3899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.45803678,  3.78794622,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.459841380393383}
done in step count: 9
reward sum = 0.8728104424519443
running average episode reward sum: 0.788824508371328
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.0886197 , 4.45440189, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.0183786751680173}
episode index:3900
target Thresh 16.56711337587062
target distance 7.0
model initialize at round 3900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 7.2155554 , 11.87898007,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 8.82831087180962}
done in step count: 7
reward sum = 0.8987675589730387
running average episode reward sum: 0.7888526916706363
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.73258006, 11.85776844,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8984876905390985}
episode index:3901
target Thresh 16.56832951512253
target distance 2.0
model initialize at round 3901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([6.20328236, 8.35103416, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.231071071483965}
done in step count: 5
reward sum = 0.9347550340183852
running average episode reward sum: 0.7888900833524272
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.61210082, 8.34904222, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.7046260594044684}
episode index:3902
target Thresh 16.569545046456817
target distance 8.0
model initialize at round 3902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.68096507,  4.97630727,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.995563811285505}
done in step count: 4
reward sum = 0.9409718917043562
running average episode reward sum: 0.7889290487145465
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.93952555, 11.11680573,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9467586003039461}
episode index:3903
target Thresh 16.570759970177345
target distance 9.0
model initialize at round 3903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([13.75302374,  8.67560554,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 8.759032847588532}
done in step count: 10
reward sum = 0.8722996370746336
running average episode reward sum: 0.7889504038857453
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.620881  , 8.30715988, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.9303336224760905}
episode index:3904
target Thresh 16.571974286587853
target distance 3.0
model initialize at round 3904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.09886575,  7.95299196,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.955492787341943}
done in step count: 2
reward sum = 0.9760455359603932
running average episode reward sum: 0.7889983155712958
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.76999323,  6.17881193,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.7904829424929475}
episode index:3905
target Thresh 16.573187995991926
target distance 11.0
model initialize at round 3905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([5., 8., 0.]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.295630140987008}
done in step count: 32
reward sum = 0.65002848468478
running average episode reward sum: 0.7889627370175615
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.70255038,  2.46424923,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8835190615058961}
episode index:3906
target Thresh 16.57440109869298
target distance 10.0
model initialize at round 3906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 7.        , 10.65373266,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 8.42865926569133}
done in step count: 7
reward sum = 0.9078531296070204
running average episode reward sum: 0.7889931671154856
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.21790579,  8.81135827,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8401102145455505}
episode index:3907
target Thresh 16.575613594994298
target distance 6.0
model initialize at round 3907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.4721359549996045}
done in step count: 2
reward sum = 0.964966111651594
running average episode reward sum: 0.7890381960163393
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.        , 11.58735313,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5873531258410942}
episode index:3908
target Thresh 16.576825485199002
target distance 2.0
model initialize at round 3908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.20105302, 10.58421886,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.77427891461826}
done in step count: 8
reward sum = 0.9088569752470134
running average episode reward sum: 0.7890688480447943
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.30411187,  9.80510676,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8606282134859298}
episode index:3909
target Thresh 16.578036769610065
target distance 6.0
model initialize at round 3909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 4.123105625617682}
done in step count: 2
reward sum = 0.9635697783678828
running average episode reward sum: 0.7891134774387388
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.50181341,  4.        ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.49818658828734286}
episode index:3910
target Thresh 16.57924744853031
target distance 3.0
model initialize at round 3910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.        , 5.53248096, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.8298901272528723}
done in step count: 12
reward sum = 0.8639153632947758
running average episode reward sum: 0.7891326034642709
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.62530071, 3.62916273, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7269946725637214}
episode index:3911
target Thresh 16.5804575222624
target distance 14.0
model initialize at round 3911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([3.86179626, 3.68022525, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.357885940280925}
done in step count: 11
reward sum = 0.8578100260095731
running average episode reward sum: 0.7891501590426312
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.08752499,  6.34340472,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9749551005912943}
episode index:3912
target Thresh 16.58166699110886
target distance 5.0
model initialize at round 3912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([5.43307495, 8.        , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 3.8626226476346424}
done in step count: 2
reward sum = 0.9697858794988473
running average episode reward sum: 0.7891963220174475
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.83546056, 4.80520688, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8578687058460042}
episode index:3913
target Thresh 16.582875855372055
target distance 9.0
model initialize at round 3913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([11.00494245, 11.86740305,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 7.058442237262242}
done in step count: 11
reward sum = 0.8668988107800879
running average episode reward sum: 0.7892161744673103
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.14406054, 10.74551253,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.29243342953526824}
episode index:3914
target Thresh 16.5840841153542
target distance 9.0
model initialize at round 3914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([9.92636824, 9.31305408, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 8.90070218117971}
done in step count: 6
reward sum = 0.9242163299151246
running average episode reward sum: 0.7892506572656366
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.19863352,  2.19989491,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8259214281463134}
episode index:3915
target Thresh 16.585291771357365
target distance 11.0
model initialize at round 3915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([11.        , 11.28801131,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 9.091698033980737}
done in step count: 32
reward sum = 0.669363131601771
running average episode reward sum: 0.7892200424735876
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.95633282, 10.58351838,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5851500021159921}
episode index:3916
target Thresh 16.586498823683456
target distance 13.0
model initialize at round 3916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.25142999,  3.99999945,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.251429986052107}
done in step count: 8
reward sum = 0.8833722361215023
running average episode reward sum: 0.789244079285854
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.57559417, 4.15695967, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.4525004370058683}
episode index:3917
target Thresh 16.587705272634242
target distance 1.0
model initialize at round 3917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([10.24288982, 10.78115833,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.7706857023943854}
done in step count: 2
reward sum = 0.9769897606594509
running average episode reward sum: 0.7892919980406712
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.88148946, 11.01787958,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.11985169306277196}
episode index:3918
target Thresh 16.588911118511337
target distance 2.0
model initialize at round 3918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([3.9399249 , 9.08920479, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.3976076503960415}
done in step count: 4
reward sum = 0.9529092472462679
running average episode reward sum: 0.7893337477853013
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.20447438, 9.77166159, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.8276469309702648}
episode index:3919
target Thresh 16.590116361616193
target distance 6.0
model initialize at round 3919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.49708188, 7.        , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 4.030767965444061}
done in step count: 2
reward sum = 0.9665822264540987
running average episode reward sum: 0.7893789642339414
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.78743005, 3.        , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.2125699520111337}
episode index:3920
target Thresh 16.591321002250133
target distance 7.0
model initialize at round 3920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([11.61894989,  9.        ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 5.5379959936496945}
done in step count: 33
reward sum = 0.6457337315031224
running average episode reward sum: 0.7893423293875423
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.7155993,  3.2968554,  0.       ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.7584827505445767}
episode index:3921
target Thresh 16.592525040714307
target distance 7.0
model initialize at round 3921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.1131741 ,  5.30419139,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.491077622671277}
done in step count: 3
reward sum = 0.9504700490132388
running average episode reward sum: 0.7893834124369115
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.03500484, 10.7256862 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0032266554628633}
episode index:3922
target Thresh 16.593728477309728
target distance 2.0
model initialize at round 3922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.49308476, 10.64143562,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6096728560606121}
done in step count: 0
reward sum = 0.9977969175247396
running average episode reward sum: 0.7894365384896996
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.49308476, 10.64143562,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6096728560606121}
episode index:3923
target Thresh 16.594931312337255
target distance 4.0
model initialize at round 3923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.7005502, 9.       , 0.       ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.6252373183781996}
done in step count: 23
reward sum = 0.7627704204019774
running average episode reward sum: 0.7894297428428882
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.91698809, 10.22188841,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7825270747965767}
episode index:3924
target Thresh 16.5961335460976
target distance 1.0
model initialize at round 3924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.06098134,  8.03754047,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.9634067656439482}
done in step count: 11
reward sum = 0.8837934172806049
running average episode reward sum: 0.7894537845433819
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.15634755,  9.80545009,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.8657939226790801}
episode index:3925
target Thresh 16.59733517889132
target distance 14.0
model initialize at round 3925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.        , 4.76789033, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.002244577452627}
done in step count: 26
reward sum = 0.6975179478131573
running average episode reward sum: 0.7894303673664257
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.08157515,  4.83135857,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9337794917750506}
episode index:3926
target Thresh 16.59853621101882
target distance 14.0
model initialize at round 3926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 3.50966057, 11.91295388,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 14.785919593559859}
done in step count: 7
reward sum = 0.8975868118460026
running average episode reward sum: 0.7894579091144469
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.22896846,  4.71657701,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0526026126430763}
episode index:3927
target Thresh 16.599736642780364
target distance 11.0
model initialize at round 3927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 6.97380116, 11.86775028,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 9.819967294377985}
done in step count: 6
reward sum = 0.9171348415440134
running average episode reward sum: 0.7894904134251469
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.76450566,  8.42240356,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.48361384248473566}
episode index:3928
target Thresh 16.60093647447605
target distance 14.0
model initialize at round 3928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([2.27844226, 4.34565723, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 13.820926044076389}
done in step count: 31
reward sum = 0.680878646148877
running average episode reward sum: 0.7894627698091438
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.86738164,  5.14411093,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.2185635040741738}
episode index:3929
target Thresh 16.602135706405846
target distance 14.0
model initialize at round 3929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([16.18152046, 10.17468035,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 14.347290937645559}
done in step count: 37
reward sum = 0.5852054049703316
running average episode reward sum: 0.7894107959249608
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.06978585, 7.96605843, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.07760215606853271}
episode index:3930
target Thresh 16.603334338869555
target distance 4.0
model initialize at round 3930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([3.59137318, 2.66148865, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 3.698395138775864}
done in step count: 2
reward sum = 0.9704877461709954
running average episode reward sum: 0.7894568597637412
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.1007214 , 6.64373005, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.1059341668132832}
episode index:3931
target Thresh 16.60453237216684
target distance 4.0
model initialize at round 3931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([11.        ,  9.27566373,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 2.018908243075337}
done in step count: 1
reward sum = 0.9817857652970963
running average episode reward sum: 0.7895057735240498
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.        , 9.13720843, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.1372084319592144}
episode index:3932
target Thresh 16.6057298065972
target distance 7.0
model initialize at round 3932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 8.42854654, 11.90672161,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 7.645029386000668}
done in step count: 27
reward sum = 0.6886263039875409
running average episode reward sum: 0.7894801240276
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.69798785,  8.70999492,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7715595444569446}
episode index:3933
target Thresh 16.606926642460003
target distance 5.0
model initialize at round 3933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([12.65861178,  8.        ,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 3.792814277036203}
done in step count: 21
reward sum = 0.7578999064411035
running average episode reward sum: 0.7894720965193168
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.76662021, 8.79161231, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.31287626913014427}
episode index:3934
target Thresh 16.608122880054452
target distance 6.0
model initialize at round 3934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 7.        , 10.59598553,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 4.306642522631408}
done in step count: 9
reward sum = 0.897377681151998
running average episode reward sum: 0.7894995185230355
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.40847319, 8.47406492, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.7915249047416716}
episode index:3935
target Thresh 16.60931851967961
target distance 10.0
model initialize at round 3935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12.      ,  9.938627,  0.      ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.07009991482851}
done in step count: 4
reward sum = 0.9421854157356916
running average episode reward sum: 0.7895383106717175
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.        , 10.78838819,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.2116118076816189}
episode index:3936
target Thresh 16.610513561634384
target distance 1.0
model initialize at round 3936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.14158553, 6.90717278, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.3896570558268462}
done in step count: 27
reward sum = 0.7157527839042934
running average episode reward sum: 0.7895195691104356
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.60090355, 7.70105584, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.49864374462312033}
episode index:3937
target Thresh 16.611708006217537
target distance 11.0
model initialize at round 3937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([14.51236057,  2.28425241,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 12.248173929791326}
done in step count: 23
reward sum = 0.7229920202560214
running average episode reward sum: 0.7895026753702491
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.40242722, 10.7556201 ,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.8561012818874861}
episode index:3938
target Thresh 16.61290185372768
target distance 3.0
model initialize at round 3938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 5.01987028, 10.        ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 3.1811344698695967}
done in step count: 5
reward sum = 0.9364602582457031
running average episode reward sum: 0.7895399837182755
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.43932294, 11.89110339,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.052817179487733}
episode index:3939
target Thresh 16.61409510446327
target distance 12.0
model initialize at round 3939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([3.49945951, 9.        , 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.920684533738266}
done in step count: 15
reward sum = 0.8235688789950653
running average episode reward sum: 0.7895486204937264
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.10316891,  6.36940002,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9699290537726043}
episode index:3940
target Thresh 16.61528775872263
target distance 12.0
model initialize at round 3940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([1.13509247, 9.17033749, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 14.728190159725985}
done in step count: 33
reward sum = 0.6481347254267098
running average episode reward sum: 0.789512737749482
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.97868091,  1.14301239,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.3008628304830567}
episode index:3941
target Thresh 16.616479816803913
target distance 13.0
model initialize at round 3941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.02997196,  1.75235128,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.49815218020393}
done in step count: 48
reward sum = 0.5172193841795814
running average episode reward sum: 0.7894436628246799
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.64184253, 5.24215029, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.4323349825636523}
episode index:3942
target Thresh 16.61767127900514
target distance 12.0
model initialize at round 3942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([12.00494245, 11.86740305,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 10.407731439784726}
done in step count: 7
reward sum = 0.8997417705471773
running average episode reward sum: 0.7894716359689159
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.5354544 , 9.32887739, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5691774348027198}
episode index:3943
target Thresh 16.61886214562417
target distance 11.0
model initialize at round 3943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.72913861, 9.78912199, 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.43477648313735}
done in step count: 22
reward sum = 0.7536579191659937
running average episode reward sum: 0.7894625554119172
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.16786496,  5.02150156,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8324127788513969}
episode index:3944
target Thresh 16.62005241695873
target distance 9.0
model initialize at round 3944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.        ,  8.79384136,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.339423406237315}
done in step count: 22
reward sum = 0.7209357374807812
running average episode reward sum: 0.7894451848623784
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.44664558, 11.90916437,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.012952182977648}
episode index:3945
target Thresh 16.62124209330638
target distance 8.0
model initialize at round 3945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 9.14633024, 11.8735458 ,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.425538835389944}
done in step count: 9
reward sum = 0.8880350476247918
running average episode reward sum: 0.7894701696223282
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.81702072, 10.28272698,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.3367728620580284}
episode index:3946
target Thresh 16.62243117496454
target distance 9.0
model initialize at round 3946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([11.06138407, 11.86842342,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.264901257834063}
done in step count: 10
reward sum = 0.8770558508352408
running average episode reward sum: 0.7894923600660103
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.8158257 , 3.99819699, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.18418312526009448}
episode index:3947
target Thresh 16.623619662230478
target distance 7.0
model initialize at round 3947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([9.        , 8.39009703, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.0370608158850505}
done in step count: 7
reward sum = 0.9114068693536838
running average episode reward sum: 0.7895232401342189
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.88046563, 9.59987256, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6116662114509721}
episode index:3948
target Thresh 16.624807555401322
target distance 3.0
model initialize at round 3948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.01620364,  7.76573539,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.5987742100488957}
done in step count: 1
reward sum = 0.9841656675331122
running average episode reward sum: 0.7895725291763559
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.44856513,  8.86472607,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5677846932714582}
episode index:3949
target Thresh 16.625994854774042
target distance 7.0
model initialize at round 3949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 9.        , 10.28417122,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 5.497039037418925}
done in step count: 3
reward sum = 0.9581010625895641
running average episode reward sum: 0.789615194627853
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.42690885, 8.91237375, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.0774318204358502}
episode index:3950
target Thresh 16.627181560645464
target distance 6.0
model initialize at round 3950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 7.94134769, 11.85951534,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 6.6995593754278415}
done in step count: 47
reward sum = 0.5391695548362633
running average episode reward sum: 0.7895518067159847
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.17812729,  9.63622428,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.660689533935198}
episode index:3951
target Thresh 16.628367673312265
target distance 1.0
model initialize at round 3951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.52961576, 10.        ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.105106933007802}
done in step count: 0
reward sum = 0.9953784774142086
running average episode reward sum: 0.7896038883634287
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.52961576, 10.        ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.105106933007802}
episode index:3952
target Thresh 16.629553193070972
target distance 9.0
model initialize at round 3952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.6927536328546795}
done in step count: 3
reward sum = 0.9483868683136488
running average episode reward sum: 0.7896440560790751
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.05293118, 10.15333178,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8483211582247866}
episode index:3953
target Thresh 16.630738120217963
target distance 12.0
model initialize at round 3953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.48504078,  6.        ,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.528493475187258}
done in step count: 9
reward sum = 0.8751861155177548
running average episode reward sum: 0.7896656903884931
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.37219995, 7.63215383, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.7335879430770746}
episode index:3954
target Thresh 16.631922455049473
target distance 12.0
model initialize at round 3954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.32713782374229}
done in step count: 12
reward sum = 0.8477613496391898
running average episode reward sum: 0.7896803795564452
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.3557981 ,  2.67718348,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7649638885802882}
episode index:3955
target Thresh 16.633106197861583
target distance 4.0
model initialize at round 3955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([16.,  8.,  0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.8284271247461907}
done in step count: 3
reward sum = 0.9584655104533484
running average episode reward sum: 0.7897230451608175
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.52931452,  9.94242513,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.4741937245870477}
episode index:3956
target Thresh 16.634289348950233
target distance 13.0
model initialize at round 3956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.71032846, 7.36177433, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.295466530845948}
done in step count: 6
reward sum = 0.9101837502296698
running average episode reward sum: 0.7897534875932333
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.22895849,  7.76492396,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7984551715552615}
episode index:3957
target Thresh 16.635471908611205
target distance 5.0
model initialize at round 3957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([15.2799052 ,  6.48561883,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 4.96383935985529}
done in step count: 3
reward sum = 0.9652973103742335
running average episode reward sum: 0.7897978392412325
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.66018404,  8.05228125,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.1549951459570331}
episode index:3958
target Thresh 16.636653877140144
target distance 5.0
model initialize at round 3958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 9.9145   , 10.8917942,  0.       ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.0873967603637564}
done in step count: 5
reward sum = 0.9394383532844202
running average episode reward sum: 0.7898356367946661
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.89080453, 10.19425026,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.201151680457815}
episode index:3959
target Thresh 16.637835254832538
target distance 13.0
model initialize at round 3959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 12.083045973594597}
done in step count: 23
reward sum = 0.7125917117478613
running average episode reward sum: 0.7898161307529875
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.76976107, 9.87437562, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.1649312572548256}
episode index:3960
target Thresh 16.639016041983734
target distance 7.0
model initialize at round 3960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86740305,  4.99505755,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 5.079550866406481}
done in step count: 3
reward sum = 0.9543103956301157
running average episode reward sum: 0.7898576592217775
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.82697636, 10.97098427,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9862796928710327}
episode index:3961
target Thresh 16.640196238888926
target distance 12.0
model initialize at round 3961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4., 9., 0.]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.661903789690607}
done in step count: 9
reward sum = 0.8860526558718086
running average episode reward sum: 0.7898819386252732
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.16220598,  2.95535769,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8389825763434744}
episode index:3962
target Thresh 16.641375845843168
target distance 2.0
model initialize at round 3962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.50964046, 11.        ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.4903595447540372}
done in step count: 0
reward sum = 0.9952907390753136
running average episode reward sum: 0.7899337702680818
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.50964046, 11.        ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.4903595447540372}
episode index:3963
target Thresh 16.64255486314136
target distance 6.0
model initialize at round 3963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.8642056 , 8.35437703, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.777171022520531}
done in step count: 50
reward sum = 0.5417356023874701
running average episode reward sum: 0.7898711572085761
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.90689941, 11.53119808,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0510175706030396}
episode index:3964
target Thresh 16.643733291078256
target distance 2.0
model initialize at round 3964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.       ,  2.7767992,  0.       ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7767992019653147}
done in step count: 0
reward sum = 0.9958778350221299
running average episode reward sum: 0.7899231134955403
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.       ,  2.7767992,  0.       ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7767992019653147}
episode index:3965
target Thresh 16.64491112994846
target distance 12.0
model initialize at round 3965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([1.13640098, 8.35795874, 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 13.294663093128662}
done in step count: 65
reward sum = 0.35996227419804894
running average episode reward sum: 0.7898147017861865
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.32976406,  4.46248565,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8591495169424873}
episode index:3966
target Thresh 16.646088380046436
target distance 13.0
model initialize at round 3966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.69477582,  4.        ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 13.629665493873116}
done in step count: 9
reward sum = 0.8807108900454964
running average episode reward sum: 0.7898376148661611
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.90914555, 10.27507051,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.162784762756966}
episode index:3967
target Thresh 16.647265041666493
target distance 11.0
model initialize at round 3967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 6.        , 11.28394401,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 9.285278684249109}
done in step count: 5
reward sum = 0.9269167405396516
running average episode reward sum: 0.7898721610167845
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.94018549,  8.41541558,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.107107809474361}
episode index:3968
target Thresh 16.6484411151028
target distance 4.0
model initialize at round 3968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.15664172,  8.61721325,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.9550043816783202}
done in step count: 2
reward sum = 0.9723139267577772
running average episode reward sum: 0.7899181277000147
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.51213658, 10.94950771,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5146196130282886}
episode index:3969
target Thresh 16.64961660064937
target distance 6.0
model initialize at round 3969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 9.99488807, 10.35664332,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 5.225703394352933}
done in step count: 3
reward sum = 0.9594818295467452
running average episode reward sum: 0.7899608389599257
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.44246853,  7.70397651,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8314814057593451}
episode index:3970
target Thresh 16.650791498600082
target distance 2.0
model initialize at round 3970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([15.15652633,  7.2253201 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.6849221034629251}
done in step count: 3
reward sum = 0.9673759871107712
running average episode reward sum: 0.7900055166602911
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.63583732,  6.59430403,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8703369275216031}
episode index:3971
target Thresh 16.651965809248654
target distance 4.0
model initialize at round 3971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.51364541, 7.92992103, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.9746039829954305}
done in step count: 2
reward sum = 0.9740369380863685
running average episode reward sum: 0.790051848840912
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.0555768 , 5.38385439, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0194505237350107}
episode index:3972
target Thresh 16.653139532888666
target distance 2.0
model initialize at round 3972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.13259695, 10.99505755,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8674171277527281}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7901031823801429
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.13259695, 10.99505755,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8674171277527281}
episode index:3973
target Thresh 16.654312669813553
target distance 5.0
model initialize at round 3973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.8672572, 4.9756388, 0.       ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.146251057389984}
done in step count: 2
reward sum = 0.968160223362406
running average episode reward sum: 0.7901479878761122
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.85362113, 8.97589959, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.2965527487243553}
episode index:3974
target Thresh 16.655485220316592
target distance 6.0
model initialize at round 3974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.17759087,  8.00367404,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.141034406744983}
done in step count: 3
reward sum = 0.9539940354098763
running average episode reward sum: 0.7901892070075672
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.22686729, 10.15640152,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.1442869278528849}
episode index:3975
target Thresh 16.656657184690925
target distance 9.0
model initialize at round 3975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.48173571,  4.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 7.016556797372405}
done in step count: 4
reward sum = 0.9408493337121878
running average episode reward sum: 0.7902270993935594
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.44604593, 11.90970797,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0650979770233426}
episode index:3976
target Thresh 16.65782856322954
target distance 7.0
model initialize at round 3976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 8.72840579, 11.66904533,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 7.2384561534617085}
done in step count: 7
reward sum = 0.9173757972373155
running average episode reward sum: 0.7902590704013148
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.96359453, 8.74675282, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.25585052458954133}
episode index:3977
target Thresh 16.658999356225284
target distance 7.0
model initialize at round 3977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([5.41167533, 8.        , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 5.195462178532201}
done in step count: 27
reward sum = 0.7034083928305099
running average episode reward sum: 0.7902372376517999
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.44713987, 3.33091932, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.5562748039491924}
episode index:3978
target Thresh 16.660169563970857
target distance 10.0
model initialize at round 3978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([10.89977145,  9.32471377,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.526376866092678}
done in step count: 15
reward sum = 0.8184337020640495
running average episode reward sum: 0.790244323971079
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.10478919, 2.55374675, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.05263377353981}
episode index:3979
target Thresh 16.66133918675881
target distance 13.0
model initialize at round 3979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14., 11.,  0.]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.60147050873545}
done in step count: 7
reward sum = 0.897125750548733
running average episode reward sum: 0.7902711786008725
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.41236376, 2.94167892, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.41646754823790466}
episode index:3980
target Thresh 16.66250822488154
target distance 11.0
model initialize at round 3980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 9.055385138137432}
done in step count: 5
reward sum = 0.9241113818940232
running average episode reward sum: 0.7903047983454826
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.05627788, 8.39304153, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.609561956255658}
episode index:3981
target Thresh 16.663676678631322
target distance 3.0
model initialize at round 3981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.95176991, 6.62058485, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 3.379759292388154}
done in step count: 13
reward sum = 0.8335365557586928
running average episode reward sum: 0.790315655140413
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.95300489, 9.82030065, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.1857428260521399}
episode index:3982
target Thresh 16.66484454830026
target distance 12.0
model initialize at round 3982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([12.83746826,  7.13045489,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.280534835649167}
done in step count: 22
reward sum = 0.7623010199839777
running average episode reward sum: 0.7903086215890306
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.73328032, 4.22009726, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.3458065839877404}
episode index:3983
target Thresh 16.66601183418032
target distance 11.0
model initialize at round 3983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([4.        , 3.74240232, 0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 10.423163311387182}
done in step count: 7
reward sum = 0.8985573371453957
running average episode reward sum: 0.7903357924513691
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.00849017,  8.41210009,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.587961214899378}
episode index:3984
target Thresh 16.667178536563327
target distance 10.0
model initialize at round 3984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([15.68474352,  4.        ,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 10.899277825519697}
done in step count: 20
reward sum = 0.7724304730159591
running average episode reward sum: 0.7903312992720879
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.42885237, 8.07645502, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.018258164275929}
episode index:3985
target Thresh 16.668344655740956
target distance 3.0
model initialize at round 3985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([6.1298672, 9.1017555, 0.       ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 3.2562112718901832}
done in step count: 2
reward sum = 0.9732588121704718
running average episode reward sum: 0.7903771917740695
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.06261539, 9.91516207, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.1054426940928177}
episode index:3986
target Thresh 16.669510192004736
target distance 8.0
model initialize at round 3986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 6.324555320336783}
done in step count: 3
reward sum = 0.9495349262707634
running average episode reward sum: 0.7904171109449991
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.10755285, 11.20442761,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9155613421731361}
episode index:3987
target Thresh 16.670675145646054
target distance 1.0
model initialize at round 3987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.98490596,  7.        ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.2293585953026476}
done in step count: 3
reward sum = 0.9597824652194374
running average episode reward sum: 0.7904595796898022
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.14618107,  5.1348997 ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8644100214370337}
episode index:3988
target Thresh 16.671839516956148
target distance 5.0
model initialize at round 3988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.7255429,  7.       ,  0.       ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 3.0125282900121038}
done in step count: 2
reward sum = 0.9731209140537606
running average episode reward sum: 0.7905053709493569
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.3345803 ,  9.81163329,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.38396093097012884}
episode index:3989
target Thresh 16.673003306226104
target distance 7.0
model initialize at round 3989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 5.904457013955612}
done in step count: 12
reward sum = 0.8571390670760962
running average episode reward sum: 0.7905220711238248
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.61225681, 9.55529957, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.6772757141397089}
episode index:3990
target Thresh 16.67416651374688
target distance 13.0
model initialize at round 3990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 3.34427172, 11.89413881,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 13.96095736735894}
done in step count: 16
reward sum = 0.8093968338551462
running average episode reward sum: 0.7905268004555039
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.12041279,  6.11831219,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.16881058697326287}
episode index:3991
target Thresh 16.67532913980927
target distance 8.0
model initialize at round 3991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([10.00000032, 11.70321113,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 6.580832333774729}
done in step count: 4
reward sum = 0.9505570829285984
running average episode reward sum: 0.7905668882016145
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.81547471, 9.8539558 , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.1807791943764618}
episode index:3992
target Thresh 16.67649118470393
target distance 2.0
model initialize at round 3992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.        , 10.38476312,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.3847631216048928}
done in step count: 0
reward sum = 0.9967161128356843
running average episode reward sum: 0.7906185158561684
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.        , 10.38476312,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.3847631216048928}
episode index:3993
target Thresh 16.677652648721377
target distance 4.0
model initialize at round 3993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([10.        ,  9.98843908,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 2.000033413420801}
done in step count: 1
reward sum = 0.9832683058724705
running average episode reward sum: 0.7906667506558721
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.        , 10.18584856,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.18584856390955906}
episode index:3994
target Thresh 16.678813532151974
target distance 2.0
model initialize at round 3994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.73657921,  8.09334254,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9441494891193566}
done in step count: 0
reward sum = 0.9990514085820597
running average episode reward sum: 0.7907189120220613
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.73657921,  8.09334254,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9441494891193566}
episode index:3995
target Thresh 16.679973835285942
target distance 10.0
model initialize at round 3995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([13.09390072,  5.58613809,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 8.784399921558409}
done in step count: 20
reward sum = 0.7388251649733812
running average episode reward sum: 0.7907059255988761
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.02316997, 8.54757833, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.0765139412255005}
episode index:3996
target Thresh 16.681133558413357
target distance 14.0
model initialize at round 3996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([1.6311729 , 2.76733208, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 14.421604022067383}
done in step count: 12
reward sum = 0.8497653260305552
running average episode reward sum: 0.790720701530933
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.29332068,  4.6464206 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9577344385167011}
episode index:3997
target Thresh 16.68229270182415
target distance 11.0
model initialize at round 3997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([14.13816583,  8.71734512,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 9.142536220722311}
done in step count: 5
reward sum = 0.9342303209146763
running average episode reward sum: 0.7907565968834552
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.14007831, 9.38679738, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.41138077643457904}
episode index:3998
target Thresh 16.683451265808102
target distance 9.0
model initialize at round 3998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 9.        , 10.78412104,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 7.0437806467782895}
done in step count: 4
reward sum = 0.9442515566904778
running average episode reward sum: 0.790794980219241
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.87771935, 10.21461763,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9035773276967186}
episode index:3999
target Thresh 16.684609250654862
target distance 7.0
model initialize at round 3999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.        ,  8.74732566,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 6.085371987666795}
done in step count: 13
reward sum = 0.846471089943478
running average episode reward sum: 0.790808899246672
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.13316358,  2.15218914,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.2125133484483075}
episode index:4000
target Thresh 16.68576665665392
target distance 12.0
model initialize at round 4000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([5.94478322, 8.045681  , 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 10.243375769965436}
done in step count: 11
reward sum = 0.855334745389305
running average episode reward sum: 0.7908250266763502
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.1915824 , 10.90095549,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.2104791631965188}
episode index:4001
target Thresh 16.686923484094635
target distance 12.0
model initialize at round 4001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([5.11034036, 9.        , 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.071796924806515}
done in step count: 21
reward sum = 0.751659370928493
running average episode reward sum: 0.7908152401556736
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.86883036,  6.27260399,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.133124508576558}
episode index:4002
target Thresh 16.688079733266207
target distance 9.0
model initialize at round 4002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.38773108, 9.        , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 7.010730018103919}
done in step count: 4
reward sum = 0.9437373375724234
running average episode reward sum: 0.7908534420286231
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.8521012, 1.1281042, 0.       ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.2191303185456528}
episode index:4003
target Thresh 16.6892354044577
target distance 5.0
model initialize at round 4003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([5.        , 7.99914086, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.162549449545982}
done in step count: 2
reward sum = 0.9729474403927959
running average episode reward sum: 0.7908989200501926
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.81424671, 8.13841712, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.881379229771052}
episode index:4004
target Thresh 16.69039049795803
target distance 8.0
model initialize at round 4004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 8.        , 10.36833477,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.03315845660754}
done in step count: 3
reward sum = 0.9564195753953955
running average episode reward sum: 0.7909402485533998
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.01542663, 11.66251296,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.6626925371209594}
episode index:4005
target Thresh 16.691545014055976
target distance 13.0
model initialize at round 4005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.08304597359458}
done in step count: 30
reward sum = 0.691173027491444
running average episode reward sum: 0.7909153441048073
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.64759449, 5.96643098, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0286780255967107}
episode index:4006
target Thresh 16.69269895304016
target distance 11.0
model initialize at round 4006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.00612653,  6.74874937,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.034101546118261}
done in step count: 31
reward sum = 0.6631764801126012
running average episode reward sum: 0.7908834651769331
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.68097633, 5.33982863, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9484487327419715}
episode index:4007
target Thresh 16.693852315199074
target distance 8.0
model initialize at round 4007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([10.,  9.,  0.]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 7.211102550928006}
done in step count: 3
reward sum = 0.9496896281469343
running average episode reward sum: 0.7909230874730833
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.15898229,  5.18539174,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.24422420885902849}
episode index:4008
target Thresh 16.695005100821053
target distance 5.0
model initialize at round 4008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4., 8., 0.]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.0000000000000178}
done in step count: 2
reward sum = 0.9680250841870118
running average episode reward sum: 0.7909672635760302
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.33315766, 4.        , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.2019478808644122}
episode index:4009
target Thresh 16.69615731019429
target distance 7.0
model initialize at round 4009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 7.        , 10.36348414,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 5.04035241175368}
done in step count: 12
reward sum = 0.8537139063273629
running average episode reward sum: 0.7909829111178635
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.12583351, 10.84210518,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.888311784129088}
episode index:4010
target Thresh 16.69730894360685
target distance 5.0
model initialize at round 4010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.11546015, 4.7438866 , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.374119914020665}
done in step count: 3
reward sum = 0.9549090841075945
running average episode reward sum: 0.7910237802709398
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.12969645, 7.15182116, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.2152512535204225}
episode index:4011
target Thresh 16.698460001346632
target distance 3.0
model initialize at round 4011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([1.13960163, 6.97495649, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.124098939162474}
done in step count: 77
reward sum = 0.3670066946054434
running average episode reward sum: 0.7909180930611529
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.67123504, 7.22124459, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0281130580267164}
episode index:4012
target Thresh 16.6996104837014
target distance 11.0
model initialize at round 4012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([12.59393213, 11.88480583,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.765912022488584}
done in step count: 10
reward sum = 0.8677588432344167
running average episode reward sum: 0.7909372410178369
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.44106262, 7.78963146, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9674341513052993}
episode index:4013
target Thresh 16.700760390958777
target distance 9.0
model initialize at round 4013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 7.280109889280531}
done in step count: 7
reward sum = 0.9042809958341769
running average episode reward sum: 0.7909654781266601
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.63670913,  9.46513884,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5901986038432703}
episode index:4014
target Thresh 16.701909723406246
target distance 8.0
model initialize at round 4014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([4.        , 4.66077209, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 7.4046538674449875}
done in step count: 61
reward sum = 0.42017952368633404
running average episode reward sum: 0.790873127951208
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.59564568, 8.09412459, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.9920245310541418}
episode index:4015
target Thresh 16.703058481331126
target distance 11.0
model initialize at round 4015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([5.9790256, 9.       , 0.       ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 10.218606944126186}
done in step count: 28
reward sum = 0.6608506034812949
running average episode reward sum: 0.7908407518245969
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.68888656, 11.88901766,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9418832096492851}
episode index:4016
target Thresh 16.704206665020617
target distance 5.0
model initialize at round 4016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([4.8674032 , 5.99505776, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 3.68478588042799}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.7908873585581254
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.10402709, 8.20442743, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.198208321029664}
episode index:4017
target Thresh 16.705354274761767
target distance 11.0
model initialize at round 4017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.067814290739566}
done in step count: 6
reward sum = 0.9161121659176847
running average episode reward sum: 0.7909185245131677
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.03421003, 10.93649518,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9678755763257163}
episode index:4018
target Thresh 16.706501310841468
target distance 7.0
model initialize at round 4018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([10.        ,  9.88118923,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 5.077055687498868}
done in step count: 3
reward sum = 0.9564418984004064
running average episode reward sum: 0.7909597097268743
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.        , 8.20429748, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.2779446371466137}
episode index:4019
target Thresh 16.707647773546487
target distance 8.0
model initialize at round 4019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 6.288603024860093}
done in step count: 3
reward sum = 0.9519975477189402
running average episode reward sum: 0.790999768890554
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.00476222, 9.34873233, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.6512850769700248}
episode index:4020
target Thresh 16.708793663163434
target distance 3.0
model initialize at round 4020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([13.2438519 ,  8.09720623,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.002819505185778}
done in step count: 36
reward sum = 0.6324715024870584
running average episode reward sum: 0.7909603438056488
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.57427854, 11.90945652,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0041662838329872}
episode index:4021
target Thresh 16.709938979978787
target distance 8.0
model initialize at round 4021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([15.63833404, 10.        ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.696022936683638}
done in step count: 66
reward sum = 0.4108165305472789
running average episode reward sum: 0.790865827690965
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.91426466, 11.84291399,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.2435368342722695}
episode index:4022
target Thresh 16.711083724278875
target distance 8.0
model initialize at round 4022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([10.99999999,  8.32740658,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 7.002576248504318}
done in step count: 50
reward sum = 0.5110764349544268
running average episode reward sum: 0.7907962802406203
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.69383424,  1.12488794,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.116793205495441}
episode index:4023
target Thresh 16.712227896349884
target distance 11.0
model initialize at round 4023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.25719681,  2.70762875,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.281576966208688}
done in step count: 35
reward sum = 0.63594459655954
running average episode reward sum: 0.7907577982118725
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.83582455, 2.59194643, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0242085959992693}
episode index:4024
target Thresh 16.71337149647785
target distance 10.0
model initialize at round 4024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([5.35246888, 7.40152162, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.19589281402368}
done in step count: 8
reward sum = 0.8862278765916074
running average episode reward sum: 0.7907815174860041
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.18797179,  1.12355816,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8963725163316527}
episode index:4025
target Thresh 16.71451452494868
target distance 4.0
model initialize at round 4025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([5.99992215, 8.00016787, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.8284907837030415}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.7908280347445542
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.95430867, 6.05522186, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.07167392591587601}
episode index:4026
target Thresh 16.71565698204813
target distance 6.0
model initialize at round 4026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.25725281, 2.39454603, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 5.611353954028086}
done in step count: 6
reward sum = 0.928764052681007
running average episode reward sum: 0.7908622875426512
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.10371437, 7.68974936, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9484636955525659}
episode index:4027
target Thresh 16.71679886806181
target distance 5.0
model initialize at round 4027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.0925771 ,  8.51326561,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.6792336109029047}
done in step count: 2
reward sum = 0.9721977956987423
running average episode reward sum: 0.7909073062884695
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.66369134,  4.74069667,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.7125478334433907}
episode index:4028
target Thresh 16.7179401832752
target distance 2.0
model initialize at round 4028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 8.45200825, 11.        ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.5479917526245472}
done in step count: 3
reward sum = 0.9620240302802736
running average episode reward sum: 0.7909497775528009
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.58333798, 10.36853886,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.7565384374040655}
episode index:4029
target Thresh 16.719080927973625
target distance 10.0
model initialize at round 4029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.86964309, 8.        , 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.610588809465561}
done in step count: 25
reward sum = 0.7172803155870122
running average episode reward sum: 0.7909314972892859
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.10070717,  5.35767426,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9678111772024778}
episode index:4030
target Thresh 16.720221102442267
target distance 5.0
model initialize at round 4030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 6.        , 10.04465453,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 3.1766811420583094}
done in step count: 2
reward sum = 0.9707439073898403
running average episode reward sum: 0.7909761046844982
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.        , 9.31380629, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0480812880465609}
episode index:4031
target Thresh 16.72136070696617
target distance 12.0
model initialize at round 4031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 12.20655561573373}
done in step count: 23
reward sum = 0.7558040980215079
running average episode reward sum: 0.79096738146856
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.77186866, 11.05725199,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.23520564633608612}
episode index:4032
target Thresh 16.72249974183024
target distance 7.0
model initialize at round 4032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([4.92833813, 7.00000316, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 5.4517649819047085}
done in step count: 5
reward sum = 0.9353567160574675
running average episode reward sum: 0.7910031834359761
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.44063058, 8.510989  , 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.7429844545076747}
episode index:4033
target Thresh 16.723638207319237
target distance 2.0
model initialize at round 4033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.43064332, 9.378865  , 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.6838901373081756}
done in step count: 0
reward sum = 0.9981755883607064
running average episode reward sum: 0.7910545400063589
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.43064332, 9.378865  , 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.6838901373081756}
episode index:4034
target Thresh 16.724776103717772
target distance 6.0
model initialize at round 4034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.21825993, 7.        , 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.234183033181163}
done in step count: 3
reward sum = 0.9548485001024287
running average episode reward sum: 0.7910951333050197
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.5715957 , 11.52292824,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6760060557947809}
episode index:4035
target Thresh 16.72591343131032
target distance 2.0
model initialize at round 4035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.99505755, 11.86740305,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.8674095874256116}
done in step count: 38
reward sum = 0.596343812391259
running average episode reward sum: 0.7910468797567258
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.37605212,  9.27363356,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9575589599343522}
episode index:4036
target Thresh 16.727050190381213
target distance 1.0
model initialize at round 4036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.41771463,  8.37173271,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5591696678592103}
done in step count: 0
reward sum = 0.9984578978403074
running average episode reward sum: 0.7910982572692558
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.41771463,  8.37173271,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5591696678592103}
episode index:4037
target Thresh 16.728186381214645
target distance 10.0
model initialize at round 4037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.86860584, 4.94319621, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.629455264118628}
done in step count: 7
reward sum = 0.8940304923014706
running average episode reward sum: 0.791123748164509
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.60988458,  7.1108256 ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.0782348142830402}
episode index:4038
target Thresh 16.72932200409466
target distance 2.0
model initialize at round 4038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.86102654,  9.02264629,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.8611643188819145}
done in step count: 8
reward sum = 0.9102196962547091
running average episode reward sum: 0.791153234658218
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.97669232,  9.01928668,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9768827297095903}
episode index:4039
target Thresh 16.730457059305163
target distance 7.0
model initialize at round 4039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([3.40019619, 4.        , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 6.793982267251231}
done in step count: 11
reward sum = 0.8760965091577347
running average episode reward sum: 0.7911742602212128
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.87020385, 9.56328195, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.036600841271973}
episode index:4040
target Thresh 16.73159154712992
target distance 7.0
model initialize at round 4040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 8.60758173, 10.45099556,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.630350085057291}
done in step count: 6
reward sum = 0.935204105400111
running average episode reward sum: 0.7912099023506807
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.69297218, 11.24591193,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7353115816384079}
episode index:4041
target Thresh 16.73272546785255
target distance 6.0
model initialize at round 4041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([9.82926059, 9.68641913, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 6.350293856774657}
done in step count: 3
reward sum = 0.956705943181291
running average episode reward sum: 0.7912508464478678
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.32092316,  5.86892304,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.69161154600022}
episode index:4042
target Thresh 16.733858821756534
target distance 9.0
model initialize at round 4042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.64071596,  2.65259528,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 11.316346842666814}
done in step count: 26
reward sum = 0.7192273397257951
running average episode reward sum: 0.7912330320756882
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.73449473, 11.11054264,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.7427665712934011}
episode index:4043
target Thresh 16.734991609125217
target distance 8.0
model initialize at round 4043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.96464145,  9.71672106,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 6.716814125495506}
done in step count: 6
reward sum = 0.9302505979225483
running average episode reward sum: 0.7912674083283704
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.22128077,  3.58188841,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.6225426089960884}
episode index:4044
target Thresh 16.736123830241787
target distance 8.0
model initialize at round 4044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([13.09035659,  3.53250282,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 7.167902320020855}
done in step count: 23
reward sum = 0.749771288674848
running average episode reward sum: 0.7912571497079369
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.61671557,  9.84202246,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.6366278367344893}
episode index:4045
target Thresh 16.7372554853893
target distance 12.0
model initialize at round 4045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.90261493, 4.61899166, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.801141466029117}
done in step count: 7
reward sum = 0.8941586718649136
running average episode reward sum: 0.7912825826101012
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.60162462, 11.13392164,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.42028317486794625}
episode index:4046
target Thresh 16.738386574850676
target distance 7.0
model initialize at round 4046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([16.87411291, 10.83661715,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 8.91346218922107}
done in step count: 42
reward sum = 0.5955640579090615
running average episode reward sum: 0.7912342212251986
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.88522675, 10.72023772,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.14121372551453}
episode index:4047
target Thresh 16.739517098908685
target distance 12.0
model initialize at round 4047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([16.57734177,  4.        ,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 12.617033167500313}
done in step count: 14
reward sum = 0.8383117032913945
running average episode reward sum: 0.791245851037962
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.85114948, 4.90168063, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8568092802865213}
episode index:4048
target Thresh 16.74064705784596
target distance 12.0
model initialize at round 4048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.80499339, 10.98179913,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.805007422033002}
done in step count: 20
reward sum = 0.7783476653702981
running average episode reward sum: 0.791242665514211
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.59549618, 11.81191357,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.006885969962242}
episode index:4049
target Thresh 16.74177645194498
target distance 13.0
model initialize at round 4049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.529964086141694}
done in step count: 6
reward sum = 0.9092098147260185
running average episode reward sum: 0.7912717932053744
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.95557642, 9.29610715, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1868409457104687}
episode index:4050
target Thresh 16.742905281488103
target distance 13.0
model initialize at round 4050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.,  8.,  0.]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.401754250991402}
done in step count: 8
reward sum = 0.8935664502201621
running average episode reward sum: 0.7912970449103891
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.8714596 , 10.90796653,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8763058805066601}
episode index:4051
target Thresh 16.744033546757535
target distance 4.0
model initialize at round 4051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.15471131,  4.        ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.005974971985866}
done in step count: 1
reward sum = 0.9857638629701949
running average episode reward sum: 0.7913450377085283
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.6607635 ,  2.81819248,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8857315276571242}
episode index:4052
target Thresh 16.745161248035345
target distance 3.0
model initialize at round 4052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.        , 10.94079602,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.3729884001040362}
done in step count: 1
reward sum = 0.9852951290589097
running average episode reward sum: 0.7913928911729622
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.9857325 , 10.28473473,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.28509196032085576}
episode index:4053
target Thresh 16.746288385603453
target distance 11.0
model initialize at round 4053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.24230897, 4.        , 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.960548888173236}
done in step count: 46
reward sum = 0.5321600171230093
running average episode reward sum: 0.7913289462114304
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.24921777,  6.24192314,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.34732736767945493}
episode index:4054
target Thresh 16.747414959743647
target distance 4.0
model initialize at round 4054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 2.7557163902527138}
done in step count: 5
reward sum = 0.9388242772665198
running average episode reward sum: 0.7913653199058952
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 7.2592842 , 10.08871206,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.27404037010204485}
episode index:4055
target Thresh 16.748540970737565
target distance 2.0
model initialize at round 4055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.22096539, 8.95061469, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.2290497227594845}
done in step count: 0
reward sum = 0.9968003694111862
running average episode reward sum: 0.791415969572933
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.22096539, 8.95061469, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.2290497227594845}
episode index:4056
target Thresh 16.749666418866717
target distance 4.0
model initialize at round 4056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([10.92530441,  8.97337604,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.2250912949165524}
done in step count: 13
reward sum = 0.8484712415353571
running average episode reward sum: 0.7914300329872692
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.35788954,  7.95852551,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.3602846824872467}
episode index:4057
target Thresh 16.750791304412463
target distance 5.0
model initialize at round 4057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([13.67558253,  7.26753533,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 4.009950586851309}
done in step count: 3
reward sum = 0.9608150793886396
running average episode reward sum: 0.7914717740041253
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.85597205,  4.93063819,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.2644269807809125}
episode index:4058
target Thresh 16.75191562765602
target distance 13.0
model initialize at round 4058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.72865462, 10.        ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.728654623031684}
done in step count: 10
reward sum = 0.8804628042637271
running average episode reward sum: 0.7914936983771874
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.71166755, 9.90358243, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.30402623034348764}
episode index:4059
target Thresh 16.753039388878477
target distance 13.0
model initialize at round 4059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([3.71209979, 6.86501718, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.964974913755153}
done in step count: 27
reward sum = 0.6973869030841362
running average episode reward sum: 0.7914705193635684
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.3590265 , 10.39922698,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8785073988898063}
episode index:4060
target Thresh 16.754162588360767
target distance 2.0
model initialize at round 4060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([6.16990888, 9.0396381 , 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.513598944670095}
done in step count: 1
reward sum = 0.9879093486993723
running average episode reward sum: 0.7915188913973866
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.27952397, 9.66835445, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.4337308137682422}
episode index:4061
target Thresh 16.755285226383695
target distance 4.0
model initialize at round 4061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 4.        , 11.42022073,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 2.043669607392945}
done in step count: 1
reward sum = 0.9817390304500837
running average episode reward sum: 0.791565720579822
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.0159724 , 11.85468055,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8548297876349631}
episode index:4062
target Thresh 16.756407303227917
target distance 1.0
model initialize at round 4062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.12033093, 5.95355868, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8808941304331734}
done in step count: 0
reward sum = 0.9965288536763985
running average episode reward sum: 0.7916161668345837
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.12033093, 5.95355868, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8808941304331734}
episode index:4063
target Thresh 16.75752881917395
target distance 14.0
model initialize at round 4063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.65152752,  8.18277574,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.671953518738844}
done in step count: 42
reward sum = 0.5552042059579523
running average episode reward sum: 0.791557994600116
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.87845849, 3.41401194, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9711308918685525}
episode index:4064
target Thresh 16.758649774502178
target distance 9.0
model initialize at round 4064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([4.        , 3.91822124, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 8.650114185003984}
done in step count: 30
reward sum = 0.6612155765191321
running average episode reward sum: 0.7915259300446226
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.64773395,  8.31952094,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.7662526483104769}
episode index:4065
target Thresh 16.75977016949284
target distance 2.0
model initialize at round 4065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.07884514, 5.91090548, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.090581849273296}
done in step count: 54
reward sum = 0.5101144075749655
running average episode reward sum: 0.7914567191438676
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.95104222, 8.28455437, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.28873526976480585}
episode index:4066
target Thresh 16.760890004426027
target distance 1.0
model initialize at round 4066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.4142135623730687}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7915065207866169
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.4142135623730687}
episode index:4067
target Thresh 16.762009279581708
target distance 7.0
model initialize at round 4067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([6.59839654, 9.32660961, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.737028446100359}
done in step count: 14
reward sum = 0.845181467632062
running average episode reward sum: 0.7915197152179947
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.24780404,  2.6337923 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.44217069248239294}
episode index:4068
target Thresh 16.763127995239692
target distance 14.0
model initialize at round 4068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.        , 5.70602107, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.301323100946655}
done in step count: 8
reward sum = 0.8875080379605595
running average episode reward sum: 0.7915433053685828
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.69593339,  3.29233695,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.7548404961574046}
episode index:4069
target Thresh 16.76424615167967
target distance 2.0
model initialize at round 4069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([3.56768858, 6.48154759, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.6399803604880219}
done in step count: 25
reward sum = 0.7401731705812716
running average episode reward sum: 0.7915306837138439
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.14804719, 6.61335374, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.0497744511175873}
episode index:4070
target Thresh 16.765363749181173
target distance 12.0
model initialize at round 4070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([12.        ,  8.43677664,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.892522587235348}
done in step count: 11
reward sum = 0.8700283004642423
running average episode reward sum: 0.7915499658599383
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.92534067, 2.73947123, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.1845138446537875}
episode index:4071
target Thresh 16.7664807880236
target distance 1.0
model initialize at round 4071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.43799412, 6.90137053, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5705947595329276}
done in step count: 0
reward sum = 0.998857871937381
running average episode reward sum: 0.7916008764459102
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.43799412, 6.90137053, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5705947595329276}
episode index:4072
target Thresh 16.76759726848621
target distance 2.0
model initialize at round 4072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.12069723, 9.23527086, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.516267612048108}
done in step count: 16
reward sum = 0.8200892372995928
running average episode reward sum: 0.7916078708875633
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.60552653, 8.72737718, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8274580819639773}
episode index:4073
target Thresh 16.76871319084813
target distance 6.0
model initialize at round 4073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([6.        , 8.40653396, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.767186394367627}
done in step count: 3
reward sum = 0.9585977027285156
running average episode reward sum: 0.7916488600460909
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.85136354, 11.51017436,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5313856216188326}
episode index:4074
target Thresh 16.769828555388333
target distance 5.0
model initialize at round 4074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([12.59346843,  9.        ,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 3.5934684276580438}
done in step count: 3
reward sum = 0.9575904224179783
running average episode reward sum: 0.7916895819018877
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.64581811, 9.62909639, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.7219467255927765}
episode index:4075
target Thresh 16.770943362385665
target distance 11.0
model initialize at round 4075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([14.        ,  6.21670341,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 9.420548809943872}
done in step count: 14
reward sum = 0.8301290352605407
running average episode reward sum: 0.7916990125822996
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.1147514 , 8.82324196, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.2107398595876599}
episode index:4076
target Thresh 16.772057612118825
target distance 4.0
model initialize at round 4076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([15.38090193,  9.20509052,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 2.6017906067501246}
done in step count: 2
reward sum = 0.9735186691720229
running average episode reward sum: 0.7917436090151152
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.92459893,  7.84903744,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8523789610334112}
episode index:4077
target Thresh 16.773171304866377
target distance 2.0
model initialize at round 4077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.85084486,  7.        ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.14915513992313656}
done in step count: 0
reward sum = 0.9960095815178555
running average episode reward sum: 0.7917936987582498
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.85084486,  7.        ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.14915513992313656}
episode index:4078
target Thresh 16.774284440906744
target distance 1.0
model initialize at round 4078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.02030165,  8.13813984,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.3048416726544363}
done in step count: 0
reward sum = 0.9953442926701712
running average episode reward sum: 0.7918436008406013
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.02030165,  8.13813984,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.3048416726544363}
episode index:4079
target Thresh 16.77539702051821
target distance 13.0
model initialize at round 4079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.04831857,  6.59273406,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.348463031175632}
done in step count: 8
reward sum = 0.8862641093777821
running average episode reward sum: 0.7918667431221056
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.39796581, 4.74107009, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8411668489791336}
episode index:4080
target Thresh 16.776509043978916
target distance 11.0
model initialize at round 4080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.13224963,  5.97380131,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.883638150958815}
done in step count: 13
reward sum = 0.8396985416360571
running average episode reward sum: 0.7918784637294356
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.88854454, 2.54333595, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.5546496849436605}
episode index:4081
target Thresh 16.777620511566877
target distance 8.0
model initialize at round 4081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([10.        , 11.35726714,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 6.010627239338921}
done in step count: 3
reward sum = 0.9567822549717332
running average episode reward sum: 0.7919188615224886
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.00042292, 11.82709173,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8270918353137837}
episode index:4082
target Thresh 16.77873142355995
target distance 1.0
model initialize at round 4082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.09055674,  3.43938567,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.8062680140272802}
done in step count: 5
reward sum = 0.9426172840291411
running average episode reward sum: 0.7919557702715717
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.13238646,  4.93430289,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8700973321810589}
episode index:4083
target Thresh 16.77984178023587
target distance 9.0
model initialize at round 4083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([11.00000429,  8.20989433,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.357502210698629}
done in step count: 24
reward sum = 0.7256734280437042
running average episode reward sum: 0.791939540510987
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.03015503, 2.7804101 , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.244845051481365}
episode index:4084
target Thresh 16.78095158187222
target distance 7.0
model initialize at round 4084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.34598982,  9.05554771,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 6.090762467091544}
done in step count: 9
reward sum = 0.8947942622741729
running average episode reward sum: 0.7919647191454455
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.74014016,  3.35964807,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.44370470738368406}
episode index:4085
target Thresh 16.782060828746456
target distance 6.0
model initialize at round 4085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([5.        , 9.62114441, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 4.047940264255282}
done in step count: 2
reward sum = 0.9696239254244146
running average episode reward sum: 0.7920081991274032
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([8.97199835, 8.14371884, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.8567388818489862}
episode index:4086
target Thresh 16.78316952113589
target distance 3.0
model initialize at round 4086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.32629812,  9.46692276,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.786034593450029}
done in step count: 2
reward sum = 0.9729652834783181
running average episode reward sum: 0.7920524753897842
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.43115723, 11.57732463,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.810484929391275}
episode index:4087
target Thresh 16.784277659317688
target distance 1.0
model initialize at round 4087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.43819652, 11.8946287 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9961810590610236}
done in step count: 0
reward sum = 0.9990676613820979
running average episode reward sum: 0.7921031151123851
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.43819652, 11.8946287 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9961810590610236}
episode index:4088
target Thresh 16.785385243568893
target distance 7.0
model initialize at round 4088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([13.4563129,  5.       ,  0.       ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 8.166025732268363}
done in step count: 27
reward sum = 0.6909973443572385
running average episode reward sum: 0.7920783888294906
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.38932243, 9.54896109, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.7591858773730621}
episode index:4089
target Thresh 16.786492274166395
target distance 9.0
model initialize at round 4089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([12.1440582 ,  7.92348808,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.778334855796322}
done in step count: 7
reward sum = 0.9141445519939836
running average episode reward sum: 0.7921082338571592
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.50003311, 11.82346693,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9633955023178141}
episode index:4090
target Thresh 16.787598751386955
target distance 2.0
model initialize at round 4090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.71810696, 3.79367816, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8422521697399263}
done in step count: 0
reward sum = 0.9988489929499405
running average episode reward sum: 0.7921587693641484
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.71810696, 3.79367816, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8422521697399263}
episode index:4091
target Thresh 16.78870467550719
target distance 4.0
model initialize at round 4091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 2.54790974, 10.        ,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 3.4520902633667063}
done in step count: 4
reward sum = 0.9492212360907338
running average episode reward sum: 0.7921971521761539
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.79640879, 10.70103003,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.7299948515360135}
episode index:4092
target Thresh 16.789810046803584
target distance 11.0
model initialize at round 4092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.62880921, 11.56812131,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.657471519942039}
done in step count: 23
reward sum = 0.7374586118761991
running average episode reward sum: 0.7921837784795256
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.86935037, 6.98765878, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8694379584600248}
episode index:4093
target Thresh 16.79091486555248
target distance 4.0
model initialize at round 4093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([8.99386023, 8.36451814, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 2.5883194709087776}
done in step count: 3
reward sum = 0.9609064033432728
running average episode reward sum: 0.7922249906497414
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.98856175,  9.82930478,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.0031905054135868}
episode index:4094
target Thresh 16.792019132030077
target distance 3.0
model initialize at round 4094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([13.8263191 ,  9.17750725,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.4331856753202874}
done in step count: 6
reward sum = 0.932324675004309
running average episode reward sum: 0.7922592030268731
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.26826426,  9.05329302,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.1965330310511133}
episode index:4095
target Thresh 16.793122846512446
target distance 10.0
model initialize at round 4095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 7.91642308, 10.48929507,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 10.366058374782638}
done in step count: 13
reward sum = 0.8541383420556462
running average episode reward sum: 0.7922743102385501
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.8673908 ,  4.95632803,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.2910964723272378}
episode index:4096
target Thresh 16.79422600927552
target distance 2.0
model initialize at round 4096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.        , 3.24826902, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7517309784889128}
done in step count: 0
reward sum = 0.9969537722498644
running average episode reward sum: 0.7923242686134614
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.        , 3.24826902, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7517309784889128}
episode index:4097
target Thresh 16.79532862059508
target distance 11.0
model initialize at round 4097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([6.        , 8.54617453, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.083040356103046}
done in step count: 7
reward sum = 0.904189649907378
running average episode reward sum: 0.792351566168682
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.13668168,  3.61841182,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.4053287795732373}
episode index:4098
target Thresh 16.796430680746784
target distance 6.0
model initialize at round 4098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.63603884, 5.54421997, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 4.470620023621168}
done in step count: 3
reward sum = 0.9601819655227569
running average episode reward sum: 0.7923925103988244
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.46218196, 10.45021393,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6452168204891835}
episode index:4099
target Thresh 16.797532190006148
target distance 11.0
model initialize at round 4099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4., 5., 0.]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.816653826391994}
done in step count: 5
reward sum = 0.9182543079307443
running average episode reward sum: 0.7924232083982226
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.0470569, 11.8496664,  0.       ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.850968472981281}
episode index:4100
target Thresh 16.79863314864855
target distance 10.0
model initialize at round 4100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([14.91527009,  4.        ,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 10.221645698198}
done in step count: 28
reward sum = 0.6981457746889239
running average episode reward sum: 0.7924002195092419
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.26266722, 9.26767535, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.7844168007067324}
episode index:4101
target Thresh 16.799733556949224
target distance 6.0
model initialize at round 4101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.20135897, 6.72078633, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.2839485341822545}
done in step count: 2
reward sum = 0.9703199749202558
running average episode reward sum: 0.7924435934135352
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.34014906, 10.7207863 ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.716493926139005}
episode index:4102
target Thresh 16.800833415183277
target distance 10.0
model initialize at round 4102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.13655188,  3.98175579,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.136570093013452}
done in step count: 57
reward sum = 0.4645690085980063
running average episode reward sum: 0.7923636824740238
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.74521995, 3.35292954, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6954229280817015}
episode index:4103
target Thresh 16.801932723625672
target distance 12.0
model initialize at round 4103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([12.87375261,  7.07247574,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 10.873994142101488}
done in step count: 6
reward sum = 0.9139165056492708
running average episode reward sum: 0.7923933006083257
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.25498306, 7.80547316, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.0971951762487897}
episode index:4104
target Thresh 16.803031482551237
target distance 7.0
model initialize at round 4104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.14381546,  5.97662597,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.926032586726639}
done in step count: 6
reward sum = 0.9232610483638232
running average episode reward sum: 0.7924251806930408
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.6714297 , 11.88225428,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.1086886238565326}
episode index:4105
target Thresh 16.80412969223466
target distance 6.0
model initialize at round 4105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.97807926, 7.        , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 4.128476958889028}
done in step count: 2
reward sum = 0.9700953745881717
running average episode reward sum: 0.7924684515634488
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.98708084, 11.        ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.012919157743396248}
episode index:4106
target Thresh 16.805227352950496
target distance 10.0
model initialize at round 4106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.86351377, 3.98177823, 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.193049345853021}
done in step count: 33
reward sum = 0.6424038296847376
running average episode reward sum: 0.7924319128193829
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.54253605,  5.90173795,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.011140239082869}
episode index:4107
target Thresh 16.80632446497316
target distance 11.0
model initialize at round 4107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.11178161,  4.70621398,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.13910839334876}
done in step count: 15
reward sum = 0.8242729419522874
running average episode reward sum: 0.7924396638001844
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.06657725, 4.89864673, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9011095788826039}
episode index:4108
target Thresh 16.807421028576925
target distance 12.0
model initialize at round 4108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.34757864, 4.        , 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.69525217514476}
done in step count: 20
reward sum = 0.758130979837332
running average episode reward sum: 0.7924313141569712
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.87086474,  2.22644419,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.1648150053210018}
episode index:4109
target Thresh 16.80851704403594
target distance 8.0
model initialize at round 4109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([3.61572957, 4.69746959, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 7.698745146409253}
done in step count: 4
reward sum = 0.9386100168340179
running average episode reward sum: 0.7924668807512966
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.71177702,  9.9035001 ,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.1501908375330834}
episode index:4110
target Thresh 16.809612511624202
target distance 10.0
model initialize at round 4110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 6., 11.,  0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 18
reward sum = 0.7800010412040191
running average episode reward sum: 0.7924638484381009
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.1456656 ,  4.90025862,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.241109523850459}
episode index:4111
target Thresh 16.810707431615583
target distance 2.0
model initialize at round 4111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.        ,  8.56318951,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.4368104934692116}
done in step count: 11
reward sum = 0.8712297102133847
running average episode reward sum: 0.7924830035601279
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.16766644,  9.38122216,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.0371427963128743}
episode index:4112
target Thresh 16.811801804283807
target distance 2.0
model initialize at round 4112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.        ,  9.29330742,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.2933074235915729}
done in step count: 0
reward sum = 0.9948153997282319
running average episode reward sum: 0.7925321969460186
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.        ,  9.29330742,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.2933074235915729}
episode index:4113
target Thresh 16.812895629902474
target distance 4.0
model initialize at round 4113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.        , 8.54405963, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.167276918793667}
done in step count: 1
reward sum = 0.9797643840548669
running average episode reward sum: 0.7925777079297591
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.       , 10.3707329,  0.       ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.6292670965194578}
episode index:4114
target Thresh 16.813988908745035
target distance 9.0
model initialize at round 4114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([12.51091242,  8.66984391,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.26840787473481}
done in step count: 6
reward sum = 0.9193848412218195
running average episode reward sum: 0.7926085237580197
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.84825039, 4.85386866, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8607456634673984}
episode index:4115
target Thresh 16.815081641084813
target distance 12.0
model initialize at round 4115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.36294472,  9.        ,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.362944722175675}
done in step count: 29
reward sum = 0.6999010320028609
running average episode reward sum: 0.7925860000719762
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.4161733 , 8.67482062, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.6682778205521492}
episode index:4116
target Thresh 16.816173827194987
target distance 12.0
model initialize at round 4116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([5.60124624, 9.        , 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 10.398753762245196}
done in step count: 37
reward sum = 0.6038654322294603
running average episode reward sum: 0.7925401607307466
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.28875485,  8.3239148 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7351670245231532}
episode index:4117
target Thresh 16.81726546734861
target distance 10.0
model initialize at round 4117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([12.        ,  8.99234521,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.995409004259466}
done in step count: 6
reward sum = 0.9188890977369124
running average episode reward sum: 0.7925708428426956
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.52653276, 3.27463071, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.5473511222813578}
episode index:4118
target Thresh 16.818356561818586
target distance 12.0
model initialize at round 4118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.13271924,  3.00080765,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.775496087568111}
done in step count: 28
reward sum = 0.6740145667696398
running average episode reward sum: 0.7925420600614203
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.67260184, 9.26606934, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.42187965735224287}
episode index:4119
target Thresh 16.81944711087769
target distance 2.0
model initialize at round 4119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([2.84399652, 9.28857183, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.1914771245614697}
done in step count: 20
reward sum = 0.7822752234555761
running average episode reward sum: 0.7925395681107879
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.92736811, 9.23777983, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.24862549732263964}
episode index:4120
target Thresh 16.820537114798565
target distance 7.0
model initialize at round 4120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.66569086, 5.99999999, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.009793467810672}
done in step count: 2
reward sum = 0.9631985038004003
running average episode reward sum: 0.7925809801310959
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.10051921, 10.20091981,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8053777112801405}
episode index:4121
target Thresh 16.821626573853706
target distance 7.0
model initialize at round 4121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.000000000000019}
done in step count: 3
reward sum = 0.951192802164578
running average episode reward sum: 0.7926194594668634
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.94509257, 11.35736329,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0104001659229875}
episode index:4122
target Thresh 16.822715488315477
target distance 7.0
model initialize at round 4122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([14.51627994, 10.91246229,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 6.791127758226124}
done in step count: 27
reward sum = 0.7213295674452797
running average episode reward sum: 0.7926021686853884
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.30198321, 9.80286922, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.857783684265165}
episode index:4123
target Thresh 16.82380385845611
target distance 8.0
model initialize at round 4123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.29031849, 5.        , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.4222705316264594}
done in step count: 14
reward sum = 0.8388978912245211
running average episode reward sum: 0.7926133946122892
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.76438149, 11.21319413,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7935557919790387}
episode index:4124
target Thresh 16.824891684547694
target distance 2.0
model initialize at round 4124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.18201661, 9.66839623, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.2276499200505395}
done in step count: 1
reward sum = 0.9851973867353172
running average episode reward sum: 0.7926600816406826
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.18224429, 9.63327861, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8962192745571435}
episode index:4125
target Thresh 16.82597896686219
target distance 1.0
model initialize at round 4125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.22627044, 4.        , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0252796256480579}
done in step count: 0
reward sum = 0.995872195535462
running average episode reward sum: 0.7927093332436624
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.22627044, 4.        , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0252796256480579}
episode index:4126
target Thresh 16.82706570567142
target distance 2.0
model initialize at round 4126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([7., 9., 0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.9999999999999805}
done in step count: 3
reward sum = 0.959801592138502
running average episode reward sum: 0.7927498208275962
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.37668568, 10.21753085,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0003892836510675}
episode index:4127
target Thresh 16.828151901247058
target distance 3.0
model initialize at round 4127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.13841734, 5.02097879, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 2.123178675396393}
done in step count: 4
reward sum = 0.9493301196165809
running average episode reward sum: 0.7927877521015276
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.34703187, 3.06701204, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.13878616114903}
episode index:4128
target Thresh 16.829237553860665
target distance 8.0
model initialize at round 4128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 6.28498888, 11.50771642,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 8.844468758156909}
done in step count: 6
reward sum = 0.9170806131228718
running average episode reward sum: 0.7928178545139813
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.26178526,  9.82798394,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.757991110424794}
episode index:4129
target Thresh 16.830322663783647
target distance 11.0
model initialize at round 4129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([5.99998179, 8.00003594, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.295663507721583}
done in step count: 13
reward sum = 0.8332926675323713
running average episode reward sum: 0.7928276547108382
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.10651538,  3.64347329,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.6522295614611007}
episode index:4130
target Thresh 16.831407231287283
target distance 13.0
model initialize at round 4130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 4.5618012 , 11.25876507,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 14.715879944843184}
done in step count: 9
reward sum = 0.8718072004021736
running average episode reward sum: 0.7928467734582821
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.88279383,  1.72335181,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9251265653861074}
episode index:4131
target Thresh 16.83249125664272
target distance 3.0
model initialize at round 4131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.99999989, 7.25122202, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 2.014503569577575}
done in step count: 1
reward sum = 0.9836028684241377
running average episode reward sum: 0.7928929390185353
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.39896671, 8.07441667, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.007908196767034}
episode index:4132
target Thresh 16.833574740120955
target distance 10.0
model initialize at round 4132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 6.        , 10.79145432,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 8.039054666627553}
done in step count: 55
reward sum = 0.4786832139766798
running average episode reward sum: 0.7928169144056532
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.2545911 ,  9.50689305,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.5549514274752928}
episode index:4133
target Thresh 16.834657681992866
target distance 4.0
model initialize at round 4133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([9.        , 8.89771366, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 2.00261391563787}
done in step count: 1
reward sum = 0.9812667169497425
running average episode reward sum: 0.7928624997473426
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.01767798, 8.13701415, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.8631668939790756}
episode index:4134
target Thresh 16.835740082529185
target distance 8.0
model initialize at round 4134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([8.23149669, 9.96248507, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 7.843081346256089}
done in step count: 13
reward sum = 0.8432022364370291
running average episode reward sum: 0.7928746738070015
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.54855511,  6.50913556,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.7484194878180565}
episode index:4135
target Thresh 16.836821942000512
target distance 3.0
model initialize at round 4135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 5.        , 10.66613868,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.054259636807133}
done in step count: 1
reward sum = 0.9868466481889263
running average episode reward sum: 0.7929215722534187
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.69709742, 10.61928844,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7942833892338825}
episode index:4136
target Thresh 16.837903260677315
target distance 1.0
model initialize at round 4136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.49704552,  9.36686707,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.6177747887659173}
done in step count: 0
reward sum = 0.9982181974805024
running average episode reward sum: 0.7929711967700316
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.49704552,  9.36686707,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.6177747887659173}
episode index:4137
target Thresh 16.83898403882992
target distance 1.0
model initialize at round 4137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 2.2360679774997645}
done in step count: 23
reward sum = 0.7522683981241196
running average episode reward sum: 0.7929613604242979
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.287802  ,  9.09507665,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9495874204334935}
episode index:4138
target Thresh 16.840064276728526
target distance 10.0
model initialize at round 4138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.14400508,  1.91387493,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.144410665774027}
done in step count: 29
reward sum = 0.6918000939951839
running average episode reward sum: 0.7929369194321672
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.69789334, 1.90363271, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7045152684995095}
episode index:4139
target Thresh 16.84114397464319
target distance 8.0
model initialize at round 4139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.        ,  4.93121338,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 6.068786621093731}
done in step count: 3
reward sum = 0.9468594178694193
running average episode reward sum: 0.7929740987796157
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.87628797, 10.8220332 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8941771521391134}
episode index:4140
target Thresh 16.842223132843838
target distance 2.0
model initialize at round 4140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([5.94478322, 8.045681  , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.9453196505797568}
done in step count: 26
reward sum = 0.7253976895371942
running average episode reward sum: 0.7929577799172051
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.00997109, 7.16148488, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8385743984412419}
episode index:4141
target Thresh 16.843301751600254
target distance 3.0
model initialize at round 4141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.75244761,  9.30298495,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.8563505780062437}
done in step count: 3
reward sum = 0.9644634314829592
running average episode reward sum: 0.7929991863999588
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.88130158, 11.1509043 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.19199328522674325}
episode index:4142
target Thresh 16.8443798311821
target distance 8.0
model initialize at round 4142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 9.96199024, 11.58479961,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 7.581421330186907}
done in step count: 3
reward sum = 0.9527688741771506
running average episode reward sum: 0.7930377501672234
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.96199024,  7.22952952,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.23265541985409594}
episode index:4143
target Thresh 16.845457371858892
target distance 6.0
model initialize at round 4143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.        , 8.16820657, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.900923792087322}
done in step count: 8
reward sum = 0.9022825049844415
running average episode reward sum: 0.7930641123184825
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.87944933, 10.36625461,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6451090520558356}
episode index:4144
target Thresh 16.846534373900013
target distance 1.0
model initialize at round 4144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.07421929,  1.96959317,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.3852105148535867}
done in step count: 1
reward sum = 0.9878131170341204
running average episode reward sum: 0.7931110963968215
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.59634203,  2.75822556,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.6434894662200463}
episode index:4145
target Thresh 16.847610837574717
target distance 8.0
model initialize at round 4145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 8.77522755, 10.17354524,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 6.987072455212988}
done in step count: 6
reward sum = 0.9121179244419065
running average episode reward sum: 0.7931398004074451
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.61031177,  7.47117706,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6114447957601088}
episode index:4146
target Thresh 16.848686763152124
target distance 13.0
model initialize at round 4146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.        ,  4.46617794,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.313536721334907}
done in step count: 14
reward sum = 0.8302504982705576
running average episode reward sum: 0.7931487492132959
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.75421656, 9.19494696, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1031559327460432}
episode index:4147
target Thresh 16.849762150901203
target distance 8.0
model initialize at round 4147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4., 8., 0.]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 6.000000000000018}
done in step count: 3
reward sum = 0.9545445596303653
running average episode reward sum: 0.7931876585214968
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.48445922, 2.        , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.48445922136308006}
episode index:4148
target Thresh 16.850837001090813
target distance 5.0
model initialize at round 4148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.        , 3.90277743, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.097222566604621}
done in step count: 4
reward sum = 0.9472573210047714
running average episode reward sum: 0.7932247926893645
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.98273342, 8.01797324, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.024923325781447223}
episode index:4149
target Thresh 16.85191131398966
target distance 3.0
model initialize at round 4149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([13.42753792, 10.96958113,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 3.126050249157916}
done in step count: 8
reward sum = 0.9126588337593292
running average episode reward sum: 0.7932535719763694
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.14119653,  8.87722089,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.86753564937439}
episode index:4150
target Thresh 16.85298508986633
target distance 12.0
model initialize at round 4150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.13560502,  3.98162708,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.310550163792742}
done in step count: 8
reward sum = 0.8841234161667542
running average episode reward sum: 0.7932754630494097
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.29216336, 2.29164797, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7655659675963062}
episode index:4151
target Thresh 16.854058328989254
target distance 7.0
model initialize at round 4151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([14.        ,  3.61219466,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 8.111969988596966}
done in step count: 18
reward sum = 0.8035029632647464
running average episode reward sum: 0.7932779263201745
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.60465985, 9.70633999, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.6721976953904589}
episode index:4152
target Thresh 16.855131031626755
target distance 4.0
model initialize at round 4152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([6.54414308, 9.76705661, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 2.4668797759567123}
done in step count: 13
reward sum = 0.8560763465845951
running average episode reward sum: 0.7932930475386345
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.78518577, 9.14064845, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.8857935672745434}
episode index:4153
target Thresh 16.856203198046998
target distance 3.0
model initialize at round 4153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([6.34721005, 9.        , 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 3.6527899503707983}
done in step count: 53
reward sum = 0.5032779730160146
running average episode reward sum: 0.7932232316805404
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.06584407, 9.13868442, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.9443943384951441}
episode index:4154
target Thresh 16.85727482851803
target distance 5.0
model initialize at round 4154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([10.56390464,  8.55543518,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 4.458315813675267}
done in step count: 4
reward sum = 0.9538708972113835
running average episode reward sum: 0.7932618953786225
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.04951149,  8.75768124,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.24732523407666462}
episode index:4155
target Thresh 16.85834592330776
target distance 7.0
model initialize at round 4155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([3.48455095, 7.26782298, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 6.155076722091631}
done in step count: 4
reward sum = 0.948663628791759
running average episode reward sum: 0.7932992875185197
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.6241293 , 9.21141106, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.0056887666338505}
episode index:4156
target Thresh 16.859416482683958
target distance 4.0
model initialize at round 4156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([5.17664066, 7.14612992, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.377601549658434}
done in step count: 2
reward sum = 0.9665527767655513
running average episode reward sum: 0.7933409650478069
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.95335744, 9.45057391, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.054470132811305}
episode index:4157
target Thresh 16.860486506914267
target distance 7.0
model initialize at round 4157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([13.1084671 , 11.11735392,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 5.529884604496534}
done in step count: 3
reward sum = 0.956197111073199
running average episode reward sum: 0.793380131990093
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.23647439, 8.11858321, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.9125873661206698}
episode index:4158
target Thresh 16.86155599626619
target distance 13.0
model initialize at round 4158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([2.9713062, 4.       , 0.       ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.193829358277206}
done in step count: 18
reward sum = 0.7701741155145826
running average episode reward sum: 0.7933745522794714
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.37951181,  1.10207296,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.0914570858287018}
episode index:4159
target Thresh 16.862624951007103
target distance 9.0
model initialize at round 4159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([9.02619884, 8.13224972, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 7.6927536328546795}
done in step count: 3
reward sum = 0.9466566068092447
running average episode reward sum: 0.7934113989271948
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.86012409, 4.96052469, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8610294675520865}
episode index:4160
target Thresh 16.863693371404242
target distance 9.0
model initialize at round 4160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 12
reward sum = 0.8443544377904074
running average episode reward sum: 0.7934236419069746
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.80970449, 11.87404294,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.19145810992414}
episode index:4161
target Thresh 16.86476125772471
target distance 7.0
model initialize at round 4161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([4., 5., 0.]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 5.385164807134529}
done in step count: 3
reward sum = 0.9536178675213556
running average episode reward sum: 0.7934621316296113
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.46188921, 11.        ,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.1355893734762466}
episode index:4162
target Thresh 16.865828610235486
target distance 2.0
model initialize at round 4162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 6., 10.,  0.]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.9999999999999813}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7935103031089713
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 6., 10.,  0.]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.9999999999999813}
episode index:4163
target Thresh 16.8668954292034
target distance 2.0
model initialize at round 4163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([16.83856065,  3.84012403,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 2.84305942594033}
done in step count: 29
reward sum = 0.7137802095624912
running average episode reward sum: 0.793491155632135
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.88209978,  3.36785027,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.085225001904273}
episode index:4164
target Thresh 16.86796171489516
target distance 10.0
model initialize at round 4164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.91129248, 3.46640108, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.640738759731802}
done in step count: 28
reward sum = 0.6604716405180969
running average episode reward sum: 0.7934592181735243
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.89620215,  8.33109235,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.118309316861855}
episode index:4165
target Thresh 16.86902746757734
target distance 3.0
model initialize at round 4165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.        , 10.72777665,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0363906378044514}
done in step count: 1
reward sum = 0.9827682165753802
running average episode reward sum: 0.7935046596037696
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.62794399, 10.54391898,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7760950667964155}
episode index:4166
target Thresh 16.870092687516376
target distance 9.0
model initialize at round 4166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([1.7328946, 9.       , 0.       ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 7.357973015529409}
done in step count: 10
reward sum = 0.8840179824592624
running average episode reward sum: 0.7935263810635381
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.38632023, 2.47403467, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.775442919584555}
episode index:4167
target Thresh 16.87115737497857
target distance 10.0
model initialize at round 4167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([5.14726615, 8.71659172, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.26017101234318}
done in step count: 17
reward sum = 0.8046853439082843
running average episode reward sum: 0.7935290583578866
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.11370043,  6.67852737,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6879877785702819}
episode index:4168
target Thresh 16.872221530230096
target distance 3.0
model initialize at round 4168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4., 7., 0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.0000000000000195}
done in step count: 16
reward sum = 0.8292919852134849
running average episode reward sum: 0.7935376366564847
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.24363881, 8.0164414 , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.2441929385426264}
episode index:4169
target Thresh 16.873285153536994
target distance 11.0
model initialize at round 4169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.236239592930875}
done in step count: 15
reward sum = 0.8271455457751543
running average episode reward sum: 0.7935456961071127
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.25284674, 7.96086143, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.2171658407671169}
episode index:4170
target Thresh 16.874348245165173
target distance 3.0
model initialize at round 4170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.86243911,  5.01920379,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.335132046222764}
done in step count: 1
reward sum = 0.9809808325292056
running average episode reward sum: 0.7935906338046486
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.8841338 ,  3.71650145,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9284740154735941}
episode index:4171
target Thresh 16.8754108053804
target distance 8.0
model initialize at round 4171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([14.9511747 , 11.86780418,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 8.16761112524116}
done in step count: 7
reward sum = 0.9177234951064557
running average episode reward sum: 0.7936203876064948
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.13042535, 9.53499493, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.4829497752351361}
episode index:4172
target Thresh 16.876472834448318
target distance 5.0
model initialize at round 4172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([12.55239134,  7.50138834,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.330702608566444}
done in step count: 4
reward sum = 0.950660488681914
running average episode reward sum: 0.79365802002947
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.38809506, 11.03047896,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.3892900451301912}
episode index:4173
target Thresh 16.877534332634433
target distance 1.0
model initialize at round 4173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.42229807, 10.39229709,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.9903323830361088}
done in step count: 7
reward sum = 0.9278561953918742
running average episode reward sum: 0.7936901710058385
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.77410728,  9.85905534,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8882587459786719}
episode index:4174
target Thresh 16.87859530020412
target distance 5.0
model initialize at round 4174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([13.13071843,  5.92561748,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 3.740559969802055}
done in step count: 2
reward sum = 0.9672917281923034
running average episode reward sum: 0.7937317522171407
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.898941  ,  8.20463167,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8017628699541873}
episode index:4175
target Thresh 16.87965573742262
target distance 11.0
model initialize at round 4175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([12.00000001, 11.63847453,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.147928675655798}
done in step count: 13
reward sum = 0.8430699040142825
running average episode reward sum: 0.7937435669086629
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.58176652, 10.43559429,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7267700288022404}
episode index:4176
target Thresh 16.880715644555043
target distance 10.0
model initialize at round 4176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([6., 9., 0.]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 8.062257748298565}
done in step count: 4
reward sum = 0.9353530142018663
running average episode reward sum: 0.7937774690985824
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.99999964,  8.37010236,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.37010236419091025}
episode index:4177
target Thresh 16.881775021866368
target distance 6.0
model initialize at round 4177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.86937666, 4.92341989, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 4.230462682902332}
done in step count: 3
reward sum = 0.9521956247640034
running average episode reward sum: 0.7938153863210968
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.57798861, 9.8004361 , 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9873037993561526}
episode index:4178
target Thresh 16.882833869621436
target distance 5.0
model initialize at round 4178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.        ,  9.18488407,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 4.30270318412037}
done in step count: 9
reward sum = 0.8948594136119272
running average episode reward sum: 0.7938395653178163
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.14105136,  5.23582701,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.890734050861109}
episode index:4179
target Thresh 16.88389218808496
target distance 10.0
model initialize at round 4179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 6.29406953, 10.25036359,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 9.709158987389603}
done in step count: 7
reward sum = 0.9081770084378531
running average episode reward sum: 0.7938669187731081
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.72939541, 10.831708  ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.1062349898972788}
episode index:4180
target Thresh 16.884949977521522
target distance 10.0
model initialize at round 4180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12.00494245, 11.86740305,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.05180052677325}
done in step count: 5
reward sum = 0.927780503835515
running average episode reward sum: 0.7938989478534866
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.17148405, 11.87570589,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8923382720950561}
episode index:4181
target Thresh 16.886007238195567
target distance 12.0
model initialize at round 4181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.181338838126695}
done in step count: 6
reward sum = 0.9075269781876045
running average episode reward sum: 0.7939261185924473
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.97895248, 4.99685427, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.397163698320168}
episode index:4182
target Thresh 16.88706397037141
target distance 7.0
model initialize at round 4182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.45904273,  7.0675019 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.088250756275313}
done in step count: 46
reward sum = 0.5259265429256086
running average episode reward sum: 0.7938620498437821
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.29397359,  1.24554009,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8097099674131577}
episode index:4183
target Thresh 16.888120174313233
target distance 1.0
model initialize at round 4183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([8.81504185, 8.12494255, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 2.2180996941982882}
done in step count: 1
reward sum = 0.9844264224286807
running average episode reward sum: 0.7939075958219334
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.09600034,  9.48953121,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.5194174130947783}
episode index:4184
target Thresh 16.889175850285092
target distance 5.0
model initialize at round 4184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 7.77880883, 11.48548156,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.522943154221619}
done in step count: 11
reward sum = 0.8830592833052928
running average episode reward sum: 0.7939288984951671
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.75026936, 9.92466836, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.1907626480745594}
episode index:4185
target Thresh 16.890230998550898
target distance 5.0
model initialize at round 4185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.34967971,  8.        ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.2896254065599853}
done in step count: 10
reward sum = 0.8821525474180228
running average episode reward sum: 0.7939499743788085
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.628568  , 11.19118154,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.4177464716948932}
episode index:4186
target Thresh 16.891285619374447
target distance 13.0
model initialize at round 4186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.08364413,  7.00000185,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.482472657426705}
done in step count: 16
reward sum = 0.79788296053229
running average episode reward sum: 0.7939509137115416
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.55475802, 3.1030346 , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.0546579473710633}
episode index:4187
target Thresh 16.89233971301939
target distance 1.0
model initialize at round 4187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([2.        , 9.30136251, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.971134015524969}
done in step count: 27
reward sum = 0.7242663792969086
running average episode reward sum: 0.793934274615454
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.36685265, 10.22222696,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0028990277344345}
episode index:4188
target Thresh 16.893393279749247
target distance 13.0
model initialize at round 4188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.11310111,  4.71605849,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.58815291929217}
done in step count: 9
reward sum = 0.8753079673331003
running average episode reward sum: 0.7939537001806766
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.11005416, 7.67063548, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9489386565928725}
episode index:4189
target Thresh 16.894446319827416
target distance 2.0
model initialize at round 4189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 5.61384177, 11.10297638,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 2.388379204372128}
done in step count: 4
reward sum = 0.9548411642189047
running average episode reward sum: 0.7939920981434543
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.14938373, 11.8341957 ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8474656114461344}
episode index:4190
target Thresh 16.895498833517152
target distance 4.0
model initialize at round 4190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([10.05466461, 10.97236419,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 3.101691903061116}
done in step count: 13
reward sum = 0.8469936837911174
running average episode reward sum: 0.7940047446683045
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.69205755,  9.01196685,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.034909685568059}
episode index:4191
target Thresh 16.896550821081586
target distance 5.0
model initialize at round 4191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([14.        ,  5.87792182,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 4.2416421966749684}
done in step count: 23
reward sum = 0.7563405835752512
running average episode reward sum: 0.7939957598970514
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.59780096, 10.92464677,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.1010620463819147}
episode index:4192
target Thresh 16.897602282783712
target distance 13.0
model initialize at round 4192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.8674032 , 5.99505776, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.831072388060079}
done in step count: 6
reward sum = 0.9117518623820188
running average episode reward sum: 0.7940238438709329
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.10402697,  9.25929593,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.7479733450774181}
episode index:4193
target Thresh 16.898653218886402
target distance 12.0
model initialize at round 4193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.00494245, 11.86740305,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.04247287949053}
done in step count: 5
reward sum = 0.9183226739067573
running average episode reward sum: 0.7940534811694632
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.19047538, 11.86093837,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8817571970598616}
episode index:4194
target Thresh 16.899703629652386
target distance 8.0
model initialize at round 4194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 6.61862862, 10.51977396,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 7.396976441939514}
done in step count: 5
reward sum = 0.9370437086473228
running average episode reward sum: 0.7940875670401373
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.18330864, 10.88523888,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8247150411005287}
episode index:4195
target Thresh 16.900753515344263
target distance 11.0
model initialize at round 4195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 5.        , 10.95548934,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.83086445327348}
done in step count: 4
reward sum = 0.940307515847924
running average episode reward sum: 0.7941224145017216
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.0395305 ,  7.28958791,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.0031763588615659}
episode index:4196
target Thresh 16.901802876224508
target distance 2.0
model initialize at round 4196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.9999999999999813}
done in step count: 3
reward sum = 0.9615049905495645
running average episode reward sum: 0.7941622959827909
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.66937464, 10.06237584,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.672274617175215}
episode index:4197
target Thresh 16.902851712555464
target distance 2.0
model initialize at round 4197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.        , 10.13645911,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.13645911216741702}
done in step count: 0
reward sum = 0.9960313452285813
running average episode reward sum: 0.7942103829406865
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.        , 10.13645911,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.13645911216741702}
episode index:4198
target Thresh 16.903900024599334
target distance 12.0
model initialize at round 4198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.5709511,  4.       ,  0.       ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.953531254610867}
done in step count: 15
reward sum = 0.8163793023094148
running average episode reward sum: 0.7942156625118627
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.33074444, 7.64418933, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.7241351929304616}
episode index:4199
target Thresh 16.9049478126182
target distance 2.0
model initialize at round 4199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 2.701289205785704e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7942632302113135
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 2.701289205785704e-14}
episode index:4200
target Thresh 16.90599507687401
target distance 9.0
model initialize at round 4200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 10.747137093366366}
done in step count: 14
reward sum = 0.8372269178116954
running average episode reward sum: 0.7942734572257387
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.07592751, 10.24113799,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.1957347197378851}
episode index:4201
target Thresh 16.907041817628574
target distance 7.0
model initialize at round 4201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4.51199591, 7.761958  , 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.2511456353577985}
done in step count: 15
reward sum = 0.8228021946787809
running average episode reward sum: 0.794280246549264
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.45649902, 10.96820418,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5444302417641734}
episode index:4202
target Thresh 16.908088035143585
target distance 5.0
model initialize at round 4202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([11.02375422, 11.86071844,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.143871155386048}
done in step count: 2
reward sum = 0.9706101353636365
running average episode reward sum: 0.794322199889453
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.42423638, 11.41383075,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7090554559998266}
episode index:4203
target Thresh 16.909133729680597
target distance 11.0
model initialize at round 4203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.92800879, 9.        , 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.268641917894378}
done in step count: 12
reward sum = 0.8599142231664274
running average episode reward sum: 0.7943378021785293
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.76812468, 10.34803204,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0075106705052654}
episode index:4204
target Thresh 16.910178901501027
target distance 1.0
model initialize at round 4204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.3625716 ,  3.00886881,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.1784124904498683}
done in step count: 0
reward sum = 0.9950156690668637
running average episode reward sum: 0.7943855258091805
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.3625716 ,  3.00886881,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.1784124904498683}
episode index:4205
target Thresh 16.911223550866172
target distance 2.0
model initialize at round 4205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 2.80753231, 10.19859052,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.4367450539002764}
done in step count: 3
reward sum = 0.9677631175569568
running average episode reward sum: 0.7944267473003236
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.3566035 , 10.81679207,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6689724937224095}
episode index:4206
target Thresh 16.912267678037193
target distance 5.0
model initialize at round 4206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([4.      , 8.393538, 0.      ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 3.060685570199596}
done in step count: 2
reward sum = 0.9696421948641192
running average episode reward sum: 0.7944683958497802
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([8.        , 8.82733054, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.0147978817632421}
episode index:4207
target Thresh 16.91331128327512
target distance 3.0
model initialize at round 4207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([16.8955675 ,  8.64526869,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 4.90840253603521}
done in step count: 7
reward sum = 0.909516585329443
running average episode reward sum: 0.7944957361989912
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.71050809,  8.2763182 ,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.779436275896625}
episode index:4208
target Thresh 16.91435436684086
target distance 1.0
model initialize at round 4208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.25831691, 2.4273939 , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 2.3466039161958734}
done in step count: 10
reward sum = 0.8801396103453881
running average episode reward sum: 0.794516083995177
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.83430381, 3.12297119, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8925439802926541}
episode index:4209
target Thresh 16.915396928995182
target distance 7.0
model initialize at round 4209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.38213713, 7.0247779 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.062622584606972}
done in step count: 3
reward sum = 0.9594279200733568
running average episode reward sum: 0.7945552554526778
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.46237101, 2.07195347, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5424225558767417}
episode index:4210
target Thresh 16.916438969998726
target distance 6.0
model initialize at round 4210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([7., 9., 0.]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 4.472135954999605}
done in step count: 4
reward sum = 0.9401052109508636
running average episode reward sum: 0.7945898196786333
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.72086064, 6.36160142, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9629085180929996}
episode index:4211
target Thresh 16.917480490112
target distance 10.0
model initialize at round 4211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([8.        , 9.55099493, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 8.752689016052006}
done in step count: 21
reward sum = 0.7531251324872691
running average episode reward sum: 0.7945799752609715
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.84502118,  5.35193659,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0649164177231476}
episode index:4212
target Thresh 16.918521489595385
target distance 6.0
model initialize at round 4212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.        , 5.67416751, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.660667509383627}
done in step count: 9
reward sum = 0.8906340654039331
running average episode reward sum: 0.7946027747127026
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.39688148, 11.83663151,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.031360381946909}
episode index:4213
target Thresh 16.919561968709132
target distance 1.0
model initialize at round 4213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.88153505,  9.81563431,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8241924967246981}
done in step count: 0
reward sum = 0.9989182274274607
running average episode reward sum: 0.7946512596326634
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.88153505,  9.81563431,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8241924967246981}
episode index:4214
target Thresh 16.92060192771336
target distance 13.0
model initialize at round 4214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.50416171, 11.91208428,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.661981094982524}
done in step count: 24
reward sum = 0.7452937680154665
running average episode reward sum: 0.7946395496702393
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.88035776, 10.32611719,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9388195854524117}
episode index:4215
target Thresh 16.92164136686806
target distance 2.0
model initialize at round 4215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.6407442894293542e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7946868363046167
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.6407442894293542e-14}
episode index:4216
target Thresh 16.922680286433092
target distance 8.0
model initialize at round 4216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([10.        , 11.35959208,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 7.416605901961669}
done in step count: 14
reward sum = 0.8196594047134816
running average episode reward sum: 0.7946927581847232
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.19228222,  6.67092397,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8721806213035022}
episode index:4217
target Thresh 16.92371868666818
target distance 1.0
model initialize at round 4217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.63912404,  6.02529389,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.7038811413554744}
done in step count: 1
reward sum = 0.9894735552257571
running average episode reward sum: 0.7947389366572317
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.35770881,  5.77940929,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0099588164036997}
episode index:4218
target Thresh 16.924756567832937
target distance 3.0
model initialize at round 4218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.25877869,  7.56547236,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.5867167783972234}
done in step count: 1
reward sum = 0.9861750437981123
running average episode reward sum: 0.7947843114159757
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.46856219,  6.64469594,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.835499253381356}
episode index:4219
target Thresh 16.92579393018682
target distance 12.0
model initialize at round 4219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.135077177234788}
done in step count: 5
reward sum = 0.9219872213984032
running average episode reward sum: 0.7948144542856398
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.83981916, 4.94021976, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.17097244967880923}
episode index:4220
target Thresh 16.926830773989174
target distance 3.0
model initialize at round 4220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.53618836, 4.        , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.102325375586635}
done in step count: 3
reward sum = 0.9649701462860515
running average episode reward sum: 0.7948547659871323
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.85113192, 2.75160921, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.28958538095845904}
episode index:4221
target Thresh 16.92786709949921
target distance 7.0
model initialize at round 4221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([7.02619869, 8.13224963, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 6.506778020272609}
done in step count: 34
reward sum = 0.6543255396553055
running average episode reward sum: 0.7948214809974754
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.71220517, 4.15821808, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.7295677895449104}
episode index:4222
target Thresh 16.928902906976013
target distance 13.0
model initialize at round 4222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.93413615, 7.42239153, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 11.327900222010697}
done in step count: 6
reward sum = 0.917091844552685
running average episode reward sum: 0.7948504344342633
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.02323639,  4.76982207,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0035183277651012}
episode index:4223
target Thresh 16.92993819667853
target distance 4.0
model initialize at round 4223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([3.25004196, 7.27453399, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 3.2464599449920946}
done in step count: 7
reward sum = 0.9203472608377785
running average episode reward sum: 0.7948801448571808
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.304092  , 8.09303354, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.1431868158108531}
episode index:4224
target Thresh 16.930972968865586
target distance 14.0
model initialize at round 4224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.        ,  9.54204047,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.830827393673754}
done in step count: 7
reward sum = 0.8982027004358075
running average episode reward sum: 0.7949045998999213
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.01011233, 4.5500199 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.4500937088372577}
episode index:4225
target Thresh 16.932007223795875
target distance 9.0
model initialize at round 4225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.8710278 , 2.88144816, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.119576204749096}
done in step count: 6
reward sum = 0.9179260135836952
running average episode reward sum: 0.7949337105042006
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.28051315, 11.48002696,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.5559797733580517}
episode index:4226
target Thresh 16.933040961727958
target distance 14.0
model initialize at round 4226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.95911348,  6.        ,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.301827774888668}
done in step count: 71
reward sum = 0.3734974033260123
running average episode reward sum: 0.794834009461575
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.74337264, 9.42665154, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.4978846619134783}
episode index:4227
target Thresh 16.93407418292027
target distance 14.0
model initialize at round 4227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 3.99505755, 11.86740305,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.612511630469585}
done in step count: 7
reward sum = 0.8974110983001631
running average episode reward sum: 0.7948582708354726
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.86605859,  7.32660644,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0970489369548995}
episode index:4228
target Thresh 16.935106887631118
target distance 13.0
model initialize at round 4228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.9061428, 3.5954065, 0.       ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.936251989134448}
done in step count: 8
reward sum = 0.8840770044392979
running average episode reward sum: 0.7948793677221133
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.79725798,  7.75122268,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8351709095471301}
episode index:4229
target Thresh 16.936139076118675
target distance 6.0
model initialize at round 4229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 8.08383453, 10.75773972,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.4460491891659215}
done in step count: 70
reward sum = 0.3653083184076859
running average episode reward sum: 0.7947778142825591
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.8245476 , 9.65446096, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.677571170912183}
episode index:4230
target Thresh 16.937170748640995
target distance 2.0
model initialize at round 4230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.24652801, 11.87994389,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.1584564234885815}
done in step count: 0
reward sum = 0.9958546763528167
running average episode reward sum: 0.7948253389486121
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.24652801, 11.87994389,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.1584564234885815}
episode index:4231
target Thresh 16.938201905455987
target distance 12.0
model initialize at round 4231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.440306508910563}
done in step count: 27
reward sum = 0.7175927594591053
running average episode reward sum: 0.7948070892842715
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.52009928, 3.46956291, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7153098665062979}
episode index:4232
target Thresh 16.939232546821444
target distance 3.0
model initialize at round 4232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 9.75555539, 10.13870025,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 2.2487261638393417}
done in step count: 11
reward sum = 0.8832192628507602
running average episode reward sum: 0.79482797569428
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.12905885, 10.17314606,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.8879853835221541}
episode index:4233
target Thresh 16.94026267299503
target distance 4.0
model initialize at round 4233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([14.17316163,  9.        ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.7087521933951204}
done in step count: 1
reward sum = 0.9831342330935275
running average episode reward sum: 0.7948724504834627
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.25765789,  7.        ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7423421144485651}
episode index:4234
target Thresh 16.941292284234272
target distance 12.0
model initialize at round 4234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([2.82500601, 2.68254673, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 12.375613113919036}
done in step count: 10
reward sum = 0.8751236493009231
running average episode reward sum: 0.794891399999122
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.12010344,  8.80230168,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1907585619570076}
episode index:4235
target Thresh 16.942321380796574
target distance 12.0
model initialize at round 4235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.        , 10.33275485,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.540742615811372}
done in step count: 6
reward sum = 0.9183393662654845
running average episode reward sum: 0.7949205425785051
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.91256112, 7.14653337, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.17063876027677066}
episode index:4236
target Thresh 16.943349962939212
target distance 13.0
model initialize at round 4236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([3.90088987, 3.77893651, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.16056996653974}
done in step count: 8
reward sum = 0.8915664741047071
running average episode reward sum: 0.7949433525694247
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.54637117,  5.73766141,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.917968306537082}
episode index:4237
target Thresh 16.94437803091933
target distance 12.0
model initialize at round 4237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.565977982391892}
done in step count: 16
reward sum = 0.7985292945629102
running average episode reward sum: 0.7949441987095837
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.98124387,  1.70248783,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.0253550722755969}
episode index:4238
target Thresh 16.945405584993946
target distance 5.0
model initialize at round 4238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.96539512, 6.64751529, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 4.458263212094718}
done in step count: 12
reward sum = 0.8498731328604351
running average episode reward sum: 0.7949571567030139
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.61379713, 11.89620413,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.0862452561554976}
episode index:4239
target Thresh 16.946432625419945
target distance 2.0
model initialize at round 4239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 3.87035191, 10.        ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.8703519105911393}
done in step count: 30
reward sum = 0.6689636758693085
running average episode reward sum: 0.794927441259421
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.15471614, 10.46569897,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9650804811850376}
episode index:4240
target Thresh 16.94745915245409
target distance 5.0
model initialize at round 4240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.26359832, 8.2160269 , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 4.279856337559252}
done in step count: 17
reward sum = 0.8193557927429155
running average episode reward sum: 0.7949332013045717
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.20983553, 4.30604392, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.37107119547711737}
episode index:4241
target Thresh 16.948485166353013
target distance 5.0
model initialize at round 4241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([5.        , 8.77918673, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.008115440295485}
done in step count: 5
reward sum = 0.9372230796188679
running average episode reward sum: 0.7949667444159141
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.81273401, 9.47771263, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.94273322076005}
episode index:4242
target Thresh 16.949510667373218
target distance 10.0
model initialize at round 4242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([15.20357466, 10.80054796,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 9.238325795684185}
done in step count: 18
reward sum = 0.7851724700612426
running average episode reward sum: 0.7949644360788047
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.20855174, 10.99105394,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.0127594650703986}
episode index:4243
target Thresh 16.950535655771077
target distance 9.0
model initialize at round 4243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.701003238415892}
done in step count: 7
reward sum = 0.8968701778996155
running average episode reward sum: 0.7949884477993092
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.59790397, 11.90123438,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0815325121009924}
episode index:4244
target Thresh 16.95156013180284
target distance 2.0
model initialize at round 4244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.02290273,  5.        ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.000262233071817}
done in step count: 0
reward sum = 0.9940683151787734
running average episode reward sum: 0.7950353452945694
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.02290273,  5.        ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.000262233071817}
episode index:4245
target Thresh 16.952584095724625
target distance 7.0
model initialize at round 4245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([7.        , 9.68379116, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 5.275903019622288}
done in step count: 3
reward sum = 0.9568540687095307
running average episode reward sum: 0.7950734561573615
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.39394797, 7.70880559, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6723787956914334}
episode index:4246
target Thresh 16.953607547792423
target distance 9.0
model initialize at round 4246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.14013811,  3.97252477,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.697134236939087}
done in step count: 4
reward sum = 0.9443695803451831
running average episode reward sum: 0.7951086094712743
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.82564182, 10.22821564,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.791234396358179}
episode index:4247
target Thresh 16.954630488262097
target distance 3.0
model initialize at round 4247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([3.76270437, 9.        , 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.0009151479592306}
done in step count: 3
reward sum = 0.961220856145831
running average episode reward sum: 0.7951477131074971
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.24269211, 11.58418345,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9564442160203535}
episode index:4248
target Thresh 16.955652917389386
target distance 1.0
model initialize at round 4248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.        ,  2.28717721,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.0404185442026859}
done in step count: 0
reward sum = 0.9969381466829047
running average episode reward sum: 0.7951952043839329
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.        ,  2.28717721,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.0404185442026859}
episode index:4249
target Thresh 16.95667483542989
target distance 10.0
model initialize at round 4249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12.57594824,  8.54722738,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.919808389176424}
done in step count: 5
reward sum = 0.9294754192124146
running average episode reward sum: 0.7952267997285983
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.41749599, 11.86975685,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9647693416010753}
episode index:4250
target Thresh 16.957696242639095
target distance 7.0
model initialize at round 4250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4., 5., 0.]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 5.099019513592807}
done in step count: 4
reward sum = 0.9393331759439187
running average episode reward sum: 0.7952606991349064
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.1671753 , 10.73426682,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.753057333778621}
episode index:4251
target Thresh 16.95871713927235
target distance 7.0
model initialize at round 4251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([13.38989104,  4.64496398,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 6.3378049860484}
done in step count: 6
reward sum = 0.9213380615299086
running average episode reward sum: 0.7952903504430895
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.88596045,  9.35295654,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.0970830255765263}
episode index:4252
target Thresh 16.959737525584877
target distance 12.0
model initialize at round 4252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.11701077,  4.26286863,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.366989987981325}
done in step count: 11
reward sum = 0.8509369458950666
running average episode reward sum: 0.7953034345238448
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.52047989, 1.30198063, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8707068170868282}
episode index:4253
target Thresh 16.960757401831774
target distance 9.0
model initialize at round 4253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([10.14149928, 10.37103426,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.217496539602362}
done in step count: 11
reward sum = 0.8665483339539491
running average episode reward sum: 0.7953201822670113
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.79760074,  1.73407231,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.3341900619387914}
episode index:4254
target Thresh 16.96177676826801
target distance 1.0
model initialize at round 4254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.       ,  9.6064868,  0.       ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.0746407030600411}
done in step count: 0
reward sum = 0.9968838604866413
running average episode reward sum: 0.7953675532842193
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.       ,  9.6064868,  0.       ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.0746407030600411}
episode index:4255
target Thresh 16.96279562514843
target distance 7.0
model initialize at round 4255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([4.86348191, 2.05126735, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 8.641105461200329}
done in step count: 15
reward sum = 0.8289294549702677
running average episode reward sum: 0.7953754390693899
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.46597481,  9.75607805,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.8881365553438827}
episode index:4256
target Thresh 16.963813972727745
target distance 3.0
model initialize at round 4256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([12.98874485,  9.        ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.4221944200552852}
done in step count: 7
reward sum = 0.9226856900833921
running average episode reward sum: 0.7954053451654701
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.38114799,  7.34849095,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7548097966605894}
episode index:4257
target Thresh 16.964831811260545
target distance 14.0
model initialize at round 4257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.165525060596456}
done in step count: 9
reward sum = 0.8767710508298491
running average episode reward sum: 0.7954244540676929
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.14681228, 2.64392138, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.660445746576737}
episode index:4258
target Thresh 16.965849141001286
target distance 12.0
model initialize at round 4258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([5.        , 7.92645824, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.1838716286465}
done in step count: 7
reward sum = 0.9057544121624503
running average episode reward sum: 0.7954503591999058
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.87851465,  5.06744334,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9404363972724699}
episode index:4259
target Thresh 16.966865962204302
target distance 4.0
model initialize at round 4259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([10.99895485,  8.13348947,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 2.736428927653304}
done in step count: 4
reward sum = 0.9519293430698792
running average episode reward sum: 0.7954870913557438
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.27728394,  9.54512731,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.8539482797886788}
episode index:4260
target Thresh 16.967882275123795
target distance 13.0
model initialize at round 4260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([2.69354677, 4.02302432, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 13.68111944706952}
done in step count: 16
reward sum = 0.8107063785684949
running average episode reward sum: 0.7954906631199335
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.66409832, 10.31687966,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7358255883729791}
episode index:4261
target Thresh 16.968898080013847
target distance 5.0
model initialize at round 4261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([12.        , 10.28961241,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 3.0139468060081582}
done in step count: 12
reward sum = 0.863249157670094
running average episode reward sum: 0.795506561405844
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.35201973, 10.69400346,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9494836635973192}
episode index:4262
target Thresh 16.96991337712841
target distance 3.0
model initialize at round 4262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 1.0000000000000195}
done in step count: 4
reward sum = 0.9477688742147514
running average episode reward sum: 0.795542278579855
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.61507511,  9.35451094,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.7099263298352674}
episode index:4263
target Thresh 16.970928166721304
target distance 1.0
model initialize at round 4263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.8562516 , 5.93511882, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.15771230204490097}
done in step count: 0
reward sum = 0.9994469677003384
running average episode reward sum: 0.7955900986288983
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.8562516 , 5.93511882, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.15771230204490097}
episode index:4264
target Thresh 16.971942449046235
target distance 13.0
model initialize at round 4264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 2.55920161, 11.85615461,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 13.750200413784066}
done in step count: 32
reward sum = 0.6701575011236288
running average episode reward sum: 0.7955606888756731
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.11393936,  6.85973666,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.234605435893138}
episode index:4265
target Thresh 16.97295622435676
target distance 12.0
model initialize at round 4265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.536241300558661}
done in step count: 8
reward sum = 0.8795174930731489
running average episode reward sum: 0.7955803693267274
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.1317773, 6.9501321, 0.       ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8696536481240406}
episode index:4266
target Thresh 16.97396949290634
target distance 7.0
model initialize at round 4266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([12.,  9.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 5.385164807134529}
done in step count: 3
reward sum = 0.9538847774252159
running average episode reward sum: 0.7956174690239616
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.66444153, 11.73791441,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8106276364447143}
episode index:4267
target Thresh 16.974982254948277
target distance 1.0
model initialize at round 4267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 3.66793454, 11.        ,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.665652544168044}
done in step count: 1
reward sum = 0.9834472962123049
running average episode reward sum: 0.7956614778869393
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.02129078, 9.92255318, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.08032002831234528}
episode index:4268
target Thresh 16.975994510735763
target distance 12.0
model initialize at round 4268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 6., 11.,  0.]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.206555615733707}
done in step count: 11
reward sum = 0.8667582690783223
running average episode reward sum: 0.7956781320896077
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.33490932,  3.34817711,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9312457796015621}
episode index:4269
target Thresh 16.977006260521872
target distance 2.0
model initialize at round 4269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2., 8., 0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9999999999999809}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7957245774919768
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2., 8., 0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9999999999999809}
episode index:4270
target Thresh 16.978017504559535
target distance 9.0
model initialize at round 4270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([3., 9., 0.]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 7.071067811865498}
done in step count: 4
reward sum = 0.9387893913214914
running average episode reward sum: 0.795758074287535
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.54753488, 2.53166187, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7631898733126649}
episode index:4271
target Thresh 16.97902824310156
target distance 13.0
model initialize at round 4271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 3.79019293, 11.85032349,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 14.000225738210252}
done in step count: 23
reward sum = 0.7532925192420713
running average episode reward sum: 0.7957481338486199
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.24331375,  4.49103164,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.5641368411155251}
episode index:4272
target Thresh 16.98003847640064
target distance 5.0
model initialize at round 4272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.        , 9.41762543, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 3.4176254272460564}
done in step count: 2
reward sum = 0.9678171031734232
running average episode reward sum: 0.795788402739171
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.14856765, 5.98184633, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8516258558280888}
episode index:4273
target Thresh 16.981048204709328
target distance 3.0
model initialize at round 4273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([5.99992215, 8.00016787, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 2.235883014884231}
done in step count: 20
reward sum = 0.7876628955973262
running average episode reward sum: 0.7957865015910329
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.5711242 , 10.10341789,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.44116857232316625}
episode index:4274
target Thresh 16.982057428280054
target distance 11.0
model initialize at round 4274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.13264402,  4.99534384,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.132645206703952}
done in step count: 20
reward sum = 0.7600448443318739
running average episode reward sum: 0.7957781409694519
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.04493979, 4.96160836, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9558315296695405}
episode index:4275
target Thresh 16.983066147365125
target distance 9.0
model initialize at round 4275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([8.99505755, 8.13259695, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 8.684052640019278}
done in step count: 13
reward sum = 0.8441742608435752
running average episode reward sum: 0.7957894590517423
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.85855534,  3.78788216,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.1652791784642995}
episode index:4276
target Thresh 16.984074362216727
target distance 11.0
model initialize at round 4276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([6.        , 8.99409652, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 9.218265613517355}
done in step count: 21
reward sum = 0.7467284494958902
running average episode reward sum: 0.7957779881586967
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.9969627 ,  6.76125331,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.2387660133137913}
episode index:4277
target Thresh 16.985082073086907
target distance 1.0
model initialize at round 4277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.25192939,  3.3193057 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8133669310724353}
done in step count: 0
reward sum = 0.9976316871767873
running average episode reward sum: 0.7958251722865645
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.25192939,  3.3193057 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8133669310724353}
episode index:4278
target Thresh 16.986089280227596
target distance 4.0
model initialize at round 4278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([13.68613029, 10.        ,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 2.686130285263065}
done in step count: 3
reward sum = 0.9620548682433874
running average episode reward sum: 0.7958640200771596
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.90715313,  9.77772661,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.24088587180336915}
episode index:4279
target Thresh 16.987095983890594
target distance 5.0
model initialize at round 4279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.00646579,  5.92427099,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.1950777352046735}
done in step count: 2
reward sum = 0.9722266898266214
running average episode reward sum: 0.7959052263084094
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.23836204,  9.92225039,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.25072188957597186}
episode index:4280
target Thresh 16.988102184327577
target distance 13.0
model initialize at round 4280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([5.        , 8.84289306, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.954736025316796}
done in step count: 12
reward sum = 0.854735859547475
running average episode reward sum: 0.7959189685726559
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.87427338,  2.68013011,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1076691374218368}
episode index:4281
target Thresh 16.989107881790094
target distance 7.0
model initialize at round 4281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.89828735, 5.64706001, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 5.465138365765264}
done in step count: 3
reward sum = 0.9511541382176819
running average episode reward sum: 0.7959552215314707
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.57125961, 11.46470672,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.7364033392584972}
episode index:4282
target Thresh 16.990113076529575
target distance 13.0
model initialize at round 4282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([15.46872067,  3.99143994,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 12.629461935472165}
done in step count: 8
reward sum = 0.884503507018943
running average episode reward sum: 0.795975895891846
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.05258016, 5.96422426, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9480950682417143}
episode index:4283
target Thresh 16.99111776879731
target distance 8.0
model initialize at round 4283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([10.        , 11.43019199,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 6.168099313312185}
done in step count: 3
reward sum = 0.9525245829575967
running average episode reward sum: 0.7960124385358857
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.        , 10.01049781,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.010497808456475965}
episode index:4284
target Thresh 16.99212195884448
target distance 5.0
model initialize at round 4284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.87264997,  7.46797252,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 3.5760804444022787}
done in step count: 29
reward sum = 0.70201024786641
running average episode reward sum: 0.795990501035146
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.14163861,  4.71846709,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.1193655473596955}
episode index:4285
target Thresh 16.993125646922127
target distance 12.0
model initialize at round 4285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.00000000000002}
done in step count: 12
reward sum = 0.8456950502658028
running average episode reward sum: 0.7960020979901695
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.07402498, 7.81175178, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9449164671864339}
episode index:4286
target Thresh 16.994128833281174
target distance 3.0
model initialize at round 4286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0000000000000195}
done in step count: 6
reward sum = 0.9261917791908181
running average episode reward sum: 0.7960324664719052
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.19939316,  2.8672366 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8115401645850226}
episode index:4287
target Thresh 16.995131518172425
target distance 1.0
model initialize at round 4287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.18931991, 4.54374132, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.5757574542231157}
done in step count: 0
reward sum = 0.9998169894829455
running average episode reward sum: 0.7960799908476074
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.18931991, 4.54374132, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.5757574542231157}
episode index:4288
target Thresh 16.996133701846542
target distance 12.0
model initialize at round 4288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 11.180339887498974}
done in step count: 12
reward sum = 0.8502466871794541
running average episode reward sum: 0.7960926200610212
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.83372751, 9.17239659, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8513648756830788}
episode index:4289
target Thresh 16.99713538455407
target distance 5.0
model initialize at round 4289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.15641367, 7.49389148, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 4.572384334362733}
done in step count: 3
reward sum = 0.9652992712150974
running average episode reward sum: 0.7961320621708473
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.80122167, 3.88092911, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9030774751487034}
episode index:4290
target Thresh 16.99813656654544
target distance 8.0
model initialize at round 4290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.36922169, 7.73137355, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 7.008130065028667}
done in step count: 24
reward sum = 0.7601872773325603
running average episode reward sum: 0.7961236853857533
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.92840589, 10.88078321,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.8836881691973296}
episode index:4291
target Thresh 16.999137248070937
target distance 3.0
model initialize at round 4291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.5483005,  4.       ,  0.       ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.84315882462844}
done in step count: 4
reward sum = 0.9527118870648775
running average episode reward sum: 0.7961601691233299
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.13205734,  4.08654181,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.2600517197100822}
episode index:4292
target Thresh 17.000137429380736
target distance 4.0
model initialize at round 4292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.46695817, 8.52537918, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.911007637507221}
done in step count: 2
reward sum = 0.9746047272082111
running average episode reward sum: 0.7962017355240019
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.71944141, 10.79670644,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.3464699013917669}
episode index:4293
target Thresh 17.001137110724883
target distance 8.0
model initialize at round 4293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.04287767, 4.53680205, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.546794753114705}
done in step count: 3
reward sum = 0.9553435539300066
running average episode reward sum: 0.7962387969628482
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.37440245, 10.53680205,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.595591748730397}
episode index:4294
target Thresh 17.0021362923533
target distance 1.0
model initialize at round 4294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.36286187, 11.2912178 ,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.3412353094062355}
done in step count: 25
reward sum = 0.737049953333343
running average episode reward sum: 0.7962250160912232
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.47569916, 9.51476595, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.6795158440261566}
episode index:4295
target Thresh 17.003134974515774
target distance 3.0
model initialize at round 4295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4.87346039, 4.17531673, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 3.4038571663709263}
done in step count: 16
reward sum = 0.8200517191094663
running average episode reward sum: 0.7962305623442535
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.0829699 , 6.79002958, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.7943744316821879}
episode index:4296
target Thresh 17.004133157461986
target distance 2.0
model initialize at round 4296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.48410082, 7.21284771, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5580825322833215}
done in step count: 0
reward sum = 0.997404951057338
running average episode reward sum: 0.7962773797491204
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.48410082, 7.21284771, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5580825322833215}
episode index:4297
target Thresh 17.005130841441474
target distance 3.0
model initialize at round 4297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 3.88978481, 10.65836304,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 2.2105316179151018}
done in step count: 12
reward sum = 0.863904098414057
running average episode reward sum: 0.7962931142113505
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.65778648, 10.18741186,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.6839636392150185}
episode index:4298
target Thresh 17.006128026703664
target distance 2.0
model initialize at round 4298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.        , 6.48278081, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.48278081417081076}
done in step count: 0
reward sum = 0.9953510210971475
running average episode reward sum: 0.796339417516046
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.        , 6.48278081, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.48278081417081076}
episode index:4299
target Thresh 17.00712471349785
target distance 3.0
model initialize at round 4299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([14.58901072, 11.41896188,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 3.543212253636954}
done in step count: 2
reward sum = 0.9747803772028896
running average episode reward sum: 0.7963809154136475
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.89155257,  8.32717645,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.1169411388938197}
episode index:4300
target Thresh 17.008120902073202
target distance 14.0
model initialize at round 4300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.27649176,  8.42768455,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.857394364730897}
done in step count: 22
reward sum = 0.7540749151660993
running average episode reward sum: 0.7963710790964544
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.14834144, 2.18242086, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.870976277490488}
episode index:4301
target Thresh 17.00911659267877
target distance 7.0
model initialize at round 4301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([2.76263264, 1.1311295 , 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 10.041109231196396}
done in step count: 27
reward sum = 0.7101528619723922
running average episode reward sum: 0.7963510376698798
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.15570305, 9.37646723, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.40739540596242574}
episode index:4302
target Thresh 17.010111785563478
target distance 11.0
model initialize at round 4302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.00000456,  7.13926815,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.041069706029434}
done in step count: 24
reward sum = 0.7253787338771587
running average episode reward sum: 0.7963345439901697
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.95749063, 8.21085054, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.21509299060212625}
episode index:4303
target Thresh 17.011106480976117
target distance 4.0
model initialize at round 4303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([1.98103547, 8.        , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.2446132654505595}
done in step count: 3
reward sum = 0.9657827817855654
running average episode reward sum: 0.7963739139338954
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.3039881 , 6.66842806, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9650018871109649}
episode index:4304
target Thresh 17.01210067916537
target distance 7.0
model initialize at round 4304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.51654971, 6.        , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.023318045531436}
done in step count: 2
reward sum = 0.9691337466993514
running average episode reward sum: 0.7964140439763496
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.40741263, 10.00000073,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0798072045600935}
episode index:4305
target Thresh 17.013094380379783
target distance 5.0
model initialize at round 4305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.3845048 , 8.96043372, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 4.1954604980043175}
done in step count: 2
reward sum = 0.97000652817842
running average episode reward sum: 0.7964543580692902
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.44846344, 5.80785918, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9239891316915503}
episode index:4306
target Thresh 17.01408758486778
target distance 11.0
model initialize at round 4306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 5.        , 11.01383191,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.824332509835658}
done in step count: 7
reward sum = 0.9057728768395416
running average episode reward sum: 0.7964797396617607
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.59858668,  5.17441303,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6234788810418773}
episode index:4307
target Thresh 17.015080292877663
target distance 10.0
model initialize at round 4307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([10.        , 10.51391983,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 9.716136675408679}
done in step count: 17
reward sum = 0.7902814259533187
running average episode reward sum: 0.7964783008702777
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.95965096, 5.79803406, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.2481139112435313}
episode index:4308
target Thresh 17.016072504657608
target distance 13.0
model initialize at round 4308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.26374388,  6.53224432,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 12.11745711402473}
done in step count: 7
reward sum = 0.8988499815052917
running average episode reward sum: 0.7965020585125695
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.15586056, 11.06233927,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8464381728956372}
episode index:4309
target Thresh 17.01706422045567
target distance 11.0
model initialize at round 4309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.19124496,  8.01286387,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.664469263455528}
done in step count: 5
reward sum = 0.92972095273832
running average episode reward sum: 0.7965329677687704
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.98103718, 11.16868067,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9954331304558343}
episode index:4310
target Thresh 17.018055440519774
target distance 12.0
model initialize at round 4310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 4.75148451, 10.92365491,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 11.248774565573326}
done in step count: 10
reward sum = 0.8539478001604028
running average episode reward sum: 0.7965462859855162
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.37664596, 11.89609941,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0915880204231352}
episode index:4311
target Thresh 17.01904616509773
target distance 2.0
model initialize at round 4311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.81364918, 8.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8136491775512766}
done in step count: 0
reward sum = 0.9965034812454423
running average episode reward sum: 0.7965926582478678
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.81364918, 8.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8136491775512766}
episode index:4312
target Thresh 17.020036394437216
target distance 12.0
model initialize at round 4312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([15.16794486, 11.87447821,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.202129465479217}
done in step count: 9
reward sum = 0.8900458750430138
running average episode reward sum: 0.7966143260468002
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.55841136, 11.24754119,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6108190321367777}
episode index:4313
target Thresh 17.021026128785792
target distance 2.0
model initialize at round 4313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.69938299,  3.00000026,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.699382985715732}
done in step count: 2
reward sum = 0.974286796523043
running average episode reward sum: 0.7966555111349958
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.39036625,  2.54344453,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.6006901963544634}
episode index:4314
target Thresh 17.022015368390885
target distance 8.0
model initialize at round 4314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4., 4., 0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 6.000000000000019}
done in step count: 3
reward sum = 0.9478596411314071
running average episode reward sum: 0.7966905526483206
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.84723397, 10.10118243,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.1832357629220172}
episode index:4315
target Thresh 17.023004113499816
target distance 14.0
model initialize at round 4315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 3.99999999, 11.65604111,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.544585952707559}
done in step count: 16
reward sum = 0.8016383726355055
running average episode reward sum: 0.7966916990384937
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.6522144 ,  8.85556202,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0758113210137363}
episode index:4316
target Thresh 17.02399236435976
target distance 4.0
model initialize at round 4316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([11.        , 11.10309243,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 2.2840343503512264}
done in step count: 1
reward sum = 0.9824825839308383
running average episode reward sum: 0.796734736074605
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.        , 10.17845356,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.17845356464377993}
episode index:4317
target Thresh 17.024980121217787
target distance 6.0
model initialize at round 4317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([4.        , 5.82489014, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 5.106987629112294}
done in step count: 6
reward sum = 0.915327595812584
running average episode reward sum: 0.7967622008406396
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.90954567, 8.76606636, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.2508125477057197}
episode index:4318
target Thresh 17.025967384320836
target distance 8.0
model initialize at round 4318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([9.47084351, 8.08844086, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 8.224395633639478}
done in step count: 9
reward sum = 0.8766256415046422
running average episode reward sum: 0.7967806920285684
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.63307905,  1.83848881,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.4008950563503553}
episode index:4319
target Thresh 17.02695415391572
target distance 7.0
model initialize at round 4319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([7.02619869, 8.13224963, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 6.506778020272609}
done in step count: 2
reward sum = 0.9622794006099732
running average episode reward sum: 0.796819001914814
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.8170896 , 4.91749021, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.228586056497106}
episode index:4320
target Thresh 17.02794043024913
target distance 3.0
model initialize at round 4320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([2.43392217, 3.        , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.858117263422561}
done in step count: 2
reward sum = 0.9721703096608006
running average episode reward sum: 0.7968595831015176
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.76679203, 1.11917905, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.911170403330134}
episode index:4321
target Thresh 17.02892621356764
target distance 4.0
model initialize at round 4321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([7., 9., 0.]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 2.2360679774997982}
done in step count: 1
reward sum = 0.9802709426174969
running average episode reward sum: 0.7969020197881247
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.        , 10.01108849,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.01108849048619831}
episode index:4322
target Thresh 17.029911504117692
target distance 6.0
model initialize at round 4322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.        ,  9.18904698,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 6.504175774658713}
done in step count: 28
reward sum = 0.6751681718015363
running average episode reward sum: 0.7968738602119075
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.88225478,  3.29175506,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9292440497174203}
episode index:4323
target Thresh 17.03089630214561
target distance 1.0
model initialize at round 4323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.9184531 , 4.71122622, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9627805488301244}
done in step count: 0
reward sum = 0.9989879509460615
running average episode reward sum: 0.7969206026010691
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.9184531 , 4.71122622, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9627805488301244}
episode index:4324
target Thresh 17.03188060789759
target distance 5.0
model initialize at round 4324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([5.56594789, 8.94083667, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.567401149157253}
done in step count: 11
reward sum = 0.8795178753882914
running average episode reward sum: 0.7969397002363957
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.48704062, 3.27102916, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8913617750584163}
episode index:4325
target Thresh 17.032864421619717
target distance 13.0
model initialize at round 4325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.3335588 , 6.64298534, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.556399293886782}
done in step count: 30
reward sum = 0.6461301709839526
running average episode reward sum: 0.7969048390414692
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.1295404 ,  2.06006032,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.1427864022389638}
episode index:4326
target Thresh 17.033847743557935
target distance 8.0
model initialize at round 4326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([11.66619732,  8.02687784,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.167337785376219}
done in step count: 67
reward sum = 0.3931366496752847
running average episode reward sum: 0.7968115253855029
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.84604974, 3.89911499, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.234588157407116}
episode index:4327
target Thresh 17.034830573958082
target distance 10.0
model initialize at round 4327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.26687872,  9.91099072,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.05837410619473}
done in step count: 29
reward sum = 0.6938663941358596
running average episode reward sum: 0.7967877395418685
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.25506309, 5.09403299, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.172905459783491}
episode index:4328
target Thresh 17.03581291306586
target distance 5.0
model initialize at round 4328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([13.      ,  9.042853,  0.      ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 3.000306047625614}
done in step count: 2
reward sum = 0.9701336167803842
running average episode reward sum: 0.7968277824795537
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.        , 9.76507688, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.259103897901371}
episode index:4329
target Thresh 17.036794761126856
target distance 9.0
model initialize at round 4329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.96517418, 7.0000199 , 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.326096876110558}
done in step count: 17
reward sum = 0.8005313368344679
running average episode reward sum: 0.796828637803885
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.15993471,  1.80388141,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.862654156323175}
episode index:4330
target Thresh 17.03777611838653
target distance 5.0
model initialize at round 4330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.14534516, 8.01339996, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.1322538542860348}
done in step count: 2
reward sum = 0.9711469174331484
running average episode reward sum: 0.7968688867717052
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.65493578, 5.96074577, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.162743868869194}
episode index:4331
target Thresh 17.038756985090227
target distance 3.0
model initialize at round 4331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 5.70719826, 11.        ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.7071982622146438}
done in step count: 1
reward sum = 0.9803277696338372
running average episode reward sum: 0.7969112364676567
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.72572168, 11.87992624,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9216825816492075}
episode index:4332
target Thresh 17.03973736148316
target distance 10.0
model initialize at round 4332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.13224963,  5.97380131,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.354328643351877}
done in step count: 5
reward sum = 0.9242351633049221
running average episode reward sum: 0.7969406211726734
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.9852198 , 8.41903551, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.070630097837709}
episode index:4333
target Thresh 17.040717247810424
target distance 5.0
model initialize at round 4333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([11.,  9.,  0.]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.605551275464015}
done in step count: 4
reward sum = 0.9429845688038152
running average episode reward sum: 0.796974318437932
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.35939044,  7.18747652,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.40535038004872126}
episode index:4334
target Thresh 17.04169664431699
target distance 13.0
model initialize at round 4334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.176978770806391}
done in step count: 21
reward sum = 0.765355103058574
running average episode reward sum: 0.7969670245012817
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.86755308,  3.94792664,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8691144846411784}
episode index:4335
target Thresh 17.042675551247704
target distance 11.0
model initialize at round 4335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([15.62474823,  5.        ,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 12.654436827075017}
done in step count: 6
reward sum = 0.9157730633772722
running average episode reward sum: 0.7969944244179966
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.9003191 , 10.74415523,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.1680502918782183}
episode index:4336
target Thresh 17.043653968847295
target distance 10.0
model initialize at round 4336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.3135165, 7.       , 0.       ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.890801921079753}
done in step count: 33
reward sum = 0.6195692259389725
running average episode reward sum: 0.7969535147572913
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.39477552,  9.31794491,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6836560778575288}
episode index:4337
target Thresh 17.044631897360368
target distance 4.0
model initialize at round 4337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([4.        , 8.16500294, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.314569475100598}
done in step count: 1
reward sum = 0.9834300700520561
running average episode reward sum: 0.7969965015150817
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.        , 7.84294087, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8429408669472753}
episode index:4338
target Thresh 17.045609337031408
target distance 7.0
model initialize at round 4338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([12.83973298,  7.48134036,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 6.3271146487247005}
done in step count: 18
reward sum = 0.8141864001787024
running average episode reward sum: 0.7970004632340638
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.90311904,  2.4102496 ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9919318208719129}
episode index:4339
target Thresh 17.04658628810477
target distance 7.0
model initialize at round 4339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.,  6.,  0.]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.000000000000019}
done in step count: 3
reward sum = 0.9491955834488367
running average episode reward sum: 0.7970355312341133
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.53576917, 11.90882676,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0205274840017222}
episode index:4340
target Thresh 17.047562750824696
target distance 12.0
model initialize at round 4340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([5.29317904, 8.71419525, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.698703006808}
done in step count: 26
reward sum = 0.7199400246049384
running average episode reward sum: 0.7970177713846249
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.86282246,  4.88383939,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.2351658503492144}
episode index:4341
target Thresh 17.048538725435296
target distance 7.0
model initialize at round 4341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.86775037, 5.97380131, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.027938263971428}
done in step count: 18
reward sum = 0.8024937342248504
running average episode reward sum: 0.7970190325460345
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.85526681, 11.57081306,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.028255254510267}
episode index:4342
target Thresh 17.049514212180572
target distance 5.0
model initialize at round 4342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 3.1622776601683915}
done in step count: 13
reward sum = 0.8431570752680828
running average episode reward sum: 0.797029656087992
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.70810116,  9.7736272 ,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.36939082015084734}
episode index:4343
target Thresh 17.050489211304388
target distance 11.0
model initialize at round 4343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 3.56847577, 11.89950392,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.793509621859373}
done in step count: 31
reward sum = 0.6487467473067733
running average episode reward sum: 0.7969955209800773
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.54576252,  8.8329298 ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5707619284150727}
episode index:4344
target Thresh 17.0514637230505
target distance 10.0
model initialize at round 4344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.48434448,  5.81490719,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 10.809161747172746}
done in step count: 5
reward sum = 0.9279007628707957
running average episode reward sum: 0.7970256487687748
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.05635945, 11.55215297,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.5550218822940453}
episode index:4345
target Thresh 17.052437747662527
target distance 7.0
model initialize at round 4345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 8.01934192, 11.86235238,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 6.333368707826763}
done in step count: 3
reward sum = 0.9540209729101291
running average episode reward sum: 0.7970617728654479
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.37776128, 7.41421476, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6970279261093968}
episode index:4346
target Thresh 17.053411285383984
target distance 12.0
model initialize at round 4346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.6028409 ,  4.39064384,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.86901159746324}
done in step count: 30
reward sum = 0.6764911378177773
running average episode reward sum: 0.7970340363494488
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.83137997, 1.13795241, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8783841796796604}
episode index:4347
target Thresh 17.05438433645825
target distance 3.0
model initialize at round 4347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([8.9532702 , 9.08887631, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 2.0486585573945417}
done in step count: 56
reward sum = 0.4581812240726577
running average episode reward sum: 0.7969561033199464
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.5612637 ,  8.93700886,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.4432351846559833}
episode index:4348
target Thresh 17.05535690112859
target distance 12.0
model initialize at round 4348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 10.428262309182273}
done in step count: 33
reward sum = 0.631052087059657
running average episode reward sum: 0.7969179556960649
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.40716702,  8.83982272,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.6140909608897287}
episode index:4349
target Thresh 17.05632897963815
target distance 12.0
model initialize at round 4349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 5.97967105, 11.86581791,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.610978080703667}
done in step count: 8
reward sum = 0.8939848298118545
running average episode reward sum: 0.7969402699199996
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.56536799,  5.81876443,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.47090478786423207}
episode index:4350
target Thresh 17.05730057222994
target distance 12.0
model initialize at round 4350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([3.81614482, 4.        , 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.61653927818156}
done in step count: 11
reward sum = 0.8522615413054624
running average episode reward sum: 0.7969529845307524
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.47313273,  6.93470932,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.5308973516778313}
episode index:4351
target Thresh 17.058271679146866
target distance 5.0
model initialize at round 4351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([11.68099594,  9.84332019,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 3.3227001828572758}
done in step count: 11
reward sum = 0.8691931360045683
running average episode reward sum: 0.7969695838302638
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.18736884,  9.55361185,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.927163303474116}
episode index:4352
target Thresh 17.0592423006317
target distance 12.0
model initialize at round 4352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([3.36881852, 4.81092644, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.697472391464082}
done in step count: 7
reward sum = 0.9011524350272136
running average episode reward sum: 0.7969935174050851
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.43975319,  5.70546126,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.5292786998519646}
episode index:4353
target Thresh 17.060212436927095
target distance 4.0
model initialize at round 4353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([5., 8., 0.]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.2360679774997974}
done in step count: 6
reward sum = 0.9197375510904147
running average episode reward sum: 0.7970217085014758
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.28313625, 6.53426075, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.6046492209720716}
episode index:4354
target Thresh 17.061182088275597
target distance 3.0
model initialize at round 4354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([9.       , 9.9127841, 0.       ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.0037961014500516}
done in step count: 2
reward sum = 0.9740756620828659
running average episode reward sum: 0.7970623638295082
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.64391944, 10.96159405,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.025405516972361}
episode index:4355
target Thresh 17.062151254919605
target distance 12.0
model initialize at round 4355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.06183124, 5.        , 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.119511496115605}
done in step count: 9
reward sum = 0.8741576543498143
running average episode reward sum: 0.7970800624728784
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.73172075,  6.15665149,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8849917843963737}
episode index:4356
target Thresh 17.06311993710142
target distance 12.0
model initialize at round 4356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.02872663, 11.85888028,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.36379637111748}
done in step count: 9
reward sum = 0.8886137255650374
running average episode reward sum: 0.7971010708876345
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.8789322 , 8.97543725, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.12353437387866538}
episode index:4357
target Thresh 17.06408813506321
target distance 8.0
model initialize at round 4357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 9.        , 10.35514891,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 7.4139950122138485}
done in step count: 26
reward sum = 0.7306168084080761
running average episode reward sum: 0.7970858152055602
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.21918814, 6.05272803, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.22544109703612844}
episode index:4358
target Thresh 17.06505584904702
target distance 4.0
model initialize at round 4358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.        ,  9.89333683,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 2.2857610037468286}
done in step count: 6
reward sum = 0.9297112919867717
running average episode reward sum: 0.7971162408712591
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.61050428, 10.62674451,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.5394687888519776}
episode index:4359
target Thresh 17.066023079294784
target distance 5.0
model initialize at round 4359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([3.        , 9.22637963, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 3.377798918809903}
done in step count: 3
reward sum = 0.9591901680049629
running average episode reward sum: 0.7971534137903264
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.22981021, 6.45337322, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8937223270593497}
episode index:4360
target Thresh 17.066989826048307
target distance 9.0
model initialize at round 4360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.18721688,  9.86806273,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 8.074193076914769}
done in step count: 5
reward sum = 0.9329498553828635
running average episode reward sum: 0.7971845526212351
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.26955467,  1.51041909,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8793405747252698}
episode index:4361
target Thresh 17.067956089549277
target distance 2.0
model initialize at round 4361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.58040619,  9.        ,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.0844625241376444}
done in step count: 0
reward sum = 0.9951267376632201
running average episode reward sum: 0.7972299313890118
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.58040619,  9.        ,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.0844625241376444}
episode index:4362
target Thresh 17.06892187003926
target distance 5.0
model initialize at round 4362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.50536835, 7.35025287, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.789850411580482}
done in step count: 11
reward sum = 0.8767826260479489
running average episode reward sum: 0.797248164873921
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.91567585, 11.44537238,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.4532848048723439}
episode index:4363
target Thresh 17.0698871677597
target distance 7.0
model initialize at round 4363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([8.26803727, 8.11773717, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 7.684180529197465}
done in step count: 19
reward sum = 0.7833712364577508
running average episode reward sum: 0.7972449850094809
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.09402831,  3.77624714,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7819213194282819}
episode index:4364
target Thresh 17.070851982951922
target distance 12.0
model initialize at round 4364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([15.17556882,  6.        ,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 11.571228907477126}
done in step count: 8
reward sum = 0.8963794573854499
running average episode reward sum: 0.7972676962288111
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.49971485, 9.23448566, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5519949778482746}
episode index:4365
target Thresh 17.07181631585713
target distance 9.0
model initialize at round 4365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 7.0710678118654915}
done in step count: 9
reward sum = 0.8829337546926511
running average episode reward sum: 0.7972873174057382
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.91700264,  9.38336544,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.3922468885433105}
episode index:4366
target Thresh 17.072780166716402
target distance 11.0
model initialize at round 4366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.131988  ,  4.05741153,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.895300023079288}
done in step count: 9
reward sum = 0.8774563557180809
running average episode reward sum: 0.7973056753261212
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.07607786, 10.13341615,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.15358290552402623}
episode index:4367
target Thresh 17.073743535770713
target distance 7.0
model initialize at round 4367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([9.62594998, 9.        , 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.340327896630724}
done in step count: 51
reward sum = 0.5324076342609351
running average episode reward sum: 0.7972450301701997
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.15786705,  4.57146718,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.592871608553061}
episode index:4368
target Thresh 17.074706423260896
target distance 2.0
model initialize at round 4368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([3.21538854, 2.        , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.2153885364532648}
done in step count: 1
reward sum = 0.9847075842029722
running average episode reward sum: 0.7972879375984516
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.8963725 , 1.13567992, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.245203943476268}
episode index:4369
target Thresh 17.075668829427674
target distance 3.0
model initialize at round 4369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([4.90930679, 6.40503382, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 4.158699304176063}
done in step count: 24
reward sum = 0.7166604220595229
running average episode reward sum: 0.797269487366063
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 6.90198233, 10.59488374,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.6029047429243272}
episode index:4370
target Thresh 17.076630754511648
target distance 2.0
model initialize at round 4370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8674171277527278}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.7973144954907115
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8674171277527278}
episode index:4371
target Thresh 17.0775921987533
target distance 6.0
model initialize at round 4371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 6.82190704, 10.55700433,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 7.357383005365115}
done in step count: 30
reward sum = 0.6908757803183884
running average episode reward sum: 0.7972901499474425
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.23939992, 4.51828039, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9003145327119889}
episode index:4372
target Thresh 17.078553162392996
target distance 6.0
model initialize at round 4372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([12.14612992,  7.82335934,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 5.604406279193081}
done in step count: 4
reward sum = 0.9388880280328273
running average episode reward sum: 0.7973225299790194
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.90931121,  2.75102654,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9427802845478843}
episode index:4373
target Thresh 17.079513645670968
target distance 7.0
model initialize at round 4373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([10.99505776,  8.1325968 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 6.490593435772951}
done in step count: 2
reward sum = 0.9622794006099732
running average episode reward sum: 0.7973602430267174
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.20442761,  4.89597299,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.1982082519139514}
episode index:4374
target Thresh 17.080473648827343
target distance 4.0
model initialize at round 4374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.69301295, 7.97678626, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.0387600086293527}
done in step count: 28
reward sum = 0.6942370702953727
running average episode reward sum: 0.7973366720158074
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.93695695, 11.83997664,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8423391173435798}
episode index:4375
target Thresh 17.081433172102123
target distance 8.0
model initialize at round 4375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([4.47411513, 9.52878594, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 7.671742515004477}
done in step count: 8
reward sum = 0.9040590537852395
running average episode reward sum: 0.7973610601286432
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.62205232, 1.50681525, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6213498631723631}
episode index:4376
target Thresh 17.082392215735183
target distance 14.0
model initialize at round 4376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([2.75529313, 8.6272769 , 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 13.50277171342346}
done in step count: 17
reward sum = 0.8063474597296785
running average episode reward sum: 0.7973631132242797
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.71385806,  6.11379189,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.3079379873577544}
episode index:4377
target Thresh 17.08335077996629
target distance 12.0
model initialize at round 4377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.06895326,  6.57506196,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.977183231617389}
done in step count: 16
reward sum = 0.8036742756556381
running average episode reward sum: 0.7973645547871923
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.64117195, 2.19282055, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.4073540594969106}
episode index:4378
target Thresh 17.084308865035077
target distance 4.0
model initialize at round 4378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 8.38361228, 11.        ,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 3.528290241514545}
done in step count: 8
reward sum = 0.9108501158198826
running average episode reward sum: 0.7973904706494971
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.04847922, 10.32044549,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.3240918807919824}
episode index:4379
target Thresh 17.085266471181072
target distance 14.0
model initialize at round 4379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([15.08910334, 10.        ,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.428500522175428}
done in step count: 29
reward sum = 0.6693496512171624
running average episode reward sum: 0.7973612375856998
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.10795522, 7.69105522, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.1284064937433305}
episode index:4380
target Thresh 17.086223598643677
target distance 3.0
model initialize at round 4380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([12.20847529,  7.76349019,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.691752085481601}
done in step count: 25
reward sum = 0.7220759557973752
running average episode reward sum: 0.7973440530886012
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.85000395,  5.9790767 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.2965715927944104}
episode index:4381
target Thresh 17.087180247662168
target distance 7.0
model initialize at round 4381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.        ,  8.13227379,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 6.213274649894272}
done in step count: 6
reward sum = 0.9267138284716194
running average episode reward sum: 0.7973735760861784
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.53198109,  2.78227002,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.9460181099435659}
episode index:4382
target Thresh 17.088136418475713
target distance 6.0
model initialize at round 4382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([12.77729082, 10.85289419,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 5.979246449188695}
done in step count: 12
reward sum = 0.8724900012876832
running average episode reward sum: 0.7973907142165004
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.38053599,  4.89602399,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.39448529833970125}
episode index:4383
target Thresh 17.089092111323357
target distance 2.0
model initialize at round 4383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.11057651,  9.        ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8894234895706834}
done in step count: 0
reward sum = 0.9943225591243829
running average episode reward sum: 0.7974356348015614
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.11057651,  9.        ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8894234895706834}
episode index:4384
target Thresh 17.090047326444015
target distance 13.0
model initialize at round 4384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.51182449,  2.0108434 ,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.469871404430737}
done in step count: 21
reward sum = 0.7500046913355841
running average episode reward sum: 0.7974248181667916
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.78543734, 6.13590621, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.1677199537385325}
episode index:4385
target Thresh 17.09100206407649
target distance 2.0
model initialize at round 4385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.95662093,  9.58143353,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.710980544459355}
done in step count: 2
reward sum = 0.9741535445797097
running average episode reward sum: 0.7974651119940631
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.72618514, 10.86937368,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7378401572163384}
episode index:4386
target Thresh 17.091956324459478
target distance 13.0
model initialize at round 4386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.25153458,  4.12358332,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.85016365248809}
done in step count: 35
reward sum = 0.6186741437483808
running average episode reward sum: 0.7974243572714176
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.14008571, 8.75455412, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.1440299391808388}
episode index:4387
target Thresh 17.092910107831536
target distance 7.0
model initialize at round 4387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.81927967,  5.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.0666773312347155}
done in step count: 3
reward sum = 0.9573418181424387
running average episode reward sum: 0.7974608015423544
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.39733727, 11.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0760468894648019}
episode index:4388
target Thresh 17.09386341443111
target distance 4.0
model initialize at round 4388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 2.        , 11.16119087,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 4.279662303866188}
done in step count: 14
reward sum = 0.8390961794154242
running average episode reward sum: 0.7974702878439887
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.07774091, 7.00863351, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9222994962991891}
episode index:4389
target Thresh 17.094816244496524
target distance 8.0
model initialize at round 4389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.12431507, 8.17270777, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 6.234512436143732}
done in step count: 4
reward sum = 0.9451946240949537
running average episode reward sum: 0.7975039380344787
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.14537727, 2.24036568, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8877813219833206}
episode index:4390
target Thresh 17.095768598265995
target distance 1.0
model initialize at round 4390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.1622776601683555}
done in step count: 41
reward sum = 0.567761400307905
running average episode reward sum: 0.7974516168006535
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.83205429,  8.26316541,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.1114133156235246}
episode index:4391
target Thresh 17.0967204759776
target distance 1.0
model initialize at round 4391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.64223599, 10.02878386,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.3589200387519366}
done in step count: 0
reward sum = 0.9979099119677997
running average episode reward sum: 0.7974972584889884
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.64223599, 10.02878386,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.3589200387519366}
episode index:4392
target Thresh 17.09767187786932
target distance 8.0
model initialize at round 4392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([9.96914881, 8.1408731 , 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 8.607060384728607}
done in step count: 7
reward sum = 0.9117585707756755
running average episode reward sum: 0.7975232683483754
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.21553377,  2.57391564,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.6130529928294678}
episode index:4393
target Thresh 17.098622804178998
target distance 2.0
model initialize at round 4393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.76367626,  5.59533143,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.4244095978085571}
done in step count: 1
reward sum = 0.9895569667707513
running average episode reward sum: 0.7975669719665871
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.80568627,  6.02018476,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9988972625936764}
episode index:4394
target Thresh 17.099573255144364
target distance 8.0
model initialize at round 4394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 6.0827625302982415}
done in step count: 3
reward sum = 0.9553641864679102
running average episode reward sum: 0.797602875769659
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.98405977,  3.44574805,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.44603297181946544}
episode index:4395
target Thresh 17.100523231003038
target distance 2.0
model initialize at round 4395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.59416687, 10.39317   ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8492802394829447}
done in step count: 0
reward sum = 0.9966382816772419
running average episode reward sum: 0.7976481522496198
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.59416687, 10.39317   ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8492802394829447}
episode index:4396
target Thresh 17.101472731992505
target distance 5.0
model initialize at round 4396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.87094102, 5.88293809, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 4.119084244923547}
done in step count: 6
reward sum = 0.9247821844822312
running average episode reward sum: 0.7976770660618174
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.50402641, 9.00786886, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.1091952040243893}
episode index:4397
target Thresh 17.10242175835015
target distance 5.0
model initialize at round 4397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([14.        ,  8.23405248,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 3.0962357152705726}
done in step count: 2
reward sum = 0.9700333425601562
running average episode reward sum: 0.7977162557563373
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.06640279,  8.81595826,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.9515645588171747}
episode index:4398
target Thresh 17.103370310313224
target distance 11.0
model initialize at round 4398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([1.91901207, 5.65737247, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.369797809226249}
done in step count: 18
reward sum = 0.7959237321908138
running average episode reward sum: 0.7977158482720078
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.14886268,  3.08162073,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8550419203601716}
episode index:4399
target Thresh 17.104318388118863
target distance 14.0
model initialize at round 4399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.        ,  4.58930957,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.78492041591575}
done in step count: 15
reward sum = 0.8126763652645332
running average episode reward sum: 0.797719248389506
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.12919615, 8.86442871, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8812938889967172}
episode index:4400
target Thresh 17.10526599200409
target distance 8.0
model initialize at round 4400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 9.        , 11.00204116,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 6.325201088548898}
done in step count: 3
reward sum = 0.9548764226714572
running average episode reward sum: 0.7977549578133375
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.        , 8.53922635, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.4607736468314201}
episode index:4401
target Thresh 17.10621312220581
target distance 2.0
model initialize at round 4401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([8.53642154, 9.4296155 , 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.637379759371548}
done in step count: 8
reward sum = 0.9142580930894124
running average episode reward sum: 0.7977814237686478
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.79723686, 10.67471308,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.8610448269935859}
episode index:4402
target Thresh 17.107159778960792
target distance 8.0
model initialize at round 4402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([9.        , 9.76060987, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 8.317729619247554}
done in step count: 4
reward sum = 0.9418995688935411
running average episode reward sum: 0.7978141555753988
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.10089644,  4.32309557,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9553941349055857}
episode index:4403
target Thresh 17.108105962505714
target distance 3.0
model initialize at round 4403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.62084743,  8.47623992,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.524152541203445}
done in step count: 1
reward sum = 0.9851665072130259
running average episode reward sum: 0.797856696981311
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.75958547,  6.47623992,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5763018028853493}
episode index:4404
target Thresh 17.109051673077115
target distance 3.0
model initialize at round 4404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.        ,  6.40322626,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.596773743629468}
done in step count: 9
reward sum = 0.8980937334880834
running average episode reward sum: 0.7978794522676916
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.46560033,  9.19471742,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.5687687392798556}
episode index:4405
target Thresh 17.109996910911427
target distance 2.0
model initialize at round 4405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.        , 11.13768589,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.13768589496612726}
done in step count: 0
reward sum = 0.9960292529188571
running average episode reward sum: 0.7979244249868589
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.        , 11.13768589,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.13768589496612726}
episode index:4406
target Thresh 17.110941676244956
target distance 1.0
model initialize at round 4406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.50052392,  8.57154775,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.6047103560221718}
done in step count: 6
reward sum = 0.933576468739428
running average episode reward sum: 0.7979552060269661
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.85410863,  8.89912981,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9108889629986845}
episode index:4407
target Thresh 17.111885969313892
target distance 14.0
model initialize at round 4407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([2.54844892, 8.82266009, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.657680476725893}
done in step count: 32
reward sum = 0.6521964585423969
running average episode reward sum: 0.7979221391604768
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.56810133,  3.53229927,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.7785124500255673}
episode index:4408
target Thresh 17.11282979035431
target distance 5.0
model initialize at round 4408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([12.14612992,  7.82335934,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.386622759140702}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.7979629960126539
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.20442761,  5.89597299,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8023447142900509}
episode index:4409
target Thresh 17.11377313960217
target distance 7.0
model initialize at round 4409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.        ,  7.18575191,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 5.281289892033752}
done in step count: 4
reward sum = 0.9451693933253866
running average episode reward sum: 0.7979963761480989
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.14287591,  1.86048055,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.1996977801121618}
episode index:4410
target Thresh 17.114716017293304
target distance 3.0
model initialize at round 4410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.41448551, 6.        , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0824963008951978}
done in step count: 1
reward sum = 0.9832390536381596
running average episode reward sum: 0.7980383717675708
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.85310808, 4.01319017, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.3044489386309437}
episode index:4411
target Thresh 17.11565842366343
target distance 8.0
model initialize at round 4411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([10.67245042,  9.3232621 ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 6.879901500105574}
done in step count: 4
reward sum = 0.9469880800340875
running average episode reward sum: 0.798072131900904
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.90778428, 11.86879131,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.2565311938872061}
episode index:4412
target Thresh 17.11660035894815
target distance 9.0
model initialize at round 4412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([15.20766795,  4.        ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.6999437437572915}
done in step count: 8
reward sum = 0.8990273692201829
running average episode reward sum: 0.7980950086825308
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.25887901, 11.23649216,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7779388589065107}
episode index:4413
target Thresh 17.117541823382957
target distance 7.0
model initialize at round 4413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([13.13585209,  2.88412443,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 6.220457701782612}
done in step count: 7
reward sum = 0.9158091186949374
running average episode reward sum: 0.7981216770355015
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.90089507,  9.09714229,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.13877468142703694}
episode index:4414
target Thresh 17.118482817203205
target distance 5.0
model initialize at round 4414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([7.01191258, 9.08179033, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 4.519877603869948}
done in step count: 20
reward sum = 0.7824160779273895
running average episode reward sum: 0.7981181197084102
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.1855975 , 7.40662265, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9102710631942714}
episode index:4415
target Thresh 17.11942334064415
target distance 1.0
model initialize at round 4415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.33919513,  8.35232592,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.394216167463316}
done in step count: 31
reward sum = 0.686094762854506
running average episode reward sum: 0.7980927521004268
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.8609049 ,  7.20717799,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8854828957244499}
episode index:4416
target Thresh 17.12036339394092
target distance 2.0
model initialize at round 4416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.7532478 , 9.61810994, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.784842297556538}
done in step count: 1
reward sum = 0.9867267055565595
running average episode reward sum: 0.7981354584516734
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.30641145, 7.61810994, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.48962025132521614}
episode index:4417
target Thresh 17.121302977328526
target distance 2.0
model initialize at round 4417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.63598806, 10.61074364,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7456549608038945}
done in step count: 0
reward sum = 0.9977507681079534
running average episode reward sum: 0.7981806407309076
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.63598806, 10.61074364,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7456549608038945}
episode index:4418
target Thresh 17.12224209104187
target distance 12.0
model initialize at round 4418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.01913464,  4.        ,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.216802775249633}
done in step count: 9
reward sum = 0.8791585072269725
running average episode reward sum: 0.7981989656610944
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.13216743, 5.92128781, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.15383055248513294}
episode index:4419
target Thresh 17.123180735315724
target distance 2.0
model initialize at round 4419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.19489086, 10.46621943,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.568246555708316}
done in step count: 0
reward sum = 0.997316827419934
running average episode reward sum: 0.7982440149510851
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.19489086, 10.46621943,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.568246555708316}
episode index:4420
target Thresh 17.124118910384755
target distance 4.0
model initialize at round 4420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.954319  , 7.05521678, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 2.8360674802257138}
done in step count: 37
reward sum = 0.6195001730280889
running average episode reward sum: 0.7982035843150473
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.62036248, 4.64626417, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7141278953785737}
episode index:4421
target Thresh 17.125056616483505
target distance 4.0
model initialize at round 4421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.99999893, 7.1482351 , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 2.9351166575401617}
done in step count: 4
reward sum = 0.9531653177956301
running average episode reward sum: 0.7982386276740434
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.66156021, 5.14849115, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.6780203076952538}
episode index:4422
target Thresh 17.125993853846396
target distance 10.0
model initialize at round 4422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 8.944271909999166}
done in step count: 4
reward sum = 0.9402792384809289
running average episode reward sum: 0.798270741761949
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.05719388,  7.92368085,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.3198748772798419}
episode index:4423
target Thresh 17.12693062270774
target distance 6.0
model initialize at round 4423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.35966527, 6.        , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.050929345289492}
done in step count: 2
reward sum = 0.9672423978045619
running average episode reward sum: 0.7983089360784144
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.89184042, 9.39950279, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6101601414527936}
episode index:4424
target Thresh 17.12786692330173
target distance 7.0
model initialize at round 4424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([3.76165676, 7.90204358, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 6.581667531974632}
done in step count: 9
reward sum = 0.8955372125714876
running average episode reward sum: 0.7983309085702772
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.34997301,  9.47353827,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.6321732829649559}
episode index:4425
target Thresh 17.128802755862445
target distance 3.0
model initialize at round 4425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.62601662, 8.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.626016616821329}
done in step count: 4
reward sum = 0.9509406271933968
running average episode reward sum: 0.7983653888501288
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.70865858, 7.96704328, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7094245010629284}
episode index:4426
target Thresh 17.129738120623834
target distance 3.0
model initialize at round 4426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.,  7.,  0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0000000000000195}
done in step count: 3
reward sum = 0.9568934735217923
running average episode reward sum: 0.7984011982209603
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.90676317,  7.58791946,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9960069368298183}
episode index:4427
target Thresh 17.130673017819745
target distance 7.0
model initialize at round 4427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([13.47647023, 11.17556906,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 8.557599558463727}
done in step count: 55
reward sum = 0.43563696207992636
running average episode reward sum: 0.7983192731450478
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.52259165, 10.13714609,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.5402879620195147}
episode index:4428
target Thresh 17.131607447683905
target distance 1.0
model initialize at round 4428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.65968409, 7.50144029, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.5367160218746272}
done in step count: 1
reward sum = 0.9876763219215858
running average episode reward sum: 0.7983620270508451
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.70660383, 9.16254652, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.3354141993717842}
episode index:4429
target Thresh 17.132541410449914
target distance 12.0
model initialize at round 4429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([15.05560207,  8.        ,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 12.701871569129553}
done in step count: 16
reward sum = 0.8030760833703872
running average episode reward sum: 0.7983630911719105
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.41817487, 3.87889818, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.5942946551948418}
episode index:4430
target Thresh 17.133474906351264
target distance 6.0
model initialize at round 4430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([11.34960306,  9.        ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.756699667660599}
done in step count: 7
reward sum = 0.9136493081578707
running average episode reward sum: 0.7983891092754957
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.46096293,  9.59709121,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6729758218014024}
episode index:4431
target Thresh 17.134407935621333
target distance 13.0
model initialize at round 4431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.401754250991393}
done in step count: 7
reward sum = 0.9024955092666785
running average episode reward sum: 0.798412598986685
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.28681525, 7.97871244, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0198730444040238}
episode index:4432
target Thresh 17.135340498493377
target distance 4.0
model initialize at round 4432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.        , 6.52775717, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 2.527757167816149}
done in step count: 2
reward sum = 0.9707347986476162
running average episode reward sum: 0.7984514715785328
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.08448291, 3.11174297, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.2756065579642242}
episode index:4433
target Thresh 17.136272595200534
target distance 10.0
model initialize at round 4433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 8.062257748298569}
done in step count: 4
reward sum = 0.9380212597216863
running average episode reward sum: 0.7984829487522231
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 5.03192517, 10.53463537,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.5355877109450342}
episode index:4434
target Thresh 17.13720422597583
target distance 3.0
model initialize at round 4434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([10.        , 10.16918576,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.0142109349574429}
done in step count: 1
reward sum = 0.9828925549412276
running average episode reward sum: 0.7985245292722207
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.        , 10.55791345,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.1451058555234495}
episode index:4435
target Thresh 17.138135391052174
target distance 7.0
model initialize at round 4435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([4.02077627, 8.        , 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 7.050500964328}
done in step count: 51
reward sum = 0.5380160659678216
running average episode reward sum: 0.7984658032886084
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.40241392,  8.26501237,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.837940198956125}
episode index:4436
target Thresh 17.139066090662357
target distance 8.0
model initialize at round 4436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([3.29626513, 8.30825734, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 6.739330024294472}
done in step count: 4
reward sum = 0.9444150890961639
running average episode reward sum: 0.7984986969748396
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.48216458,  8.09813803,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.0226620661092454}
episode index:4437
target Thresh 17.139996325039053
target distance 12.0
model initialize at round 4437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.        , 8.87119353, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.22408025176265}
done in step count: 11
reward sum = 0.8647152617114335
running average episode reward sum: 0.7985136173364296
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.22980644, 11.44576591,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5015159440795428}
episode index:4438
target Thresh 17.14092609441482
target distance 5.0
model initialize at round 4438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.86740305,  7.99505755,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 3.1276296443239566}
done in step count: 3
reward sum = 0.9612373417008555
running average episode reward sum: 0.7985502750801476
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.88208207, 10.54974847,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9903510609622453}
episode index:4439
target Thresh 17.1418553990221
target distance 12.0
model initialize at round 4439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.        , 6.26513195, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.519557341132854}
done in step count: 15
reward sum = 0.8178869558105764
running average episode reward sum: 0.7985546301884202
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.13342705,  3.18609537,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.22898529887779914}
episode index:4440
target Thresh 17.14278423909322
target distance 10.0
model initialize at round 4440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([12.,  9.,  0.]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 8.544003745317543}
done in step count: 25
reward sum = 0.7203493162471246
running average episode reward sum: 0.7985370203451548
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.87882288, 6.20750556, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9029884944839873}
episode index:4441
target Thresh 17.143712614860387
target distance 13.0
model initialize at round 4441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.        , 6.17199051, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.001344496780296}
done in step count: 5
reward sum = 0.9214080727038504
running average episode reward sum: 0.7985646815455958
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.05915413,  6.04124647,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9417495501808324}
episode index:4442
target Thresh 17.1446405265557
target distance 12.0
model initialize at round 4442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([14.        ,  3.04989839,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.045033254147185}
done in step count: 39
reward sum = 0.5886273893253339
running average episode reward sum: 0.7985174302981908
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.39154295, 4.68636401, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7901907602849839}
episode index:4443
target Thresh 17.14556797441114
target distance 7.0
model initialize at round 4443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 5.        , 10.74909776,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.055803344748202}
done in step count: 7
reward sum = 0.9126280278361756
running average episode reward sum: 0.7985431077503821
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.94673647, 10.14045752,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.957098877304635}
episode index:4444
target Thresh 17.146494958658558
target distance 2.0
model initialize at round 4444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.7113235,  9.       ,  0.       ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.28867650032050207}
done in step count: 0
reward sum = 0.9966205144754186
running average episode reward sum: 0.7985876695966644
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.7113235,  9.       ,  0.       ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.28867650032050207}
episode index:4445
target Thresh 17.14742147952971
target distance 5.0
model initialize at round 4445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([14.        ,  9.10260987,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 3.0017542847474776}
done in step count: 2
reward sum = 0.9685382806582344
running average episode reward sum: 0.7986258951052253
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.03568888,  8.13804923,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.2933889820678668}
episode index:4446
target Thresh 17.14834753725622
target distance 3.0
model initialize at round 4446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([14.41778851,  9.13184512,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.6624732021979172}
done in step count: 15
reward sum = 0.8395114102974894
running average episode reward sum: 0.7986350890596198
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.61997493,  9.50660922,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.6227788669655121}
episode index:4447
target Thresh 17.149273132069606
target distance 4.0
model initialize at round 4447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([4., 9., 0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.828427124746218}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.7986754273940058
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2., 7., 0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 5.432610663077293e-14}
episode index:4448
target Thresh 17.150198264201265
target distance 11.0
model initialize at round 4448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([3.66239679, 3.97032654, 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 11.967581299335922}
done in step count: 16
reward sum = 0.8036077765344889
running average episode reward sum: 0.7986765360362041
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.54729652,  9.48595362,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.684970161991397}
episode index:4449
target Thresh 17.15112293388248
target distance 8.0
model initialize at round 4449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 7.324500627510404}
done in step count: 10
reward sum = 0.8771435852175148
running average episode reward sum: 0.798694169080964
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.38545497, 8.09954644, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.9794856529402054}
episode index:4450
target Thresh 17.15204714134442
target distance 2.0
model initialize at round 4450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.        , 2.66701436, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.3329856395721333}
done in step count: 11
reward sum = 0.8754202919128055
running average episode reward sum: 0.7987114070326224
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.49147484, 3.02855674, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0886916583939275}
episode index:4451
target Thresh 17.152970886818135
target distance 13.0
model initialize at round 4451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.        ,  9.58779716,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.015693600817192}
done in step count: 23
reward sum = 0.7313316222541487
running average episode reward sum: 0.7986962723100756
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.70792262, 8.30000025, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9955672211154122}
episode index:4452
target Thresh 17.15389417053456
target distance 5.0
model initialize at round 4452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([16.31686953,  3.31949389,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 4.349023986225851}
done in step count: 5
reward sum = 0.938468952812037
running average episode reward sum: 0.79872766074046
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.71136772,  6.06399759,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9794943095764204}
episode index:4453
target Thresh 17.154816992724523
target distance 9.0
model initialize at round 4453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([12.        ,  8.83265662,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.327849433391155}
done in step count: 4
reward sum = 0.9365898586625001
running average episode reward sum: 0.7987586131872318
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.08182462, 11.56091182,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0759498538585517}
episode index:4454
target Thresh 17.15573935361872
target distance 3.0
model initialize at round 4454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([ 8.19868433, 10.36472654,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 4.991503847104714}
done in step count: 57
reward sum = 0.4146827351843081
running average episode reward sum: 0.7986724008689371
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.71327383,  8.38989291,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.6741235507825833}
episode index:4455
target Thresh 17.156661253447744
target distance 3.0
model initialize at round 4455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.2475462, 7.       , 0.       ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.251473822010828}
done in step count: 4
reward sum = 0.9478149408426557
running average episode reward sum: 0.7987058709183029
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.27088829, 6.03543627, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7299723349548363}
episode index:4456
target Thresh 17.157582692442077
target distance 12.0
model initialize at round 4456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.87912158, 5.23948467, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 11.123456732529622}
done in step count: 17
reward sum = 0.7750090480220505
running average episode reward sum: 0.7987005541530132
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.55391551,  4.50639498,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.741935515586689}
episode index:4457
target Thresh 17.15850367083207
target distance 12.0
model initialize at round 4457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([12.05521678,  8.045681  ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 10.24337576996544}
done in step count: 16
reward sum = 0.8090995244205708
running average episode reward sum: 0.7987028868067296
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.13534646, 10.05310697,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8662829218577228}
episode index:4458
target Thresh 17.159424188847975
target distance 11.0
model initialize at round 4458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.        , 10.32365274,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.456638013330167}
done in step count: 4
reward sum = 0.9362589388635002
running average episode reward sum: 0.7987337358877021
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.85542892, 5.95582952, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.282719269453444}
episode index:4459
target Thresh 17.160344246719912
target distance 2.0
model initialize at round 4459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([1.83236563, 7.        , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.1676343679428176}
done in step count: 29
reward sum = 0.696603358024049
running average episode reward sum: 0.7987108366998403
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.87574101, 7.11531742, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.16952405218863578}
episode index:4460
target Thresh 17.161263844677908
target distance 8.0
model initialize at round 4460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 9.99999603, 11.78860792,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 6.616372251899557}
done in step count: 4
reward sum = 0.9425206148718194
running average episode reward sum: 0.7987430738166688
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.85737447,  9.90765464,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.2485703498730012}
episode index:4461
target Thresh 17.16218298295185
target distance 1.0
model initialize at round 4461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.8971421 ,  5.65441672,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.1104617005351496}
done in step count: 0
reward sum = 0.9949274495208233
running average episode reward sum: 0.798787041628346
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.8971421 ,  5.65441672,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.1104617005351496}
episode index:4462
target Thresh 17.163101661771528
target distance 6.0
model initialize at round 4462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.67804218, 3.99999991, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.337721325953775}
done in step count: 2
reward sum = 0.9689097512761865
running average episode reward sum: 0.7988251600934251
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.93012417, 7.95913321, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9310215212437599}
episode index:4463
target Thresh 17.164019881366617
target distance 6.0
model initialize at round 4463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([12.96753526,  7.32930011,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.450710376808429}
done in step count: 4
reward sum = 0.9490240669261986
running average episode reward sum: 0.7988588068019449
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.86409801,  3.56626715,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5823468311889375}
episode index:4464
target Thresh 17.164937641966663
target distance 6.0
model initialize at round 4464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.54090464, 7.        , 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.2882266121648}
done in step count: 2
reward sum = 0.9637777853352949
running average episode reward sum: 0.7988957427433858
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.54090448, 10.90368276,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.5494130217017795}
episode index:4465
target Thresh 17.16585494380111
target distance 2.0
model initialize at round 4465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.        , 10.78509176,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.785091757774433}
done in step count: 5
reward sum = 0.9427556850370016
running average episode reward sum: 0.7989279550009526
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.17557466,  9.60363555,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.628651205371352}
episode index:4466
target Thresh 17.166771787099286
target distance 1.0
model initialize at round 4466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.61389303, 10.12026727,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.4044042623344752}
done in step count: 0
reward sum = 0.9991369042798688
running average episode reward sum: 0.7989727745553021
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.61389303, 10.12026727,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.4044042623344752}
episode index:4467
target Thresh 17.167688172090394
target distance 6.0
model initialize at round 4467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.20926112,  6.        ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 4.0054700368469}
done in step count: 2
reward sum = 0.9704289945750556
running average episode reward sum: 0.7990111488211973
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.47293932,  2.        ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.5270606818094095}
episode index:4468
target Thresh 17.168604099003538
target distance 3.0
model initialize at round 4468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([10.98786223,  9.        ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.0121377706527266}
done in step count: 3
reward sum = 0.9646794358365356
running average episode reward sum: 0.799048219370988
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.44759214,  8.95994882,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.553857871132579}
episode index:4469
target Thresh 17.1695195680677
target distance 6.0
model initialize at round 4469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([10.        , 10.89859468,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.001285173471393}
done in step count: 3
reward sum = 0.9558440630837849
running average episode reward sum: 0.7990832967409462
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.81037676, 10.74620527,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8491891799805046}
episode index:4470
target Thresh 17.170434579511742
target distance 10.0
model initialize at round 4470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.14308032,  3.95126543,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.192432959905762}
done in step count: 82
reward sum = 0.33034592039519456
running average episode reward sum: 0.7989784572472434
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.28791891, 3.38344042, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.4795037600770875}
episode index:4471
target Thresh 17.171349133564423
target distance 1.0
model initialize at round 4471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.83742464, 10.70926285,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8864581842374587}
done in step count: 0
reward sum = 0.9999167808697487
running average episode reward sum: 0.7990233897883038
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.83742464, 10.70926285,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8864581842374587}
episode index:4472
target Thresh 17.172263230454373
target distance 9.0
model initialize at round 4472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([14.55578995,  2.56850219,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 11.332638764193428}
done in step count: 40
reward sum = 0.5619175026828445
running average episode reward sum: 0.7989703815416895
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([6.99718896, 9.66728045, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.0512317133314586}
episode index:4473
target Thresh 17.17317687041012
target distance 2.0
model initialize at round 4473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([6.22609657, 9.62412906, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.5785903110647408}
done in step count: 3
reward sum = 0.9687383071280551
running average episode reward sum: 0.7990083269877303
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.42518264, 10.07144582,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0212703549890403}
episode index:4474
target Thresh 17.17409005366008
target distance 6.0
model initialize at round 4474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.89766521, 6.37512747, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.18211910519883}
done in step count: 8
reward sum = 0.8927990039068472
running average episode reward sum: 0.799029285798215
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.86137898, 11.67968121,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.6936730736512606}
episode index:4475
target Thresh 17.175002780432543
target distance 2.0
model initialize at round 4475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.17490542, 10.89524078,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.20387839296855553}
done in step count: 0
reward sum = 0.9962707444147447
running average episode reward sum: 0.7990733522545636
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.17490542, 10.89524078,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.20387839296855553}
episode index:4476
target Thresh 17.175915050955687
target distance 8.0
model initialize at round 4476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.29877165, 10.18302941,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 8.188481841970532}
done in step count: 16
reward sum = 0.8300120864544077
running average episode reward sum: 0.7990802628496495
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.85109214,  1.14871944,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.2037592847859584}
episode index:4477
target Thresh 17.17682686545759
target distance 12.0
model initialize at round 4477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.56465042,  7.24907874,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.700156173316113}
done in step count: 9
reward sum = 0.8845399174135189
running average episode reward sum: 0.7990993471851929
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.87052662, 1.83192024, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.21216540656671706}
episode index:4478
target Thresh 17.177738224166195
target distance 8.0
model initialize at round 4478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.20942748,  2.41520381,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 8.621121188346095}
done in step count: 16
reward sum = 0.81594564389458
running average episode reward sum: 0.7991031083588276
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.44833042, 11.03634021,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.4498008230062929}
episode index:4479
target Thresh 17.178649127309352
target distance 1.0
model initialize at round 4479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.12043167,  4.70181649,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.1252496784795727}
done in step count: 0
reward sum = 0.9969092873358056
running average episode reward sum: 0.7991472615237777
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.12043167,  4.70181649,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.1252496784795727}
episode index:4480
target Thresh 17.179559575114776
target distance 6.0
model initialize at round 4480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([11.40316665,  8.66406941,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 5.338266285847325}
done in step count: 69
reward sum = 0.41442644891697933
running average episode reward sum: 0.7990614055066818
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.22163141,  3.65731985,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.40810558391205826}
episode index:4481
target Thresh 17.180469567810086
target distance 2.0
model initialize at round 4481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.22500122,  5.62000966,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9924893423557339}
done in step count: 0
reward sum = 0.9981212527644174
running average episode reward sum: 0.7991058186809918
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.22500122,  5.62000966,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9924893423557339}
episode index:4482
target Thresh 17.18137910562278
target distance 5.0
model initialize at round 4482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([1.64522183, 8.        , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 4.500504035531797}
done in step count: 32
reward sum = 0.6293151973158974
running average episode reward sum: 0.7990679443509974
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.04019105, 10.84679065,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9719600381709452}
episode index:4483
target Thresh 17.18228818878024
target distance 6.0
model initialize at round 4483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 7.        , 10.75993109,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.85975509750395}
done in step count: 7
reward sum = 0.9149708023629688
running average episode reward sum: 0.7990937924460044
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.10733056, 8.4802959 , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0136778956414623}
episode index:4484
target Thresh 17.18319681750974
target distance 2.0
model initialize at round 4484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.        , 10.41262871,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5873712897301555}
done in step count: 0
reward sum = 0.9967412459637666
running average episode reward sum: 0.7991378609975134
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.        , 10.41262871,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5873712897301555}
episode index:4485
target Thresh 17.18410499203843
target distance 11.0
model initialize at round 4485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 4.73780668, 11.0604955 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.918146560451328}
done in step count: 39
reward sum = 0.620049960260799
running average episode reward sum: 0.7990979394859805
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.65073145,  5.27616139,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.44525681542239964}
episode index:4486
target Thresh 17.185012712593362
target distance 5.0
model initialize at round 4486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([5.       , 7.6787473, 0.       ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.017151520824455}
done in step count: 4
reward sum = 0.9469843013724684
running average episode reward sum: 0.7991308983364122
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.3298431 , 7.82862104, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.3717085092335301}
episode index:4487
target Thresh 17.18591997940146
target distance 6.0
model initialize at round 4487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.2526778, 10.       ,  0.       ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.368668957006041}
done in step count: 2
reward sum = 0.9674231608225868
running average episode reward sum: 0.7991683966123672
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.86562567, 11.49732966,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9983208883570132}
episode index:4488
target Thresh 17.186826792689548
target distance 7.0
model initialize at round 4488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([4.74297028, 5.43758944, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 6.350364590351996}
done in step count: 4
reward sum = 0.9419509822131874
running average episode reward sum: 0.7992002038268026
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.48249431, 8.59269516, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.6585661508203897}
episode index:4489
target Thresh 17.18773315268432
target distance 13.0
model initialize at round 4489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.40132143,  5.        ,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.789407552866189}
done in step count: 39
reward sum = 0.6013680114461547
running average episode reward sum: 0.7991561432048915
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.16646692, 2.88212251, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8976922390463379}
episode index:4490
target Thresh 17.188639059612374
target distance 4.0
model initialize at round 4490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 5.37386918, 11.45669818,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 3.00308718914415}
done in step count: 4
reward sum = 0.9521096503740479
running average episode reward sum: 0.799190200988719
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.95418024, 10.85343516,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.8546642735626959}
episode index:4491
target Thresh 17.18954451370018
target distance 4.0
model initialize at round 4491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.86392001, 4.01818777, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 2.1953222188734975}
done in step count: 1
reward sum = 0.9814246162725064
running average episode reward sum: 0.7992307696475087
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.85224028, 2.04096227, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8532241205943363}
episode index:4492
target Thresh 17.19044951517411
target distance 7.0
model initialize at round 4492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([13.13282979,  5.01276326,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 6.49953832425564}
done in step count: 20
reward sum = 0.7774331670343839
running average episode reward sum: 0.7992259181891038
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.86482112, 9.57420902, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.0380903480242556}
episode index:4493
target Thresh 17.19135406426041
target distance 5.0
model initialize at round 4493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([ 4.        , 11.45112944,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 3.874020587766996}
done in step count: 9
reward sum = 0.8989869221927151
running average episode reward sum: 0.799248116899385
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.53250041, 9.08462661, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.47509739353622144}
episode index:4494
target Thresh 17.192258161185208
target distance 9.0
model initialize at round 4494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([8.55793059, 9.        , 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 9.513162371142467}
done in step count: 7
reward sum = 0.9005972716539314
running average episode reward sum: 0.7992706639860936
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.85840815,  2.81353652,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.1826691035464911}
episode index:4495
target Thresh 17.193161806174544
target distance 7.0
model initialize at round 4495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.68224143,  3.73898475,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 7.292996358881374}
done in step count: 14
reward sum = 0.8472287487571099
running average episode reward sum: 0.7992813308198949
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.25644063, 10.2791413 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.7651137629415025}
episode index:4496
target Thresh 17.19406499945432
target distance 5.0
model initialize at round 4496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.13945997,  5.        ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 3.0032397644155524}
done in step count: 5
reward sum = 0.9268284861237561
running average episode reward sum: 0.7993096935406652
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.60153794,  1.10035067,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0822277057647172}
episode index:4497
target Thresh 17.194967741250338
target distance 7.0
model initialize at round 4497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.09983873,  2.        ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.344484284364848}
done in step count: 17
reward sum = 0.7871140635545936
running average episode reward sum: 0.799306982195626
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.66621437, 10.88816726,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.6755354516774611}
episode index:4498
target Thresh 17.195870031788278
target distance 6.0
model initialize at round 4498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([3.99386215, 5.        , 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 4.474884251550927}
done in step count: 2
reward sum = 0.9652790418829283
running average episode reward sum: 0.7993438730735295
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.61416493, 9.07477544, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.6187001917020918}
episode index:4499
target Thresh 17.19677187129372
target distance 9.0
model initialize at round 4499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([10.72783554, 11.        ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 7.727835536003099}
done in step count: 6
reward sum = 0.9255688116855563
running average episode reward sum: 0.7993719230598877
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.68343645, 11.74457431,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0106810980205194}
episode index:4500
target Thresh 17.19767325999212
target distance 13.0
model initialize at round 4500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([3.81117105, 9.4990356 , 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.464513599728518}
done in step count: 28
reward sum = 0.6976547868876962
running average episode reward sum: 0.7993493242738019
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.87722309,  7.22468159,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.25603903641255776}
episode index:4501
target Thresh 17.198574198108826
target distance 13.0
model initialize at round 4501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.00000000000002}
done in step count: 7
reward sum = 0.8954523686566914
running average episode reward sum: 0.7993706710184448
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.28005552, 6.19810114, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0776649862588497}
episode index:4502
target Thresh 17.19947468586907
target distance 3.0
model initialize at round 4502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([11.85839093,  8.68870473,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 2.577859015581203}
done in step count: 5
reward sum = 0.9407018054205216
running average episode reward sum: 0.7994020570132043
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.94481432, 10.2224288 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7795270565686796}
episode index:4503
target Thresh 17.200374723497976
target distance 4.0
model initialize at round 4503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.25709975,  8.20341992,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.593794567525359}
done in step count: 11
reward sum = 0.8790136418355816
running average episode reward sum: 0.7994197327647191
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.61200588, 11.6867453 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7887702713622976}
episode index:4504
target Thresh 17.201274311220555
target distance 6.0
model initialize at round 4504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4.85261539, 5.99616794, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 5.764674343443161}
done in step count: 8
reward sum = 0.9020205017485641
running average episode reward sum: 0.7994425076301983
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.60366901, 9.06792289, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.0128405543738161}
episode index:4505
target Thresh 17.202173449261704
target distance 11.0
model initialize at round 4505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([3.97864795, 8.        , 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.507091714007439}
done in step count: 7
reward sum = 0.9007421812329627
running average episode reward sum: 0.7994649886940249
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.39568802, 11.8832244 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9678090445331231}
episode index:4506
target Thresh 17.203072137846203
target distance 5.0
model initialize at round 4506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([16.04010069,  6.        ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 5.865374242336243}
done in step count: 7
reward sum = 0.9039779284997671
running average episode reward sum: 0.7994881777199415
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.00909747,  8.90554071,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.09489637462636043}
episode index:4507
target Thresh 17.20397037719873
target distance 8.0
model initialize at round 4507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.63738356,  4.99999999,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 6.219407132269226}
done in step count: 3
reward sum = 0.955502650632281
running average episode reward sum: 0.7995227860768431
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.31497213, 10.8859661 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6944543985194689}
episode index:4508
target Thresh 17.20486816754384
target distance 11.0
model initialize at round 4508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([14.03406227,  6.84771812,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.430335759399314}
done in step count: 40
reward sum = 0.5803717628960158
running average episode reward sum: 0.7994741830555122
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.44356089, 3.02673329, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.1211032822035065}
episode index:4509
target Thresh 17.205765509105984
target distance 12.0
model initialize at round 4509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.38090849, 5.99239254, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.032656870739736}
done in step count: 10
reward sum = 0.8793179343668718
running average episode reward sum: 0.799491886769772
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.69568539,  3.83246617,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.084886215134656}
episode index:4510
target Thresh 17.206662402109494
target distance 13.0
model initialize at round 4510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 4.        , 10.18538702,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.001562086606523}
done in step count: 33
reward sum = 0.6559541329045514
running average episode reward sum: 0.7994600672721296
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.53837291, 10.71078079,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.891658407811618}
episode index:4511
target Thresh 17.207558846778596
target distance 1.0
model initialize at round 4511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.65019107, 11.21247101,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6840265748635453}
done in step count: 0
reward sum = 0.9968550875662725
running average episode reward sum: 0.7995038161684713
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.65019107, 11.21247101,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6840265748635453}
episode index:4512
target Thresh 17.2084548433374
target distance 12.0
model initialize at round 4512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.77032961426902}
done in step count: 19
reward sum = 0.7782523275934475
running average episode reward sum: 0.7994991072190861
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.11224539, 3.35252495, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.36996333610842586}
episode index:4513
target Thresh 17.209350392009906
target distance 7.0
model initialize at round 4513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.00543499, 5.78513753, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.013502318729085}
done in step count: 5
reward sum = 0.9414844381305462
running average episode reward sum: 0.7995305616565942
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.73219476, 10.82793641,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.3183167072640695}
episode index:4514
target Thresh 17.21024549302
target distance 10.0
model initialize at round 4514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.91258492, 4.53364953, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.726083400843631}
done in step count: 6
reward sum = 0.9140638059596689
running average episode reward sum: 0.79955592893108
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.26528983,  8.92673375,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.182638777686289}
episode index:4515
target Thresh 17.211140146591458
target distance 8.0
model initialize at round 4515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 8.0196625 , 10.36329108,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 8.802840614360242}
done in step count: 4
reward sum = 0.9414120891242832
running average episode reward sum: 0.7995873408354628
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.31248063,  5.46645484,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5614483574339197}
episode index:4516
target Thresh 17.212034352947946
target distance 5.0
model initialize at round 4516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([11.        ,  8.73527381,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.465714239455852}
done in step count: 5
reward sum = 0.942501909169582
running average episode reward sum: 0.7996189801023068
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.26348084,  7.20863244,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7654985073096924}
episode index:4517
target Thresh 17.212928112313012
target distance 3.0
model initialize at round 4517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.31742632,  6.69285619,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 3.706473570692247}
done in step count: 6
reward sum = 0.9324876957012338
running average episode reward sum: 0.7996483888485659
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.55969992,  3.32508779,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5473081640239786}
episode index:4518
target Thresh 17.213821424910094
target distance 7.0
model initialize at round 4518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 6.50549221, 10.4465847 ,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 5.512626755527076}
done in step count: 41
reward sum = 0.5846446900918112
running average episode reward sum: 0.7996008111325321
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.62701438, 10.40164026,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.5481178494025323}
episode index:4519
target Thresh 17.214714290962522
target distance 1.0
model initialize at round 4519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.4142135623730687}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.799643819802681
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.4142135623730687}
episode index:4520
target Thresh 17.215606710693518
target distance 11.0
model initialize at round 4520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.15439153, 6.        , 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.292521858133007}
done in step count: 16
reward sum = 0.8033345838481489
running average episode reward sum: 0.7996446361627884
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.71858587,  3.87259089,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9168471894346039}
episode index:4521
target Thresh 17.21649868432618
target distance 7.0
model initialize at round 4521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 7.        , 11.14468527,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 5.440558326174001}
done in step count: 3
reward sum = 0.9544663564984298
running average episode reward sum: 0.7996788736064715
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.09471999, 8.39554271, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0885313558794947}
episode index:4522
target Thresh 17.217390212083505
target distance 12.0
model initialize at round 4522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.1097066 ,  4.69171828,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.131219831010197}
done in step count: 8
reward sum = 0.8850497225401189
running average episode reward sum: 0.7996977484348893
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.12632932, 3.1680682 , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.2064041502494656}
episode index:4523
target Thresh 17.218281294188372
target distance 2.0
model initialize at round 4523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.49165791,  4.        ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.508342087268856}
done in step count: 0
reward sum = 0.9968187041976535
running average episode reward sum: 0.799741320706278
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.49165791,  4.        ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.508342087268856}
episode index:4524
target Thresh 17.219171930863553
target distance 6.0
model initialize at round 4524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.99983213, 7.00007785, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.656917905780964}
done in step count: 2
reward sum = 0.9622794006099732
running average episode reward sum: 0.7997772407239363
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.94477814, 11.04569133,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.07167392591587947}
episode index:4525
target Thresh 17.22006212233171
target distance 13.0
model initialize at round 4525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([15.87184644,  7.        ,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 13.216823775378083}
done in step count: 36
reward sum = 0.6291055443484275
running average episode reward sum: 0.7997395315554928
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.21926339, 9.81345338, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8027137042370325}
episode index:4526
target Thresh 17.220951868815387
target distance 4.0
model initialize at round 4526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([11.52715075, 10.3103447 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.5672179203773693}
done in step count: 47
reward sum = 0.5283790997040017
running average episode reward sum: 0.799679588893277
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.72178965, 11.74737852,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7974808149020267}
episode index:4527
target Thresh 17.221841170537022
target distance 4.0
model initialize at round 4527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 4.        , 11.11800778,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 5.214787016470293}
done in step count: 11
reward sum = 0.8735959961340102
running average episode reward sum: 0.7996959131881622
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.78236628, 6.26376021, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.3419559679456432}
episode index:4528
target Thresh 17.22273002771894
target distance 3.0
model initialize at round 4528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.49759078, 9.79409444, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.8618193940750052}
done in step count: 1
reward sum = 0.9876796572701978
running average episode reward sum: 0.7997374198660342
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.06468505, 8.6923815 , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6953965044950599}
episode index:4529
target Thresh 17.223618440583355
target distance 11.0
model initialize at round 4529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.        ,  9.71397901,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.00454374241781}
done in step count: 7
reward sum = 0.903897972395481
running average episode reward sum: 0.7997604133654889
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.00960543, 9.0621936 , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9378555861242532}
episode index:4530
target Thresh 17.224506409352372
target distance 7.0
model initialize at round 4530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 9.0020877 , 11.86684263,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 6.322448360908609}
done in step count: 19
reward sum = 0.7934332957045948
running average episode reward sum: 0.799759016959031
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.1961461 , 8.71534506, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.7417491759839934}
episode index:4531
target Thresh 17.225393934247982
target distance 8.0
model initialize at round 4531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.14264297,  4.        ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 6.001695345339954}
done in step count: 5
reward sum = 0.9407839173765978
running average episode reward sum: 0.7997901345451778
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.2397714 ,  9.55579253,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.504787673743871}
episode index:4532
target Thresh 17.226281015492066
target distance 11.0
model initialize at round 4532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.0421548 ,  6.62261976,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.675591178069839}
done in step count: 7
reward sum = 0.8981873283211865
running average episode reward sum: 0.7998118414046033
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.01282571, 3.70890011, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.215340465833446}
episode index:4533
target Thresh 17.227167653306395
target distance 7.0
model initialize at round 4533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([13.27029881,  4.41270059,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 6.218455050605526}
done in step count: 8
reward sum = 0.8998082208300309
running average episode reward sum: 0.7998338961861263
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.24685375, 10.77883809,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8170223642612034}
episode index:4534
target Thresh 17.228053847912626
target distance 8.0
model initialize at round 4534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 9.        , 11.49125421,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 6.941819355337614}
done in step count: 6
reward sum = 0.9214851140032063
running average episode reward sum: 0.7998607211514663
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.04582852,  7.03454071,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9665463716430364}
episode index:4535
target Thresh 17.228939599532314
target distance 6.0
model initialize at round 4535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([5.       , 9.1477809, 0.       ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 4.089777182503922}
done in step count: 2
reward sum = 0.9691801781874809
running average episode reward sum: 0.7998980490740932
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.        , 9.84995564, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.15004435642464922}
episode index:4536
target Thresh 17.229824908386888
target distance 13.0
model initialize at round 4536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([3.94087827, 9.59290957, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.075004070293337}
done in step count: 6
reward sum = 0.9158048055002638
running average episode reward sum: 0.7999235960779342
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.81322896,  9.82972783,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.1618044665908633}
episode index:4537
target Thresh 17.230709774697683
target distance 2.0
model initialize at round 4537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.59112883,  6.4095819 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7181707109328236}
done in step count: 0
reward sum = 0.9978397866167926
running average episode reward sum: 0.7999672091653159
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.59112883,  6.4095819 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7181707109328236}
episode index:4538
target Thresh 17.23159419868591
target distance 8.0
model initialize at round 4538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([7.00422041, 8.13271827, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 7.32420742796277}
done in step count: 16
reward sum = 0.8068176690852051
running average episode reward sum: 0.7999687184096252
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.50223923, 1.15175542, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9835063059798518}
episode index:4539
target Thresh 17.23247818057268
target distance 14.0
model initialize at round 4539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([2.66307425, 8.        , 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 13.336925745010367}
done in step count: 24
reward sum = 0.7349099924015521
running average episode reward sum: 0.7999543882937645
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.55928109,  8.09045576,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.4499059937289978}
episode index:4540
target Thresh 17.233361720578987
target distance 2.0
model initialize at round 4540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.969584  , 11.85923192,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8597700955499806}
done in step count: 0
reward sum = 0.995774797379245
running average episode reward sum: 0.7999975110440587
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.969584  , 11.85923192,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8597700955499806}
episode index:4541
target Thresh 17.234244818925713
target distance 4.0
model initialize at round 4541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4., 9., 0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.0000000000000178}
done in step count: 5
reward sum = 0.9331032557818634
running average episode reward sum: 0.8000268165801084
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.61623209, 11.25102085,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.45857308352556964}
episode index:4542
target Thresh 17.235127475833632
target distance 3.0
model initialize at round 4542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 6., 11.,  0.]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 1.4142135623730956}
done in step count: 27
reward sum = 0.714241306219045
running average episode reward sum: 0.8000079335710041
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([4.50819519, 9.44563753, 0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.7410733501834714}
episode index:4543
target Thresh 17.236009691523414
target distance 6.0
model initialize at round 4543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.86740357, 6.00494981, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 4.09780574199391}
done in step count: 2
reward sum = 0.9668003372080368
running average episode reward sum: 0.8000446396457482
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.84316936, 2.04989337, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8446442568152354}
episode index:4544
target Thresh 17.23689146621561
target distance 4.0
model initialize at round 4544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.98438672, 11.86742441,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.0593019398941985}
done in step count: 16
reward sum = 0.800601291794521
running average episode reward sum: 0.8000447621214686
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.38207475, 11.89703758,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.08926949325005}
episode index:4545
target Thresh 17.237772800130664
target distance 3.0
model initialize at round 4545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.54570407,  6.        ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.139207149060854}
done in step count: 16
reward sum = 0.8227362445607375
running average episode reward sum: 0.800049753648622
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.1139498 ,  7.64733783,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.0973291343368639}
episode index:4546
target Thresh 17.238653693488907
target distance 3.0
model initialize at round 4546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 1.89429429, 10.43646884,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.438760772056757}
done in step count: 12
reward sum = 0.8559856276868443
running average episode reward sum: 0.8000620553583291
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.76561934, 7.31854771, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0249635104657864}
episode index:4547
target Thresh 17.239534146510564
target distance 14.0
model initialize at round 4547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.369316876853006}
done in step count: 12
reward sum = 0.850407035061666
running average episode reward sum: 0.8000731250548337
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.13353982, 8.34885315, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9340512660019336}
episode index:4548
target Thresh 17.24041415941575
target distance 12.0
model initialize at round 4548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([5.        , 7.79358745, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 10.072508687012178}
done in step count: 8
reward sum = 0.9057224874889072
running average episode reward sum: 0.8000963497992687
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.55120761,  8.99700619,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5512157391422549}
episode index:4549
target Thresh 17.241293732424463
target distance 11.0
model initialize at round 4549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([12.02619884, 11.86775028,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 9.819967294377982}
done in step count: 30
reward sum = 0.6588047100902981
running average episode reward sum: 0.8000652966916403
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.2014013 , 8.10602098, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8056055674303235}
episode index:4550
target Thresh 17.2421728657566
target distance 12.0
model initialize at round 4550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([3.17803562, 7.        , 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 12.480338211598427}
done in step count: 15
reward sum = 0.8247549393696346
running average episode reward sum: 0.8000707217944041
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.01327162, 11.61147203,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.1608320036461877}
episode index:4551
target Thresh 17.24305155963195
target distance 9.0
model initialize at round 4551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 7.0000000000000195}
done in step count: 4
reward sum = 0.9396394998324652
running average episode reward sum: 0.8001013827737622
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.15355216,  1.13767119,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.2083397408375856}
episode index:4552
target Thresh 17.243929814270178
target distance 4.0
model initialize at round 4552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([5., 9., 0.]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.828427124746218}
done in step count: 10
reward sum = 0.881165286293377
running average episode reward sum: 0.800119187277061
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.53721637, 7.28460503, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5432943157032479}
episode index:4553
target Thresh 17.24480762989085
target distance 12.0
model initialize at round 4553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([14.        , 10.24962342,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 11.79227683125667}
done in step count: 34
reward sum = 0.6453943922456113
running average episode reward sum: 0.800085211696246
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.65857176, 4.62484109, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7120390665687645}
episode index:4554
target Thresh 17.24568500671342
target distance 4.0
model initialize at round 4554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 8.27755058, 10.5256778 ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.3264174571916643}
done in step count: 3
reward sum = 0.9604129654015748
running average episode reward sum: 0.8001204098858631
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.57109171, 11.84036697,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9434929588140943}
episode index:4555
target Thresh 17.246561944957232
target distance 7.0
model initialize at round 4555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([6.71419632, 9.4039886 , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 7.95202596641747}
done in step count: 3
reward sum = 0.9517417029347353
running average episode reward sum: 0.8001536893619493
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.09538336, 3.40449702, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9909334559905434}
episode index:4556
target Thresh 17.24743844484152
target distance 7.0
model initialize at round 4556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([10.51177216,  9.1348455 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 5.888820778284107}
done in step count: 13
reward sum = 0.8281992034999975
running average episode reward sum: 0.8001598437429319
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.88764387,  6.30718236,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.1260141744573144}
episode index:4557
target Thresh 17.248314506585412
target distance 3.0
model initialize at round 4557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.90362436, 6.37539884, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.6456788691648065}
done in step count: 1
reward sum = 0.9794532722442123
running average episode reward sum: 0.8001991797298783
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.86742213, 5.01186486, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8675032687042996}
episode index:4558
target Thresh 17.24919013040792
target distance 4.0
model initialize at round 4558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([12.50661266, 11.13538678,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 3.685842573515373}
done in step count: 31
reward sum = 0.6575731620273879
running average episode reward sum: 0.8001678952337821
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.22234489, 9.83606582, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.2762456644521317}
episode index:4559
target Thresh 17.250065316527955
target distance 9.0
model initialize at round 4559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.91941553,  3.4745295 ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.071907657547662}
done in step count: 5
reward sum = 0.9361841608877602
running average episode reward sum: 0.8001977233622151
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.9877793 , 10.56946652,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.077528203248318}
episode index:4560
target Thresh 17.25094006516431
target distance 10.0
model initialize at round 4560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 6.99298589, 11.85346841,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 8.500268048466042}
done in step count: 6
reward sum = 0.9170246109665412
running average episode reward sum: 0.800223337676533
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.87379547,  9.09890284,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.16034137158354714}
episode index:4561
target Thresh 17.25181437653567
target distance 3.0
model initialize at round 4561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.11376274,  9.08009779,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.0860723760089983}
done in step count: 20
reward sum = 0.7860583720920437
running average episode reward sum: 0.800220232686269
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.89997834,  7.29559653,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1428671272486308}
episode index:4562
target Thresh 17.252688250860615
target distance 9.0
model initialize at round 4562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([14.16321743, 11.35011339,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 7.289340851799203}
done in step count: 7
reward sum = 0.9126719022073922
running average episode reward sum: 0.8002448769267952
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([7.59806192, 9.25652471, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.9541664287238607}
episode index:4563
target Thresh 17.253561688357614
target distance 6.0
model initialize at round 4563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.12268132,  6.        ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 4.15456536181997}
done in step count: 3
reward sum = 0.9593077141039842
running average episode reward sum: 0.8002797285563257
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.31804627,  1.69307572,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.4419908899641517}
episode index:4564
target Thresh 17.254434689245024
target distance 1.0
model initialize at round 4564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.1065858 , 10.94372189,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.2995383557701117}
done in step count: 0
reward sum = 0.9999891041815528
running average episode reward sum: 0.8003234765027935
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.1065858 , 10.94372189,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.2995383557701117}
episode index:4565
target Thresh 17.2553072537411
target distance 11.0
model initialize at round 4565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([11.        , 10.41999578,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 9.11133294467584}
done in step count: 5
reward sum = 0.929233477650981
running average episode reward sum: 0.800351709091744
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.41421874, 8.63351397, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.690978798344706}
episode index:4566
target Thresh 17.25617938206398
target distance 9.0
model initialize at round 4566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([3.43947315, 8.72645259, 0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 8.654741066060522}
done in step count: 7
reward sum = 0.914968754718197
running average episode reward sum: 0.8003768058829913
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.65182714,  9.89515782,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.6602048938243196}
episode index:4567
target Thresh 17.257051074431697
target distance 6.0
model initialize at round 4567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.15858793, 6.        , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.003142532106778}
done in step count: 2
reward sum = 0.9703403546390397
running average episode reward sum: 0.8004140133148556
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.7777161 , 9.81276035, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.2906351965589176}
episode index:4568
target Thresh 17.257922331062172
target distance 6.0
model initialize at round 4568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.        ,  9.89910066,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.148732258654351}
done in step count: 16
reward sum = 0.828456083863771
running average episode reward sum: 0.8004201507783157
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.55297825, 11.88831571,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9944512335874031}
episode index:4569
target Thresh 17.25879315217322
target distance 13.0
model initialize at round 4569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([3.75326955, 7.89769888, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.279587530318327}
done in step count: 9
reward sum = 0.8686870807364873
running average episode reward sum: 0.8004350888373876
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.90303   ,  6.49258343,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0358256415169675}
episode index:4570
target Thresh 17.259663537982547
target distance 13.0
model initialize at round 4570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 4.97399766, 11.86017911,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.390932890981196}
done in step count: 6
reward sum = 0.9153063313322913
running average episode reward sum: 0.8004602192776622
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.33368121,  8.71518774,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7246369777027631}
episode index:4571
target Thresh 17.260533488707754
target distance 10.0
model initialize at round 4571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([ 3.9772281 , 11.86031018,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 9.465293825209843}
done in step count: 6
reward sum = 0.9234994261305312
running average episode reward sum: 0.8004871307402284
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.59873943,  8.1359538 ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.9526730213978535}
episode index:4572
target Thresh 17.26140300456632
target distance 5.0
model initialize at round 4572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([16.79800303,  9.00000635,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 4.102299240078051}
done in step count: 2
reward sum = 0.966537794493369
running average episode reward sum: 0.8005234418409836
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.83238618,  5.26461824,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1106993736399762}
episode index:4573
target Thresh 17.262272085775628
target distance 9.0
model initialize at round 4573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([10.52676678,  9.        ,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 9.418373090401987}
done in step count: 6
reward sum = 0.9205748514204527
running average episode reward sum: 0.8005496883231829
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.70910966, 4.57625271, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8260740176081858}
episode index:4574
target Thresh 17.263140732552948
target distance 6.0
model initialize at round 4574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([11.        , 11.09312844,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 4.001083966483564}
done in step count: 2
reward sum = 0.9702260459746879
running average episode reward sum: 0.8005867760516313
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.       , 10.0832006,  0.       ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9167993962763159}
episode index:4575
target Thresh 17.264008945115442
target distance 9.0
model initialize at round 4575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([15.10651541,  7.        ,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 8.643818140905406}
done in step count: 10
reward sum = 0.8768052300436707
running average episode reward sum: 0.8006034321823113
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([6.48605892, 9.85816457, 0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.5331535603571167}
episode index:4576
target Thresh 17.264876723680164
target distance 13.0
model initialize at round 4576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.13596507,  2.93953331,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.678287522130772}
done in step count: 8
reward sum = 0.8870071849711688
running average episode reward sum: 0.8006223099958986
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.33617641, 8.47949484, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.6196290824743337}
episode index:4577
target Thresh 17.26574406846406
target distance 1.0
model initialize at round 4577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.44428754, 4.42750049, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.667804263152358}
done in step count: 9
reward sum = 0.9027146152800768
running average episode reward sum: 0.8006446106305173
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.84363762, 5.22175159, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1477782998953863}
episode index:4578
target Thresh 17.26661097968396
target distance 6.0
model initialize at round 4578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([8.        , 9.76531291, 0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 4.072554953629866}
done in step count: 6
reward sum = 0.9299251566604104
running average episode reward sum: 0.8006728439884623
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.21968023,  8.14449766,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.1579219289583542}
episode index:4579
target Thresh 17.2674774575566
target distance 13.0
model initialize at round 4579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.2364092 ,  5.97649181,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.919285998028151}
done in step count: 14
reward sum = 0.8085678983581245
running average episode reward sum: 0.8006745677994599
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.12690727, 1.82084626, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8912838953926925}
episode index:4580
target Thresh 17.268343502298595
target distance 6.0
model initialize at round 4580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.        ,  5.21334755,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 6.122527789108302}
done in step count: 11
reward sum = 0.8673386799308118
running average episode reward sum: 0.8006891201050986
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.6049143 , 11.45529698,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6028167658190022}
episode index:4581
target Thresh 17.269209114126454
target distance 1.0
model initialize at round 4581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.71418935, 6.20865351, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.3538700529615399}
done in step count: 0
reward sum = 0.9994690622539854
running average episode reward sum: 0.8007325028947426
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.71418935, 6.20865351, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.3538700529615399}
episode index:4582
target Thresh 17.270074293256584
target distance 8.0
model initialize at round 4582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([ 4.16951701, 11.87111593,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 7.082130519396492}
done in step count: 5
reward sum = 0.9351694291066631
running average episode reward sum: 0.8007618367211036
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.3199119 , 10.32559675,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.4564612462066587}
episode index:4583
target Thresh 17.270939039905276
target distance 7.0
model initialize at round 4583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 5.99505755, 11.86740305,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 5.768140878289246}
done in step count: 3
reward sum = 0.9519283837646725
running average episode reward sum: 0.8007948137165319
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.99563722,  8.41618641,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.1541801367421873}
episode index:4584
target Thresh 17.27180335428872
target distance 2.0
model initialize at round 4584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.2265684 ,  4.43772721,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.8898490894031543}
done in step count: 2
reward sum = 0.9759772414021337
running average episode reward sum: 0.8008330214433991
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.49936369,  3.42240775,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.6540584058583536}
episode index:4585
target Thresh 17.272667236622997
target distance 1.0
model initialize at round 4585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 8.        , 10.37175274,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.06686461133488}
done in step count: 0
reward sum = 0.9955887213770336
running average episode reward sum: 0.8008754888877806
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 8.        , 10.37175274,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 1.06686461133488}
episode index:4586
target Thresh 17.273530687124072
target distance 6.0
model initialize at round 4586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.85103986, 10.53698766,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 5.5389910179502335}
done in step count: 3
reward sum = 0.9612903237991103
running average episode reward sum: 0.8009104605108264
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.44087842,  5.55022049,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.7844485494340175}
episode index:4587
target Thresh 17.27439370600781
target distance 12.0
model initialize at round 4587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([3.90021837, 5.59597242, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.428070858108425}
done in step count: 64
reward sum = 0.39520009564174174
running average episode reward sum: 0.8008220319221453
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.38011753,  2.72978305,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.6762185117855052}
episode index:4588
target Thresh 17.275256293489967
target distance 2.0
model initialize at round 4588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([12.95674223,  9.85112858,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.3464050792270164}
done in step count: 18
reward sum = 0.8139461752911793
running average episode reward sum: 0.8008248918357145
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.7479269 ,  9.42380265,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8596530295159354}
episode index:4589
target Thresh 17.276118449786185
target distance 10.0
model initialize at round 4589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([15.00690603, 10.        ,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 9.062248963662743}
done in step count: 17
reward sum = 0.8081792013418472
running average episode reward sum: 0.8008264940817942
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.49714229, 8.30553191, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.8540704793460273}
episode index:4590
target Thresh 17.27698017511201
target distance 10.0
model initialize at round 4590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.87695415, 6.31738799, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.412770719231725}
done in step count: 8
reward sum = 0.8989732529791354
running average episode reward sum: 0.800847872160404
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.611858  ,  4.84733615,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9320047054462055}
episode index:4591
target Thresh 17.277841469682862
target distance 12.0
model initialize at round 4591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.84568799, 4.76727951, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.156739458598658}
done in step count: 10
reward sum = 0.880177970922561
running average episode reward sum: 0.8008651478787755
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.8260624 ,  4.78856111,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.27378950459984946}
episode index:4592
target Thresh 17.27870233371408
target distance 10.0
model initialize at round 4592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.12651768,  3.85198246,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.592561397929403}
done in step count: 12
reward sum = 0.8559029747165559
running average episode reward sum: 0.8008771308587097
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.72527136, 10.26541164,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.0322977343413502}
episode index:4593
target Thresh 17.27956276742087
target distance 3.0
model initialize at round 4593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.50182319, 10.9413774 ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.5025099135577014}
done in step count: 1
reward sum = 0.987302993283109
running average episode reward sum: 0.8009177111509223
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.9372319 , 10.74501885,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9712975937606014}
episode index:4594
target Thresh 17.280422771018344
target distance 13.0
model initialize at round 4594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.        , 5.65554154, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.01951608327729}
done in step count: 24
reward sum = 0.7282456502958621
running average episode reward sum: 0.8009018956861007
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.91115824,  5.79781056,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.2110784575049258}
episode index:4595
target Thresh 17.2812823447215
target distance 5.0
model initialize at round 4595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 6.3623929 , 11.48744541,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 3.9299719154879695}
done in step count: 12
reward sum = 0.8686121505434914
running average episode reward sum: 0.8009166281175317
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.0757714 , 10.83732891,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.8407502666371863}
episode index:4596
target Thresh 17.282141488745236
target distance 9.0
model initialize at round 4596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.78229141,  4.        ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 7.043577205784165}
done in step count: 5
reward sum = 0.9274720362919183
running average episode reward sum: 0.8009441581171346
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.45105492, 11.90881059,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0617332020136068}
episode index:4597
target Thresh 17.283000203304333
target distance 4.0
model initialize at round 4597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 5.46385598, 11.        ,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 3.1734187084567718}
done in step count: 3
reward sum = 0.9619844750913232
running average episode reward sum: 0.800979182109517
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.79070503, 9.74040848, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0832447342932807}
episode index:4598
target Thresh 17.283858488613472
target distance 3.0
model initialize at round 4598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([13.       ,  7.5741812,  0.       ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 2.6238515306562187}
done in step count: 17
reward sum = 0.8180362193076897
running average episode reward sum: 0.8009828909673552
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([12.2692031 ,  9.09801982,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.9412962119979921}
episode index:4599
target Thresh 17.284716344887226
target distance 9.0
model initialize at round 4599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 4.98131307, 11.86477008,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.071760288585582}
done in step count: 12
reward sum = 0.852376237942865
running average episode reward sum: 0.8009940634340891
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.68977822, 11.44633545,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.5435557803047851}
episode index:4600
target Thresh 17.285573772340058
target distance 8.0
model initialize at round 4600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 9., 11.,  0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 6.3245553203367715}
done in step count: 16
reward sum = 0.8138662812273894
running average episode reward sum: 0.8009968611341094
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.79320766, 8.86663671, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8043408268328913}
episode index:4601
target Thresh 17.28643077118632
target distance 12.0
model initialize at round 4601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.95760405,  7.        ,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.360857649425958}
done in step count: 10
reward sum = 0.8728409238332137
running average episode reward sum: 0.8010124726210063
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.64404708, 9.50702171, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8110636430647725}
episode index:4602
target Thresh 17.28728734164027
target distance 10.0
model initialize at round 4602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([13.12948959,  6.12964507,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 8.621342031812583}
done in step count: 32
reward sum = 0.6659318110094634
running average episode reward sum: 0.8009831263986271
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.78992616, 8.26088222, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.7683918951147741}
episode index:4603
target Thresh 17.288143483916045
target distance 7.0
model initialize at round 4603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 7.168891158489506}
done in step count: 53
reward sum = 0.4725406853858263
running average episode reward sum: 0.8009117879014479
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.60283609, 8.80558321, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.4421957278036351}
episode index:4604
target Thresh 17.288999198227682
target distance 10.0
model initialize at round 4604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([12.46849132,  9.50420976,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 8.483488235707684}
done in step count: 9
reward sum = 0.8912096120098758
running average episode reward sum: 0.8009313965494628
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.96477671, 8.7043235 , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.0090682266213056}
episode index:4605
target Thresh 17.289854484789107
target distance 4.0
model initialize at round 4605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([6.32561207, 9.61981714, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 3.3472727162232583}
done in step count: 41
reward sum = 0.5315933732538749
running average episode reward sum: 0.8008729210776228
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.84134996, 9.50092859, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.5236813009730173}
episode index:4606
target Thresh 17.290709343814147
target distance 9.0
model initialize at round 4606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 8.72475603, 11.79834647,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 7.494212752359534}
done in step count: 4
reward sum = 0.9411826838837914
running average episode reward sum: 0.800903376854225
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.62592795,  9.87925894,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6374670174698136}
episode index:4607
target Thresh 17.291563775516515
target distance 12.0
model initialize at round 4607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 4.7727778 , 11.88008214,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.383550725748313}
done in step count: 7
reward sum = 0.9048296077656071
running average episode reward sum: 0.8009259302897527
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.2904804 , 10.53823478,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8905699011358149}
episode index:4608
target Thresh 17.292417780109815
target distance 5.0
model initialize at round 4608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([4.67004049, 7.45719743, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 5.021393660618245}
done in step count: 30
reward sum = 0.6867337330875409
running average episode reward sum: 0.8009011543736749
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.42147949, 10.2888028 ,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.646601141627479}
episode index:4609
target Thresh 17.293271357807555
target distance 6.0
model initialize at round 4609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([10.79229657,  8.25676621,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 8.015943566497137}
done in step count: 58
reward sum = 0.45258973851311995
running average episode reward sum: 0.8008255987520133
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.90840849, 3.54688835, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0151434107579158}
episode index:4610
target Thresh 17.29412450882312
target distance 4.0
model initialize at round 4610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.,  8.,  0.]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 2.2360679774998142}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.8008640360544762
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.86740305,  6.00494245,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8674171277527271}
episode index:4611
target Thresh 17.294977233369806
target distance 5.0
model initialize at round 4611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([12.50982111,  7.51031544,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.949418405871024}
done in step count: 4
reward sum = 0.943980209576252
running average episode reward sum: 0.800895067314997
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.76331677, 10.82043062,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.2970927709745572}
episode index:4612
target Thresh 17.295829531660793
target distance 2.0
model initialize at round 4612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.77881002,  3.29267156,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.36685371488727875}
done in step count: 0
reward sum = 0.9966952934992239
running average episode reward sum: 0.8009375126274151
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.77881002,  3.29267156,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.36685371488727875}
episode index:4613
target Thresh 17.296681403909155
target distance 10.0
model initialize at round 4613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.13966756,  3.97467423,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.986829862073876}
done in step count: 15
reward sum = 0.8108053628192948
running average episode reward sum: 0.8009396513032261
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.6383928 , 8.40347356, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.5418031716523756}
episode index:4614
target Thresh 17.297532850327855
target distance 10.0
model initialize at round 4614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.12634255,  4.84344783,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.008781176281119}
done in step count: 28
reward sum = 0.6761060560159203
running average episode reward sum: 0.8009126017701195
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.82049186, 11.86826503,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.194609163271545}
episode index:4615
target Thresh 17.29838387112976
target distance 3.0
model initialize at round 4615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.17685282,  6.59493208,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.9821177823210416}
done in step count: 2
reward sum = 0.9757778547736458
running average episode reward sum: 0.8009504841906142
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.88910317,  4.99953526,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.11089779957795125}
episode index:4616
target Thresh 17.29923446652763
target distance 9.0
model initialize at round 4616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([14.01071882,  7.        ,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 7.625626427242868}
done in step count: 7
reward sum = 0.9101744891661084
running average episode reward sum: 0.8009741411117698
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'previousTarget': array([ 7., 10.]), 'currentState': array([ 7.15911349, 10.61866416,  0.        ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.6387976568092549}
episode index:4617
target Thresh 17.3000846367341
target distance 12.0
model initialize at round 4617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 3.76843739, 11.21183372,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 11.064556781892938}
done in step count: 5
reward sum = 0.9268045175289068
running average episode reward sum: 0.8010013889195692
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.86531418,  7.2824389 ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.3129089359470399}
episode index:4618
target Thresh 17.300934381961724
target distance 4.0
model initialize at round 4618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 3.26896644, 11.08283722,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 4.275492441159171}
done in step count: 3
reward sum = 0.9630395357477588
running average episode reward sum: 0.8010364697047668
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.87295115, 6.09724927, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.2557876379494066}
episode index:4619
target Thresh 17.30178370242293
target distance 6.0
model initialize at round 4619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([9.       , 9.4884212, 0.       ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 4.029709080104425}
done in step count: 25
reward sum = 0.7284588523880019
running average episode reward sum: 0.8010207602637892
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.47100491,  9.7143199 ,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.8888693521330071}
episode index:4620
target Thresh 17.30263259833006
target distance 8.0
model initialize at round 4620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.40399587,  5.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 6.029529079549819}
done in step count: 8
reward sum = 0.905486290686214
running average episode reward sum: 0.8010433669572371
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.46495186, 11.56073867,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7750512035150796}
episode index:4621
target Thresh 17.303481069895327
target distance 4.0
model initialize at round 4621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([12.02619884, 11.86775028,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 2.2041942457664434}
done in step count: 1
reward sum = 0.9809941191109879
running average episode reward sum: 0.8010823004821513
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.02580179, 11.60231391,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6028663019450688}
episode index:4622
target Thresh 17.30432911733085
target distance 4.0
model initialize at round 4622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([ 9.00010689, 11.82126473,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 3.4583178394810004}
done in step count: 30
reward sum = 0.6554442786406345
running average episode reward sum: 0.8010507975572451
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.745877  , 8.12826993, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.9080153186217415}
episode index:4623
target Thresh 17.305176740848648
target distance 11.0
model initialize at round 4623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.46663666, 3.51093829, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.544710743303181}
done in step count: 15
reward sum = 0.8064292858564308
running average episode reward sum: 0.8010519607251299
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.8395464 ,  3.53475451,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.49213689552514167}
episode index:4624
target Thresh 17.30602394066062
target distance 7.0
model initialize at round 4624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.18588972,  8.73102832,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 5.734042254352849}
done in step count: 3
reward sum = 0.962739808503368
running average episode reward sum: 0.8010869202597846
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.81695098,  3.86887777,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8879501796265965}
episode index:4625
target Thresh 17.30687071697857
target distance 6.0
model initialize at round 4625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.000000000000018}
done in step count: 2
reward sum = 0.9637054677937578
running average episode reward sum: 0.8011220734261344
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.56474054,  4.        ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.5647405385971034}
episode index:4626
target Thresh 17.30771707001419
target distance 5.0
model initialize at round 4626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([13.12286426,  6.18841595,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 4.92752008545428}
done in step count: 22
reward sum = 0.7418399112273688
running average episode reward sum: 0.801109261201756
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.01111395, 10.98155379,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.9816167090700134}
episode index:4627
target Thresh 17.308562999979067
target distance 7.0
model initialize at round 4627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([12.       , 10.2043277,  0.       ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 7.381983625035023}
done in step count: 23
reward sum = 0.7352595004612836
running average episode reward sum: 0.801095032644984
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.01096849,  3.92063859,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.08011579514816201}
episode index:4628
target Thresh 17.309408507084687
target distance 2.0
model initialize at round 4628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.20450878,  7.        ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7954912185669389}
done in step count: 0
reward sum = 0.9969686324309719
running average episode reward sum: 0.8011373470973034
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.20450878,  7.        ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7954912185669389}
episode index:4629
target Thresh 17.310253591542423
target distance 9.0
model initialize at round 4629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 9.00494245, 11.86740305,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 7.2495801885666395}
done in step count: 27
reward sum = 0.6957298481768528
running average episode reward sum: 0.8011145808988325
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.14346916, 9.8414333 , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8710846520249313}
episode index:4630
target Thresh 17.31109825356355
target distance 7.0
model initialize at round 4630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.13224963,  5.97380131,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.922301997568521}
done in step count: 2
reward sum = 0.9622794006099732
running average episode reward sum: 0.8011493821987052
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.91749021, 10.1829104 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8212449618480495}
episode index:4631
target Thresh 17.311942493359233
target distance 1.0
model initialize at round 4631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.33419864, 8.99999999, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.1079116253004693}
done in step count: 35
reward sum = 0.6509063591072325
running average episode reward sum: 0.801116946312891
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.95652455, 7.45004828, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.45214331005371716}
episode index:4632
target Thresh 17.312786311140528
target distance 4.0
model initialize at round 4632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.77294505, 7.59948635, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.7119685348278955}
done in step count: 11
reward sum = 0.8737779581166027
running average episode reward sum: 0.8011326296739537
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.83518329, 4.81469981, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8554924233811049}
episode index:4633
target Thresh 17.31362970711839
target distance 11.0
model initialize at round 4633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.90479973, 5.40524106, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.654047830189054}
done in step count: 39
reward sum = 0.563278144649938
running average episode reward sum: 0.8010813015589291
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.14005702,  1.12941904,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8817750185305446}
episode index:4634
target Thresh 17.314472681503673
target distance 6.0
model initialize at round 4634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([10.,  9.,  0.]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 5.0000000000000275}
done in step count: 2
reward sum = 0.9622794006099732
running average episode reward sum: 0.8011160800053263
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.20442761,  5.89597299,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.22937364454250284}
episode index:4635
target Thresh 17.315315234507118
target distance 2.0
model initialize at round 4635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.34520292,  7.96968102,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.831815476007687}
done in step count: 14
reward sum = 0.8414060859120629
running average episode reward sum: 0.8011247706882225
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.89883534, 11.44909784,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0047854747769878}
episode index:4636
target Thresh 17.316157366339365
target distance 9.0
model initialize at round 4636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 8.713402794252564}
done in step count: 24
reward sum = 0.7292996154018417
running average episode reward sum: 0.8011092811140827
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.77236402,  8.74054002,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.3451631757584256}
episode index:4637
target Thresh 17.316999077210937
target distance 4.0
model initialize at round 4637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.57176232, 8.90939522, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.015400593011685}
done in step count: 3
reward sum = 0.9634028150774282
running average episode reward sum: 0.8011442732516342
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.65349042, 11.6257189 ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7152573171471172}
episode index:4638
target Thresh 17.317840367332277
target distance 9.0
model initialize at round 4638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([13.        ,  8.38248777,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 7.184451670211067}
done in step count: 5
reward sum = 0.9280371657009379
running average episode reward sum: 0.8011716267529166
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 6.59743619, 10.53865631,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.8044132193538814}
episode index:4639
target Thresh 17.318681236913697
target distance 11.0
model initialize at round 4639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.2195344, 6.       , 0.       ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.83145499959272}
done in step count: 7
reward sum = 0.907152805431814
running average episode reward sum: 0.8011944675241837
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.63117739,  6.23221269,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8517790007503161}
episode index:4640
target Thresh 17.31952168616542
target distance 10.0
model initialize at round 4640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([12.38620522,  7.67067232,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.154358158175127}
done in step count: 8
reward sum = 0.9013645947645698
running average episode reward sum: 0.8012160512620075
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.13049433, 4.86694435, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.2278569192227462}
episode index:4641
target Thresh 17.32036171529755
target distance 7.0
model initialize at round 4641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 6.6096094 , 11.71659943,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.430443742150994}
done in step count: 4
reward sum = 0.9490788008986049
running average episode reward sum: 0.8012479045040661
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.9727731 , 11.84406987,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8445088783053861}
episode index:4642
target Thresh 17.321201324520107
target distance 6.0
model initialize at round 4642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([16.        ,  7.86481071,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 6.196450968711958}
done in step count: 7
reward sum = 0.9230706973694898
running average episode reward sum: 0.801274142452131
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.88761061,  2.29440844,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.9351625166290208}
episode index:4643
target Thresh 17.322040514042982
target distance 7.0
model initialize at round 4643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.87026554, 3.90489828, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.168889987200555}
done in step count: 3
reward sum = 0.955067749074
running average episode reward sum: 0.8013072590771573
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.3710677 , 9.90375499, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9769668984368898}
episode index:4644
target Thresh 17.322879284075977
target distance 10.0
model initialize at round 4644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 7., 11.,  0.]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 8.544003745317543}
done in step count: 25
reward sum = 0.7221641840763101
running average episode reward sum: 0.8012902207402358
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.96011385,  7.32584398,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6753349142392148}
episode index:4645
target Thresh 17.323717634828785
target distance 13.0
model initialize at round 4645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.38864207, 9.97188371, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.631043527116471}
done in step count: 29
reward sum = 0.6994980327991797
running average episode reward sum: 0.801268311100128
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.62541949,  5.66844649,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9154071476911224}
episode index:4646
target Thresh 17.324555566510995
target distance 2.0
model initialize at round 4646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.40024704, 6.71677303, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.4903215329878673}
done in step count: 0
reward sum = 0.9975197401945329
running average episode reward sum: 0.8013105429548933
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.40024704, 6.71677303, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.4903215329878673}
episode index:4647
target Thresh 17.325393079332084
target distance 13.0
model initialize at round 4647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.09484601,  5.60222285,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 12.338217299244466}
done in step count: 46
reward sum = 0.554513978974817
running average episode reward sum: 0.8012574455874277
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.9637402 , 10.46628637,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.101655760662919}
episode index:4648
target Thresh 17.326230173501436
target distance 13.0
model initialize at round 4648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.55212903,  8.5893867 ,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.833274283318017}
done in step count: 16
reward sum = 0.8092995226272411
running average episode reward sum: 0.801259175438372
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.12612096, 2.34508735, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0920508943346794}
episode index:4649
target Thresh 17.327066849228324
target distance 7.0
model initialize at round 4649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([6.4513123 , 9.45809662, 0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 5.567565686114652}
done in step count: 4
reward sum = 0.9505722882490918
running average episode reward sum: 0.801291285785213
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.3919987 ,  8.99502305,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.6080216679333673}
episode index:4650
target Thresh 17.327903106721912
target distance 3.0
model initialize at round 4650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([8.55239344, 9.63798803, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.6783784165265125}
done in step count: 1
reward sum = 0.9878560981555566
running average episode reward sum: 0.801331398623822
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.88488698, 9.19897044, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.9069808169382292}
episode index:4651
target Thresh 17.328738946191272
target distance 9.0
model initialize at round 4651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.19168751,  4.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 7.046514676237593}
done in step count: 11
reward sum = 0.8718919271550423
running average episode reward sum: 0.8013465664072552
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.16602534, 10.0465751 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.2667015331174218}
episode index:4652
target Thresh 17.329574367845357
target distance 1.0
model initialize at round 4652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.63231964, 1.1026387 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9697660306037528}
done in step count: 0
reward sum = 0.9969569160507075
running average episode reward sum: 0.8013886060267789
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.63231964, 1.1026387 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9697660306037528}
episode index:4653
target Thresh 17.33040937189303
target distance 12.0
model initialize at round 4653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.27617889,  6.76877083,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.302354578728773}
done in step count: 20
reward sum = 0.7500644357977354
running average episode reward sum: 0.8013775780572411
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.14319728, 6.30554434, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9096528156567325}
episode index:4654
target Thresh 17.33124395854303
target distance 14.0
model initialize at round 4654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.165525060596464}
done in step count: 45
reward sum = 0.5571936150429174
running average episode reward sum: 0.8013251217816204
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.77727308, 7.73725298, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7701618214363286}
episode index:4655
target Thresh 17.332078128004017
target distance 4.0
model initialize at round 4655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([5.68759669, 8.00008813, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 2.5193318229760675}
done in step count: 7
reward sum = 0.9203703744234836
running average episode reward sum: 0.8013506899200743
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([8.24340527, 9.94880055, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.979524687889769}
episode index:4656
target Thresh 17.332911880484524
target distance 2.0
model initialize at round 4656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.96836197, 3.27454424, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0065284156056375}
done in step count: 0
reward sum = 0.9970638079843052
running average episode reward sum: 0.8013927154983574
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.96836197, 3.27454424, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0065284156056375}
episode index:4657
target Thresh 17.333745216192995
target distance 13.0
model initialize at round 4657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.372368206704436}
done in step count: 8
reward sum = 0.8899761882965619
running average episode reward sum: 0.8014117329892974
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.90969196,  9.4940363 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.502222465136284}
episode index:4658
target Thresh 17.334578135337757
target distance 10.0
model initialize at round 4658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 7.      , 10.373503,  0.      ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.228467162053077}
done in step count: 29
reward sum = 0.6911226066475461
running average episode reward sum: 0.8013880607149162
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.99746756,  3.75597641,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.026883174353193}
episode index:4659
target Thresh 17.335410638127048
target distance 6.0
model initialize at round 4659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.62564933,  7.17203283,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.804503144885323}
done in step count: 17
reward sum = 0.8117434238073493
running average episode reward sum: 0.8013902828958374
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.52500277, 11.01934192,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5253589451853471}
episode index:4660
target Thresh 17.33624272476899
target distance 9.0
model initialize at round 4660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.83771086,  4.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 7.049947480697396}
done in step count: 4
reward sum = 0.9366239656354838
running average episode reward sum: 0.8014192967732756
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.82447875, 10.9908616 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.1757589837620654}
episode index:4661
target Thresh 17.337074395471603
target distance 8.0
model initialize at round 4661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([12.,  9.,  0.]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 6.0827625302982415}
done in step count: 17
reward sum = 0.7950947314860762
running average episode reward sum: 0.8014179401526649
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.64574579, 10.61286615,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.7078848560808684}
episode index:4662
target Thresh 17.337905650442806
target distance 6.0
model initialize at round 4662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 8.00494245, 11.86740305,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 4.0977984452913745}
done in step count: 3
reward sum = 0.9575788425424582
running average episode reward sum: 0.8014514295162484
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.12060462, 11.86499899,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8733663143083861}
episode index:4663
target Thresh 17.338736489890415
target distance 11.0
model initialize at round 4663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.94442511, 8.        , 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.235022751902587}
done in step count: 6
reward sum = 0.9198391672826255
running average episode reward sum: 0.8014768128219444
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.06608939,  6.85223758,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.2643171757138205}
episode index:4664
target Thresh 17.339566914022136
target distance 1.0
model initialize at round 4664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.80435985, 10.65221864,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.39903250679712227}
done in step count: 0
reward sum = 0.9994240399941433
running average episode reward sum: 0.8015192452393447
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.80435985, 10.65221864,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.39903250679712227}
episode index:4665
target Thresh 17.340396923045578
target distance 5.0
model initialize at round 4665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([12.        , 10.81932095,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 3.1098692599659943}
done in step count: 2
reward sum = 0.9708525276099316
running average episode reward sum: 0.8015555361271224
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 8.        , 10.24147353,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.0287416894653905}
episode index:4666
target Thresh 17.34122651716824
target distance 8.0
model initialize at round 4666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.23312449,  5.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 6.048809638338409}
done in step count: 20
reward sum = 0.7817928785279783
running average episode reward sum: 0.8015513015743906
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.96231605, 10.41363914,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.5875705360393879}
episode index:4667
target Thresh 17.342055696597523
target distance 14.0
model initialize at round 4667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.        , 7.72463679, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.213815035571326}
done in step count: 6
reward sum = 0.9173013987816205
running average episode reward sum: 0.8015760980819329
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.97191195, 10.38329328,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.3843210573492122}
episode index:4668
target Thresh 17.342884461540724
target distance 4.0
model initialize at round 4668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4., 6., 0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.0000000000000187}
done in step count: 4
reward sum = 0.9373678729978988
running average episode reward sum: 0.801605181777567
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.48008227, 8.98282636, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1118732371734694}
episode index:4669
target Thresh 17.34371281220503
target distance 2.0
model initialize at round 4669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.58999693, 8.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.4100030660629326}
done in step count: 0
reward sum = 0.9955089182445124
running average episode reward sum: 0.8016467029202794
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.58999693, 8.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.4100030660629326}
episode index:4670
target Thresh 17.344540748797527
target distance 12.0
model initialize at round 4670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([5.09038568, 9.40742648, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.429358682947328}
done in step count: 6
reward sum = 0.9172163815340288
running average episode reward sum: 0.8016714448767371
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.84562011,  5.6592351 ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9116984624461795}
episode index:4671
target Thresh 17.34536827152521
target distance 6.0
model initialize at round 4671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.0267421 , 10.00123549,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 5.105541667499648}
done in step count: 3
reward sum = 0.9616701145781045
running average episode reward sum: 0.801705691167341
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.85271466,  4.61632597,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.41097290653080387}
episode index:4672
target Thresh 17.34619538059495
target distance 9.0
model initialize at round 4672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([11.,  9.,  0.]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 8.062257748298576}
done in step count: 5
reward sum = 0.9238598297980614
running average episode reward sum: 0.8017318315779189
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.68929528,  1.79921776,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.36993368078759986}
episode index:4673
target Thresh 17.347022076213523
target distance 12.0
model initialize at round 4673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.895369891272122}
done in step count: 10
reward sum = 0.8665971820426283
running average episode reward sum: 0.8017457094877317
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.73769318, 7.46914938, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9088419019719371}
episode index:4674
target Thresh 17.34784835858761
target distance 2.0
model initialize at round 4674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.51977444, 2.28558993, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.5587290948776648}
done in step count: 0
reward sum = 0.9961522976824029
running average episode reward sum: 0.8017872937846717
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.51977444, 2.28558993, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.5587290948776648}
episode index:4675
target Thresh 17.348674227923777
target distance 2.0
model initialize at round 4675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.       , 10.1712193,  0.       ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.17121930420390896}
done in step count: 0
reward sum = 0.9969780129540625
running average episode reward sum: 0.8018290368811579
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.       , 10.1712193,  0.       ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.17121930420390896}
episode index:4676
target Thresh 17.34949968442849
target distance 8.0
model initialize at round 4676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.        ,  7.32753181,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.03470131369892}
done in step count: 4
reward sum = 0.9421307142268629
running average episode reward sum: 0.8018590351016723
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.46070106, 10.77987525,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.582493138717063}
episode index:4677
target Thresh 17.350324728308117
target distance 10.0
model initialize at round 4677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([12.        , 10.87786192,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.927198029431121}
done in step count: 5
reward sum = 0.9329375865090387
running average episode reward sum: 0.8018870553136022
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.83387231, 5.90941246, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.2338452265679203}
episode index:4678
target Thresh 17.351149359768918
target distance 9.0
model initialize at round 4678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([12.6580261 ,  9.33149698,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.276868441848276}
done in step count: 24
reward sum = 0.7421702583175239
running average episode reward sum: 0.8018742925871657
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.04167429, 6.66542182, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.6667255382979174}
episode index:4679
target Thresh 17.351973579017052
target distance 3.0
model initialize at round 4679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([10.38123775,  9.2388978 ,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.577052398357947}
done in step count: 1
reward sum = 0.9870725854676875
running average episode reward sum: 0.8019138648719691
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.29499638, 9.48018068, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.5976913830332655}
episode index:4680
target Thresh 17.35279738625857
target distance 2.0
model initialize at round 4680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.        , 10.73218858,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.2678114175796793}
done in step count: 0
reward sum = 0.9947496416766926
running average episode reward sum: 0.8019550602953412
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.        , 10.73218858,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.2678114175796793}
episode index:4681
target Thresh 17.353620781699426
target distance 7.0
model initialize at round 4681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4.90580334, 2.57795852, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 7.970595511288847}
done in step count: 15
reward sum = 0.8113027066298678
running average episode reward sum: 0.8019570568024609
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.37489188, 10.64380384,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8973536337896244}
episode index:4682
target Thresh 17.35444376554547
target distance 6.0
model initialize at round 4682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([12.05521678,  8.045681  ,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 4.5015714873224155}
done in step count: 2
reward sum = 0.9651770869086129
running average episode reward sum: 0.8019919105351336
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.05522186, 10.28646389,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.29173791884812156}
episode index:4683
target Thresh 17.355266338002444
target distance 1.0
model initialize at round 4683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([ 5.73004591, 10.34164453,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.3685339751267063}
done in step count: 11
reward sum = 0.8765149707968108
running average episode reward sum: 0.802007820667555
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.12143408, 9.42888576, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.4457456978498125}
episode index:4684
target Thresh 17.356088499275998
target distance 2.0
model initialize at round 4684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.,  4.,  0.]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 2.5757174171303632e-14}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.80204880085529
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.,  4.,  0.]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 2.5757174171303632e-14}
episode index:4685
target Thresh 17.356910249571666
target distance 14.0
model initialize at round 4685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.57231367,  5.        ,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.193296438909055}
done in step count: 13
reward sum = 0.8308184379610274
running average episode reward sum: 0.8020549403425085
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.175049  , 8.74719715, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8628171470897362}
episode index:4686
target Thresh 17.357731589094886
target distance 4.0
model initialize at round 4686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.54624367,  8.12540269,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.1581698462044696}
done in step count: 53
reward sum = 0.5211132825158421
running average episode reward sum: 0.8019949997285065
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.09778885,  5.3875314 ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9819193193922973}
episode index:4687
target Thresh 17.358552518051
target distance 11.0
model initialize at round 4687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.78138804, 7.97613215, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.96492850910447}
done in step count: 6
reward sum = 0.9098708482685666
running average episode reward sum: 0.8020180107883487
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.14055517,  4.02135576,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.1421682923220961}
episode index:4688
target Thresh 17.35937303664523
target distance 2.0
model initialize at round 4688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.80013679,  4.68824553,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7166779047417884}
done in step count: 0
reward sum = 0.9986795160433908
running average episode reward sum: 0.8020599518216724
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.80013679,  4.68824553,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7166779047417884}
episode index:4689
target Thresh 17.360193145082714
target distance 6.0
model initialize at round 4689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 4.123105625617684}
done in step count: 11
reward sum = 0.8714050345169724
running average episode reward sum: 0.8020747375535904
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.68055592, 9.82365025, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.8834275602207037}
episode index:4690
target Thresh 17.361012843568474
target distance 3.0
model initialize at round 4690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.        , 8.76928055, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.0262706579098628}
done in step count: 5
reward sum = 0.9387234633717383
running average episode reward sum: 0.8021038675313815
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.5655663 , 8.38930347, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.7494550631461381}
episode index:4691
target Thresh 17.361832132307434
target distance 1.0
model initialize at round 4691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.01282585,  3.29827833,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.29855395749905383}
done in step count: 0
reward sum = 0.9979664930334805
running average episode reward sum: 0.8021456114839608
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.01282585,  3.29827833,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.29855395749905383}
episode index:4692
target Thresh 17.362651011504422
target distance 3.0
model initialize at round 4692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 2.91269696, 10.        ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.3144835257269833}
done in step count: 5
reward sum = 0.9407213458626308
running average episode reward sum: 0.8021751396609007
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.18443841, 11.47242789,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9425119777336917}
episode index:4693
target Thresh 17.363469481364152
target distance 2.0
model initialize at round 4693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.35037628,  5.88908184,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.164861667309213}
done in step count: 1
reward sum = 0.987533139684019
running average episode reward sum: 0.80221462794382
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.58070102,  7.23352087,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.4799412739325643}
episode index:4694
target Thresh 17.364287542091247
target distance 1.0
model initialize at round 4694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.09260601,  3.44461746,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.7059552890084333}
done in step count: 1
reward sum = 0.9844125998289673
running average episode reward sum: 0.8022534347535931
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.4941461 ,  2.31426321,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.5955245880536177}
episode index:4695
target Thresh 17.36510519389022
target distance 3.0
model initialize at round 4695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 4.38122439, 10.98534131,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 1.8950809906871824}
done in step count: 3
reward sum = 0.9633048560977653
running average episode reward sum: 0.8022877302010685
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.49030325, 10.4405181 ,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.673681659740747}
episode index:4696
target Thresh 17.365922436965477
target distance 7.0
model initialize at round 4696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([7.        , 9.91733456, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 6.351811554837957}
done in step count: 3
reward sum = 0.9553049580652092
running average episode reward sum: 0.8023203078523063
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.56595862, 6.9603422 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.053873359920342}
episode index:4697
target Thresh 17.36673927152134
target distance 7.0
model initialize at round 4697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([2.52866578, 8.        , 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.132893282092984}
done in step count: 5
reward sum = 0.9330625043727124
running average episode reward sum: 0.8023481371831962
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.4868462 , 10.99368356,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5131926778270971}
episode index:4698
target Thresh 17.367555697762008
target distance 6.0
model initialize at round 4698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([11.        , 10.45570996,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 4.025875255310658}
done in step count: 2
reward sum = 0.9690606042973785
running average episode reward sum: 0.8023836154694516
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.        , 10.63373771,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6337377130985811}
episode index:4699
target Thresh 17.368371715891595
target distance 11.0
model initialize at round 4699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.310400490210087}
done in step count: 21
reward sum = 0.746222249513183
running average episode reward sum: 0.8023716662426523
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.15342928,  9.88756195,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.190217929740326}
episode index:4700
target Thresh 17.3691873261141
target distance 10.0
model initialize at round 4700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([11.       , 10.3139298,  0.       ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 8.327921175289193}
done in step count: 5
reward sum = 0.9252630219713471
running average episode reward sum: 0.8023978077775871
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.01664405, 7.68205949, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.3183758664471092}
episode index:4701
target Thresh 17.37000252863343
target distance 6.0
model initialize at round 4701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([5.09950259, 7.00047711, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 5.292727713863201}
done in step count: 8
reward sum = 0.9028078665226882
running average episode reward sum: 0.8024191625327435
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.53330185,  8.12504478,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.0246743361382968}
episode index:4702
target Thresh 17.370817323653384
target distance 5.0
model initialize at round 4702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([16.86887595,  5.91581069,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.388832606639787}
done in step count: 24
reward sum = 0.75069520246419
running average episode reward sum: 0.8024081644549063
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.68966799, 10.80963425,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.36406740982888186}
episode index:4703
target Thresh 17.37163171137766
target distance 7.0
model initialize at round 4703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 5.000000000000018}
done in step count: 8
reward sum = 0.901377404576275
running average episode reward sum: 0.8024292038341837
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.94023628, 10.34647162,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.3515882304759136}
episode index:4704
target Thresh 17.372445692009855
target distance 4.0
model initialize at round 4704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([14.47527218,  5.        ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.514914497393348}
done in step count: 3
reward sum = 0.9563654099151904
running average episode reward sum: 0.8024619214125218
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.49357665,  7.88113356,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0099575483571628}
episode index:4705
target Thresh 17.373259265753465
target distance 5.0
model initialize at round 4705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([3.64943361, 5.12554777, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 3.5340741481955837}
done in step count: 3
reward sum = 0.9604368262232624
running average episode reward sum: 0.8024954902405734
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.92856783, 2.30212361, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.3104532634742513}
episode index:4706
target Thresh 17.374072432811882
target distance 5.0
model initialize at round 4706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.90070663, 2.95923972, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 4.139929476492477}
done in step count: 2
reward sum = 0.9721844629168895
running average episode reward sum: 0.8025315405853104
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.1502949 , 6.95868148, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8507091020619074}
episode index:4707
target Thresh 17.3748851933884
target distance 5.0
model initialize at round 4707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.32389843, 7.        , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.075241994816624}
done in step count: 3
reward sum = 0.9620876913001242
running average episode reward sum: 0.8025654310166432
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.17219877, 10.16540682,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8441648473568124}
episode index:4708
target Thresh 17.375697547686208
target distance 9.0
model initialize at round 4708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4.91379035, 6.46129553, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.272897322430424}
done in step count: 16
reward sum = 0.8119044441198136
running average episode reward sum: 0.8025674142430401
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.68117209, 11.16050331,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.699826210006168}
episode index:4709
target Thresh 17.376509495908394
target distance 6.0
model initialize at round 4709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4., 7., 0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 4.123105625617675}
done in step count: 2
reward sum = 0.9688003897655557
running average episode reward sum: 0.8026027078684164
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.70695451, 10.47301173,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.6029861476738708}
episode index:4710
target Thresh 17.377321038257946
target distance 11.0
model initialize at round 4710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([6.        , 9.39839607, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 9.620244065564313}
done in step count: 9
reward sum = 0.8974453645297198
running average episode reward sum: 0.8026228400392211
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.15151942,  6.27434988,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8917326713024769}
episode index:4711
target Thresh 17.378132174937747
target distance 12.0
model initialize at round 4711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.0881157,  5.5453204,  0.       ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.382462627896361}
done in step count: 6
reward sum = 0.9131006781774146
running average episode reward sum: 0.8026462860999466
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.50287637, 8.53075537, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7272091660252699}
episode index:4712
target Thresh 17.378942906150584
target distance 2.0
model initialize at round 4712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.13259695, 7.99505755, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8674171277527281}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.8026868873547961
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.13259695, 7.99505755, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8674171277527281}
episode index:4713
target Thresh 17.37975323209914
target distance 1.0
model initialize at round 4713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([13.09480795,  3.59242502,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 2.9336424871067073}
done in step count: 13
reward sum = 0.8461161841112365
running average episode reward sum: 0.8026961001882191
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.69590151,  4.41545169,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8104807332990402}
episode index:4714
target Thresh 17.380563152985996
target distance 11.0
model initialize at round 4714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([3.95568597, 6.        , 0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 9.528883261744813}
done in step count: 33
reward sum = 0.6412246886025366
running average episode reward sum: 0.8026618538655074
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.3471946,  9.0379507,  0.       ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.3492625797287561}
episode index:4715
target Thresh 17.38137266901363
target distance 12.0
model initialize at round 4715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.        , 9.91890264, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 11.620387618724658}
done in step count: 16
reward sum = 0.8072881647285131
running average episode reward sum: 0.8026628348474546
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.73495726,  3.09875549,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9394090280030718}
episode index:4716
target Thresh 17.382181780384425
target distance 4.0
model initialize at round 4716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.43888682,  7.        ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.0772212206080853}
done in step count: 1
reward sum = 0.9834296170295022
running average episode reward sum: 0.8027011572519875
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.22248715,  5.        ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.222487151622893}
episode index:4717
target Thresh 17.382990487300656
target distance 5.0
model initialize at round 4717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([12.31472212,  7.74155549,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.996923704713829}
done in step count: 9
reward sum = 0.8816572071332998
running average episode reward sum: 0.8027178923197877
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.66014526, 10.41716389,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6746844990958841}
episode index:4718
target Thresh 17.3837987899645
target distance 4.0
model initialize at round 4718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.        , 6.83711839, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.162881612777727}
done in step count: 1
reward sum = 0.9809448735280398
running average episode reward sum: 0.802755660275119
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.14984444, 8.83578771, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8658695891277235}
episode index:4719
target Thresh 17.38460668857803
target distance 11.0
model initialize at round 4719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([5.07605471, 7.02181887, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.122200991100096}
done in step count: 20
reward sum = 0.7788941145268253
running average episode reward sum: 0.8027506048628842
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.78411756,  1.36343692,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.0099766831263997}
episode index:4720
target Thresh 17.38541418334323
target distance 1.0
model initialize at round 4720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.52629054, 4.2779091 , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.7860564727979211}
done in step count: 16
reward sum = 0.8297179403544848
running average episode reward sum: 0.8027563170712069
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.90486683, 5.71407567, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9489661226123763}
episode index:4721
target Thresh 17.386221274461963
target distance 12.0
model initialize at round 4721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.        , 9.32561743, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.266864002351793}
done in step count: 5
reward sum = 0.9289156378816603
running average episode reward sum: 0.8027830344199597
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.00000158,  7.23607579,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.23607579097857342}
episode index:4722
target Thresh 17.38702796213601
target distance 11.0
model initialize at round 4722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 6., 10.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 9.055385138137432}
done in step count: 13
reward sum = 0.8329102158984796
running average episode reward sum: 0.8027894132430549
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.18276306,  8.50671864,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5260502172459532}
episode index:4723
target Thresh 17.387834246567035
target distance 9.0
model initialize at round 4723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([4.        , 4.46523786, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 8.34050764063319}
done in step count: 4
reward sum = 0.9394535652596953
running average episode reward sum: 0.8028183429958102
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.5186621 ,  8.10838015,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.0132483027193155}
episode index:4724
target Thresh 17.388640127956617
target distance 11.0
model initialize at round 4724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.33757019, 7.93935978, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.720468481117631}
done in step count: 41
reward sum = 0.5913257056779497
running average episode reward sum: 0.802773582649288
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.20548194,  8.70293904,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8482359088439229}
episode index:4725
target Thresh 17.389445606506218
target distance 5.0
model initialize at round 4725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 5.30773044, 10.63716006,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 4.230330227791418}
done in step count: 2
reward sum = 0.9698621207209479
running average episode reward sum: 0.8028089378202723
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.60538232, 7.04552424, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.1302706466272627}
episode index:4726
target Thresh 17.390250682417218
target distance 6.0
model initialize at round 4726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([11.54577541,  9.        ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 4.692890188457085}
done in step count: 2
reward sum = 0.9664147528869262
running average episode reward sum: 0.802843548739474
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.67815319,  5.        ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.3218468129636136}
episode index:4727
target Thresh 17.391055355890874
target distance 10.0
model initialize at round 4727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12.        , 11.14076996,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.0012384154714}
done in step count: 10
reward sum = 0.8752867189722522
running average episode reward sum: 0.8028588708989988
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.81653225, 10.61717409,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9018207117385272}
episode index:4728
target Thresh 17.391859627128362
target distance 13.0
model initialize at round 4728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([16.08285642, 11.        ,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 14.005753532746597}
done in step count: 40
reward sum = 0.5990933597278292
running average episode reward sum: 0.8028157824001255
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.15070809, 5.53450399, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9684953707747955}
episode index:4729
target Thresh 17.39266349633075
target distance 2.0
model initialize at round 4729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.12433112,  6.28793269,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.5574230694926021}
done in step count: 19
reward sum = 0.8024884994252953
running average episode reward sum: 0.8028157132071075
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.06413032,  4.82304097,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.18822113890986877}
episode index:4730
target Thresh 17.393466963699005
target distance 11.0
model initialize at round 4730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([ 2.51529937, 11.91288391,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 10.881812348794396}
done in step count: 10
reward sum = 0.8778456731624729
running average episode reward sum: 0.8028315724250223
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.91736457,  8.68557107,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.9697542458415532}
episode index:4731
target Thresh 17.394270029433994
target distance 6.0
model initialize at round 4731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([4.        , 7.88301003, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 4.525665313957508}
done in step count: 17
reward sum = 0.8029260249405535
running average episode reward sum: 0.8028315923854017
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 8.32388031, 10.73330074,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.8016410822565453}
episode index:4732
target Thresh 17.395072693736477
target distance 10.0
model initialize at round 4732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.246211251235344}
done in step count: 4
reward sum = 0.9363036015304359
running average episode reward sum: 0.8028597926831295
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.53956264, 11.89922758,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.048683979767178}
episode index:4733
target Thresh 17.395874956807127
target distance 13.0
model initialize at round 4733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([3.30003202, 9.        , 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.699967980384832}
done in step count: 16
reward sum = 0.7981620526107216
running average episode reward sum: 0.8028588003425987
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.75230085,  9.2620062 ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.36055806395236534}
episode index:4734
target Thresh 17.396676818846508
target distance 11.0
model initialize at round 4734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([5.99999895, 8.16850378, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 9.001578328007842}
done in step count: 36
reward sum = 0.6379460368849651
running average episode reward sum: 0.8028239718814671
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.06307795,  8.95965709,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.3411803231500476}
episode index:4735
target Thresh 17.397478280055086
target distance 6.0
model initialize at round 4735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([1.14678379, 7.99408165, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 7.140747921156261}
done in step count: 15
reward sum = 0.8039043353597844
running average episode reward sum: 0.8028241999987556
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([7.36211266, 9.09189369, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 1.1097555299640505}
episode index:4736
target Thresh 17.398279340633223
target distance 13.0
model initialize at round 4736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 3.48394132, 10.17625475,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.946053818388961}
done in step count: 13
reward sum = 0.8514777195484978
running average episode reward sum: 0.8028344709549621
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.04816804,  7.91547326,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9167395728989414}
episode index:4737
target Thresh 17.39908000078119
target distance 9.0
model initialize at round 4737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 8.52695179, 11.00702047,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 10.244256218335604}
done in step count: 14
reward sum = 0.8361844793803628
running average episode reward sum: 0.8028415097916918
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.88760023,  4.32792573,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9462396401237885}
episode index:4738
target Thresh 17.39988026069915
target distance 8.0
model initialize at round 4738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([16.08053672,  8.78915429,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.377524245401691}
done in step count: 6
reward sum = 0.9248090874199364
running average episode reward sum: 0.8028672467778973
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.69740372, 11.25014929,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7409093145033637}
episode index:4739
target Thresh 17.400680120587165
target distance 10.0
model initialize at round 4739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([10., 11.,  0.]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 8.544003745317543}
done in step count: 37
reward sum = 0.6159941612486521
running average episode reward sum: 0.8028278220763089
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.58762355, 8.45205698, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.741388523840361}
episode index:4740
target Thresh 17.4014795806452
target distance 2.0
model initialize at round 4740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.77497268,  3.55303276,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.7356535919366796}
done in step count: 27
reward sum = 0.7153657593073259
running average episode reward sum: 0.8028093740563196
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.21543874,  1.93992492,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.22365792434099857}
episode index:4741
target Thresh 17.402278641073124
target distance 9.0
model initialize at round 4741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([13.46499351, 11.90534159,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 8.949699712596123}
done in step count: 8
reward sum = 0.9068306652256632
running average episode reward sum: 0.8028313102206319
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.40285693, 9.80283276, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.8982394748918365}
episode index:4742
target Thresh 17.4030773020707
target distance 8.0
model initialize at round 4742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 7.2111025509279845}
done in step count: 3
reward sum = 0.9494709218282976
running average episode reward sum: 0.8028622272797944
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.00196789,  6.15471176,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8452905309818227}
episode index:4743
target Thresh 17.40387556383759
target distance 11.0
model initialize at round 4743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.69960559,  7.29485185,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.607926329279282}
done in step count: 15
reward sum = 0.8291430888389462
running average episode reward sum: 0.8028677670904096
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.43004843, 2.18592393, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9937628680845074}
episode index:4744
target Thresh 17.404673426573364
target distance 6.0
model initialize at round 4744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([4.89630991, 6.45432229, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 5.703343540024599}
done in step count: 8
reward sum = 0.901820748637453
running average episode reward sum: 0.8028886212487968
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.92370704,  9.55030579,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.075207491442573}
episode index:4745
target Thresh 17.40547089047749
target distance 3.0
model initialize at round 4745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.8674032 , 5.99505776, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.3275154321449871}
done in step count: 1
reward sum = 0.9837731237416156
running average episode reward sum: 0.8029267342918842
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.87584438, 6.48299224, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0170547700401764}
episode index:4746
target Thresh 17.40626795574933
target distance 11.0
model initialize at round 4746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([12.61313319,  9.90691447,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.662703648273181}
done in step count: 18
reward sum = 0.7940505646362164
running average episode reward sum: 0.8029248644436315
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.14114981, 2.95801137, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8598759797915214}
episode index:4747
target Thresh 17.407064622588145
target distance 2.0
model initialize at round 4747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([1.12891858, 8.27301516, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.3504604422114643}
done in step count: 2
reward sum = 0.9726913536223575
running average episode reward sum: 0.8029606198120348
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.29547615, 10.04402834,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.29873842674939166}
episode index:4748
target Thresh 17.407860891193113
target distance 12.0
model initialize at round 4748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.3801831, 9.       , 0.       ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.289025379306876}
done in step count: 24
reward sum = 0.7501703323428333
running average episode reward sum: 0.802949503727076
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.55739874,  5.96247459,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0593645311520727}
episode index:4749
target Thresh 17.408656761763293
target distance 3.0
model initialize at round 4749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2., 3., 0.]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.0000000000000187}
done in step count: 1
reward sum = 0.9818828617373151
running average episode reward sum: 0.8029871739077097
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13130196, 1.74280414, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.905972407831476}
episode index:4750
target Thresh 17.409452234497653
target distance 6.0
model initialize at round 4750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([8.53089263, 8.0896571 , 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 8.082916920168353}
done in step count: 10
reward sum = 0.8695875284711716
running average episode reward sum: 0.8030011920837913
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.85762911,  4.74205313,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.895580415451701}
episode index:4751
target Thresh 17.410247309595068
target distance 10.0
model initialize at round 4751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([12.35279524, 10.75136435,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 9.74909147821549}
done in step count: 13
reward sum = 0.846018146964786
running average episode reward sum: 0.8030102444732865
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.44963466, 7.28898678, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8991339153177677}
episode index:4752
target Thresh 17.4110419872543
target distance 4.0
model initialize at round 4752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.28267288, 5.17092669, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.8431601697834483}
done in step count: 2
reward sum = 0.973109156805287
running average episode reward sum: 0.8030460321678651
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.00276524, 8.01453328, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.014794012691934434}
episode index:4753
target Thresh 17.41183626767402
target distance 2.0
model initialize at round 4753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.05766317,  9.        ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.057663172483497505}
done in step count: 0
reward sum = 0.9969975061004205
running average episode reward sum: 0.8030868297012964
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.05766317,  9.        ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.057663172483497505}
episode index:4754
target Thresh 17.4126301510528
target distance 3.0
model initialize at round 4754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 9.        , 10.34040171,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.197944032659705}
done in step count: 1
reward sum = 0.983616474477847
running average episode reward sum: 0.8031247959778004
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.        , 11.00364602,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0000066466946496}
episode index:4755
target Thresh 17.41342363758911
target distance 13.0
model initialize at round 4755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.00000734, 11.80087916,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 11.350994909207376}
done in step count: 27
reward sum = 0.6851299528915252
running average episode reward sum: 0.8030999862967478
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.86465754, 9.87218683, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.228145971535261}
episode index:4756
target Thresh 17.414216727481318
target distance 3.0
model initialize at round 4756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([2.70843935, 3.        , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.6334408199556525}
done in step count: 1
reward sum = 0.9849681494690372
running average episode reward sum: 0.8031382179896577
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.81775427, 2.00051838, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.18224646859719365}
episode index:4757
target Thresh 17.4150094209277
target distance 7.0
model initialize at round 4757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.92284982, 8.        , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 5.114709425987982}
done in step count: 10
reward sum = 0.8809841212485616
running average episode reward sum: 0.8031545790454078
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.65629085, 2.36038201, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9164217700191417}
episode index:4758
target Thresh 17.41580171812643
target distance 7.0
model initialize at round 4758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.12903605, 7.74143874, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 6.0385946137597255}
done in step count: 9
reward sum = 0.8930399854188618
running average episode reward sum: 0.8031734665020948
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.04688091, 1.13832429, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.284881713754525}
episode index:4759
target Thresh 17.41659361927558
target distance 9.0
model initialize at round 4759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([14.        ,  5.06143081,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 8.031956626476047}
done in step count: 10
reward sum = 0.8701892252543594
running average episode reward sum: 0.8031875454430091
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.33672136, 8.32505359, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.9463039733596682}
episode index:4760
target Thresh 17.41738512457313
target distance 7.0
model initialize at round 4760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.6091264 , 4.36097277, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 8.523021385503704}
done in step count: 21
reward sum = 0.7519250126630591
running average episode reward sum: 0.8031767782653615
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.10853123, 10.16903331,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.20087629953243}
episode index:4761
target Thresh 17.41817623421695
target distance 2.0
model initialize at round 4761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([4.05230296, 5.        , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 2.052302956581096}
done in step count: 19
reward sum = 0.8017637726068889
running average episode reward sum: 0.8031764815401077
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.78987921, 4.34902559, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0235608670587961}
episode index:4762
target Thresh 17.418966948404822
target distance 6.0
model initialize at round 4762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([7.58340105, 8.09481283, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 7.558537230392286}
done in step count: 6
reward sum = 0.9237537323575976
running average episode reward sum: 0.8032017969402373
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.74674827, 3.35031768, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.4322718054877276}
episode index:4763
target Thresh 17.419757267334422
target distance 10.0
model initialize at round 4763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.86071866, 4.97624663, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.350508760765683}
done in step count: 9
reward sum = 0.8775006686515877
running average episode reward sum: 0.8032173928411003
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.37450413,  2.10372519,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9713711318178547}
episode index:4764
target Thresh 17.42054719120333
target distance 6.0
model initialize at round 4764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([3.50171375, 2.70767844, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 7.199397010652013}
done in step count: 6
reward sum = 0.9299142334994023
running average episode reward sum: 0.8032439818947537
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.13332549, 9.88588459, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.239320871008373}
episode index:4765
target Thresh 17.421336720209027
target distance 2.0
model initialize at round 4765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.        , 9.65053916, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.6505391597747376}
done in step count: 19
reward sum = 0.797207343481656
running average episode reward sum: 0.803242715289967
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.39774003, 7.10946907, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6121278826795417}
episode index:4766
target Thresh 17.422125854548902
target distance 2.0
model initialize at round 4766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.        , 7.12070823, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.12070822715754481}
done in step count: 0
reward sum = 0.9943511974536007
running average episode reward sum: 0.8032828051750444
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.        , 7.12070823, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.12070822715754481}
episode index:4767
target Thresh 17.422914594420227
target distance 1.0
model initialize at round 4767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.46736252,  4.27136648,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8656411143191044}
done in step count: 0
reward sum = 0.9983299056048672
running average episode reward sum: 0.8033237127044969
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.46736252,  4.27136648,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8656411143191044}
episode index:4768
target Thresh 17.423702940020192
target distance 10.0
model initialize at round 4768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([12.1546731 , 10.57139051,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.2325779132312}
done in step count: 5
reward sum = 0.9363614932203104
running average episode reward sum: 0.8033516090728164
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.62892436, 6.54371675, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8313686075361975}
episode index:4769
target Thresh 17.424490891545883
target distance 3.0
model initialize at round 4769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.65856326, 10.73494005,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.6796097317664662}
done in step count: 1
reward sum = 0.9852555367490549
running average episode reward sum: 0.8033897440681365
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.65856326, 10.89811923,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.3563127026420482}
episode index:4770
target Thresh 17.42527844919429
target distance 11.0
model initialize at round 4770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([12.,  9.,  0.]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 9.219544457292903}
done in step count: 31
reward sum = 0.6584952975888365
running average episode reward sum: 0.8033593742407461
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.86260965, 6.20458012, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.1733661800207027}
episode index:4771
target Thresh 17.426065613162304
target distance 5.0
model initialize at round 4771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([10.99505776,  8.1325968 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.6847858804279876}
done in step count: 1
reward sum = 0.9780600004086888
running average episode reward sum: 0.8033959837600605
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.20442743,  6.89597291,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.198208321029659}
episode index:4772
target Thresh 17.42685238364671
target distance 2.0
model initialize at round 4772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.87614935, 2.81252144, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.4757177996161128}
done in step count: 1
reward sum = 0.9855624418182625
running average episode reward sum: 0.8034341497894043
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.8764569 , 3.66137293, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9395983174097918}
episode index:4773
target Thresh 17.427638760844204
target distance 6.0
model initialize at round 4773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([10.970748  , 11.85948944,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 4.437631427686266}
done in step count: 2
reward sum = 0.9692926167555956
running average episode reward sum: 0.8034688918227026
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.97086445, 10.574018  ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5747569465961199}
episode index:4774
target Thresh 17.428424744951382
target distance 8.0
model initialize at round 4774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.03824949,  5.        ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.512370817547254}
done in step count: 5
reward sum = 0.9313851900364334
running average episode reward sum: 0.8034956805762552
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.99022378, 11.27584211,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0279260729054949}
episode index:4775
target Thresh 17.429210336164736
target distance 9.0
model initialize at round 4775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([7.99999999, 8.37661798, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 8.255591139899147}
done in step count: 13
reward sum = 0.8329012140031645
running average episode reward sum: 0.8035018375137399
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.42266222,  3.22123642,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9694285016642352}
episode index:4776
target Thresh 17.429995534680664
target distance 11.0
model initialize at round 4776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([3.05892992, 9.        , 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.140260072185697}
done in step count: 12
reward sum = 0.8476310805891072
running average episode reward sum: 0.8035110753707789
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.06945247, 11.86821499,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.2726806200947063}
episode index:4777
target Thresh 17.430780340695474
target distance 10.0
model initialize at round 4777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([12.05521678,  8.045681  ,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.505020465526599}
done in step count: 34
reward sum = 0.632897748768362
running average episode reward sum: 0.8034753672655879
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.91260922, 3.50969834, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0452980400590963}
episode index:4778
target Thresh 17.431564754405354
target distance 6.0
model initialize at round 4778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([9.09842074, 8.95272671, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 6.403426983481501}
done in step count: 4
reward sum = 0.9464609074296172
running average episode reward sum: 0.8035052868178298
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.97185845, 6.07225905, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.3435818228504521}
episode index:4779
target Thresh 17.432348776006418
target distance 4.0
model initialize at round 4779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([15.40369368,  5.69953525,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 4.926635804309205}
done in step count: 27
reward sum = 0.7119570608990924
running average episode reward sum: 0.8034861344693114
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.61884376,  9.70481823,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.6856382948190386}
episode index:4780
target Thresh 17.433132405694668
target distance 12.0
model initialize at round 4780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.2347782 , 11.86917915,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 11.595362441565431}
done in step count: 12
reward sum = 0.8406714067105558
running average episode reward sum: 0.8034939121878307
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.83659269, 9.64617156, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.6665130367125909}
episode index:4781
target Thresh 17.43391564366601
target distance 5.0
model initialize at round 4781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.05725539,  6.9850446 ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 4.985373389145636}
done in step count: 9
reward sum = 0.9026374506404342
running average episode reward sum: 0.8035146448391173
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.85412438,  2.9962087 ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.3122348289973649}
episode index:4782
target Thresh 17.434698490116254
target distance 7.0
model initialize at round 4782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 8., 11.,  0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 5.830951894845307}
done in step count: 12
reward sum = 0.8623183831615584
running average episode reward sum: 0.803526939160322
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.39981674, 7.52790505, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7636056532958972}
episode index:4783
target Thresh 17.435480945241114
target distance 8.0
model initialize at round 4783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([8., 9., 0.]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 8.485281374238598}
done in step count: 3
reward sum = 0.9466566068092447
running average episode reward sum: 0.8035568575691115
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.79557239, 3.89597299, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9189985001164133}
episode index:4784
target Thresh 17.4362630092362
target distance 13.0
model initialize at round 4784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([2.14899778, 3.03325176, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 13.449288068330379}
done in step count: 57
reward sum = 0.47550157694210243
running average episode reward sum: 0.8034882984718018
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.98414466,  7.06193877,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.06393592557407816}
episode index:4785
target Thresh 17.437044682297035
target distance 4.0
model initialize at round 4785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([11.        , 11.21213478,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 2.982203929688186}
done in step count: 7
reward sum = 0.9174678085170129
running average episode reward sum: 0.8035121136640386
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.00433524, 9.29732951, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.29736111047479635}
episode index:4786
target Thresh 17.43782596461903
target distance 1.0
model initialize at round 4786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.88880122, 10.11462963,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.8961626831671151}
done in step count: 0
reward sum = 0.9963925233893516
running average episode reward sum: 0.8035524062083722
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([ 9.88880122, 10.11462963,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.8961626831671151}
episode index:4787
target Thresh 17.438606856397506
target distance 1.0
model initialize at round 4787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.99505755, 11.86740305,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.175463780705619}
done in step count: 2
reward sum = 0.9677310082921284
running average episode reward sum: 0.803586695807805
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.49152377, 11.89786972,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0318517903777569}
episode index:4788
target Thresh 17.439387357827687
target distance 2.0
model initialize at round 4788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([ 5.10997665, 11.18637121,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 2.1891354273373707}
done in step count: 17
reward sum = 0.8140105854094111
running average episode reward sum: 0.8035888724395864
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.86940103, 8.07294471, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.2709404623083276}
episode index:4789
target Thresh 17.440167469104704
target distance 5.0
model initialize at round 4789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([10.        ,  9.17116094,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 3.112390430749809}
done in step count: 21
reward sum = 0.7726619480240597
running average episode reward sum: 0.8035824158791657
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.41940877,  9.74479995,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.4909488573758855}
episode index:4790
target Thresh 17.440947190423575
target distance 6.0
model initialize at round 4790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([9.98491552, 8.14507339, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 4.175176223113076}
done in step count: 7
reward sum = 0.9127362797227221
running average episode reward sum: 0.8036051989857913
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.57829754,  7.17225754,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.45552785734565776}
episode index:4791
target Thresh 17.441726521979238
target distance 3.0
model initialize at round 4791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([1.56436396, 7.83006978, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.733847599766445}
done in step count: 31
reward sum = 0.6820681152486937
running average episode reward sum: 0.8035798364891851
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.88757338, 5.30278548, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9377982493192946}
episode index:4792
target Thresh 17.442505463966526
target distance 3.0
model initialize at round 4792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0000000000000195}
done in step count: 1
reward sum = 0.9796731371852572
running average episode reward sum: 0.8036165761722012
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.47952885, 11.9103569 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0289303189559895}
episode index:4793
target Thresh 17.44328401658017
target distance 9.0
model initialize at round 4793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([4.12548304, 9.93152595, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 8.21138124576085}
done in step count: 10
reward sum = 0.8870347296723797
running average episode reward sum: 0.8036339767048463
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.22098816, 1.66622997, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.40029763383770856}
episode index:4794
target Thresh 17.444062180014807
target distance 6.0
model initialize at round 4794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([13.        ,  9.89169431,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 4.001465996739878}
done in step count: 2
reward sum = 0.9679507810526071
running average episode reward sum: 0.8036682450686309
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([9.        , 9.24715012, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.752849876880644}
episode index:4795
target Thresh 17.444839954464985
target distance 2.0
model initialize at round 4795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.13172534, 7.06670044, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.119327269854606}
done in step count: 1
reward sum = 0.9837127248730363
running average episode reward sum: 0.8037057856190488
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.15417109, 9.04205628, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8468738299933447}
episode index:4796
target Thresh 17.445617340125143
target distance 12.0
model initialize at round 4796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([2.63952053, 9.64329219, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 12.684921781671667}
done in step count: 41
reward sum = 0.5838895334223364
running average episode reward sum: 0.8036599619267001
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.19816005,  3.61000726,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.43744913125292756}
episode index:4797
target Thresh 17.446394337189627
target distance 4.0
model initialize at round 4797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([12.84292221, 10.97630537,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 2.843020951691171}
done in step count: 27
reward sum = 0.6996930135315662
running average episode reward sum: 0.8036382931171138
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.08476213, 10.96056835,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.09348515139954486}
episode index:4798
target Thresh 17.447170945852687
target distance 5.0
model initialize at round 4798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([11.65603304, 11.08101046,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 3.8125006491035194}
done in step count: 4
reward sum = 0.9482860224199782
running average episode reward sum: 0.80366843434014
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.33883558, 9.36337341, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.721181647148485}
episode index:4799
target Thresh 17.447947166308474
target distance 5.0
model initialize at round 4799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([10.        , 10.53224289,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 3.0468479611554926}
done in step count: 2
reward sum = 0.9734724693950695
running average episode reward sum: 0.8037038101807764
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.20740628,  9.17908376,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.1410995866969809}
episode index:4800
target Thresh 17.448722998751045
target distance 5.0
model initialize at round 4800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.85920658,  5.03052363,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 3.149969754579856}
done in step count: 3
reward sum = 0.9568126259004489
running average episode reward sum: 0.8037357012067542
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.14666029,  1.15062509,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.204004318592097}
episode index:4801
target Thresh 17.44949844337436
target distance 4.0
model initialize at round 4801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.91438118,  6.        ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 2.0018318068667162}
done in step count: 1
reward sum = 0.9845186245206748
running average episode reward sum: 0.8037733486293519
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.98917273,  4.17546678,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.17580050991431198}
episode index:4802
target Thresh 17.450273500372276
target distance 8.0
model initialize at round 4802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([9.98096595, 8.1374455 , 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 7.303918547991555}
done in step count: 3
reward sum = 0.9503948074944762
running average episode reward sum: 0.8038038756872042
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.34473543,  3.95218484,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6570068122385491}
episode index:4803
target Thresh 17.45104816993856
target distance 13.0
model initialize at round 4803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.1319398 ,  6.04914014,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.301590063788755}
done in step count: 50
reward sum = 0.520745531115747
running average episode reward sum: 0.8037449542999079
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.03539938, 8.66149842, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6624449252910886}
episode index:4804
target Thresh 17.451822452266878
target distance 4.0
model initialize at round 4804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.51788688,  4.35007417,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 2.70005809404659}
done in step count: 5
reward sum = 0.9444847840704311
running average episode reward sum: 0.8037742445870609
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.9575634 ,  6.58795589,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.4142236254288532}
episode index:4805
target Thresh 17.4525963475508
target distance 9.0
model initialize at round 4805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.43682028,  3.78907931,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.224139338469965}
done in step count: 23
reward sum = 0.7522442600431513
running average episode reward sum: 0.8037635225761279
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.42735055, 10.43993647,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8009984754405525}
episode index:4806
target Thresh 17.4533698559838
target distance 7.0
model initialize at round 4806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([5.74401915, 9.90492344, 0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 6.321090318677778}
done in step count: 4
reward sum = 0.9444519201752815
running average episode reward sum: 0.8037927899773343
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.69617014,  8.38520892,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.9287738865036773}
episode index:4807
target Thresh 17.45414297775926
target distance 1.0
model initialize at round 4807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.17094946, 4.52055502, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.2653023848682505}
done in step count: 1
reward sum = 0.9867045283233341
running average episode reward sum: 0.8038308331841449
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.51846135, 4.7417832 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.5464022232935479}
episode index:4808
target Thresh 17.454915713070452
target distance 8.0
model initialize at round 4808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([12.21226168,  8.00000031,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 6.2606718458626895}
done in step count: 3
reward sum = 0.9556103752489568
running average episode reward sum: 0.8038623947441501
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.92261712,  2.00133299,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.07739435875437453}
episode index:4809
target Thresh 17.455688062110568
target distance 10.0
model initialize at round 4809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([2.51084447, 6.        , 0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 9.95208886064484}
done in step count: 22
reward sum = 0.7501035943439263
running average episode reward sum: 0.8038512182783704
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.15541875,  8.13234429,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.2108443052660605}
episode index:4810
target Thresh 17.456460025072687
target distance 12.0
model initialize at round 4810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.6510303 ,  8.37680048,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.173505625362889}
done in step count: 9
reward sum = 0.8816909640831827
running average episode reward sum: 0.8038673978139773
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.48844211, 4.02307285, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0922281585146116}
episode index:4811
target Thresh 17.457231602149808
target distance 8.0
model initialize at round 4811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([2.23678458, 8.        , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 6.25371318710745}
done in step count: 8
reward sum = 0.906100889842079
running average episode reward sum: 0.803888643344324
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.84655857, 2.38439685, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.41389033126530284}
episode index:4812
target Thresh 17.45800279353482
target distance 4.0
model initialize at round 4812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.62525058, 11.39611673,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 4.440358163504868}
done in step count: 18
reward sum = 0.8163695201051426
running average episode reward sum: 0.8038912365038421
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.91315349,  6.47229381,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.0546672982991823}
episode index:4813
target Thresh 17.458773599420525
target distance 8.0
model initialize at round 4813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.16659248,  5.44349313,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.877651610305163}
done in step count: 15
reward sum = 0.8302866488299583
running average episode reward sum: 0.8038967195558416
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.01314012, 11.10121292,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.102062322183076}
episode index:4814
target Thresh 17.45954401999962
target distance 6.0
model initialize at round 4814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([9.80965322, 8.122693  , 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 5.878401460432068}
done in step count: 3
reward sum = 0.9509280621626285
running average episode reward sum: 0.8039272556602253
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.30079974,  4.71757211,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.778068264061265}
episode index:4815
target Thresh 17.46031405546471
target distance 9.0
model initialize at round 4815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.95791137,  1.84132433,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 9.977294346151426}
done in step count: 5
reward sum = 0.9338365564674583
running average episode reward sum: 0.8039542301828182
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.8417276 , 10.54249021,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9580295200670198}
episode index:4816
target Thresh 17.461083706008306
target distance 13.0
model initialize at round 4816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.1325968 ,  5.99505776,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.831072388060079}
done in step count: 37
reward sum = 0.6071897827299539
running average episode reward sum: 0.8039133822593278
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.10655245, 9.34391619, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.1084649237106854}
episode index:4817
target Thresh 17.46185297182282
target distance 5.0
model initialize at round 4817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([11.        , 10.62622416,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 3.4124192326265885}
done in step count: 2
reward sum = 0.9707923345293942
running average episode reward sum: 0.8039480188206126
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.        , 9.74058253, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.2443723226657577}
episode index:4818
target Thresh 17.462621853100572
target distance 5.0
model initialize at round 4818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([3.65707231, 6.        , 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 3.806482652110065}
done in step count: 2
reward sum = 0.9707888020408362
running average episode reward sum: 0.8039826402738643
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.70750391, 9.51692915, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.5939441939885607}
episode index:4819
target Thresh 17.46339035003378
target distance 9.0
model initialize at round 4819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([3.32033992, 4.        , 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 9.745623580508646}
done in step count: 7
reward sum = 0.901929001309719
running average episode reward sum: 0.8040029610956559
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.89282506, 10.49126137,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.0190556005574234}
episode index:4820
target Thresh 17.464158462814567
target distance 8.0
model initialize at round 4820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.0827625302982415}
done in step count: 3
reward sum = 0.9541967370606993
running average episode reward sum: 0.8040341151665883
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.50114096, 11.73384956,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.88735309759339}
episode index:4821
target Thresh 17.46492619163496
target distance 7.0
model initialize at round 4821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([5.69481099, 9.33147025, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.594368536145325}
done in step count: 4
reward sum = 0.9453337894843215
running average episode reward sum: 0.8040634182927429
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.65568817, 3.67412955, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.47406981722078906}
episode index:4822
target Thresh 17.465693536686892
target distance 13.0
model initialize at round 4822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([5.       , 9.8178139, 0.       ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.44375178999592}
done in step count: 8
reward sum = 0.8987435116640284
running average episode reward sum: 0.8040830492472052
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.5067473 ,  4.24184936,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.5615015065067585}
episode index:4823
target Thresh 17.466460498162203
target distance 8.0
model initialize at round 4823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 9.        , 10.26564288,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 7.361773508576802}
done in step count: 3
reward sum = 0.9490667857791517
running average episode reward sum: 0.8041131039189573
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.20578533,  5.11708471,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9065798423999192}
episode index:4824
target Thresh 17.467227076252627
target distance 11.0
model initialize at round 4824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.60112035,  3.20203209,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.669050279988868}
done in step count: 27
reward sum = 0.7124532936335101
running average episode reward sum: 0.8040941070670846
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.16270145, 1.1782974 , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.173142796159366}
episode index:4825
target Thresh 17.467993271149815
target distance 10.0
model initialize at round 4825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([12.93474687,  6.90427302,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.192232116566327}
done in step count: 10
reward sum = 0.8787897969516584
running average episode reward sum: 0.8041095848312547
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.85386209, 2.75232275, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.1380114175984521}
episode index:4826
target Thresh 17.468759083045313
target distance 4.0
model initialize at round 4826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.31870818, 6.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.3956191825166724}
done in step count: 1
reward sum = 0.982440369238075
running average episode reward sum: 0.8041465292655632
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.69874012, 8.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6987401247024652}
episode index:4827
target Thresh 17.46952451213057
target distance 12.0
model initialize at round 4827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([12.58737457,  9.        ,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 10.774622974509441}
done in step count: 6
reward sum = 0.9101797868976151
running average episode reward sum: 0.8041684914150313
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.85617787, 7.48322886, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5041774855612672}
episode index:4828
target Thresh 17.47028955859695
target distance 9.0
model initialize at round 4828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.06374943, 2.75980461, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.464625590301262}
done in step count: 7
reward sum = 0.9160429231546687
running average episode reward sum: 0.8041916586197817
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.18534051, 11.08321251,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8188982880687594}
episode index:4829
target Thresh 17.47105422263571
target distance 11.0
model initialize at round 4829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([3.18075812, 6.        , 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 11.018961435855076}
done in step count: 25
reward sum = 0.7206420112871825
running average episode reward sum: 0.8041743605561518
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.10354183, 11.305486  ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.32255636664040616}
episode index:4830
target Thresh 17.47181850443802
target distance 1.0
model initialize at round 4830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.62961066, 10.27598846,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.959480219144915}
done in step count: 0
reward sum = 0.9976151323085589
running average episode reward sum: 0.804214402115198
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.62961066, 10.27598846,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.959480219144915}
episode index:4831
target Thresh 17.47258240419495
target distance 8.0
model initialize at round 4831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([16.12529397,  5.        ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.281349486174261}
done in step count: 3
reward sum = 0.9563390602200702
running average episode reward sum: 0.8042458848672892
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.94658875, 10.8341223 ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9610128416132736}
episode index:4832
target Thresh 17.47334592209747
target distance 8.0
model initialize at round 4832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([15.6284416,  4.       ,  0.       ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 6.5504736664671785}
done in step count: 3
reward sum = 0.9550243703628799
running average episode reward sum: 0.804277082567578
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.15903991, 10.        ,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.15903991460806566}
episode index:4833
target Thresh 17.47410905833646
target distance 4.0
model initialize at round 4833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.65336524,  7.00000003,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.104016696587935}
done in step count: 1
reward sum = 0.9837087734536862
running average episode reward sum: 0.8043142012458747
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.61554856,  4.99999986,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6155485567076853}
episode index:4834
target Thresh 17.47487181310271
target distance 12.0
model initialize at round 4834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.28412998,  6.        ,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 11.034642241073572}
done in step count: 44
reward sum = 0.5430298531847418
running average episode reward sum: 0.8042601610497918
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.59006942, 2.93673779, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.022507097346331}
episode index:4835
target Thresh 17.475634186586905
target distance 10.0
model initialize at round 4835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12., 10.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.062257748298572}
done in step count: 6
reward sum = 0.9185521776863752
running average episode reward sum: 0.8042837946347042
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.62503686, 11.17253749,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.4127548246224739}
episode index:4836
target Thresh 17.47639617897964
target distance 7.0
model initialize at round 4836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([12.43983848,  7.564652  ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.779226229547077}
done in step count: 18
reward sum = 0.7957409372170253
running average episode reward sum: 0.804282028486799
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.12478272,  2.73261927,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.1413747297943468}
episode index:4837
target Thresh 17.47715779047141
target distance 11.0
model initialize at round 4837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.00012256,  6.9928481 ,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.054719952870572}
done in step count: 8
reward sum = 0.9029965671549143
running average episode reward sum: 0.8043024324840433
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.62578811, 6.68383044, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7795246069583978}
episode index:4838
target Thresh 17.477919021252617
target distance 1.0
model initialize at round 4838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 1.19786994, 10.37905689,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.8415637100888753}
done in step count: 7
reward sum = 0.9168967460826728
running average episode reward sum: 0.8043257005794348
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.57984658, 9.77574436, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.4762557000103775}
episode index:4839
target Thresh 17.478679871513577
target distance 2.0
model initialize at round 4839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.        , 10.76710713,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.23289287090309152}
done in step count: 0
reward sum = 0.996558660336277
running average episode reward sum: 0.8043654181331036
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.        , 10.76710713,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.23289287090309152}
episode index:4840
target Thresh 17.47944034144449
target distance 10.0
model initialize at round 4840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 5.33294475, 11.05913246,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.970100594135191}
done in step count: 11
reward sum = 0.8746654066416076
running average episode reward sum: 0.8043799399237478
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.29572459,  4.56470974,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.6374559780207906}
episode index:4841
target Thresh 17.480200431235488
target distance 6.0
model initialize at round 4841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([12.        ,  9.01819859,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.118729660291886}
done in step count: 6
reward sum = 0.927032333786489
running average episode reward sum: 0.8044052708601093
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.62800542,  9.80951098,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.4179306599383931}
episode index:4842
target Thresh 17.480960141076583
target distance 12.0
model initialize at round 4842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.305877894274122}
done in step count: 32
reward sum = 0.6482457435794541
running average episode reward sum: 0.8043730264811539
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.54672764, 1.69952572, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5438204016752227}
episode index:4843
target Thresh 17.481719471157703
target distance 1.0
model initialize at round 4843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.46074319, 2.12620428, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5538279771181442}
done in step count: 0
reward sum = 0.9982110707200622
running average episode reward sum: 0.8044130425926813
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.46074319, 2.12620428, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5538279771181442}
episode index:4844
target Thresh 17.482478421668688
target distance 9.0
model initialize at round 4844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.85085893,  4.        ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.558266773564004}
done in step count: 47
reward sum = 0.5314022655638789
running average episode reward sum: 0.8043566936190943
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.25355712, 11.37998397,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.45681399893853103}
episode index:4845
target Thresh 17.483236992799267
target distance 1.0
model initialize at round 4845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.18689096, 9.09578788, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.21000845060292483}
done in step count: 0
reward sum = 0.998330267679037
running average episode reward sum: 0.8043967211828706
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.18689096, 9.09578788, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.21000845060292483}
episode index:4846
target Thresh 17.483995184739086
target distance 10.0
model initialize at round 4846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([5.10625243, 9.72126722, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.985204689915813}
done in step count: 5
reward sum = 0.9351284124486048
running average episode reward sum: 0.8044236928542686
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.8921836, 10.3849138,  0.       ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.6244640954434472}
episode index:4847
target Thresh 17.484752997677692
target distance 13.0
model initialize at round 4847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 2.97431396, 11.8526766 ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 12.627756795961163}
done in step count: 30
reward sum = 0.6613817881006443
running average episode reward sum: 0.8043941875108788
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.35983933,  8.25794131,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.44273927091794846}
episode index:4848
target Thresh 17.485510431804542
target distance 11.0
model initialize at round 4848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.88164888, 5.75866155, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.677324117759733}
done in step count: 5
reward sum = 0.9254365844638971
running average episode reward sum: 0.8044191498530014
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.83427837,  9.25437215,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.30359323537917854}
episode index:4849
target Thresh 17.486267487308993
target distance 12.0
model initialize at round 4849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([15.74874973, 10.90622562,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 15.55157466556237}
done in step count: 29
reward sum = 0.7092988670996501
running average episode reward sum: 0.8043995374235678
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.9892452 , 2.29044012, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0310002522243011}
episode index:4850
target Thresh 17.48702416438031
target distance 9.0
model initialize at round 4850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([14.01201749, 11.        ,  0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 9.231276142151955}
done in step count: 25
reward sum = 0.7022339413705961
running average episode reward sum: 0.8043784766946351
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.2486797 , 9.21408928, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.32813992560880556}
episode index:4851
target Thresh 17.487780463207656
target distance 1.0
model initialize at round 4851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.09699803, 10.41423071,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9934785557394045}
done in step count: 0
reward sum = 0.9955304627288069
running average episode reward sum: 0.804417873229267
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.09699803, 10.41423071,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9934785557394045}
episode index:4852
target Thresh 17.488536383980108
target distance 3.0
model initialize at round 4852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.10034672,  4.54418447,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.711366383784365}
done in step count: 1
reward sum = 0.9839129579213908
running average episode reward sum: 0.8044548596468833
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.09830877,  5.61030988,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9822960161715425}
episode index:4853
target Thresh 17.48929192688665
target distance 6.0
model initialize at round 4853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([7.36880755, 8.65935016, 0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 5.788581031932031}
done in step count: 5
reward sum = 0.9376350870050579
running average episode reward sum: 0.8044822968589473
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.76878971, 10.6377032 ,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.9988508361422}
episode index:4854
target Thresh 17.490047092116168
target distance 7.0
model initialize at round 4854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([15.5997858,  6.       ,  0.       ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.793970080120051}
done in step count: 18
reward sum = 0.7999883464569565
running average episode reward sum: 0.8044813712254968
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.61513355, 11.89617829,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0869796728314518}
episode index:4855
target Thresh 17.49080187985745
target distance 7.0
model initialize at round 4855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 9.99319496, 11.8534222 ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 8.816503092306078}
done in step count: 99
reward sum = -0.16267185981156848
running average episode reward sum: 0.8042822045798961
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([5.66622853, 7.68014545, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.110171098410849}
episode index:4856
target Thresh 17.491556290299194
target distance 3.0
model initialize at round 4856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([11.00494981,  8.13259643,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.327521399710712}
done in step count: 1
reward sum = 0.983138111208231
running average episode reward sum: 0.8043190289378594
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.01073237,  8.16423115,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.8358377543502146}
episode index:4857
target Thresh 17.492310323630004
target distance 3.0
model initialize at round 4857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.29094797,  9.        ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0414656611797029}
done in step count: 1
reward sum = 0.9839651210851047
running average episode reward sum: 0.8043560083722249
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.33422476,  7.        ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0543747865691995}
episode index:4858
target Thresh 17.493063980038382
target distance 9.0
model initialize at round 4858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.63895565,  9.        ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 7.131089798570752}
done in step count: 4
reward sum = 0.9452739135715433
running average episode reward sum: 0.8043850097933402
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.72998047,  2.64920473,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.7031197124196528}
episode index:4859
target Thresh 17.493817259712753
target distance 8.0
model initialize at round 4859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([14.37175477,  9.        ,  0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 6.3717547655106355}
done in step count: 8
reward sum = 0.8974312986366539
running average episode reward sum: 0.8044041551202625
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.44006496, 8.11101138, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.050632196576158}
episode index:4860
target Thresh 17.49457016284143
target distance 10.0
model initialize at round 4860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([4.14637876, 7.88603497, 0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 9.102497311554467}
done in step count: 5
reward sum = 0.9333582696934759
running average episode reward sum: 0.8044306834301932
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.20596455,  9.53162939,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.9218803197754696}
episode index:4861
target Thresh 17.49532268961264
target distance 5.0
model initialize at round 4861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([2.65751851, 2.30461717, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 8.56564415558261}
done in step count: 31
reward sum = 0.6464739715410428
running average episode reward sum: 0.8043981954186982
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.29021566, 8.129847  , 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.122924780641745}
episode index:4862
target Thresh 17.49607484021451
target distance 12.0
model initialize at round 4862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.12353202,  3.7990002 ,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.381401475619553}
done in step count: 34
reward sum = 0.6290602804687513
running average episode reward sum: 0.8043621399149042
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.48984403, 8.19803515, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9397312412138512}
episode index:4863
target Thresh 17.496826614835086
target distance 4.0
model initialize at round 4863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([4.        , 6.50736737, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 2.6857433636916044}
done in step count: 1
reward sum = 0.9807997678190659
running average episode reward sum: 0.8043984140982726
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'previousTarget': array([5., 9.]), 'currentState': array([5.43365717, 8.50736737, 0.        ]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.6563120036893063}
episode index:4864
target Thresh 17.49757801366231
target distance 5.0
model initialize at round 4864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([13.00080441,  7.23335035,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.7393887178869907}
done in step count: 3
reward sum = 0.9570139087886733
running average episode reward sum: 0.8044297841896786
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.93972305,  4.73404703,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.272698173735222}
episode index:4865
target Thresh 17.498329036884027
target distance 6.0
model initialize at round 4865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.81553012, 4.68114531, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 4.684939215326806}
done in step count: 5
reward sum = 0.9338334279578872
running average episode reward sum: 0.8044563776224299
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.13276779, 8.8555773 , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8791755357659611}
episode index:4866
target Thresh 17.49907968468799
target distance 10.0
model initialize at round 4866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 5.52295451, 11.912613  ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.52088513743043}
done in step count: 6
reward sum = 0.9091877785704235
running average episode reward sum: 0.8044778962994277
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.36277795, 11.84639824,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9208679752042854}
episode index:4867
target Thresh 17.499829957261877
target distance 11.0
model initialize at round 4867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([3.38511777, 6.        , 0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 10.413739016864167}
done in step count: 25
reward sum = 0.7254168488444619
running average episode reward sum: 0.8044616553282989
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.58011563,  9.95611248,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.42217176135699136}
episode index:4868
target Thresh 17.50057985479324
target distance 11.0
model initialize at round 4868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.72953963,  8.75279987,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.985682244934944}
done in step count: 43
reward sum = 0.5185177829000289
running average episode reward sum: 0.8044029278950624
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.72964058, 11.50773613,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5752305593151988}
episode index:4869
target Thresh 17.50132937746956
target distance 3.0
model initialize at round 4869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.0000000000000178}
done in step count: 1
reward sum = 0.980429259126011
running average episode reward sum: 0.8044390729322762
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.00176319,  8.14882915,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 1.3118569067229462}
episode index:4870
target Thresh 17.502078525478215
target distance 14.0
model initialize at round 4870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([2.93328881, 5.8826071 , 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.70006808476526}
done in step count: 12
reward sum = 0.8496534369828943
running average episode reward sum: 0.8044483552899134
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.56304873,  9.65757119,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5551431358296876}
episode index:4871
target Thresh 17.502827299006494
target distance 2.0
model initialize at round 4871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.19575748,  4.98857406,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 2.06839877235027}
done in step count: 1
reward sum = 0.9863192508621437
running average episode reward sum: 0.8044856851124856
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.85106033,  5.74928576,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.29161730847238987}
episode index:4872
target Thresh 17.50357569824159
target distance 6.0
model initialize at round 4872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([ 5.23716378, 10.        ,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 4.762836217880282}
done in step count: 3
reward sum = 0.9569875368392008
running average episode reward sum: 0.8045169803826944
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.8071686 ,  9.68103567,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.8679051785097366}
episode index:4873
target Thresh 17.504323723370604
target distance 6.0
model initialize at round 4873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4., 8., 0.]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 4.000000000000018}
done in step count: 5
reward sum = 0.9291687919167753
running average episode reward sum: 0.804542555231183
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.9067431 , 4.48129216, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0265598811745165}
episode index:4874
target Thresh 17.505071374580538
target distance 12.0
model initialize at round 4874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.8789269 , 5.79329966, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.57416062016982}
done in step count: 18
reward sum = 0.773764958008603
running average episode reward sum: 0.8045362418779065
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.09098257,  9.55735879,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0662839774730755}
episode index:4875
target Thresh 17.50581865205831
target distance 10.0
model initialize at round 4875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([12.26993598, 11.88629726,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.15087861283725}
done in step count: 22
reward sum = 0.762523293109685
running average episode reward sum: 0.8045276256045742
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.12231192, 6.76134463, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7711069014585593}
episode index:4876
target Thresh 17.50656555599074
target distance 2.0
model initialize at round 4876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.23662353, 6.45525801, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8888214100492666}
done in step count: 0
reward sum = 0.9958782064353436
running average episode reward sum: 0.804566860909235
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.23662353, 6.45525801, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8888214100492666}
episode index:4877
target Thresh 17.507312086564546
target distance 13.0
model initialize at round 4877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       , 10.5002749,  0.       ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.31368181702167}
done in step count: 8
reward sum = 0.895165387914629
running average episode reward sum: 0.804585433793
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.37669772, 3.73592289, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8267307151512114}
episode index:4878
target Thresh 17.50805824396637
target distance 3.0
model initialize at round 4878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([12.,  9.,  0.]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.4142135623731213}
done in step count: 1
reward sum = 0.9785758622593076
running average episode reward sum: 0.8046210948769241
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.18196893, 11.        ,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.291965493256726}
episode index:4879
target Thresh 17.508804028382745
target distance 12.0
model initialize at round 4879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.0430029 ,  6.63382548,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.06117750783793}
done in step count: 7
reward sum = 0.8977367760708392
running average episode reward sum: 0.8046401759591361
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.11248229, 6.2768563 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9296973130901147}
episode index:4880
target Thresh 17.509549440000118
target distance 6.0
model initialize at round 4880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([9.68358529, 9.        , 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 6.653139515277111}
done in step count: 8
reward sum = 0.9011224222245278
running average episode reward sum: 0.804659942860645
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.56733094,  5.63832286,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8540026170972363}
episode index:4881
target Thresh 17.51029447900485
target distance 9.0
model initialize at round 4881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([11.50435972, 11.        ,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 8.503847061138774}
done in step count: 43
reward sum = 0.582728416690568
running average episode reward sum: 0.8046144837196844
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.98381669, 6.73656453, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0184760847252852}
episode index:4882
target Thresh 17.511039145583187
target distance 3.0
model initialize at round 4882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([7., 9., 0.]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 2.236067977499781}
done in step count: 7
reward sum = 0.9191478918565107
running average episode reward sum: 0.804637939260978
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.93202782, 10.36092061,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.1300877490092363}
episode index:4883
target Thresh 17.511783439921302
target distance 12.0
model initialize at round 4883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.        , 4.17795897, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.390568588942694}
done in step count: 9
reward sum = 0.872937775179926
running average episode reward sum: 0.8046519236663667
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.85365327,  6.21161231,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.16201508401767}
episode index:4884
target Thresh 17.512527362205272
target distance 1.0
model initialize at round 4884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.66583979,  9.42183352,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.8818271439798006}
done in step count: 0
reward sum = 0.9976680260688069
running average episode reward sum: 0.8046914356627644
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.66583979,  9.42183352,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.8818271439798006}
episode index:4885
target Thresh 17.513270912621074
target distance 4.0
model initialize at round 4885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([15.12825793,  8.        ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.296293961416355}
done in step count: 1
reward sum = 0.9829880891145174
running average episode reward sum: 0.8047279269958489
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.33098996, 10.        ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.3309899568558148}
episode index:4886
target Thresh 17.5140140913546
target distance 3.0
model initialize at round 4886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.        , 10.97328353,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0003568212431417}
done in step count: 1
reward sum = 0.9832256197730019
running average episode reward sum: 0.8047644519994865
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.        , 11.32893878,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0527111296291622}
episode index:4887
target Thresh 17.514756898591635
target distance 4.0
model initialize at round 4887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([14.        ,  9.17508435,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.9548251962613454}
done in step count: 1
reward sum = 0.9785622622231485
running average episode reward sum: 0.804800008016308
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.        ,  7.17508435,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.17508435249323373}
episode index:4888
target Thresh 17.515499334517894
target distance 5.0
model initialize at round 4888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([6.       , 9.7158047, 0.       ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 3.0134310960711486}
done in step count: 2
reward sum = 0.9702772848399971
running average episode reward sum: 0.8048338548718662
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([10.        , 10.91431594,  0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 1.3549810464420033}
episode index:4889
target Thresh 17.516241399318975
target distance 6.0
model initialize at round 4889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([14.25458503,  6.        ,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 5.156774544470059}
done in step count: 2
reward sum = 0.9636356910347484
running average episode reward sum: 0.8048663296849874
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.48356755, 10.00000007,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.5164324463038038}
episode index:4890
target Thresh 17.516983093180396
target distance 10.0
model initialize at round 4890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([5.62212188, 8.00001376, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.898806617080359}
done in step count: 18
reward sum = 0.793293431166045
running average episode reward sum: 0.8048639635229512
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.48288068, 10.26407255,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8994451604956902}
episode index:4891
target Thresh 17.517724416287585
target distance 7.0
model initialize at round 4891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([13.70581448,  7.        ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 5.164776486756723}
done in step count: 3
reward sum = 0.9579596582266386
running average episode reward sum: 0.8048952586363411
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.53544945,  2.13778675,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5528935664268504}
episode index:4892
target Thresh 17.518465368825872
target distance 6.0
model initialize at round 4892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 6.60972574, 11.90354969,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 4.482288495344158}
done in step count: 2
reward sum = 0.9676963028041209
running average episode reward sum: 0.8049285308709963
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.57065732, 11.76492208,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8771778229357895}
episode index:4893
target Thresh 17.519205950980492
target distance 1.0
model initialize at round 4893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 4.        , 10.43315381,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.4604734986826777}
done in step count: 17
reward sum = 0.8138950719512534
running average episode reward sum: 0.8049303630207878
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.923129 , 8.7992763, 0.       ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.21493988431760222}
episode index:4894
target Thresh 17.51994616293659
target distance 13.0
model initialize at round 4894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.08920002,  8.16530225,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.68784098554389}
done in step count: 18
reward sum = 0.781089649330979
running average episode reward sum: 0.8049254925991964
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.67904569, 1.1127651 , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9435027521705406}
episode index:4895
target Thresh 17.520686004879224
target distance 3.0
model initialize at round 4895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.40457612, 7.        , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1638425982866538}
done in step count: 1
reward sum = 0.983714756392393
running average episode reward sum: 0.8049620100141869
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.14677793, 5.        , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.31452953727191}
episode index:4896
target Thresh 17.52142547699335
target distance 7.0
model initialize at round 4896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.11190632,  4.70713253,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.517177303723878}
done in step count: 8
reward sum = 0.9060748447583451
running average episode reward sum: 0.8049826579281636
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.5675775 , 10.46970065,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7767635488870456}
episode index:4897
target Thresh 17.522164579463837
target distance 13.0
model initialize at round 4897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([16.88340388,  4.44942234,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 14.115748290961115}
done in step count: 24
reward sum = 0.7492472423145664
running average episode reward sum: 0.8049712787089693
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.06985134, 7.20370076, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.21534439802764171}
episode index:4898
target Thresh 17.52290331247546
target distance 6.0
model initialize at round 4898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([13.14572512,  5.98883494,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 5.0975515392423585}
done in step count: 6
reward sum = 0.9254925481401306
running average episode reward sum: 0.8049958799070569
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.14221109,  9.03214097,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.9782510419050063}
episode index:4899
target Thresh 17.523641676212907
target distance 12.0
model initialize at round 4899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.19054699,  9.16380912,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.424192425662413}
done in step count: 25
reward sum = 0.7210439118611598
running average episode reward sum: 0.8049787468523537
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.88393262, 4.08695   , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.14502393212156794}
episode index:4900
target Thresh 17.524379670860764
target distance 1.0
model initialize at round 4900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.44684076, 7.04241753, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 2.007933218290088}
done in step count: 4
reward sum = 0.9540401161127843
running average episode reward sum: 0.805009161332921
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.87946714, 8.049932  , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.2946395858805593}
episode index:4901
target Thresh 17.525117296603533
target distance 1.0
model initialize at round 4901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.93681765,  9.6540665 ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.044276298441433}
done in step count: 7
reward sum = 0.9165086693630456
running average episode reward sum: 0.8050319070505934
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.13339121,  9.71766129,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7299526947923342}
episode index:4902
target Thresh 17.525854553625614
target distance 12.0
model initialize at round 4902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.12745865,  5.16257486,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.223462065956802}
done in step count: 7
reward sum = 0.9008451673148953
running average episode reward sum: 0.8050514488128336
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.23469798, 10.72881594,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.05681590357646}
episode index:4903
target Thresh 17.526591442111325
target distance 5.0
model initialize at round 4903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([12.05521678,  8.045681  ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 4.171300636053921}
done in step count: 9
reward sum = 0.8996593118605212
running average episode reward sum: 0.8050707407914323
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.08487641, 10.75145854,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9482742473533927}
episode index:4904
target Thresh 17.527327962244897
target distance 5.0
model initialize at round 4904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.        ,  7.39931524,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.686675874967116}
done in step count: 5
reward sum = 0.9333687362042368
running average episode reward sum: 0.8050968973654206
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.54445928, 10.41038574,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.745092156100226}
episode index:4905
target Thresh 17.528064114210444
target distance 3.0
model initialize at round 4905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([14.35880065,  7.38771248,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.300653463508348}
done in step count: 21
reward sum = 0.7517681514321292
running average episode reward sum: 0.8050860272582185
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.72447668,  9.9838995 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.2218529770797}
episode index:4906
target Thresh 17.528799898192016
target distance 2.0
model initialize at round 4906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.        ,  9.93466139,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0653386116028223}
done in step count: 1
reward sum = 0.9865278506758351
running average episode reward sum: 0.8051230033787439
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.99645962, 10.72937322,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.27064994029372025}
episode index:4907
target Thresh 17.52953531437355
target distance 2.0
model initialize at round 4907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([14.22901839,  9.        ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.770981609821261}
done in step count: 31
reward sum = 0.6500884423354385
running average episode reward sum: 0.8050914152448719
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.06621117,  8.7657445 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.2434328640909353}
episode index:4908
target Thresh 17.53027036293891
target distance 13.0
model initialize at round 4908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 4.86798638, 11.87251157,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.151670508661383}
done in step count: 9
reward sum = 0.879864900702442
running average episode reward sum: 0.8051066471628711
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.8196937,  7.005401 ,  0.       ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8197114890078318}
episode index:4909
target Thresh 17.53100504407185
target distance 6.0
model initialize at round 4909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([4., 9., 0.]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 4.123105625617683}
done in step count: 28
reward sum = 0.6944368418496171
running average episode reward sum: 0.8050841074876545
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.49602631, 9.80294721, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.5337339292457425}
episode index:4910
target Thresh 17.53173935795605
target distance 6.0
model initialize at round 4910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([12.91029839,  7.15503692,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 4.650933735738345}
done in step count: 5
reward sum = 0.9342400644850924
running average episode reward sum: 0.8051104068069371
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.04658663,  3.72231691,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7238176831530386}
episode index:4911
target Thresh 17.532473304775074
target distance 5.0
model initialize at round 4911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 5.89083338, 11.        ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.017285697345845}
done in step count: 2
reward sum = 0.9700303238069771
running average episode reward sum: 0.8051439817086066
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.92017829, 10.53249413,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5384435911109076}
episode index:4912
target Thresh 17.53320688471242
target distance 8.0
model initialize at round 4912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 7.915692916515819}
done in step count: 6
reward sum = 0.9137015050775464
running average episode reward sum: 0.8051660776832389
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.65452387, 10.66609451,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9338540537668971}
episode index:4913
target Thresh 17.53394009795148
target distance 8.0
model initialize at round 4913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 2.33558619, 10.29274809,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 8.299535471040542}
done in step count: 20
reward sum = 0.7702784658831947
running average episode reward sum: 0.805158978047138
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.36637051, 2.26679425, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.6875067335777921}
episode index:4914
target Thresh 17.534672944675552
target distance 13.0
model initialize at round 4914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 2.79817283, 10.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 12.201827168464671}
done in step count: 10
reward sum = 0.8570700362741285
running average episode reward sum: 0.8051695398087304
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.0570025 , 10.14639517,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.95429336801725}
episode index:4915
target Thresh 17.535405425067857
target distance 4.0
model initialize at round 4915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([4.       , 7.0930959, 0.       ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.19601344465034}
done in step count: 1
reward sum = 0.981350671298645
running average episode reward sum: 0.8052053781186347
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.01825881, 7.18372142, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8164827610978109}
episode index:4916
target Thresh 17.53613753931151
target distance 1.0
model initialize at round 4916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.        ,  7.77476515,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0250515783186591}
done in step count: 0
reward sum = 0.9969619522570998
running average episode reward sum: 0.8052443768117684
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.        ,  7.77476515,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0250515783186591}
episode index:4917
target Thresh 17.53686928758954
target distance 5.0
model initialize at round 4917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.       ,  8.8825624,  0.       ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.8825623989105242}
done in step count: 9
reward sum = 0.8922411828247544
running average episode reward sum: 0.8052620662802542
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.96397024,  4.58680468,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.048794062197053}
episode index:4918
target Thresh 17.537600670084885
target distance 9.0
model initialize at round 4918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([1.13320865, 9.00188853, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 7.566038305385353}
done in step count: 4
reward sum = 0.9427885232439649
running average episode reward sum: 0.8052900244947213
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.16892357, 1.83730083, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8468524438919028}
episode index:4919
target Thresh 17.53833168698039
target distance 2.0
model initialize at round 4919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([7.95689738, 9.94953415, 0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 2.043725795300167}
done in step count: 6
reward sum = 0.9292237534120367
running average episode reward sum: 0.8053152142770216
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'previousTarget': array([10., 10.]), 'currentState': array([10.00112256,  9.90760195,  0.        ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.09240486908864638}
episode index:4920
target Thresh 17.539062338458805
target distance 9.0
model initialize at round 4920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 3.94042706, 10.        ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.121374015292425}
done in step count: 4
reward sum = 0.9423452368624708
running average episode reward sum: 0.8053430602478782
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.8814574, 11.8540424,  0.       ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8622301145302941}
episode index:4921
target Thresh 17.5397926247028
target distance 2.0
model initialize at round 4921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.79185331, 9.        , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7918533086776574}
done in step count: 0
reward sum = 0.9965297260910434
running average episode reward sum: 0.8053819035363469
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.79185331, 9.        , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7918533086776574}
episode index:4922
target Thresh 17.540522545894945
target distance 5.0
model initialize at round 4922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([ 8.11773051, 11.73806588,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 4.750686388060522}
done in step count: 2
reward sum = 0.9706178248658655
running average episode reward sum: 0.8054154676073055
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([11.5115469 ,  8.62434436,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.6162009358660068}
episode index:4923
target Thresh 17.54125210221772
target distance 4.0
model initialize at round 4923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.53014642,  7.95700181,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.0888336374034573}
done in step count: 6
reward sum = 0.9331851224008426
running average episode reward sum: 0.8054414159531207
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.02720487, 10.45761985,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.1137803168746796}
episode index:4924
target Thresh 17.54198129385351
target distance 10.0
model initialize at round 4924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([3.43225932, 6.42518806, 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 9.712625009926569}
done in step count: 19
reward sum = 0.7877713934153067
running average episode reward sum: 0.8054378281312856
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.87379517, 10.2710386 ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.1379379228715123}
episode index:4925
target Thresh 17.542710120984616
target distance 14.0
model initialize at round 4925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.        , 2.98521471, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.042831443822612}
done in step count: 11
reward sum = 0.8529014745262283
running average episode reward sum: 0.8054474634634811
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.17961475,  3.52571683,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9476162101195733}
episode index:4926
target Thresh 17.543438583793247
target distance 13.0
model initialize at round 4926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([2.83087456, 9.40858209, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 12.183488378213363}
done in step count: 6
reward sum = 0.9222242889906838
running average episode reward sum: 0.8054711648691086
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.70923567,  9.13276468,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9146808130629025}
episode index:4927
target Thresh 17.544166682461512
target distance 6.0
model initialize at round 4927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 7.31467881, 11.89249926,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 4.406019538471407}
done in step count: 6
reward sum = 0.9275242973394076
running average episode reward sum: 0.8054959321443664
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.43217699, 11.88537989,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.985228147683857}
episode index:4928
target Thresh 17.544894417171445
target distance 2.0
model initialize at round 4928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.        ,  9.67785108,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6778510808944276}
done in step count: 0
reward sum = 0.9966553880856911
running average episode reward sum: 0.8055347147485339
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.        ,  9.67785108,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6778510808944276}
episode index:4929
target Thresh 17.54562178810497
target distance 6.0
model initialize at round 4929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([3.83043242, 7.        , 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 5.136661741424264}
done in step count: 2
reward sum = 0.965902916320638
running average episode reward sum: 0.8055672437955059
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.12602949, 10.43341637,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.9755379011994536}
episode index:4930
target Thresh 17.546348795443937
target distance 7.0
model initialize at round 4930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([8.        , 8.75038993, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 5.706544031791205}
done in step count: 2
reward sum = 0.9658914042405019
running average episode reward sum: 0.8055997573141522
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.86785169, 5.96832617, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8684294958610052}
episode index:4931
target Thresh 17.547075439370097
target distance 6.0
model initialize at round 4931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.        ,  7.83467638,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.100909094283687}
done in step count: 2
reward sum = 0.9674255612111182
running average episode reward sum: 0.8056325687099141
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.        , 10.78966582,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.21033418178552665}
episode index:4932
target Thresh 17.547801720065106
target distance 4.0
model initialize at round 4932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([5.44855416, 7.96565657, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 2.753133164272804}
done in step count: 7
reward sum = 0.9233446799842744
running average episode reward sum: 0.8056564308853194
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'previousTarget': array([8., 9.]), 'currentState': array([7.64007575, 9.65957482, 0.        ]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 0.751388319615381}
episode index:4933
target Thresh 17.54852763771054
target distance 4.0
model initialize at round 4933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.        ,  7.31939912,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.3193991184234246}
done in step count: 1
reward sum = 0.9828425765922608
running average episode reward sum: 0.805692342143063
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.37502509,  5.70520222,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.7987202229361638}
episode index:4934
target Thresh 17.549253192487875
target distance 11.0
model initialize at round 4934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([15.44243239, 11.81657354,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 13.342170158665148}
done in step count: 32
reward sum = 0.6305905857826365
running average episode reward sum: 0.8056568605308319
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.07218835, 6.77100977, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9556521272741467}
episode index:4935
target Thresh 17.5499783845785
target distance 9.0
model initialize at round 4935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 7.        , 10.60525641,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 7.873872857158173}
done in step count: 5
reward sum = 0.9315163311989468
running average episode reward sum: 0.8056823588028473
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.62029717,  6.70708332,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.47955648213899604}
episode index:4936
target Thresh 17.55070321416371
target distance 1.0
model initialize at round 4936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.60415608, 11.07053041,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.6082590769121224}
done in step count: 0
reward sum = 0.9988667200622731
running average episode reward sum: 0.805721488711954
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.60415608, 11.07053041,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.6082590769121224}
episode index:4937
target Thresh 17.551427681424723
target distance 5.0
model initialize at round 4937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([4.21992552, 5.        , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 3.238551880268298}
done in step count: 2
reward sum = 0.9697305721776784
running average episode reward sum: 0.8057547023781074
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.87294844, 1.12935144, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.232910338149703}
episode index:4938
target Thresh 17.552151786542645
target distance 4.0
model initialize at round 4938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([5.        , 7.80021393, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.773411039712594}
done in step count: 12
reward sum = 0.8547312550327038
running average episode reward sum: 0.8057646186673673
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.33884615, 11.88073887,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9436724399736665}
episode index:4939
target Thresh 17.552875529698508
target distance 8.0
model initialize at round 4939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([11.       , 10.8063668,  0.       ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 8.771508572408905}
done in step count: 51
reward sum = 0.5097147513884861
running average episode reward sum: 0.8057046895444363
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.47210506,  2.66279035,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.5801668167116343}
episode index:4940
target Thresh 17.553598911073244
target distance 5.0
model initialize at round 4940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 8.79259776, 11.87609981,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 4.231258471631376}
done in step count: 2
reward sum = 0.9668922592702176
running average episode reward sum: 0.8057373120033972
{'scaleFactor': 20, 'currentTarget': array([ 5., 10.]), 'previousTarget': array([ 5., 10.]), 'currentState': array([ 4.83354314, 10.95259676,  0.        ]), 'targetState': array([ 5, 10], dtype=int32), 'currentDistance': 0.9670307554902225}
episode index:4941
target Thresh 17.5543219308477
target distance 9.0
model initialize at round 4941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.75515532,  5.        ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.805224837497681}
done in step count: 11
reward sum = 0.8551723708107121
running average episode reward sum: 0.8057473150505051
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.0811484 , 11.85575995,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8595988380415593}
episode index:4942
target Thresh 17.555044589202634
target distance 3.0
model initialize at round 4942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.6081812,  8.       ,  0.       ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.1704205970820996}
done in step count: 1
reward sum = 0.9820549120373573
running average episode reward sum: 0.8057829831866545
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.81461099,  9.99940867,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.2893443121397419}
episode index:4943
target Thresh 17.555766886318704
target distance 6.0
model initialize at round 4943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([13.39081387,  5.        ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 4.660041947245213}
done in step count: 35
reward sum = 0.6411763938246149
running average episode reward sum: 0.8057496889735959
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.62128833,  8.03939932,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.1440073662827355}
episode index:4944
target Thresh 17.55648882237649
target distance 12.0
model initialize at round 4944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([3.48593807, 8.25970304, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.865299151419103}
done in step count: 37
reward sum = 0.6272893715039635
running average episode reward sum: 0.8057135999306294
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.08846371, 11.6764583 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.1351185993032158}
episode index:4945
target Thresh 17.557210397556474
target distance 7.0
model initialize at round 4945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([10.6890313 , 11.86005826,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.32102977587498}
done in step count: 9
reward sum = 0.8941509009800794
running average episode reward sum: 0.8057314805009993
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.42767483, 4.11206134, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.4421125471155808}
episode index:4946
target Thresh 17.55793161203905
target distance 6.0
model initialize at round 4946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([ 9.        , 10.22160149,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 4.18238092372048}
done in step count: 2
reward sum = 0.9690139342600943
running average episode reward sum: 0.8057644868591475
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.        ,  8.55583638,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.4441636204720396}
episode index:4947
target Thresh 17.55865246600452
target distance 4.0
model initialize at round 4947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([5.06255456, 6.90666332, 0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 3.232264804706081}
done in step count: 15
reward sum = 0.8295899303936122
running average episode reward sum: 0.8057693020255853
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'previousTarget': array([ 6., 10.]), 'currentState': array([ 5.66315763, 10.48760885,  0.        ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.5926425339862431}
episode index:4948
target Thresh 17.5593729596331
target distance 13.0
model initialize at round 4948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([5.        , 8.15817022, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.759693006426984}
done in step count: 7
reward sum = 0.9031960432660076
running average episode reward sum: 0.8057889881725323
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.21614233,  3.46358777,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9498267927799279}
episode index:4949
target Thresh 17.560093093104914
target distance 4.0
model initialize at round 4949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.99875939, 8.63013673, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.650395505152453}
done in step count: 3
reward sum = 0.9585651377935164
running average episode reward sum: 0.8058198520411426
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.43870135, 10.76996744,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.4953522512332328}
episode index:4950
target Thresh 17.560812866599992
target distance 10.0
model initialize at round 4950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.05059261, 11.8664305 ,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.222425081422978}
done in step count: 5
reward sum = 0.9265792293641791
running average episode reward sum: 0.8058442429474894
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.19726685, 10.52933286,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.5648960005203255}
episode index:4951
target Thresh 17.56153228029828
target distance 1.0
model initialize at round 4951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.        , 9.82263231, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.2948837484581524}
done in step count: 0
reward sum = 0.9969764054249043
running average episode reward sum: 0.8058828399108329
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.        , 9.82263231, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.2948837484581524}
episode index:4952
target Thresh 17.56225133437963
target distance 8.0
model initialize at round 4952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 9., 11.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.082762530298236}
done in step count: 8
reward sum = 0.9012613810560577
running average episode reward sum: 0.8059020966322432
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.82242476, 10.3178544 ,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.3640939183725579}
episode index:4953
target Thresh 17.562970029023806
target distance 1.0
model initialize at round 4953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 5.        , 11.07598639,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.4689270579060942}
done in step count: 18
reward sum = 0.802893990033595
running average episode reward sum: 0.8059014894246133
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.87544411, 10.52249906,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.5371400514290356}
episode index:4954
target Thresh 17.56368836441048
target distance 9.0
model initialize at round 4954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4.86045933, 5.97521227, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.56559533114617}
done in step count: 5
reward sum = 0.9258480170952471
running average episode reward sum: 0.8059256965946779
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.55575676, 11.87396748,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0357049455520655}
episode index:4955
target Thresh 17.56440634071924
target distance 3.0
model initialize at round 4955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([12.89572358, 10.        ,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 1.8957235813141047}
done in step count: 3
reward sum = 0.9596568640129626
running average episode reward sum: 0.8059567157971433
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([10.62661067, 10.1796678 ,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.41436712227184946}
episode index:4956
target Thresh 17.56512395812958
target distance 10.0
model initialize at round 4956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([ 3.59239083, 11.90519705,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 9.845977866332747}
done in step count: 35
reward sum = 0.6206812495724019
running average episode reward sum: 0.8059193392657282
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([13.56527304,  8.89400673,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.5751244900819522}
episode index:4957
target Thresh 17.5658412168209
target distance 13.0
model initialize at round 4957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([3.49254179, 5.74663699, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 12.268361397404782}
done in step count: 9
reward sum = 0.8768719118801626
running average episode reward sum: 0.8059336499903379
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.60763225, 10.76234906,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.857396374257195}
episode index:4958
target Thresh 17.566558116972516
target distance 6.0
model initialize at round 4958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([10.        ,  8.92986703,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 6.348510764383698}
done in step count: 7
reward sum = 0.9074877146561009
running average episode reward sum: 0.8059541287289276
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.33394304,  3.22204144,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0241344591589259}
episode index:4959
target Thresh 17.567274658763655
target distance 2.0
model initialize at round 4959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.00494245, 1.13259695, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8674171277527281}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.8059920412030156
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.00494245, 1.13259695, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8674171277527281}
episode index:4960
target Thresh 17.567990842373447
target distance 8.0
model initialize at round 4960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([ 3.78541243, 11.23624754,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 7.553216324996655}
done in step count: 4
reward sum = 0.9494783292093975
running average episode reward sum: 0.8060209640588927
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.4693011 ,  8.61326569,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.6566618168450225}
episode index:4961
target Thresh 17.568706667980948
target distance 5.0
model initialize at round 4961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([ 9.029881 , 10.9807846,  0.       ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 3.6199014474353346}
done in step count: 2
reward sum = 0.9693728425015808
running average episode reward sum: 0.8060538846309288
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([5.029881  , 9.19054914, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9886555750526522}
episode index:4962
target Thresh 17.569422135765105
target distance 12.0
model initialize at round 4962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([5., 8., 0.]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.198039027185583}
done in step count: 14
reward sum = 0.8185895608747078
running average episode reward sum: 0.8060564104572926
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.79608962,  6.14395343,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8090001729185482}
episode index:4963
target Thresh 17.57013724590479
target distance 6.0
model initialize at round 4963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([15.11449945,  4.80027604,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 5.228555073240265}
done in step count: 27
reward sum = 0.7180661001954357
running average episode reward sum: 0.8060386847702938
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'previousTarget': array([12.,  9.]), 'currentState': array([12.73194309,  8.46476662,  0.        ]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.9067609691637535}
episode index:4964
target Thresh 17.570851998578778
target distance 5.0
model initialize at round 4964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.        ,  4.60294437,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.3970556259155433}
done in step count: 2
reward sum = 0.9690475521986214
running average episode reward sum: 0.8060715163649421
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.68692592,  8.11548161,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.6965653016427974}
episode index:4965
target Thresh 17.57156639396576
target distance 13.0
model initialize at round 4965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.        , 8.64421749, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.005752186634226}
done in step count: 21
reward sum = 0.7636236533193503
running average episode reward sum: 0.8060629686679938
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.90326666,  8.39851902,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.08520506027651}
episode index:4966
target Thresh 17.57228043224433
target distance 6.0
model initialize at round 4966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([7.31781667, 8.10704483, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 4.804495679646899}
done in step count: 2
reward sum = 0.9633594378145504
running average episode reward sum: 0.8060946369726337
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.21972031, 5.23934787, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7917503835139857}
episode index:4967
target Thresh 17.572994113593005
target distance 4.0
model initialize at round 4967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.85739475, 7.19074166, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.4708087257375375}
done in step count: 1
reward sum = 0.9839485042861066
running average episode reward sum: 0.8061304368654102
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.21288836, 5.19706309, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8114053207879626}
episode index:4968
target Thresh 17.573707438190194
target distance 12.0
model initialize at round 4968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.63844097, 8.72430944, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.504053915638863}
done in step count: 11
reward sum = 0.8594225608080969
running average episode reward sum: 0.8061411617846983
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.16196007,  6.10989571,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.2225369386678526}
episode index:4969
target Thresh 17.57442040621424
target distance 10.0
model initialize at round 4969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([ 4.        , 11.43203104,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 8.127158967709182}
done in step count: 7
reward sum = 0.908622799791231
running average episode reward sum: 0.806161781832587
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'previousTarget': array([12., 10.]), 'currentState': array([11.5115331 ,  9.34945059,  0.        ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.8135197924323992}
episode index:4970
target Thresh 17.57513301784338
target distance 8.0
model initialize at round 4970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([7.        , 9.57172471, 0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 6.015265557029449}
done in step count: 3
reward sum = 0.9576419665830609
running average episode reward sum: 0.8061922546116557
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([13.        , 10.44540983,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.44540983438493953}
episode index:4971
target Thresh 17.57584527325577
target distance 10.0
model initialize at round 4971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.88957119,  9.57734573,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.458723313991682}
done in step count: 13
reward sum = 0.8420428304757291
running average episode reward sum: 0.8061994651055946
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.28246813, 2.01110883, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7176178576943153}
episode index:4972
target Thresh 17.576557172629464
target distance 7.0
model initialize at round 4972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.79844207, 8.        , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 5.004060910987503}
done in step count: 4
reward sum = 0.946377578452488
running average episode reward sum: 0.8062276529425837
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.3378479 , 3.07997285, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.34718418385387845}
episode index:4973
target Thresh 17.57726871614245
target distance 11.0
model initialize at round 4973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.13259829055323}
done in step count: 8
reward sum = 0.8853078312439668
running average episode reward sum: 0.8062435516515305
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.03145963, 4.7095632 , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0111498375878185}
episode index:4974
target Thresh 17.577979903972604
target distance 2.0
model initialize at round 4974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([14.34545827,  9.        ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.6545417308806876}
done in step count: 2
reward sum = 0.97641038992576
running average episode reward sum: 0.8062777560411334
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.01109529,  8.30175218,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.2105711680260118}
episode index:4975
target Thresh 17.578690736297727
target distance 5.0
model initialize at round 4975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([3.99793184, 7.28041422, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 5.693578197561667}
done in step count: 22
reward sum = 0.7722019980190955
running average episode reward sum: 0.8062709080190229
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'previousTarget': array([ 9., 10.]), 'currentState': array([8.35300896, 9.52360032, 0.        ]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.8034637960736066}
episode index:4976
target Thresh 17.579401213295526
target distance 9.0
model initialize at round 4976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([7.25954387, 8.11871603, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 9.866779969502087}
done in step count: 7
reward sum = 0.9016791166993744
running average episode reward sum: 0.8062900778419445
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.79307825,  1.86644368,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8042452371488814}
episode index:4977
target Thresh 17.580111335143624
target distance 7.0
model initialize at round 4977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([13.38251479,  4.99999999,  0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 7.532363177822249}
done in step count: 8
reward sum = 0.8984154362797164
running average episode reward sum: 0.8063085843422333
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([7.59592496, 8.09965691, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.079696363723407}
episode index:4978
target Thresh 17.580821102019545
target distance 11.0
model initialize at round 4978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.,  9.,  0.]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 9.84885780179613}
done in step count: 23
reward sum = 0.7434250330978522
running average episode reward sum: 0.8062959545870125
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.63796673, 5.60832452, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7079031095891039}
episode index:4979
target Thresh 17.581530514100734
target distance 7.0
model initialize at round 4979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.107471  ,  5.62465761,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.6987693214057975}
done in step count: 7
reward sum = 0.9220917466924462
running average episode reward sum: 0.806319206754102
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.93662388, 10.4787215 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0719121099525144}
episode index:4980
target Thresh 17.582239571564546
target distance 6.0
model initialize at round 4980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([9.96759592, 8.14196423, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 4.190986157693372}
done in step count: 49
reward sum = 0.5248544145433605
running average episode reward sum: 0.8062626990664468
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.01041935,  6.94798396,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9909467858134254}
episode index:4981
target Thresh 17.582948274588244
target distance 3.0
model initialize at round 4981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.39403343, 9.93036437, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.8559920471600115}
done in step count: 8
reward sum = 0.911974873106142
running average episode reward sum: 0.806283917889016
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([6.95671523, 9.28585833, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9985083928545191}
episode index:4982
target Thresh 17.583656623349
target distance 4.0
model initialize at round 4982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([4.8685613 , 5.93449777, 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 3.733675781396012}
done in step count: 1
reward sum = 0.9805451580289785
running average episode reward sum: 0.8063188890389538
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'previousTarget': array([7., 9.]), 'currentState': array([6.01497973, 8.101635  , 0.        ]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 1.333163384095965}
episode index:4983
target Thresh 17.58436461802391
target distance 6.0
model initialize at round 4983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.75636601,  9.        ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 4.007412821122903}
done in step count: 2
reward sum = 0.9682325352117284
running average episode reward sum: 0.8063513757255856
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.08686399,  5.        ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.08686399459831229}
episode index:4984
target Thresh 17.585072258789964
target distance 14.0
model initialize at round 4984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.        , 5.50427699, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.092856871723079}
done in step count: 12
reward sum = 0.8483752923424082
running average episode reward sum: 0.8063598057991296
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.20258419,  6.61588871,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8851064680912963}
episode index:4985
target Thresh 17.585779545824074
target distance 3.0
model initialize at round 4985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.5294739, 8.       , 0.       ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.1315222541439804}
done in step count: 1
reward sum = 0.9825476214869585
running average episode reward sum: 0.8063951423044822
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.22054517, 6.        , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.2678918833302428}
episode index:4986
target Thresh 17.586486479303066
target distance 7.0
model initialize at round 4986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([6.        , 8.98204565, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 5.000032235762765}
done in step count: 5
reward sum = 0.9332199501333167
running average episode reward sum: 0.806420573386862
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([11.12631385,  8.13167024,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8774689561086016}
episode index:4987
target Thresh 17.587193059403667
target distance 5.0
model initialize at round 4987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([7.02619869, 8.13224963, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.355326188690743}
done in step count: 26
reward sum = 0.7062866807998212
running average episode reward sum: 0.8064004984284443
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.84454468, 4.10003881, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.2341741616721962}
episode index:4988
target Thresh 17.58789928630253
target distance 7.0
model initialize at round 4988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([10.23451984,  9.48288584,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 6.408446616277755}
done in step count: 4
reward sum = 0.9486753748275607
running average episode reward sum: 0.8064290161426955
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.79023886, 7.6094978 , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.44327384686863014}
episode index:4989
target Thresh 17.588605160176204
target distance 6.0
model initialize at round 4989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.13259695, 7.00494245, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.418909163121906}
done in step count: 15
reward sum = 0.8378829874047241
running average episode reward sum: 0.8064353195437499
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.10315744, 2.20551126, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1981397780074494}
episode index:4990
target Thresh 17.58931068120116
target distance 9.0
model initialize at round 4990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([13.48660994,  3.70131361,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 7.71932332292545}
done in step count: 6
reward sum = 0.929535371690197
running average episode reward sum: 0.8064599839501106
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.85977148, 10.22772811,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.784899939680522}
episode index:4991
target Thresh 17.59001584955378
target distance 10.0
model initialize at round 4991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([ 5.        , 10.42692041,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 8.01138321639063}
done in step count: 4
reward sum = 0.939723890886448
running average episode reward sum: 0.8064866794442885
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.96165925,  9.45185802,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.5494812536996239}
episode index:4992
target Thresh 17.590720665410355
target distance 7.0
model initialize at round 4992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([10.,  9.,  0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 6.403124237432876}
done in step count: 8
reward sum = 0.8916645124146463
running average episode reward sum: 0.8065037388941123
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.71851094,  3.4761152 ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.594719573910377}
episode index:4993
target Thresh 17.59142512894709
target distance 9.0
model initialize at round 4993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([13.,  9.,  0.]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 7.280109889280543}
done in step count: 22
reward sum = 0.7544536948191006
running average episode reward sum: 0.8064933163782784
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.281009  ,  1.17590012,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8706932095697935}
episode index:4994
target Thresh 17.5921292403401
target distance 5.0
model initialize at round 4994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([ 6.79523408, 10.43602392,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 4.227312771342451}
done in step count: 3
reward sum = 0.9591796331096957
running average episode reward sum: 0.8065238842094559
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.70435984,  9.60058545,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.8097251172848262}
episode index:4995
target Thresh 17.59283299976541
target distance 5.0
model initialize at round 4995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([13.21675003, 11.46307296,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 4.051445311590254}
done in step count: 4
reward sum = 0.9499877830638783
running average episode reward sum: 0.8065525999618287
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([9.9745183 , 9.71254915, 0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.71300462948795}
episode index:4996
target Thresh 17.593536407398965
target distance 6.0
model initialize at round 4996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 4.        , 11.11853075,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 4.153445682720565}
done in step count: 2
reward sum = 0.9690224634864819
running average episode reward sum: 0.8065851134426221
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([8.        , 9.36179185, 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.6382081508636688}
episode index:4997
target Thresh 17.594239463416617
target distance 3.0
model initialize at round 4997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([12.49912119,  9.        ,  0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 3.4991211891174743}
done in step count: 11
reward sum = 0.8709455342678016
running average episode reward sum: 0.8065979906776811
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'previousTarget': array([9., 9.]), 'currentState': array([9.59624435, 9.71532765, 0.        ]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.9312362646339346}
episode index:4998
target Thresh 17.594942167994123
target distance 7.0
model initialize at round 4998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([4.05919111, 9.32026631, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 6.948193908185952}
done in step count: 13
reward sum = 0.8283842081793135
running average episode reward sum: 0.8066023487928045
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.01077612,  8.1327199 ,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.3155754077962512}
episode index:4999
target Thresh 17.595644521307165
target distance 4.0
model initialize at round 4999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([4.        , 5.27673769, 0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 4.226426650171028}
done in step count: 99
reward sum = -0.172994560156887
running average episode reward sum: 0.8064064294110145
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'previousTarget': array([6., 9.]), 'currentState': array([15.34023694, 11.89261075,  0.        ]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 9.777894611019098}

Process finished with exit code 0
