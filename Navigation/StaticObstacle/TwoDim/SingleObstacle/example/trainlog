scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.78960966, 11.09782276,  3.75536041]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.2320202288486133}
episode index:2463
target Thresh 19.0
target distance 9.0
model initialize at round 2463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.92547618,  2.02792071,  1.04015988]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 8.972388785197897}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8141304450217084
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.73220318, 10.89883964,  1.62423334]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7391582449930721}
episode index:2464
target Thresh 19.0
target distance 14.0
model initialize at round 2464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([1.39787548, 4.57156694, 2.94668114]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 14.671825433260203}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8140742984107707
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.31538012,  5.77844079,  5.54845746]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.7195782529893189}
episode index:2465
target Thresh 19.0
target distance 14.0
model initialize at round 2465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([3.68276728, 2.97387786, 0.99447792]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.683518324405188}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8140209651977129
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.8159254 ,  6.76731645,  1.59625424]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.7890868068452641}
episode index:2466
target Thresh 19.0
target distance 2.0
model initialize at round 2466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([13.99965989,  7.00089768,  2.94295728]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.8293023994073105}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8140764857022538
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.35132034,  5.03292532,  0.37658667]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.3528598303597081}
episode index:2467
target Thresh 19.0
target distance 9.0
model initialize at round 2467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([2.1734209 , 5.46599968, 3.09419787]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 11.277713259973055}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8140649790795248
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.15989085, 11.12853584,  0.82871541]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8498851954789031}
episode index:2468
target Thresh 19.0
target distance 12.0
model initialize at round 2468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.        , 3.        , 0.53088063]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.892443989071193}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8140201766153369
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.28398062, 10.42318769,  1.41584225]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.509639885964021}
episode index:2469
target Thresh 19.0
target distance 12.0
model initialize at round 2469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.55108183, 11.45021422,  2.88099414]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 12.055350247330471}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8140119151202067
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.23042306, 7.4929974 , 4.89869699]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5569079115847976}
episode index:2470
target Thresh 19.0
target distance 11.0
model initialize at round 2470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([2.71367363, 3.65841342, 2.75176167]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.526673009414635}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8139643235795063
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.55721772,  6.84622928,  5.6367233 ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9550707560786171}
episode index:2471
target Thresh 19.0
target distance 13.0
model initialize at round 2471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.37846575,  9.45057161,  3.88056254]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.217899596654865}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8139403571408429
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.7843357 , 5.99137509, 3.33189478]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.2641230369240928}
episode index:2472
target Thresh 19.0
target distance 1.0
model initialize at round 2472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.46868747,  7.33169017,  4.56310415]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.6263476457342262}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8140155935512187
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.46868747,  7.33169017,  4.56310415]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.6263476457342262}
episode index:2473
target Thresh 19.0
target distance 14.0
model initialize at round 2473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 2.        , 10.        ,  3.76446986]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8139947071812897
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.25004575,  9.36700644,  1.2158021 ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.4440907615540167}
episode index:2474
target Thresh 19.0
target distance 14.0
model initialize at round 2474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([16.24665489,  3.66479687,  2.43370706]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 15.21287506998509}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8139558125652966
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.80608982, 8.30435044, 5.60185399]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0647577690883159}
episode index:2475
target Thresh 19.0
target distance 10.0
model initialize at round 2475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([12.62461769,  9.96990252,  3.53739357]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 8.62467020426906}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8139709587521017
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.18869914, 9.99382646, 4.12146703]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.18880010529018423}
episode index:2476
target Thresh 19.0
target distance 6.0
model initialize at round 2476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.39495863, 6.36403063, 5.95927787]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 4.405773307401298}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8140111469994701
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.47653687, 1.93942888, 5.10972195]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.5269558888539878}
episode index:2477
target Thresh 19.0
target distance 6.0
model initialize at round 2477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([1.99990342, 9.99982781, 5.21123409]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.082886103571098}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.814047616300523
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.42210552, 10.56173065,  0.07849286]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7252875629298275}
episode index:2478
target Thresh 19.0
target distance 9.0
model initialize at round 2478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([2.32385083, 4.15135605, 4.06153703]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 12.68400903186924}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8140267591395898
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.01488546, 11.48221144,  1.51286926]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.096803781739617}
episode index:2479
target Thresh 19.0
target distance 2.0
model initialize at round 2479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([1.98957156, 6.01303672, 3.2554934 ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.010470704029117}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8140937241560657
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.66601113, 5.94618299, 0.9723081 ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.3382969695291246}
episode index:2480
target Thresh 19.0
target distance 2.0
model initialize at round 2480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.59050664,  8.63178335,  2.82666737]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.6959034342310311}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8141686561495538
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.59050664,  8.63178335,  2.82666737]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.6959034342310311}
episode index:2481
target Thresh 19.0
target distance 13.0
model initialize at round 2481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([2.        , 9.        , 5.81228352]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 13.03840481039893}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8141447039461107
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.25739072,  7.68016749,  5.26361575]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8085551154841195}
episode index:2482
target Thresh 19.0
target distance 1.0
model initialize at round 2482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.57597991, 5.18025497, 4.67391944]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.46074384240665867}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8142195550520527
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.57597991, 5.18025497, 4.67391944]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.46074384240665867}
episode index:2483
target Thresh 19.0
target distance 13.0
model initialize at round 2483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([2.99999905, 3.99994359, 5.70560741]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.000000946870752}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.8141353306205251
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.46584229,  3.77535088,  5.1746425 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.5794753513266941}
episode index:2484
target Thresh 19.0
target distance 6.0
model initialize at round 2484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.        , 2.        , 6.23194647]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 6.0000000016459545}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8141790366019365
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.05073198, 7.09401822, 3.38239055]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.3122167392148343}
episode index:2485
target Thresh 19.0
target distance 8.0
model initialize at round 2485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.56808585,  4.62660329,  2.84033847]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.463082516304722}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8141940320703568
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.96417345, 11.00354765,  3.42441193]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.964179980812961}
episode index:2486
target Thresh 19.0
target distance 2.0
model initialize at round 2486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 9.03521123, 10.5947709 ,  5.80906343]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.0061419529511615}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8142647220453989
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.49860052, 11.42466124,  1.52587813]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.6570681878400072}
episode index:2487
target Thresh 19.0
target distance 5.0
model initialize at round 2487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([10.99999976, 11.00000012,  3.71194899]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 5.385165077261827}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8143083233204723
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.1968682 ,  9.83928586,  0.86239307]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.1616460072186254}
episode index:2488
target Thresh 19.0
target distance 2.0
model initialize at round 2488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.07666871, 6.05080812, 1.59524935]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.2267834370095927}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8143709953480656
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.90474599, 7.33097769, 1.31206404]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.1252360471714578}
episode index:2489
target Thresh 19.0
target distance 2.0
model initialize at round 2489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.34740615, 10.68412374,  4.34045434]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.46954110026981616}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8144455451491306
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.34740615, 10.68412374,  4.34045434]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.46954110026981616}
episode index:2490
target Thresh 19.0
target distance 7.0
model initialize at round 2490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 8.38143581, 10.53887305,  4.42913818]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 5.4083486934061105}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.814485317008759
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.86757338, 10.7716986 ,  3.57958226]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1611211436010185}
episode index:2491
target Thresh 19.0
target distance 2.0
model initialize at round 2491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 8.99998651, 10.99999392,  4.57514501]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.0000134894866908}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8145557482619658
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.15408285, 10.22321   ,  0.2919597 ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.1484679076030342}
episode index:2492
target Thresh 19.0
target distance 8.0
model initialize at round 2492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.84652613,  7.22551463,  3.33592165]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.818034268199281}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8145705505174141
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.82195464, 10.61067741,  3.91999511]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.4281030529573298}
episode index:2493
target Thresh 19.0
target distance 6.0
model initialize at round 2493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([15.00503658,  6.35736358,  3.21333599]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 5.530309015741425}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8146065615536977
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.79138943, 10.1567402 ,  4.36378007]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8686802959207491}
episode index:2494
target Thresh 19.0
target distance 2.0
model initialize at round 2494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 5.99927025, 10.99976429,  4.46402025]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 2.0007297627883}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8146728915891471
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.55718361, 11.67744713,  2.18083495]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8093336611718707}
episode index:2495
target Thresh 19.0
target distance 4.0
model initialize at round 2495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([15.        ,  6.99999999,  5.61064911]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 5.000000007085848}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8147161895870793
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.63982836, 11.05684662,  2.76109319]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6423487073374843}
episode index:2496
target Thresh 19.0
target distance 6.0
model initialize at round 2496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.0000004 ,  9.00000013,  1.32635182]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 6.000000131882445}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.814748477158033
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.59248355,  3.6504766 ,  4.4767959 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7675867810212609}
episode index:2497
target Thresh 19.0
target distance 9.0
model initialize at round 2497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([15.3175355 ,  8.53838561,  2.99833518]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.674153687664436}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8147631726319869
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.45259305, 11.55465468,  3.58240865]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.7158786726042201}
episode index:2498
target Thresh 19.0
target distance 12.0
model initialize at round 2498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([2.        , 6.        , 5.09083343]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 13.000000000252077}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8147452774631887
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.05566945, 10.69565931,  0.54216566]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9921609953802665}
episode index:2499
target Thresh 19.0
target distance 12.0
model initialize at round 2499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.99999473, 3.99994919, 5.61905241]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.000005269925454}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.8146687895101599
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.31622622,  3.86316222,  5.3712728 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6973314550856728}
episode index:2500
target Thresh 19.0
target distance 1.0
model initialize at round 2500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.68876094, 10.10079315,  2.8563447 ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.3271528913656576}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8147428923532186
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.68876094, 10.10079315,  2.8563447 ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.3271528913656576}
episode index:2501
target Thresh 19.0
target distance 4.0
model initialize at round 2501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.1579642 ,  3.97228192,  0.83629769]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 4.030814514873121}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8148011869645883
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.80920968,  7.26663139,  2.55311238]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7577799568564709}
episode index:2502
target Thresh 19.0
target distance 14.0
model initialize at round 2502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([16.00000482, 10.99999432,  0.1429879 ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 15.231548403856012}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8147682273891547
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.90030445, 5.43684944, 5.59432014]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0006925266792566}
episode index:2503
target Thresh 19.0
target distance 13.0
model initialize at round 2503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([16.        ,  3.        ,  0.12789362]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 13.341664064126332}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8147181798086753
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.21800574, 6.31854087, 5.01285524]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.3859984283973944}
episode index:2504
target Thresh 19.0
target distance 13.0
model initialize at round 2504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([15.35579486,  4.64492693,  2.36777958]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 13.770757846580771}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8146794621850385
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.11677153, 7.45014248, 5.53592651]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.5621199864786083}
episode index:2505
target Thresh 19.0
target distance 11.0
model initialize at round 2505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.49213717,  5.60451346,  2.88734061]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.499588181492738}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8146407754613407
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.94273767, 6.54815529, 6.05548753]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0905175520748944}
episode index:2506
target Thresh 19.0
target distance 13.0
model initialize at round 2506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 1.99999997, 10.99999992,  5.31291342]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 13.038404837658634}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8146323883485295
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.85943701, 10.6262009 ,  1.04743096]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6417830761857913}
episode index:2507
target Thresh 19.0
target distance 1.0
model initialize at round 2507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 1.99771973, 10.00238656,  3.34342337]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.4141422560743493}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8146983642702406
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.55923125, 10.08050525,  1.06023806]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.076201737481317}
episode index:2508
target Thresh 19.0
target distance 5.0
model initialize at round 2508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.63240611, 8.00624317, 4.13461041]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.0197207010423845}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8147451426614868
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.14385327, 3.18146563, 5.5682398 ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.23156756435463866}
episode index:2509
target Thresh 19.0
target distance 12.0
model initialize at round 2509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([12.38973531, 10.51067833,  4.44660211]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.326641986856439}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8147304319908644
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.73255299, 6.13245898, 4.18111965]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.2984514790175865}
episode index:2510
target Thresh 19.0
target distance 10.0
model initialize at round 2510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.87177007,  7.96052911,  1.84380167]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.949988847319036}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8146918020030537
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.23325545, 3.54657363, 5.0119486 ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9416155990107749}
episode index:2511
target Thresh 19.0
target distance 12.0
model initialize at round 2511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([2.35322444, 3.34583721, 3.94459212]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 14.4185892020667}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8146447165795054
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.1262838 ,  8.53949325,  0.54636843]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5540763172765087}
episode index:2512
target Thresh 19.0
target distance 12.0
model initialize at round 2512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.98010256,  7.66127027,  5.07135344]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.353648582338986}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8146004678641932
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.62384578, 1.96482608, 5.95631506]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.6248365874760161}
episode index:2513
target Thresh 19.0
target distance 8.0
model initialize at round 2513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.68402093, 9.95106544, 4.82454777]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.958243561515103}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8146254959290836
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.49599391, 2.91459986, 5.69180654]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5032923076391005}
episode index:2514
target Thresh 19.0
target distance 13.0
model initialize at round 2514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([15.        ,  5.        ,  0.37379521]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.038404810405297}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8145841153053632
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.53420954, 5.84175943, 5.54194214]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5571533945896023}
episode index:2515
target Thresh 19.0
target distance 11.0
model initialize at round 2515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([2.76437444, 8.87989848, 4.24848723]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 13.172848673547614}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8145573219742926
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.36983384,  4.88530192,  5.69981947]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0866779132643165}
episode index:2516
target Thresh 19.0
target distance 9.0
model initialize at round 2516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([3.        , 5.        , 5.39013672]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 10.816653827968262}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8145396365646108
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.43084279, 11.395711  ,  0.84146895]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6932006354581607}
episode index:2517
target Thresh 19.0
target distance 11.0
model initialize at round 2517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([3.00000003, 5.99999993, 6.09637165]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 11.045360996454598}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8144983393409045
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.27178505,  7.46911256,  4.98133327]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8662353019708953}
episode index:2518
target Thresh 19.0
target distance 11.0
model initialize at round 2518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([2.99905051, 2.99934364, 4.75643969]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.000949507277845}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.8144127727141183
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.14742397,  3.55893268,  6.22547477]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5780480680057075}
episode index:2519
target Thresh 19.0
target distance 5.0
model initialize at round 2519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 6.70072339, 11.74050682,  3.54113543]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 4.604968105105686}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8144594602439568
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.77878255, 8.48570102, 4.97476482]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9332768603401723}
episode index:2520
target Thresh 19.0
target distance 5.0
model initialize at round 2520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.99999824,  5.99999755,  5.10029078]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.385166428846968}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8144987532972052
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.72867663, 10.33041538,  4.25073485]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7224679420543565}
episode index:2521
target Thresh 19.0
target distance 3.0
model initialize at round 2521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.76556534,  8.66653006,  2.72055161]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.8171326328589832}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8145605297629875
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.86235287,  9.77682002,  2.4373663 ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.26221372432452683}
episode index:2522
target Thresh 19.0
target distance 1.0
model initialize at round 2522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.90604077, 10.99696269,  4.18390727]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0013805146058194}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8146340293548373
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.90604077, 10.99696269,  4.18390727]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0013805146058194}
episode index:2523
target Thresh 19.0
target distance 3.0
model initialize at round 2523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 1.12643151, 10.46619372,  4.56572413]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.616339690382928}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8146918589826682
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.24084067, 7.80636083, 6.28253883]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.30903132287250185}
episode index:2524
target Thresh 19.0
target distance 12.0
model initialize at round 2524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([15.27884454,  5.70307319,  1.51266831]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 13.47603744079506}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8146651184818162
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.98644533, 8.52788007, 2.96400054]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.1188081890459023}
episode index:2525
target Thresh 19.0
target distance 13.0
model initialize at round 2525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 4.68295181, 10.00782826,  1.01465147]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.005759745080045}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8146413869571614
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.97036977,  5.97445408,  0.46598371]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.039122170991317616}
episode index:2526
target Thresh 19.0
target distance 6.0
model initialize at round 2526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.        , 4.99999999, 5.70006084]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.211102558910214}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8146559561633891
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([7.72313186e+00, 1.03480350e+01, 9.49001344e-04]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7083179474853628}
episode index:2527
target Thresh 19.0
target distance 4.0
model initialize at round 2527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.42304947,  5.37106889,  5.97648525]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 2.408513763164151}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8147175238231347
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.36467584,  3.69385773,  5.69329995]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7838539499657686}
episode index:2528
target Thresh 19.0
target distance 4.0
model initialize at round 2528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([1.83275406, 5.32569527, 5.62282944]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 3.1789642092854344}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8147676474394169
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.1556167 , 3.38593259, 5.05645882]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9284002970522386}
episode index:2529
target Thresh 19.0
target distance 5.0
model initialize at round 2529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 1.9983075 , 11.00048655,  3.87166798]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 5.099828778314506}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8148103261141159
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.38586793, 6.3845518 , 1.02211206]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.5447698114328526}
episode index:2530
target Thresh 19.0
target distance 12.0
model initialize at round 2530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.00001402, 10.99999325,  0.56142443]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.422213024095536}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8147777275536813
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.87243772, 3.97645158, 6.01275667]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.3094293676242128}
episode index:2531
target Thresh 19.0
target distance 6.0
model initialize at round 2531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 8.00000665, 11.0000049 ,  1.64508026]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.000006647568747}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.814820368535859
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.87046867, 10.03505775,  5.07870965]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.2995496300872886}
episode index:2532
target Thresh 19.0
target distance 10.0
model initialize at round 2532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.00015915, 4.99992095, 0.54900807]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.049725131061225}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8147877917498814
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.32724003,  6.73511543,  6.0003403 ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8046618793168355}
episode index:2533
target Thresh 19.0
target distance 3.0
model initialize at round 2533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.41286125,  6.82549194,  1.53877276]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.2133548563591114}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8148491616031767
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.44747832,  8.14240904,  1.25558745]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9673154077928929}
episode index:2534
target Thresh 19.0
target distance 12.0
model initialize at round 2534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([3.29771447, 9.65637962, 2.40295711]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.818928885102967}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8148376542971427
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.48150841,  8.9487279 ,  0.13747465]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.081165188367448}
episode index:2535
target Thresh 19.0
target distance 2.0
model initialize at round 2535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.65636824, 7.22442981, 1.14467495]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8482876301714385}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.814910667840401
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.65636824, 7.22442981, 1.14467495]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8482876301714385}
episode index:2536
target Thresh 19.0
target distance 1.0
model initialize at round 2536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.48769867, 11.55088814,  3.80185425]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7522834605879279}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8149836238246972
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.48769867, 11.55088814,  3.80185425]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7522834605879279}
episode index:2537
target Thresh 19.0
target distance 5.0
model initialize at round 2537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([2.        , 5.99999999, 5.86983657]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 6.403124241778597}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8150152844355854
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.46975284, 11.40336058,  2.73709534]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.6662295475460432}
episode index:2538
target Thresh 19.0
target distance 2.0
model initialize at round 2538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.07078647,  4.03274277,  1.44324702]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.2350572577066794}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8150764438351774
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.25662633,  2.67186255,  1.16006171]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.7192053657339452}
episode index:2539
target Thresh 19.0
target distance 9.0
model initialize at round 2539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([2.56148182, 3.58654538, 2.24064945]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.640587538864365}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8150840963224274
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.32594413, 10.18023288,  0.5415376 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0613055445608952}
episode index:2540
target Thresh 19.0
target distance 13.0
model initialize at round 2540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([16.        ,  3.        ,  5.64938545]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8150429586329153
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.24809981, 8.57211461, 4.53434707]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.4946103764175613}
episode index:2541
target Thresh 19.0
target distance 14.0
model initialize at round 2541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([2.        , 7.        , 3.76498926]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 14.000000000000002}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8150046767973391
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.00450049,  6.93372741,  0.64995088]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.06642522884192731}
episode index:2542
target Thresh 19.0
target distance 8.0
model initialize at round 2542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.33671191,  4.02133941,  1.66244429]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.785339929629155}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8150156632372081
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.74972267, 10.4089379 ,  4.24651775]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.641866923726369}
episode index:2543
target Thresh 19.0
target distance 12.0
model initialize at round 2543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([16.46844886, 10.11506114,  6.20924568]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 13.476871573387822}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8149831505431896
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.01410024, 5.00522269, 5.37739261]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.015036398067097818}
episode index:2544
target Thresh 19.0
target distance 1.0
model initialize at round 2544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([1.4854220e+01, 6.6468158e+00, 2.2966226e-03]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.3610140656823948}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8150519194427797
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.67779996,  7.90799015,  2.00229662]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6840165185883098}
episode index:2545
target Thresh 19.0
target distance 3.0
model initialize at round 2545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([2.00113937, 9.00121903, 1.82916277]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.605933927132357}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8151015770350648
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.93856718, 6.88101194, 1.26279216]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8831511975713963}
episode index:2546
target Thresh 19.0
target distance 8.0
model initialize at round 2546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.35726826,  4.55540476,  2.97265112]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.454490566897937}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8151295641943428
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.68625985, 10.57865428,  1.8399099 ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8052855383062312}
episode index:2547
target Thresh 19.0
target distance 4.0
model initialize at round 2547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 6.00012622, 11.00005704,  1.43446511]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.123241909865137}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8151828846361425
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.83208484, 9.77328842, 5.1512798 ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8624171373074276}
episode index:2548
target Thresh 19.0
target distance 11.0
model initialize at round 2548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.00592797,  6.00261703,  1.42573851]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.709377486424366}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8151651758331488
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.78667568, 10.70362927,  3.16025605]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0554396115664384}
episode index:2549
target Thresh 19.0
target distance 8.0
model initialize at round 2549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.40557861, 10.46134887,  4.4773891 ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.839549440055175}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8151896291065474
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.63042643, 3.81197521, 5.34464787]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0279791923288109}
episode index:2550
target Thresh 19.0
target distance 7.0
model initialize at round 2550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([16.       ,  4.       ,  5.2058785]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.615773105934444}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8152175378649202
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.14243374, 10.04073772,  4.07313727]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9697790916596892}
episode index:2551
target Thresh 19.0
target distance 12.0
model initialize at round 2551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([3.54343672, 7.59281657, 2.25200072]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.706723464349238}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8152059628660733
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.65683309,  9.41891995,  6.26970357]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6748463156645877}
episode index:2552
target Thresh 19.0
target distance 12.0
model initialize at round 2552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([2.        , 7.        , 5.31173539]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 12.369316876858692}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8151975054907413
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.31536403, 10.70759474,  1.04625293]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.9845896247595781}
episode index:2553
target Thresh 19.0
target distance 14.0
model initialize at round 2553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([3.67995057, 2.89923215, 0.95008915]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 13.747835695459248}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8151565327897944
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.39625855,  8.4781024 ,  6.11823608]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7980481468280316}
episode index:2554
target Thresh 19.0
target distance 11.0
model initialize at round 2554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.75784912, 11.4206884 ,  3.10308611]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 13.068968361152297}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.815138875886865
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.27441337, 4.71772418, 4.83760366]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0205900025161323}
episode index:2555
target Thresh 19.0
target distance 9.0
model initialize at round 2555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 5.99999984, 10.99999968,  5.24795699]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 12.04159444978098}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8151122848142691
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.58751415,  2.14087288,  0.41610392]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6041672365038431}
episode index:2556
target Thresh 19.0
target distance 5.0
model initialize at round 2556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.00231235,  8.00072504,  1.31384247]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.832561969441967}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8151543780522876
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.53969925, 10.2761365 ,  4.74747186]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.902913974519926}
episode index:2557
target Thresh 19.0
target distance 11.0
model initialize at round 2557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 3.        , 10.        ,  2.83206457]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 11.000000000038398}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8151522566645507
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.29209138, 10.47265362,  0.84976742]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.5556247126307154}
episode index:2558
target Thresh 19.0
target distance 7.0
model initialize at round 2558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([3.99999696, 9.00000279, 3.40976906]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.280112048069506}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8151698049638774
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.36000084, 11.80325284,  1.99384252]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0270414000491865}
episode index:2559
target Thresh 19.0
target distance 1.0
model initialize at round 2559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.53149172, 4.40327733, 4.53755784]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6181687566851238}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8152420042588133
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.53149172, 4.40327733, 4.53755784]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6181687566851238}
episode index:2560
target Thresh 19.0
target distance 2.0
model initialize at round 2560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.34079111,  4.27937977,  3.98477614]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.7540446305463544}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8153025497471934
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.6173885 ,  5.6655682 ,  3.70159083]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.5081694466658196}
episode index:2561
target Thresh 19.0
target distance 11.0
model initialize at round 2561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([3.99988988, 9.99990337, 4.87183022]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.530014491706437}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8152759570635808
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.12360499,  3.94918002,  0.03997715]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.13364454326498426}
episode index:2562
target Thresh 19.0
target distance 12.0
model initialize at round 2562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([14.16050678,  3.67529868,  2.48528001]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 14.196097163010837}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8152323252797088
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.09102249, 11.8057372 ,  3.37024163]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8108622091781361}
episode index:2563
target Thresh 19.0
target distance 14.0
model initialize at round 2563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.37624564,  3.55749591,  4.41765094]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.460027055198891}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8151805788950485
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.91353564, 5.81330851, 5.01942725]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.223118188453341}
episode index:2564
target Thresh 19.0
target distance 13.0
model initialize at round 2564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([2.        , 6.        , 5.93919158]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 13.152946437912071}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8151454131083282
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.15560451,  8.0201211 ,  0.8241532 ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.15690003551971482}
episode index:2565
target Thresh 19.0
target distance 5.0
model initialize at round 2565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.17373319,  4.67397794,  2.47738191]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.3305564029305157}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.815194647222238
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.7751703,  7.0775465,  1.9110113]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.2049105568418899}
episode index:2566
target Thresh 19.0
target distance 4.0
model initialize at round 2566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 6.04707984, 10.93661166,  0.07802981]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 5.6458627847002365}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8152329497544785
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.87299336, 7.05554741, 5.51165919]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.874758776897755}
episode index:2567
target Thresh 19.0
target distance 10.0
model initialize at round 2567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 5.99999996, 10.99999999,  4.49581027]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.206555639219065}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8152153524787973
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.36894048,  4.05249193,  6.23032781]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.37265598670774036}
episode index:2568
target Thresh 19.0
target distance 1.0
model initialize at round 2568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.00124622,  6.99948681,  0.61936444]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.4154577534084445}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8152833885424489
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.94590161,  8.43375908,  2.61936444]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0406136622684086}
episode index:2569
target Thresh 19.0
target distance 13.0
model initialize at round 2569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([3.53647861, 9.31325291, 0.5896756 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.484073500604126}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8152780761692958
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.11351251, 10.14161373,  0.60737845]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.1814925250667276}
episode index:2570
target Thresh 19.0
target distance 7.0
model initialize at round 2570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.        ,  4.        ,  5.45054197]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.602325267042628}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8152855579605369
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.03265054, 10.86946903,  3.75143013]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.13455255787082956}
episode index:2571
target Thresh 19.0
target distance 2.0
model initialize at round 2571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.65294712, 10.78348328,  0.87975329]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.899250524884619}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8153496382257156
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.42928   ,  9.69498911,  4.87975329]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8992948252270356}
episode index:2572
target Thresh 19.0
target distance 10.0
model initialize at round 2572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.76082574,  7.61653896,  2.72768521]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.797506710516155}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8153034134219161
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.46266252, 3.17847273, 5.61264683]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.49589224476061117}
episode index:2573
target Thresh 19.0
target distance 4.0
model initialize at round 2573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([15.00000082,  9.99978531,  5.72619772]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 4.123158493632087}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8153524331328637
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.94996095, 10.39891959,  5.15982711]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.1241545570090665}
episode index:2574
target Thresh 19.0
target distance 5.0
model initialize at round 2574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.35531458, 2.96407443, 0.90923327]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 5.077023301501035}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8153977585366595
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.21897244, 7.83231739, 2.34286265]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.798825074460849}
episode index:2575
target Thresh 19.0
target distance 8.0
model initialize at round 2575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([15.        ,  3.        ,  5.04539967]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 11.313708498984763}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8153831714251931
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.01141157, 10.18944519,  4.77991721]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8106351381030182}
episode index:2576
target Thresh 19.0
target distance 11.0
model initialize at round 2576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.00000074,  8.00000052,  1.61774176]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.045361803874162}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8153716444439677
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.24121865, 6.50378239, 5.63544461]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5517411986655695}
episode index:2577
target Thresh 19.0
target distance 14.0
model initialize at round 2577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([2.37296211, 6.63513825, 2.35654074]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.10355954216234}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8153096923788514
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.58171963,  3.73426819,  4.67513175]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8450492567217811}
episode index:2578
target Thresh 19.0
target distance 1.0
model initialize at round 2578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.90228735, 4.601628  , 0.03000992]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.9435527830760195}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8153735893573785
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.15347924, 4.79492506, 4.03000992]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.2561476327595603}
episode index:2579
target Thresh 19.0
target distance 12.0
model initialize at round 2579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.45103933, 1.31076146, 0.51406639]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.57145311969859}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.815239951005529
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.11929505,  2.20695209,  6.00080433]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.23887334987251588}
episode index:2580
target Thresh 19.0
target distance 13.0
model initialize at round 2580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([15.        ,  4.        ,  5.50707817]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.815199390477155
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.52592202, 7.68846845, 4.39203979]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.5672758077313509}
episode index:2581
target Thresh 19.0
target distance 11.0
model initialize at round 2581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.        ,  9.        ,  1.71400135]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.40175425102727}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8151879569954857
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.5527181, 5.6066751, 5.7317042]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.6783817324635114}
episode index:2582
target Thresh 19.0
target distance 12.0
model initialize at round 2582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.58982612, 7.66787557, 4.83223009]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.940854949311055}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8151366084232887
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.50274942,  3.80165235,  5.4340064 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.5353502917469075}
episode index:2583
target Thresh 19.0
target distance 8.0
model initialize at round 2583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.        , 2.        , 4.87741733]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 8.000000000000941}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8151573550193977
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.69425011, 9.19609604, 1.46149079]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0621886819954947}
episode index:2584
target Thresh 19.0
target distance 3.0
model initialize at round 2584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.00000269, 6.00000359, 1.93761652]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622819116292975}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8152099015164502
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.522445  , 2.96106622, 5.65443122]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5238937121882113}
episode index:2585
target Thresh 19.0
target distance 1.0
model initialize at round 2585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.99313833,  8.25610661,  0.36711806]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7439250337821377}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8152813594044949
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.99313833,  8.25610661,  0.36711806]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7439250337821377}
episode index:2586
target Thresh 19.0
target distance 12.0
model initialize at round 2586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.08604324,  8.46291317,  4.00303316]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.950646368508819}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8152550318957694
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.77267888, 4.02312768, 5.4543654 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.7730249256992376}
episode index:2587
target Thresh 19.0
target distance 6.0
model initialize at round 2587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([5.80291983, 9.76782738, 5.79876041]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 5.530690512332095}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8152930002943737
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.8474973 , 5.77049144, 4.94920449]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7854388114861313}
episode index:2588
target Thresh 19.0
target distance 3.0
model initialize at round 2588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([16.00960339,  3.00124467,  1.13888853]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.6098530911728406}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8153491235117183
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.67150546,  6.03496473,  2.85570323]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6724151368467093}
episode index:2589
target Thresh 19.0
target distance 3.0
model initialize at round 2589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.84017217,  5.4542419 ,  2.0569061 ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.5539990446556435}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8154089497188567
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.770562  ,  6.51432551,  1.77372079]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9108487801024598}
episode index:2590
target Thresh 19.0
target distance 6.0
model initialize at round 2590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.1347816 , 6.24245858, 1.84066218]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 4.89100496894654}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8154539734155716
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.7032003 , 10.18253788,  3.27429156]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8696748703869165}
episode index:2591
target Thresh 19.0
target distance 10.0
model initialize at round 2591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.31205315, 8.65331723, 2.39424631]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.044720332074805}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8154334829606841
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.46387471,  6.91925395,  6.12876385]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.029663817396749}
episode index:2592
target Thresh 19.0
target distance 12.0
model initialize at round 2592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([2.9999995 , 8.99999975, 4.61406732]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.649111039553357}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8153985993714041
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.06610963,  4.85280749,  5.78221424]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.16135711131196673}
episode index:2593
target Thresh 19.0
target distance 14.0
model initialize at round 2593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([16.        ,  7.        ,  1.88356369]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 14.142135624485912}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8153609478421932
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.12095318, 4.54295957, 5.05171062]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.47277438972398794}
episode index:2594
target Thresh 19.0
target distance 13.0
model initialize at round 2594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([15.        ,  6.        ,  5.97247887]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 13.92838827718412}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.815337579186841
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.9214951, 10.356696 ,  5.4238111]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.123829720740488}
episode index:2595
target Thresh 19.0
target distance 9.0
model initialize at round 2595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([3.        , 6.        , 5.43671584]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 10.295630141123889}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8153292111993437
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.0374239 , 11.44337793,  1.17123338]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.44495453234564214}
episode index:2596
target Thresh 19.0
target distance 1.0
model initialize at round 2596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.00271445,  9.00008418,  1.04100197]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0027144495834608}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8153964698781271
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.67280486,  9.66562042,  3.04100197]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.946423224591083}
episode index:2597
target Thresh 19.0
target distance 12.0
model initialize at round 2597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.14761297,  5.24874918,  4.65519261]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.117906553163989}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8153731145345263
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.81954197, 9.98290374, 4.10652485]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8197202751395923}
episode index:2598
target Thresh 19.0
target distance 12.0
model initialize at round 2598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([12.40528907, 10.46214767,  4.47688437]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 10.415547084684333}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8153772682510716
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.54643414, 9.51505717, 4.77777252]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.730588680167263}
episode index:2599
target Thresh 19.0
target distance 7.0
model initialize at round 2599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([10.00000113, 10.99999116,  5.84998322]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 7.071067684397852}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8154045788677889
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.6095838 , 9.40422243, 4.71724199]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8523751069485256}
episode index:2600
target Thresh 19.0
target distance 6.0
model initialize at round 2600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.28559956,  8.91423398,  0.3086726 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 4.922526051489559}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8154423000014358
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.52612379,  4.2519208 ,  5.74230199]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.5833269512490175}
episode index:2601
target Thresh 19.0
target distance 7.0
model initialize at round 2601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([3.00000027, 9.00000016, 1.54790371]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 7.071068007659405}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8154695646331478
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.78460016, 2.50453693, 0.41516248]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9328209440834404}
episode index:2602
target Thresh 19.0
target distance 5.0
model initialize at round 2602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.16939698, 11.02579943,  1.16114014]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 5.093973037079577}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8155072318182612
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.54045488,  6.17011087,  0.31158422]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.5665943721567759}
episode index:2603
target Thresh 19.0
target distance 11.0
model initialize at round 2603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.       ,  8.       ,  0.1619727]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8154957767141864
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.65568746, 9.58952623, 4.17967555]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.5357609889780711}
episode index:2604
target Thresh 19.0
target distance 1.0
model initialize at round 2604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.00005549, 3.99990397, 6.24636602]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.000096030946832}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8155627648997087
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.798052  , 5.12334352, 1.96318072]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8075274696684914}
episode index:2605
target Thresh 19.0
target distance 1.0
model initialize at round 2605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.52111732, 10.18511075,  1.35131663]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9672681952923139}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8156335389730396
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.52111732, 10.18511075,  1.35131663]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9672681952923139}
episode index:2606
target Thresh 19.0
target distance 6.0
model initialize at round 2606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([1.40000000e+01, 5.00000000e+00, 2.43204435e-03]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.48528137434984}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8156407810990377
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.36132232, 10.2137731 ,  4.58650551]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8652783152611538}
episode index:2607
target Thresh 19.0
target distance 8.0
model initialize at round 2607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.00038761, 11.00002702,  1.07959002]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.000027026465663}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8156512498154047
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.21682174, 3.28049458, 3.66366348]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.35452627123913893}
episode index:2608
target Thresh 19.0
target distance 1.0
model initialize at round 2608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.04787883, 7.86566981, 6.06477737]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9615504809930085}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8157219085927847
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.04787883, 7.86566981, 6.06477737]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9615504809930085}
episode index:2609
target Thresh 19.0
target distance 8.0
model initialize at round 2609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.82402776, 5.97392378, 1.61772173]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.761051294713319}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8157165096199672
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.32751097, 11.63388781,  1.63542458]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9241511011120607}
episode index:2610
target Thresh 19.0
target distance 3.0
model initialize at round 2610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([4.03529803, 4.67715524, 2.55975306]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.6798095988509048}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.815771997747267
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.65717667, 5.80367283, 4.27656775]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.395059732425526}
episode index:2611
target Thresh 19.0
target distance 3.0
model initialize at round 2611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 5.14817467, 11.22691968,  1.8285405 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.865676713262615}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8158311581616058
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.54723911, 11.90922602,  1.54535519]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0157186507023408}
episode index:2612
target Thresh 19.0
target distance 14.0
model initialize at round 2612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 3.68037964, 10.09333999,  1.06548994]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.702039133959284}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8158077705339907
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.79269226,  7.98065989,  0.51682218]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0023324378926233}
episode index:2613
target Thresh 19.0
target distance 12.0
model initialize at round 2613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([15.        ,  3.        ,  0.43897074]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8157594313279214
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.03300356, 5.68007248, 5.32393236]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.32162532701706903}
episode index:2614
target Thresh 19.0
target distance 3.0
model initialize at round 2614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 5.0027294 , 10.99955917,  0.84987276]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.162723745761781}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8158111447575858
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.70783697, 8.6548229 , 4.56668746]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.7170441179012297}
episode index:2615
target Thresh 19.0
target distance 11.0
model initialize at round 2615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.71633869, 3.40958352, 2.02171881]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 12.21422447178537}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.815770908550596
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.39280889, 10.40953705,  0.90668043]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5674675522782406}
episode index:2616
target Thresh 19.0
target distance 7.0
model initialize at round 2616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 3.62821198, 10.56128435,  2.19824962]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.386873572297032}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.815797891341259
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.73246519, 11.77298831,  1.06550839]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8179766478951978}
episode index:2617
target Thresh 19.0
target distance 12.0
model initialize at round 2617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([15.88553648,  6.39803921,  2.86080939]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 12.125038943161583}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8157443891096774
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.16876359, 3.60862638, 5.4625857 ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.426209404519825}
episode index:2618
target Thresh 19.0
target distance 13.0
model initialize at round 2618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([2.        , 9.99999877, 5.72510433]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 13.152946247252121}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8157269392267814
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.38654308,  8.69202456,  1.17643657]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7926623133810292}
episode index:2619
target Thresh 19.0
target distance 1.0
model initialize at round 2619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.99898762, 7.99858633, 5.10091591]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.0014141830308227}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.815789677036237
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.73829001, 9.4479508 , 2.8177306 ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5187986441365866}
episode index:2620
target Thresh 19.0
target distance 3.0
model initialize at round 2620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.00044473, 8.00049629, 1.85013979]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 3.000496326713149}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8158412605436249
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.9041301 , 4.76000253, 5.56695449]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.25843727575670333}
episode index:2621
target Thresh 19.0
target distance 13.0
model initialize at round 2621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([16.        , 11.        ,  2.18409899]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8158297566840764
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.82705992, 7.99085194, 6.20180184]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8271105157240132}
episode index:2622
target Thresh 19.0
target distance 11.0
model initialize at round 2622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 3.99999962, 10.99999896,  5.37123179]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 14.212670039420715}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8157869118264003
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.48081493,  2.20364287,  6.25619342]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5221622525216267}
episode index:2623
target Thresh 19.0
target distance 12.0
model initialize at round 2623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.        , 7.        , 5.58164191]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 12.041594579072484}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8157523056618162
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.09506396,  6.57227646,  0.46660353]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.5801185278367682}
episode index:2624
target Thresh 19.0
target distance 12.0
model initialize at round 2624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([2.85575903, 4.23413965, 3.3284142 ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 14.353265002867367}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8157041901495142
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.83372347, 10.84073062,  1.93019052]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8570156697144785}
episode index:2625
target Thresh 19.0
target distance 2.0
model initialize at round 2625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.25167432, 6.90125182, 0.63609284]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 2.2281685887157705}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.815770563268269
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.15884813, 8.57823935, 2.63609284]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9409667954857704}
episode index:2626
target Thresh 19.0
target distance 5.0
model initialize at round 2626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 9.99999996, 11.00000002,  3.5555743 ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 5.830951937635288}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8158008516927039
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.18291996,  8.51429472,  0.42283307]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5458560017549835}
episode index:2627
target Thresh 19.0
target distance 2.0
model initialize at round 2627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4.00114462, 7.9991893 , 0.39374274]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.8298097543383727}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8158633703944952
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.64573476, 9.01521965, 4.39374274]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.1776101711397327}
episode index:2628
target Thresh 19.0
target distance 1.0
model initialize at round 2628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.99969866, 11.00024072,  3.47756791]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.000301373188033}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8159258415354635
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.50717619, 11.23367611,  1.1943826 ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.5584193868302473}
episode index:2629
target Thresh 19.0
target distance 5.0
model initialize at round 2629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([16.86222079,  5.55793731,  6.26196766]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 4.566314239054337}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8159629485339228
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.33107645,  1.47545737,  5.41241174]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6202875045259086}
episode index:2630
target Thresh 19.0
target distance 11.0
model initialize at round 2630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([3.00001278, 4.9999601 , 6.03238678]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 11.045344675444916}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.8158970605269107
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.94680664,  4.23344627,  6.06779248]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.23942993704036797}
episode index:2631
target Thresh 19.0
target distance 2.0
model initialize at round 2631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.99973838, 9.00176242, 2.72816324]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.237761404343286}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8159594476619689
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.15363568, 7.92717759, 0.44497794]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9398203099783059}
episode index:2632
target Thresh 19.0
target distance 5.0
model initialize at round 2632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.99054346,  1.99902662,  4.25416303]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 5.000982317861275}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8160000041552335
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.32238112,  6.03021805,  1.40460711]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0219621358596744}
episode index:2633
target Thresh 19.0
target distance 13.0
model initialize at round 2633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.86071618,  5.23871727,  3.32440495]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.070146782848932}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8159391306610874
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.8148209 , 3.07442468, 5.64299596]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8182127691918755}
episode index:2634
target Thresh 19.0
target distance 5.0
model initialize at round 2634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([1.47713379, 7.59968568, 2.89670563]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 5.658485326665384}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8159692631558113
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.06521716, 10.19467205,  6.04714971]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.2338444275397802}
episode index:2635
target Thresh 19.0
target distance 12.0
model initialize at round 2635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([3.42671349, 8.58231813, 2.9283936 ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.652958375133775}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8159489192450341
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.94544431,  9.82695072,  0.37972584]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.1814452440938245}
episode index:2636
target Thresh 19.0
target distance 2.0
model initialize at round 2636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.35018917, 11.88787331,  1.59944552]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.100260480728777}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8160187148767197
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.35018917, 11.88787331,  1.59944552]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.100260480728777}
episode index:2637
target Thresh 19.0
target distance 4.0
model initialize at round 2637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 4.99966967, 10.99984084,  4.60059977]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.000330333892813}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8160698791432184
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.35452539, 11.66242953,  2.03422915]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9249055947305145}
episode index:2638
target Thresh 19.0
target distance 13.0
model initialize at round 2638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([3.72527409, 7.25979437, 2.05842989]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.992872821672428}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8160166995941154
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.32330517,  3.54144487,  4.66020621]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8666478130488524}
episode index:2639
target Thresh 19.0
target distance 6.0
model initialize at round 2639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([1.36656699, 5.40530122, 3.90837622]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 5.828272283975512}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8160536316198311
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.16027849, 10.51788485,  3.0588203 ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9682805579162171}
episode index:2640
target Thresh 19.0
target distance 12.0
model initialize at round 2640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([15.00155447,  8.00325986,  2.13583887]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 12.043414799763946}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8160332942789478
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.53133467, 6.63189121, 5.87035641]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.6463904504210767}
episode index:2641
target Thresh 19.0
target distance 1.0
model initialize at round 2641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.83929617, 5.02718529, 3.98401523]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9859990812937457}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8161029258859581
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.83929617, 5.02718529, 3.98401523]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9859990812937457}
episode index:2642
target Thresh 19.0
target distance 7.0
model initialize at round 2642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.82549953,  5.20537825,  3.3532207 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.853126574824175}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8161397833666987
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.1156937 , 10.00971989,  2.50366478]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.997015406380947}
episode index:2643
target Thresh 19.0
target distance 8.0
model initialize at round 2643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 3.       , 11.       ,  3.1386826]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 8.062257748375403}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8161596797469567
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.65506124, 2.82826698, 6.00594138]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.6771982461176629}
episode index:2644
target Thresh 19.0
target distance 13.0
model initialize at round 2644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([3.        , 5.        , 4.93280339]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 13.601470509019125}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8161252074052594
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.23068588,  8.6104647 ,  6.10095032]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.862311990039415}
episode index:2645
target Thresh 19.0
target distance 9.0
model initialize at round 2645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4.18423388, 2.01220234, 1.07613629]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 12.589608373581488}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8161020003303533
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.46211114, 10.36866505,  0.52746852]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8294023413535624}
episode index:2646
target Thresh 19.0
target distance 5.0
model initialize at round 2646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([15.00000286,  8.00000576,  2.11979707]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 5.09902572707097}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8161422884656375
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.52265818,  3.29010809,  5.55342646]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5977744352820505}
episode index:2647
target Thresh 19.0
target distance 4.0
model initialize at round 2647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.21020947, 3.48613119, 3.06927896]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.52264234801632}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8162005047464285
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.87287492, 5.13135279, 2.78609365]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8779001981176735}
episode index:2648
target Thresh 19.0
target distance 12.0
model initialize at round 2648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.72314265,  3.81458385,  4.74170423]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.782923445511818}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8161500532893745
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.2042441 , 4.80372768, 5.34348054]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.2832639717292038}
episode index:2649
target Thresh 19.0
target distance 4.0
model initialize at round 2649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.00001524,  7.00000586,  1.37742394]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 4.123115010284592}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8161973476652656
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.47203633,  3.88549631,  0.81105332]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0034550400076083}
episode index:2650
target Thresh 19.0
target distance 2.0
model initialize at round 2650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.00000738, 6.99991263, 5.80663943]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.2361494200100616}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8162591743919103
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.00280457, 8.19363855, 3.52345413]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8063663292844165}
episode index:2651
target Thresh 19.0
target distance 1.0
model initialize at round 2651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.18717042,  2.78956097,  3.38071465]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8114427204987513}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8163284582628032
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.18717042,  2.78956097,  3.38071465]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8114427204987513}
episode index:2652
target Thresh 19.0
target distance 14.0
model initialize at round 2652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.51747222,  2.20344525,  4.64462924]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.519125399462578}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.8162605566524757
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.98877587, 2.67608653, 0.39684963]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.1978191553851898}
episode index:2653
target Thresh 19.0
target distance 7.0
model initialize at round 2653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([10.00000133, 11.00000452,  2.29415041]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 8.062261013528017}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8162580952024262
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.93666882,  4.65559402,  0.31185326]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.1433074844472009}
episode index:2654
target Thresh 19.0
target distance 3.0
model initialize at round 2654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.00000158, 4.99999795, 0.09443348]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.1622801079977796}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8163088417013707
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.57674573, 8.01985695, 3.81124818]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.42371980778866736}
episode index:2655
target Thresh 19.0
target distance 5.0
model initialize at round 2655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([10.99999994, 11.00000001,  4.04822612]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 5.830951952248533}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8163420018042726
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.04076668,  8.28147404,  5.1986702 ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9996780451803808}
episode index:2656
target Thresh 19.0
target distance 4.0
model initialize at round 2656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.00111532, 6.00011045, 1.10870999]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 4.000110607986003}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8163890993381818
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.94152442, 2.62697489, 0.54233937]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.1311789165145782}
episode index:2657
target Thresh 19.0
target distance 4.0
model initialize at round 2657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([11.67459633, 10.52611555,  5.1206634 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 5.580556224185727}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8164222042951685
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.47681645,  6.7577198 ,  6.27110748]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5348398061082931}
episode index:2658
target Thresh 19.0
target distance 12.0
model initialize at round 2658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([3.        , 3.        , 4.53035426]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8163692922397964
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.23052231,  9.06607676,  0.84894527]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.23980549358406233}
episode index:2659
target Thresh 19.0
target distance 12.0
model initialize at round 2659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([16.        ,  8.        ,  0.62135237]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8163461155461736
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.80574514, 7.82307924, 0.07268461]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.1518179833709883}
episode index:2660
target Thresh 19.0
target distance 8.0
model initialize at round 2660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.8853237 , 5.68834784, 0.81709641]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.87875371571606}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8163405854725144
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.03804884, 11.2073619 ,  0.83479926]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9840472520997526}
episode index:2661
target Thresh 19.0
target distance 13.0
model initialize at round 2661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.        ,  9.00000001,  2.47883233]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.31782106661815}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8162955338694254
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.31927412, 2.73572444, 5.36379396]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.4144605345159523}
episode index:2662
target Thresh 19.0
target distance 3.0
model initialize at round 2662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.02273409,  8.7124351 ,  4.50634599]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.0535890951703903}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8163425427374432
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.84975645, 10.85015905,  3.93997537]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.2121919713444587}
episode index:2663
target Thresh 19.0
target distance 7.0
model initialize at round 2663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 7.        , 11.        ,  1.07675141]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 8.062257748446912}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8163589525767465
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.85807974, 4.0300119 , 5.94401018]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8586044273016068}
episode index:2664
target Thresh 19.0
target distance 12.0
model initialize at round 2664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.        ,  9.        ,  6.11012316]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8163474400770206
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.65177985, 8.36377556, 3.8446407 ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.5035771358830846}
episode index:2665
target Thresh 19.0
target distance 3.0
model initialize at round 2665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.23812398, 6.52108033, 5.24668932]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.7012173423745889}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8164051863485596
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.70205389, 5.30780276, 4.96350401]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.4283858311729783}
episode index:2666
target Thresh 19.0
target distance 13.0
model initialize at round 2666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([2.        , 7.        , 5.41526461]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8163878027563048
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.08722383, 10.31378757,  0.86659684]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.3256848666369014}
episode index:2667
target Thresh 19.0
target distance 12.0
model initialize at round 2667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([15.93929347,  7.660195  ,  2.67249089]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 12.232063032814814}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8163508247689892
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.70631671, 5.10654718, 5.84063782]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.714307769640271}
episode index:2668
target Thresh 19.0
target distance 2.0
model initialize at round 2668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.91642682, 9.24020072, 1.88869685]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.1904339833529145}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8164196330024965
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.91642682, 9.24020072, 1.88869685]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.1904339833529145}
episode index:2669
target Thresh 19.0
target distance 6.0
model initialize at round 2669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.44576951,  5.64442589,  3.75854075]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.5765407215660145}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8164559991502423
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.51949994, 10.57678534,  2.90898483]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6403053634135295}
episode index:2670
target Thresh 19.0
target distance 10.0
model initialize at round 2670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.23121198,  9.48065995,  5.87340951]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.527648001173317}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8164504486412152
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.80881682, 6.79825423, 5.89111236]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8335982242100038}
episode index:2671
target Thresh 19.0
target distance 2.0
model initialize at round 2671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.00002621, 11.00001966,  1.65367668]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.2360972846354676}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8165116947307957
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.80970516,  9.36685198,  5.65367668]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8889335311816777}
episode index:2672
target Thresh 19.0
target distance 11.0
model initialize at round 2672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([15.        ,  3.        ,  5.76450658]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 11.045361017187332}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.816439496339535
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.2700424 , 3.62600884, 5.51672697]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.46129414664400153}
episode index:2673
target Thresh 19.0
target distance 4.0
model initialize at round 2673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.49439613, 5.70935725, 1.45318573]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.3275752831839394}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8164934067784506
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.6599651 , 8.40503914, 3.17000043]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6852752447471466}
episode index:2674
target Thresh 19.0
target distance 2.0
model initialize at round 2674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.37072965,  5.82753649,  4.41909313]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.203868907560312}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8165509041964774
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.41809084,  7.22552345,  4.13590782]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9687271028995914}
episode index:2675
target Thresh 19.0
target distance 5.0
model initialize at round 2675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([11.        , 11.00000001,  2.92360789]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 6.403124243151073}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8165803464050208
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.39711012,  6.62477245,  6.07405196]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7101210651799894}
episode index:2676
target Thresh 19.0
target distance 14.0
model initialize at round 2676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.3267082 ,  4.81977061,  4.25888848]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.518032401011691}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8165461289935724
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.99600558, 6.77851328, 5.42703541]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.0203349890728242}
episode index:2677
target Thresh 19.0
target distance 12.0
model initialize at round 2677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.        , 7.        , 3.69667518]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.000000000160288}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.81648605173128
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.21270456,  2.30774987,  6.01526619]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.37410321089952725}
episode index:2678
target Thresh 19.0
target distance 13.0
model initialize at round 2678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([15.      ,  4.      ,  0.105138]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8164386321844855
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.28981482, 4.83214294, 4.99009962]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.33491584492752535}
episode index:2679
target Thresh 19.0
target distance 2.0
model initialize at round 2679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.33554391,  6.94750948,  4.18311834]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.3396247442186012}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8165071252321778
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.33554391,  6.94750948,  4.18311834]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.3396247442186012}
episode index:2680
target Thresh 19.0
target distance 14.0
model initialize at round 2680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([2.        , 5.        , 4.20629478]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 14.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8164471297436818
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.24888501,  5.85223699,  0.24170047]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8878353651125115}
episode index:2681
target Thresh 19.0
target distance 14.0
model initialize at round 2681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([16.       ,  8.       ,  1.7876026]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8164298277362476
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.02256604, 10.47718289,  3.52212014]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.47771616776439196}
episode index:2682
target Thresh 19.0
target distance 1.0
model initialize at round 2682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.9126643 , 2.13981926, 3.13913155]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.386437817052091}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8164908304094731
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.73254012, 2.01015578, 0.85594624]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0253420729324074}
episode index:2683
target Thresh 19.0
target distance 13.0
model initialize at round 2683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([2.        , 7.        , 3.23658323]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 13.601470508760253}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8164309080511143
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.5794483 ,  3.59558487,  5.55517424]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7290988078356765}
episode index:2684
target Thresh 19.0
target distance 11.0
model initialize at round 2684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([3.        , 2.        , 4.65116882]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 12.529964086141668}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8163734969274393
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.41405632,  8.10038065,  4.96975983]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.5944798365888895}
episode index:2685
target Thresh 19.0
target distance 11.0
model initialize at round 2685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 4.90887468, 10.26386235,  1.95734995]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 13.04309132106553}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8163449514607456
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.18746038,  2.89393179,  5.40868219]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.208029260551916}
episode index:2686
target Thresh 19.0
target distance 12.0
model initialize at round 2686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.38463141,  9.36157186,  5.95297003]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 14.40735351639669}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8163055797732917
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.36899524, 2.26799324, 4.83793165]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.6855562594918221}
episode index:2687
target Thresh 19.0
target distance 2.0
model initialize at round 2687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.77475111,  8.06576705,  3.43569422]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.317610974629325}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8163701982331973
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.32087399,  6.52624421,  5.43569422]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.572192860299785}
episode index:2688
target Thresh 19.0
target distance 7.0
model initialize at round 2688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 4.        , 11.        ,  2.16846929]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 7.00000000023552}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8163995653049807
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.36638443, 4.68381216, 5.31891337]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9322380383879973}
episode index:2689
target Thresh 19.0
target distance 14.0
model initialize at round 2689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([3.61408921, 6.47655441, 1.29709023]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.864572154683902}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8163373731351158
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.94271058,  2.83107983,  5.61568124]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.17837068347536977}
episode index:2690
target Thresh 19.0
target distance 9.0
model initialize at round 2690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([3.        , 3.        , 6.00126123]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 12.041594578792306}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8163201697804782
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.6907145 , 11.03964846,  1.45259347]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.31181648920535504}
episode index:2691
target Thresh 19.0
target distance 10.0
model initialize at round 2691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.49415957,  3.60868966,  2.28276417]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 11.375895200298897}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8162917077461571
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.94663675, 8.58743916, 5.7340964 ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1140942072710662}
episode index:2692
target Thresh 19.0
target distance 12.0
model initialize at round 2692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.98952214,  8.68074773,  2.58703029]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.28556264868236}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8162716746999636
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.96723055, 6.25999032, 0.03836253]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.0015637328656999}
episode index:2693
target Thresh 19.0
target distance 3.0
model initialize at round 2693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([16.00026817, 10.00001063,  1.04961365]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 3.162372555781227}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8163216815207506
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.14723491,  7.75188555,  4.76642834]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.136899366468441}
episode index:2694
target Thresh 19.0
target distance 1.0
model initialize at round 2694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.0020751 , 9.00248452, 1.88495034]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0024866688187997}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8163824526964387
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.21879769, 7.46109328, 5.88495034]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5816295065711389}
episode index:2695
target Thresh 19.0
target distance 5.0
model initialize at round 2695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.89665232,  3.42415579,  2.01889406]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 3.57733735839101}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8164253617821994
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.11679679,  7.00629695,  3.45252344]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8832256565530164}
episode index:2696
target Thresh 19.0
target distance 5.0
model initialize at round 2696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.        ,  5.        ,  4.83520937]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.099019514159538}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8164647831142889
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.49949229,  9.56423098,  1.98565345]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6628628754701741}
episode index:2697
target Thresh 19.0
target distance 7.0
model initialize at round 2697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([1.46760245, 3.69579958, 3.72536802]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 6.7938192699948265}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8164906986400865
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.58714312, 9.96052636, 2.59262679]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.4147396408100928}
episode index:2698
target Thresh 19.0
target distance 1.0
model initialize at round 2698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.51578563, 6.52152925, 2.25396201]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7116574416707016}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.816558690230068
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.51578563, 6.52152925, 2.25396201]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7116574416707016}
episode index:2699
target Thresh 19.0
target distance 5.0
model initialize at round 2699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.04074379, 6.32055875, 1.91334837]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 3.8237985790040194}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8166014704736521
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.54492809, 10.1343642 ,  3.34697776]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.4744936085222388}
episode index:2700
target Thresh 19.0
target distance 8.0
model initialize at round 2700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([11.99999781, 11.00000846,  2.83456534]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.31371292781406}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8165842330339377
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.64420151, 3.77759068, 4.56908288]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8551256173662746}
episode index:2701
target Thresh 19.0
target distance 7.0
model initialize at round 2701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([4.87427514, 7.82388748, 0.26117438]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 6.1180530182052815}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8166100659868178
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.22780699, 1.73720446, 5.41161846]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.3477894717946418}
episode index:2702
target Thresh 19.0
target distance 9.0
model initialize at round 2702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.20803779, 9.8190613 , 4.92827082]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 8.021773379653542}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8166326005621091
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.3795421 , 2.74629001, 5.79552959]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8372580125690993}
episode index:2703
target Thresh 19.0
target distance 10.0
model initialize at round 2703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.41136459, 3.83708495, 2.12403211]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.763032839240978}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.8165433705877804
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.77065738,  2.39941082,  5.30988189]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.460572516414183}
episode index:2704
target Thresh 19.0
target distance 11.0
model initialize at round 2704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([3.64469703, 6.64312783, 0.79632872]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.320078915901838}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8165290556113706
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.96573212, 10.69538851,  0.53084626]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.3065329437268522}
episode index:2705
target Thresh 19.0
target distance 2.0
model initialize at round 2705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.00492648,  5.99802796,  0.62924021]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.2400377851406157}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8165931616514254
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.92605339,  7.62488712,  2.62924021]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9991419116277661}
episode index:2706
target Thresh 19.0
target distance 12.0
model initialize at round 2706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([2.11564058, 5.43188565, 3.13407016]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 14.036046989066067}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8165620239373519
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.19091927, 11.42361602,  2.30221709]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9132700352789678}
episode index:2707
target Thresh 19.0
target distance 10.0
model initialize at round 2707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([5.43654784, 9.79132352, 1.4676978 ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.006897354646892}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8165595002461716
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.64493995,  7.25702603,  5.76858595]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.4383263832509259}
episode index:2708
target Thresh 19.0
target distance 13.0
model initialize at round 2708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 2.        , 10.        ,  2.67678654]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 13.341664064128286}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8165394866670284
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.02967999,  7.80995813,  0.12811878]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8105017435062698}
episode index:2709
target Thresh 19.0
target distance 10.0
model initialize at round 2709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.1197273 ,  6.27941327,  3.99037528]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.124006676190305}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8165138898432884
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.34842091, 6.93143668, 5.44170752]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.99447042359461}
episode index:2710
target Thresh 19.0
target distance 2.0
model initialize at round 2710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([4.00023473, 5.99783557, 5.83041668]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.237246639772142}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8165742314552976
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.6343078 , 6.94361517, 3.54723137]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.6368089421188043}
episode index:2711
target Thresh 19.0
target distance 11.0
model initialize at round 2711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 5.30654029, 10.06081001,  1.69195431]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.607133662690524}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8165458856374262
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.24927153,  2.17877008,  5.14328655]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7717201446852557}
episode index:2712
target Thresh 19.0
target distance 2.0
model initialize at round 2712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.95498893,  8.04828513,  4.10542011]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 2.0487796339005846}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8166098200695538
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.57482614,  6.50996675,  6.10542011]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.7684342429591822}
episode index:2713
target Thresh 19.0
target distance 8.0
model initialize at round 2713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.      ,  4.      ,  0.493025]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 10.630145812734655}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8165984229880277
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.58987678, 10.09661254,  4.51072785]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.078917752170987}
episode index:2714
target Thresh 19.0
target distance 5.0
model initialize at round 2714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.99999982,  4.99999966,  5.2343297 ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 5.099019881598035}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8166375192206021
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.53970813,  9.61224176,  2.38477378]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6018513583227799}
episode index:2715
target Thresh 19.0
target distance 3.0
model initialize at round 2715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.44571326,  8.14309562,  0.47495001]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 2.188953888669483}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8166940956126417
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.23136687,  6.47404769,  0.1917647 ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.5274958222282812}
episode index:2716
target Thresh 19.0
target distance 4.0
model initialize at round 2716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 8.58816222, 11.71381985,  3.59904122]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.684794674959511}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8167470591438847
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.62950691, 10.10237929,  5.31585592]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0963584637160972}
episode index:2717
target Thresh 19.0
target distance 7.0
model initialize at round 2717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.6598661 , 10.72210822,  0.8441202 ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 6.730707988971266}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8167726801198126
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.2375054 ,  4.10405498,  5.99456428]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.25929954532797334}
episode index:2718
target Thresh 19.0
target distance 6.0
model initialize at round 2718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.99999991,  3.99999995,  4.62513471]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 6.08276256231231}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8168015751452407
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.80214596,  9.32906746,  1.49239348]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0457478732030292}
episode index:2719
target Thresh 19.0
target distance 7.0
model initialize at round 2719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([16.15309143,  4.95328451,  1.70082301]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.626894128786734}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8168206722914257
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.47615278, 10.10404561,  4.56808178]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0146209857793367}
episode index:2720
target Thresh 19.0
target distance 11.0
model initialize at round 2720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.3179304 ,  9.05504494,  4.1188798 ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 9.318092985425237}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8168241077752715
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.98345753, 9.26889372, 4.41976795]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.2694020900307504}
episode index:2721
target Thresh 19.0
target distance 10.0
model initialize at round 2721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.87119594, 2.78551899, 0.91227597]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.162538058779623}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.8167151865463497
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.70412772,  2.04653512,  5.24856982]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.29950946803185463}
episode index:2722
target Thresh 19.0
target distance 5.0
model initialize at round 2722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 7.33560146, 10.75124594,  4.29995036]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.32383987815879}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8167575479717485
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.93846712, 8.58450722, 5.73357975]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1056080778097317}
episode index:2723
target Thresh 19.0
target distance 11.0
model initialize at round 2723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([15.60462811,  2.37891981,  1.5698201 ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 13.360617373017364}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8167211944418756
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.94244181, 9.24772571, 4.73796703]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9744560517786538}
episode index:2724
target Thresh 19.0
target distance 1.0
model initialize at round 2724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.03786258,  7.93839365,  6.27346659]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.432730421750893}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.816784782994374
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.91048909,  9.24023478,  1.99028128]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.2563687801180592}
episode index:2725
target Thresh 19.0
target distance 5.0
model initialize at round 2725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.11481208, 5.3248538 , 3.94621027]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.758208660388545}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.816820268124414
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.56432559, 10.35685779,  3.09665434]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5631693056213132}
episode index:2726
target Thresh 19.0
target distance 6.0
model initialize at round 2726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.00000014,  1.99999989,  0.36087435]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 6.000000105710881}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8168523773311923
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.36526485,  7.65400309,  1.51131843]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5031225260160969}
episode index:2727
target Thresh 19.0
target distance 6.0
model initialize at round 2727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.00000003, 3.99999979, 5.86134028]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.082762730984659}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8168911941629726
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.32703161, 9.02754444, 3.01178436]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1826057102182106}
episode index:2728
target Thresh 19.0
target distance 1.0
model initialize at round 2728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([15.00420534,  4.0013767 ,  1.32637471]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.41816207148188}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8169509995150566
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.29738988,  2.56303057,  5.32637471]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5285669551104424}
episode index:2729
target Thresh 19.0
target distance 1.0
model initialize at round 2729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.00797747, 4.01091973, 1.94985646]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.416357090658231}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8170107610536959
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.31092172, 2.65051949, 5.94985646]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.7726354288588587}
episode index:2730
target Thresh 19.0
target distance 12.0
model initialize at round 2730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([15.        , 10.        ,  1.29975622]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8170080943042148
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.53987185, 9.53328221, 5.60064438]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7136435476841323}
episode index:2731
target Thresh 19.0
target distance 10.0
model initialize at round 2731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([15.9999802 , 11.00022416,  2.66889101]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.999980203785945}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8170175873858693
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.33296957, 10.04256589,  5.25296448]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.013680824168536}
episode index:2732
target Thresh 19.0
target distance 8.0
model initialize at round 2732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([10.       , 11.       ,  0.2379052]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 9.433981132105222}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8170239892058708
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.19120612, 5.82873167, 4.82197866]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.25669558272680654}
episode index:2733
target Thresh 19.0
target distance 9.0
model initialize at round 2733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 7.64394916, 10.63970614,  0.79424399]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 7.3838139143680355}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8170397296467763
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.32178903,  9.74575637,  5.66150276]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7242996194257549}
episode index:2734
target Thresh 19.0
target distance 13.0
model initialize at round 2734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([16.        ,  5.        ,  5.58346176]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 13.601470508735465}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.817008747431057
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.85492617, 8.68505771, 4.75160869]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.34674928307557523}
episode index:2735
target Thresh 19.0
target distance 14.0
model initialize at round 2735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([16.        ,  5.        ,  0.52106875]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 14.866068747318506}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8169804914463922
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.78717631, 10.21438742,  3.97240098]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.815848337302833}
episode index:2736
target Thresh 19.0
target distance 13.0
model initialize at round 2736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([14.78743574,  5.16708012,  3.385306  ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.845071439453855}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8169239702734062
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.17218726, 4.925869  , 3.70389701]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9417440532157634}
episode index:2737
target Thresh 19.0
target distance 4.0
model initialize at round 2737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 8.99999345, 11.0000098 ,  3.16975844]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 4.000006549483353}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8169729352403992
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.17988671, 10.28877983,  0.60338782]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0855505251773794}
episode index:2738
target Thresh 19.0
target distance 9.0
model initialize at round 2738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.14804226,  2.0459222 ,  4.15111828]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 11.457312831901278}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8169502066357854
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.12388024, 11.36416569,  3.60245051]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.3846595462267899}
episode index:2739
target Thresh 19.0
target distance 7.0
model initialize at round 2739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.83585248,  7.78465941,  4.95850182]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.844735572054964}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8169821160767976
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.74909688,  2.3368306 ,  6.10894589]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8213409641834561}
episode index:2740
target Thresh 19.0
target distance 11.0
model initialize at round 2740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([15.        ,  5.        ,  0.36052721]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8169355881562548
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.73826928, 4.76697265, 5.24548883]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.35043503411066634}
episode index:2741
target Thresh 19.0
target distance 11.0
model initialize at round 2741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([14.20134935,  8.47740074,  3.07637715]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 11.140675290425115}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8168916339731378
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.69674221, 4.39470589, 5.96133877]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.8007761492781769}
episode index:2742
target Thresh 19.0
target distance 14.0
model initialize at round 2742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([16.        ,  3.        ,  1.31486433]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 14.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.816823435649124
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.98217407, 3.96212261, 5.35027003]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.3748984806696964}
episode index:2743
target Thresh 19.0
target distance 6.0
model initialize at round 2743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.82700766,  8.79369509,  4.95099092]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 4.935121442457354}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8168654334305592
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.0298371 ,  4.79624244,  0.10143499]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7968012733366225}
episode index:2744
target Thresh 19.0
target distance 6.0
model initialize at round 2744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([2.99999977, 1.99999978, 4.90774965]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 6.082762786393454}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8168940209791303
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.82862176, 7.70478514, 1.77500842]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8796396069809241}
episode index:2745
target Thresh 19.0
target distance 11.0
model initialize at round 2745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.00003073, 5.99986488, 5.94598699]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.704624859813029}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.8168305532373626
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.39654285,  1.86497309,  5.98139269]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.41890153470147773}
episode index:2746
target Thresh 19.0
target distance 1.0
model initialize at round 2746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.90170098, 10.40276165,  2.65075731]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9875634723888275}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.816897233050527
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.90170098, 10.40276165,  2.65075731]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9875634723888275}
episode index:2747
target Thresh 19.0
target distance 7.0
model initialize at round 2747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.95071226,  4.68224814,  2.61008668]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 5.420285011174902}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.816929068873656
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.3657501 ,  9.40168459,  3.76053076]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8719256051766435}
episode index:2748
target Thresh 19.0
target distance 11.0
model initialize at round 2748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.99999997, 6.99999999, 4.39226484]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.045361047531925}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8169120132450388
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.70814217,  6.99145866,  6.12678239]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.0335237143762244}
episode index:2749
target Thresh 19.0
target distance 4.0
model initialize at round 2749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.61057139,  4.43184357,  6.0936861 ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 2.4628271908928614}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8169677903311315
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.74545148,  2.76971817,  5.8105008 ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8107163598982147}
episode index:2750
target Thresh 19.0
target distance 2.0
model initialize at round 2750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.99779755,  2.99924554,  4.48162007]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.0007556739461405}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8170270895712876
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.5822595 ,  4.44185384,  2.19843477]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8065688264880176}
episode index:2751
target Thresh 19.0
target distance 8.0
model initialize at round 2751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 6.9961722 , 10.98043497,  5.52918434]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 8.938966496743737}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.817039600720097
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.94341947,  6.93079973,  6.11325781]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.08938698711927968}
episode index:2752
target Thresh 19.0
target distance 11.0
model initialize at round 2752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([16.        ,  4.        ,  1.01779288]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 13.038404810405297}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8170197326175278
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.93895457, 10.70104558,  4.75231042]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.3051233985478704}
episode index:2753
target Thresh 19.0
target distance 1.0
model initialize at round 2753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([14.99620882,  8.00091204,  3.91550922]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.003791594748083}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8170825431721328
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.05246486,  7.7132419 ,  5.91550922]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9899762904353347}
episode index:2754
target Thresh 19.0
target distance 4.0
model initialize at round 2754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.91979405, 5.73150307, 5.0169878 ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 2.937337900145573}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8171346351746112
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.7152148 , 3.56472551, 0.45061719]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9112887101365802}
episode index:2755
target Thresh 19.0
target distance 7.0
model initialize at round 2755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4.89326493, 8.3674699 , 0.58384341]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.649994581433291}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8171565460555344
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.55685523, 11.10317463,  1.45110218]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.4549970223962017}
episode index:2756
target Thresh 19.0
target distance 12.0
model initialize at round 2756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.09257365,  8.63263505,  5.11942959]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.924280946668}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.817123112537182
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.6137688 , 1.8150088 , 0.00439121]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.6410412470770043}
episode index:2757
target Thresh 19.0
target distance 6.0
model initialize at round 2757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 9.        , 11.        ,  3.52758753]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 6.32455532158486}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8171514719069142
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.06431874,  9.21793817,  0.3948463 ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.22723104404771882}
episode index:2758
target Thresh 19.0
target distance 2.0
model initialize at round 2758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([1.39856602, 5.57614829, 5.32181573]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.701921748322282}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8172141208841134
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.06190571, 5.62378342, 1.03863042]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.6268477278016708}
episode index:2759
target Thresh 19.0
target distance 10.0
model initialize at round 2759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.        , 10.        ,  1.22596472]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8172114084737283
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.38006558, 6.68561845, 5.52685288]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.4932399048142328}
episode index:2760
target Thresh 19.0
target distance 2.0
model initialize at round 2760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 2.04119754, 11.07391446,  2.07231409]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.233875412108107}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8172668549031112
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.73172757, 10.72134572,  1.78912878]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.76961661839515}
episode index:2761
target Thresh 19.0
target distance 8.0
model initialize at round 2761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.        ,  3.        ,  0.09631556]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8172730992574005
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.73824783, 10.22729643,  4.68038902]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.06868174162102}
episode index:2762
target Thresh 19.0
target distance 14.0
model initialize at round 2762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([1.09839461, 4.38957126, 3.18993497]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 15.59852224786933}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8172293569913102
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.37300167,  8.47476479,  6.07489659]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.817923551819402}
episode index:2763
target Thresh 19.0
target distance 2.0
model initialize at round 2763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.97000018, 9.05313865, 3.0947473 ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.845396113364535}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8172918644598371
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.9825857 , 7.69214296, 5.0947473 ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.201888736823067}
episode index:2764
target Thresh 19.0
target distance 5.0
model initialize at round 2764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([3.68107765, 9.07315837, 1.05349129]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.837702987457203}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8173333738571056
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.00636776, 11.43759693,  2.48712068]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.085723774026065}
episode index:2765
target Thresh 19.0
target distance 7.0
model initialize at round 2765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([16.        , 11.        ,  2.22597609]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8173615751876918
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.72039626,  4.61666892,  5.37642017]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.94828862967369}
episode index:2766
target Thresh 19.0
target distance 3.0
model initialize at round 2766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.0000723 , 5.00006891, 1.77135342]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 3.0000689075079605}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8174098688178734
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.66469643, 1.80092304, 5.48816811]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.38994887288964536}
episode index:2767
target Thresh 19.0
target distance 11.0
model initialize at round 2767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 5.08506143, 10.2864796 ,  1.88012617]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.2362303913466}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8173984050433031
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.83997322,  5.64712489,  5.89782902]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6666177300221447}
episode index:2768
target Thresh 19.0
target distance 2.0
model initialize at round 2768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.99991175,  4.99899579,  5.6347301 ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.237005673707288}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8174571632935584
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.16095287,  6.61519657,  3.35154479]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9230784168658667}
episode index:2769
target Thresh 19.0
target distance 8.0
model initialize at round 2769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.86774745, 3.34695717, 1.24037999]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.697389016304598}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8174514497290261
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.64372159, 11.54320914,  1.25808283]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.6496233342361392}
episode index:2770
target Thresh 19.0
target distance 12.0
model initialize at round 2770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([2.        , 3.        , 3.98593259]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 12.369316876854116}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8174003048713326
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.48973828,  6.32735867,  0.3045236 ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.5890732356596164}
episode index:2771
target Thresh 19.0
target distance 14.0
model initialize at round 2771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([16.        , 10.        ,  1.21253794]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 14.866068747318506}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8173722745930198
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.59935524, 5.31638909, 4.66387018]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.5105078638792641}
episode index:2772
target Thresh 19.0
target distance 8.0
model initialize at round 2772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.97861789, 10.0062608 ,  3.86674941]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 8.065848059810882}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8173939654507212
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.47857187,  2.56396145,  4.73400818]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7680753954274099}
episode index:2773
target Thresh 19.0
target distance 13.0
model initialize at round 2773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([2.99999999, 6.99999997, 5.5145483 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.928388271742609}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.817330958109926
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.49739342,  2.55209269,  5.549954  ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.7466054585609091}
episode index:2774
target Thresh 19.0
target distance 7.0
model initialize at round 2774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.51597034, 11.66246584,  1.42198342]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.804852771977122}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8173494859854787
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.01365646,  4.87302622,  4.28924219]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.3172123455100384}
episode index:2775
target Thresh 19.0
target distance 2.0
model initialize at round 2775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.06982265,  9.87277696,  1.69430989]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.091057967067828}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8174081136922564
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.02329002,  8.27253186,  5.69430989]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0140197211920305}
episode index:2776
target Thresh 19.0
target distance 7.0
model initialize at round 2776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 5.87766193, 11.08580659,  3.53993499]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 8.077432582029491}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8174297604006853
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.95733145, 4.75816725, 4.40719376]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.7593669644984435}
episode index:2777
target Thresh 19.0
target distance 5.0
model initialize at round 2777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.00004384,  9.99970712,  5.87098432]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.0991199508642975}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8174643491289367
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.04366837, 10.44230652,  5.02142839]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5594005200710306}
episode index:2778
target Thresh 19.0
target distance 9.0
model initialize at round 2778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 5.33870855, 10.73101497,  4.31211281]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 9.347598475810607}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8174765813786546
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.77770271, 2.90328612, 4.89618627]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9302375519054142}
episode index:2779
target Thresh 19.0
target distance 10.0
model initialize at round 2779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([5.69702384, 9.69858103, 6.24017286]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.909591701075104}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8174540068124045
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.28688787,  3.73521517,  5.6915051 ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.39040447318045357}
episode index:2780
target Thresh 19.0
target distance 4.0
model initialize at round 2780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([13.99999818,  4.99999912,  4.60341096]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 4.123106923457187}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8175020240878766
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.27585784,  8.00833601,  2.03704034]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0293177395970161}
episode index:2781
target Thresh 19.0
target distance 3.0
model initialize at round 2781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.99943076,  5.00107241,  3.06878233]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 3.606127975231126}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8175534597406128
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.67342365,  2.6871084 ,  4.78559702]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7607693898379588}
episode index:2782
target Thresh 19.0
target distance 4.0
model initialize at round 2782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([14.40104963,  8.65675886,  4.67197609]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.9910559606698026}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8175946066641364
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.15832151,  4.8010565 ,  6.10560548]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.25425227200254014}
episode index:2783
target Thresh 19.0
target distance 1.0
model initialize at round 2783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.59809557, 10.39603635,  4.55837965]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7254648556728962}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8176601258427773
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.59809557, 10.39603635,  4.55837965]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7254648556728962}
episode index:2784
target Thresh 19.0
target distance 11.0
model initialize at round 2784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.7977964 ,  4.6707788 ,  2.70123416]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.299385863830809}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8176268476417413
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.21540819, 8.8606495 , 3.86938109]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8871968446047555}
episode index:2785
target Thresh 19.0
target distance 2.0
model initialize at round 2785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([15.63576806,  5.26317603,  3.52589178]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.792436735955383}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8176781646418698
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.37363789,  2.41521224,  5.24270647]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.6939610932565239}
episode index:2786
target Thresh 19.0
target distance 9.0
model initialize at round 2786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.        , 2.        , 5.27414656]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8176782467276091
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.95435387, 11.1987815 ,  1.29184941]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.20395502721745007}
episode index:2787
target Thresh 19.0
target distance 5.0
model initialize at round 2787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([10.68225066, 10.95311033,  0.98213404]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.3180806706133605}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8177192750996247
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.48029469, 11.47070136,  2.41576343]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7011799930041696}
episode index:2788
target Thresh 19.0
target distance 3.0
model initialize at round 2788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.72350675, 6.90243558, 4.49081635]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.544768101501255}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8177636497408227
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.843665  , 9.76801661, 3.92444574]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.27974439277703955}
episode index:2789
target Thresh 19.0
target distance 7.0
model initialize at round 2789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.76818757, 5.2753493 , 1.87841051]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 5.855679936770322}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8177882451608856
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.03898203, 10.34368443,  0.74566929]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.6574722214797823}
episode index:2790
target Thresh 19.0
target distance 12.0
model initialize at round 2790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([3.90997624, 9.41574409, 2.0095313 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.105403280106623}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8177824559614509
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.81361048, 10.61686369,  2.02723415]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6444081483615209}
episode index:2791
target Thresh 19.0
target distance 12.0
model initialize at round 2791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([2.        , 8.        , 3.58350837]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.8177106463930344
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.84798578,  2.68195782,  5.33572876]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6986950634760307}
episode index:2792
target Thresh 19.0
target distance 5.0
model initialize at round 2792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.00000312,  8.99999875,  0.62928551]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.385168170190316}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8177482525684854
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.26907936, 10.47239259,  4.0629149 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5922611627769326}
episode index:2793
target Thresh 19.0
target distance 5.0
model initialize at round 2793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.13166839,  7.08692879,  3.46726561]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.073426035960021}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8177925374277668
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.61979642, 10.26961341,  2.90089499]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9579207560649807}
episode index:2794
target Thresh 19.0
target distance 12.0
model initialize at round 2794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([15.        , 11.        ,  1.78989714]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8177726984928543
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.88859238, 5.55646576, 5.52441468]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0484515056356913}
episode index:2795
target Thresh 19.0
target distance 2.0
model initialize at round 2795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.20344211, 7.17102482, 1.78174656]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8147109701712747}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8178378727780857
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.20344211, 7.17102482, 1.78174656]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8147109701712747}
episode index:2796
target Thresh 19.0
target distance 14.0
model initialize at round 2796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([2.00000007, 2.99999978, 6.02874613]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 14.035668762933696}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.8177553554056007
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.4481671 ,  2.1101767 ,  5.21459591]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.562724136600129}
episode index:2797
target Thresh 19.0
target distance 9.0
model initialize at round 2797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.00000005, 10.99999993,  0.08498114]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.816653831493877}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8177438910687178
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.71888826, 5.52577362, 4.10268399]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5962060924924689}
episode index:2798
target Thresh 19.0
target distance 12.0
model initialize at round 2798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.58113452,  6.09489843,  4.71941829]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.63763180464827}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8176955919275752
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.0117392 , 5.66789928, 5.3211946 ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6680024422115534}
episode index:2799
target Thresh 19.0
target distance 1.0
model initialize at round 2799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.5434116 ,  4.83061398,  3.63333833]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.48699547645420177}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8177607006447439
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.5434116 ,  4.83061398,  3.63333833]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.48699547645420177}
episode index:2800
target Thresh 19.0
target distance 14.0
model initialize at round 2800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([15.18409388,  9.47196578,  3.08693254]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 13.266009751499643}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8177436647451225
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.57657105, 7.79853741, 4.82145008]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6107547400701319}
episode index:2801
target Thresh 19.0
target distance 12.0
model initialize at round 2801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 4.27264328, 11.87921396,  2.42640027]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 11.760268403350311}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8177408040040361
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.6352149 , 10.42037829,  0.44410312]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6848572869415354}
episode index:2802
target Thresh 19.0
target distance 3.0
model initialize at round 2802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.02596853, 6.00156857, 1.07032936]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.1691012101413456}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8177917691149872
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.94358409, 9.10876966, 2.78714406]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.12252997253396895}
episode index:2803
target Thresh 19.0
target distance 10.0
model initialize at round 2803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.        , 10.        ,  0.29898804]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 12.206555615869483}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8177612810980611
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.57889892, 3.49568374, 5.75032028]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.7621196317563032}
episode index:2804
target Thresh 19.0
target distance 2.0
model initialize at round 2804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.95213528,  9.7065304 ,  0.07375544]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0881842133302255}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8178156617465111
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.24547304,  9.59373658,  6.07375544]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.47466511908864994}
episode index:2805
target Thresh 19.0
target distance 14.0
model initialize at round 2805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([16.        ,  6.        ,  1.41786402]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8177555359327626
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.74326973, 4.49492531, 5.73645503]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.5665782185554287}
episode index:2806
target Thresh 19.0
target distance 12.0
model initialize at round 2806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.13233974,  7.65388606,  4.3817904 ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.151527030767902}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8177224845612004
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.84436182, 6.50344951, 5.54993733]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5203706695360251}
episode index:2807
target Thresh 19.0
target distance 9.0
model initialize at round 2807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 5.99999999, 11.00000001,  3.07726145]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.000000006313892}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8177375614380096
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.41957679, 11.60247376,  1.66133491]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8365797833805232}
episode index:2808
target Thresh 19.0
target distance 3.0
model initialize at round 2808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.46875304, 5.09380548, 3.9768182 ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.9543510179978987}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8177849991341514
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.51598549, 7.39962096, 1.41044759]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7916413441689155}
episode index:2809
target Thresh 19.0
target distance 5.0
model initialize at round 2809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([16.00002779,  7.00023025,  2.46067968]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.385388912079343}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8178223513388821
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.1645463 ,  2.30829701,  5.89430907]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.349460341674295}
episode index:2810
target Thresh 19.0
target distance 6.0
model initialize at round 2810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.68948482, 8.11179779, 0.09930676]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.04510409062246}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8178531445525677
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.76618435, 11.68349824,  1.24975084]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7223846643658902}
episode index:2811
target Thresh 19.0
target distance 12.0
model initialize at round 2811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([3.42691825, 6.10780752, 0.45121497]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 11.26673066512691}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8178389084981036
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.43972841,  9.18401205,  0.18573251]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.9898184602155237}
episode index:2812
target Thresh 19.0
target distance 1.0
model initialize at round 2812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.99946587,  5.00235275,  2.80403775]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.4162554823952007}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8178965910759571
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.26628089,  4.06580236,  0.52085244]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.2742908313698767}
episode index:2813
target Thresh 19.0
target distance 14.0
model initialize at round 2813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([16.00816699,  8.02678711,  2.28486404]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 14.041933115908918}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8178795855872326
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.49894104, 9.84580521, 4.01938158]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.982002346778716}
episode index:2814
target Thresh 19.0
target distance 2.0
model initialize at round 2814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([16.14605052,  5.7565729 ,  6.26279116]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 2.773279864773853}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8179337310275213
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.98904595,  4.40948122,  5.97960585]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0704610001260173}
episode index:2815
target Thresh 19.0
target distance 4.0
model initialize at round 2815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([15.45945851,  7.59378024,  2.90777886]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.814233941599267}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8179776040454096
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.38833363, 10.02828963,  2.34140824]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.3893626981551501}
episode index:2816
target Thresh 19.0
target distance 3.0
model initialize at round 2816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.00079981, 11.00216888,  2.22750175]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.606912719089793}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8180248218110663
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.17537751,  7.96669587,  5.94431644]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8252947407371896}
episode index:2817
target Thresh 19.0
target distance 13.0
model initialize at round 2817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([2.        , 2.        , 5.87831163]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 14.317821063276412}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8179695546780544
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.36456842,  7.65420847,  6.19690264]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7234259303558251}
episode index:2818
target Thresh 19.0
target distance 11.0
model initialize at round 2818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.13080381,  6.86001843,  3.60529757]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.606256174232017}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8179553126790198
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.38181502, 10.70423154,  3.33981511]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8010772525127824}
episode index:2819
target Thresh 19.0
target distance 2.0
model initialize at round 2819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.98568608, 8.01868371, 3.23453331]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.246477456720506}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8180163214333889
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.20396582, 6.54551011, 5.23453331]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.5823944877269738}
episode index:2820
target Thresh 19.0
target distance 11.0
model initialize at round 2820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.99712653, 11.01012741,  2.85726207]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 11.705465126127633}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.818004858058477
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.90730731, 7.52111672, 0.59177962]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0463121898557182}
episode index:2821
target Thresh 19.0
target distance 13.0
model initialize at round 2821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([15.        ,  6.        ,  0.35387009]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 13.601470508735442}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.817985133698551
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.4535231 , 9.82670657, 4.08838763]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.4855036703271788}
episode index:2822
target Thresh 19.0
target distance 6.0
model initialize at round 2822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.1145037 ,  6.43118286,  3.13486433]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 5.034403235040679}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.818015738353638
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.47272377, 10.49644974,  4.28530841]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7290974430703405}
episode index:2823
target Thresh 19.0
target distance 12.0
model initialize at round 2823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.27591884, 11.85975756,  3.03808498]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.27858254394427}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8180070976118852
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.23475702, 6.72448997, 5.05578783]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.36196220166870535}
episode index:2824
target Thresh 19.0
target distance 7.0
model initialize at round 2824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.4295363 ,  5.62708135,  2.32269278]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.378973198266863}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8180099866477165
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.97230737, 11.49647343,  2.62358094]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0917268382431893}
episode index:2825
target Thresh 19.0
target distance 12.0
model initialize at round 2825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([16.        ,  4.        ,  1.02527016]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 13.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8179745020567578
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.15651478, 9.79561153, 4.19341709]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8108603931177129}
episode index:2826
target Thresh 19.0
target distance 5.0
model initialize at round 2826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([1.99999768, 2.99999748, 4.97759724]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 5.385168003736372}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8180050671692276
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.00248904, 7.54008349, 6.12804132]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.0984312919362695}
episode index:2827
target Thresh 19.0
target distance 9.0
model initialize at round 2827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([12.44435936, 10.64215327,  3.76010907]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.4529551538649965}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8180261124152778
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.69257088, 10.36587938,  4.62736784]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9390225709871425}
episode index:2828
target Thresh 19.0
target distance 5.0
model initialize at round 2828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.64913857,  7.64598279,  2.79081529]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.737524506824324}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8180664232090182
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.6749616 , 10.31228643,  4.22444468]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7606575540015074}
episode index:2829
target Thresh 19.0
target distance 4.0
model initialize at round 2829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.99188722, 7.35893683, 1.95028418]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.6410756322848266}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8181167870206051
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.42773724, 9.16210723, 3.66709887]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0146669238068875}
episode index:2830
target Thresh 19.0
target distance 14.0
model initialize at round 2830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([16.36128321,  7.5942742 ,  2.35794517]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 14.449503991303597}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8180737974165886
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.83801892, 5.85542145, 5.2429068 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.2171193877072885}
episode index:2831
target Thresh 19.0
target distance 5.0
model initialize at round 2831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.33061933, 6.26717345, 1.2081539 ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 4.744360473006131}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8181140486702928
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.64352204, 10.6834863 ,  2.64178329]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.4767152833727747}
episode index:2832
target Thresh 19.0
target distance 12.0
model initialize at round 2832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.90222637,  7.7243507 ,  5.01179576]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.926263028630517}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8180998260478887
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.53468488, 7.9179324 , 4.7463133 ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0623030702393526}
episode index:2833
target Thresh 19.0
target distance 1.0
model initialize at round 2833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.01268781, 10.11643845,  2.47225872]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9941545491782178}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8181640110069403
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.01268781, 10.11643845,  2.47225872]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9941545491782178}
episode index:2834
target Thresh 19.0
target distance 5.0
model initialize at round 2834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.21845046,  9.33127236,  5.85255718]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.3384271923450157}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8182075087629874
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.35654432,  6.32951961,  5.28618657]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7229234979152661}
episode index:2835
target Thresh 19.0
target distance 6.0
model initialize at round 2835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([3.99998084, 9.0000721 , 2.84049362]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 6.3246176660859845}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8182347057818505
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.04489107, 2.55290455, 5.9909377 ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.4493434677096068}
episode index:2836
target Thresh 19.0
target distance 1.0
model initialize at round 2836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.02211506, 9.06072188, 2.23152533]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0239171606755135}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8182952504749128
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.39523011, 8.93040294, 4.23152533]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.6087613458986058}
episode index:2837
target Thresh 19.0
target distance 12.0
model initialize at round 2837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([15.47335632,  7.08716232,  2.17014535]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.646772884128769}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8182450157316379
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.44584669, 5.79418757, 4.77192166]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9684109581131829}
episode index:2838
target Thresh 19.0
target distance 13.0
model initialize at round 2838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([16.08505351,  8.31918056,  5.77294827]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 13.288988822433055}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8182147439295676
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.98680579, 6.31844992, 4.94109519]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.3187231406528127}
episode index:2839
target Thresh 19.0
target distance 6.0
model initialize at round 2839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([2.00000025, 4.99999931, 6.06891346]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 6.708204439953055}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8182419000951765
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.09681969, 10.93571862,  2.93617223]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9054649444093982}
episode index:2840
target Thresh 19.0
target distance 1.0
model initialize at round 2840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.61457654, 10.55076892,  2.20347987]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6722333276660052}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8183058768990853
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.61457654, 10.55076892,  2.20347987]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6722333276660052}
episode index:2841
target Thresh 19.0
target distance 10.0
model initialize at round 2841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.        ,  3.        ,  4.83470893]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 12.806248473839394}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8182756156368598
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.33209993, 10.74320279,  4.00285586]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.41980373031336266}
episode index:2842
target Thresh 19.0
target distance 2.0
model initialize at round 2842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.00130712,  5.00398191,  2.26361343]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.0039823320009895}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8183325359268223
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.1301815 ,  3.47988052,  6.26361343]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.4972248356644753}
episode index:2843
target Thresh 19.0
target distance 12.0
model initialize at round 2843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.64085916, 4.54090211, 2.16842474]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 13.067135351231885}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8182946388422039
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.49058427, 11.64771248,  1.05338636]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.824036307629787}
episode index:2844
target Thresh 19.0
target distance 9.0
model initialize at round 2844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 8.03810758, 10.14668956,  5.80364227]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 9.578678341468255}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8182887815313767
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.63721328, 2.1450179 , 5.82134512]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6535066597579432}
episode index:2845
target Thresh 19.0
target distance 9.0
model initialize at round 2845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 5.85140169, 11.0755237 ,  3.68137431]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 9.858926175023502}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8182974443605591
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.70124806, 1.75166306, 6.26544778]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7439220947057169}
episode index:2846
target Thresh 19.0
target distance 1.0
model initialize at round 2846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.18097259,  8.13298663,  4.87342429]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8297537817788595}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8183612668247808
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.18097259,  8.13298663,  4.87342429]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8297537817788595}
episode index:2847
target Thresh 19.0
target distance 13.0
model initialize at round 2847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([2.       , 6.       , 5.7934835]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 13.34166406415049}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.818336270626574
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.91037443,  9.42505805,  0.96163042]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.434404294551477}
episode index:2848
target Thresh 19.0
target distance 11.0
model initialize at round 2848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.        ,  9.        ,  0.04237669]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.704699910727827}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8183139410430629
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.47448758, 5.11142556, 5.77689424]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.48739523996152384}
episode index:2849
target Thresh 19.0
target distance 5.0
model initialize at round 2849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.00573406,  6.00286752,  1.47371643]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.073097373316714}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8183316364366509
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.61386585, 10.34505439,  4.34097521]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7602981939005202}
episode index:2850
target Thresh 19.0
target distance 1.0
model initialize at round 2850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.98012535, 2.99274179, 4.50174356]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.019900480018324}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8183918498226779
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.72593653, 2.23786092, 0.21855825]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8099177456249516}
episode index:2851
target Thresh 19.0
target distance 12.0
model initialize at round 2851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([2.00000331, 1.99999735, 0.33588427]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 12.649108335444698}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.818337112863057
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.77372226,  6.65811908,  0.65447528]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0157592522034007}
episode index:2852
target Thresh 19.0
target distance 9.0
model initialize at round 2852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([16.12480652,  9.58542459,  2.37075108]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.233802986319633}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8183547815275876
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.96737582, 10.17649415,  5.23800985]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.2704242837521194}
episode index:2853
target Thresh 19.0
target distance 12.0
model initialize at round 2853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.32017357,  1.8971835 ,  4.21272302]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.378929937957045}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.8182597576526273
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.69700198, 3.36841936, 4.83220218]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.47701218682428487}
episode index:2854
target Thresh 19.0
target distance 14.0
model initialize at round 2854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 3.60417263, 10.50893836,  1.31721466]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.492780665969176}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8182220320728092
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.41280116,  3.35865905,  0.20217628]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5468465160149748}
episode index:2855
target Thresh 19.0
target distance 7.0
model initialize at round 2855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 9.39600258, 11.89744622,  2.29664361]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 8.853962612090525}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8182306879416157
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.23181003,  6.26354212,  4.88071708]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8121393188944568}
episode index:2856
target Thresh 19.0
target distance 1.0
model initialize at round 2856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.98438116,  2.99701576,  4.34038377]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.423208368759531}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8182908102069494
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.91451413,  1.68860232,  0.05719846]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.3229184911653425}
episode index:2857
target Thresh 19.0
target distance 4.0
model initialize at round 2857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.3462065 ,  9.5697634 ,  4.73361444]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.934239904858524}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8183306193524008
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.06742808,  5.7472364 ,  6.16724383]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.26160272296459963}
episode index:2858
target Thresh 19.0
target distance 11.0
model initialize at round 2858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.        , 9.        , 3.34386075]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 13.038404810595225}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8182879759801368
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.24830957,  1.94591077,  6.22882237]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.25413242137087977}
episode index:2859
target Thresh 19.0
target distance 5.0
model initialize at round 2859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.17489073, 5.67382695, 2.47668853]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.5275764225188926}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8183277582780134
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.42020395, 8.73114228, 3.91031792]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.6390993166646391}
episode index:2860
target Thresh 19.0
target distance 2.0
model initialize at round 2860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.93135667,  6.97940901,  1.82054108]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.3816769521175822}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8183877625568395
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.34710503,  7.5239167 ,  3.82054108]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8080390799412299}
episode index:2861
target Thresh 19.0
target distance 12.0
model initialize at round 2861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([2.99997836, 8.00001575, 3.52240169]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.369341691238338}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8183551254406275
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.16903008,  5.87928233,  4.69054862]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.2098133810793548}
episode index:2862
target Thresh 19.0
target distance 11.0
model initialize at round 2862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([3.17238986, 8.64226889, 2.47620881]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 11.423802523953576}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8183199788835746
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.08252222,  4.86760207,  5.64435573]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.15601002976325729}
episode index:2863
target Thresh 19.0
target distance 10.0
model initialize at round 2863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 6.        , 11.        ,  3.84959686]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 10.000000000000217}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8183256331372639
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.58834954, 11.89910666,  2.15048501]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9888624192063558}
episode index:2864
target Thresh 19.0
target distance 12.0
model initialize at round 2864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.58780059,  6.0845324 ,  4.72674131]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 11.348344652635484}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8182711676600726
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.58095585, 2.38915451, 5.04533232]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.5718734396898492}
episode index:2865
target Thresh 19.0
target distance 1.0
model initialize at round 2865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.19038551,  2.73523988,  0.79114693]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.2790093091990296}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8183310870014332
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.84247719,  4.36471833,  2.79114693]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.39728188452517493}
episode index:2866
target Thresh 19.0
target distance 12.0
model initialize at round 2866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.      ,  2.      ,  6.021667]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.649110640676296}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8182720606817147
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.68876543, 6.54756146, 4.0570727 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.6298337187593263}
episode index:2867
target Thresh 19.0
target distance 12.0
model initialize at round 2867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([2.        , 6.        , 4.20820236]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 12.16552506059728}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8182176708561576
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.8250739 ,  4.99180619,  4.52679337]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0071140245893406}
episode index:2868
target Thresh 19.0
target distance 11.0
model initialize at round 2868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 2.84331161, 10.66379226,  2.67469484]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 12.512003776670813}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8181826457121151
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.53080359,  5.6548519 ,  5.84284176]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8429611240037236}
episode index:2869
target Thresh 19.0
target distance 14.0
model initialize at round 2869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([3.58051615, 7.57823593, 1.36072701]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.614708783137564}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.818137790813215
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.22622335,  2.94391311,  6.24568863]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9706435765081058}
episode index:2870
target Thresh 19.0
target distance 13.0
model initialize at round 2870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.3361736 ,  7.25312012,  4.00061893]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.405224324707257}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8181078937664861
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.79343357, 6.93336353, 3.16876586]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.2250323761296609}
episode index:2871
target Thresh 19.0
target distance 14.0
model initialize at round 2871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([2.       , 4.       , 5.1974237]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 15.652475842498529}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8180729434318174
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.26907182, 11.55656567,  2.08238532]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9187063468857242}
episode index:2872
target Thresh 19.0
target distance 13.0
model initialize at round 2872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([16.        , 11.        ,  0.41517371]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 13.15294643796658}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8180616678304862
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.08094048, 8.92770124, 4.43287656]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.10852866531138392}
episode index:2873
target Thresh 19.0
target distance 12.0
model initialize at round 2873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([3.        , 6.        , 6.05634069]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 12.369316876848112}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8180370020081136
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.94447002,  9.69690104,  1.22448762]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.6991098892663214}
episode index:2874
target Thresh 19.0
target distance 4.0
model initialize at round 2874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([2.9999774 , 6.99996953, 5.08416057]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 4.000030467236266}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8180832465465109
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.82702348, 10.29375785,  2.51778995]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7271168080939358}
episode index:2875
target Thresh 19.0
target distance 4.0
model initialize at round 2875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([11.30380323, 11.88880864,  1.72065848]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.9515430360230845}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8181261522846383
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.17617001,  9.81428888,  1.15428786]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8331279960776719}
episode index:2876
target Thresh 19.0
target distance 5.0
model initialize at round 2876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.57615478,  7.56023006,  2.22705379]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.551756969800718}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8181467970085571
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.67935044, 11.47478859,  3.09431256]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.8288191712993925}
episode index:2877
target Thresh 19.0
target distance 6.0
model initialize at round 2877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 8.9999338 , 11.0002686 ,  2.82243907]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 7.810370779832946}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.818164378320496
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.12374679, 5.77653247, 5.68969785]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.255442766789869}
episode index:2878
target Thresh 19.0
target distance 12.0
model initialize at round 2878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.23257925,  6.72213847,  5.02256823]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.833222353378298}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8181245323033648
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.84119371, 3.17024658, 5.90752985]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8582486539324677}
episode index:2879
target Thresh 19.0
target distance 11.0
model initialize at round 2879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.25731817,  7.67179896,  5.09744501]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.518226853233646}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.818121616795003
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.68435567, 10.79202089,  3.11514786]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0467281271309428}
episode index:2880
target Thresh 19.0
target distance 3.0
model initialize at round 2880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.4006329 , 5.36648593, 5.96289992]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.9567974661545702}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8181744378235365
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.47370017, 4.12666318, 5.67971462]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.49034213332174675}
episode index:2881
target Thresh 19.0
target distance 6.0
model initialize at round 2881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.60991063, 6.63713725, 2.8147099 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.380267241143969}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8182075199920513
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.87070619, 10.98891878,  1.96515398]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8707767037536316}
episode index:2882
target Thresh 19.0
target distance 7.0
model initialize at round 2882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.        , 11.        ,  2.81246883]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.280109890158509}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8182250497502119
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.18349336,  3.53670088,  5.6797276 ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.498313045295473}
episode index:2883
target Thresh 19.0
target distance 4.0
model initialize at round 2883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 2.49662595, 10.60770896,  2.28119189]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.5252690662391197}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8182677873021018
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.60402266, 11.35580874,  1.71482127]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.5323513078314737}
episode index:2884
target Thresh 19.0
target distance 2.0
model initialize at round 2884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.9801106 , 9.00351849, 3.97650135]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.2418993747289937}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8183238816565899
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.24155406, 10.02730416,  1.69331604]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0022402734089695}
episode index:2885
target Thresh 19.0
target distance 8.0
model initialize at round 2885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.52933395,  4.59724361,  2.26026547]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 6.424599891729947}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.818344393486577
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.69960249, 11.28588272,  3.12752424]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.41468975256129986}
episode index:2886
target Thresh 19.0
target distance 3.0
model initialize at round 2886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 2.00860352, 11.00592472,  1.61306017]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.165193579222321}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8183903393322346
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.3628134 , 8.20752384, 5.32987487]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6701290244849751}
episode index:2887
target Thresh 19.0
target distance 5.0
model initialize at round 2887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([2.00061526, 5.99936102, 0.20568865]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.3855296453726735}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8184264731116998
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.31390626, 10.70314523,  3.63931804]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7475609493602786}
episode index:2888
target Thresh 19.0
target distance 10.0
model initialize at round 2888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([10.69711299, 10.11131702,  4.86331272]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 8.949716969961848}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8184291529838784
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.55847586, 7.68013373, 5.16420088]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.5452137129956346}
episode index:2889
target Thresh 19.0
target distance 11.0
model initialize at round 2889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.57453175,  4.62830122,  2.83637774]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.837235666689732}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8183869329371883
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.52749892, 6.61934557, 5.72133936]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.6505020396529693}
episode index:2890
target Thresh 19.0
target distance 3.0
model initialize at round 2890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([3.08583998, 5.65262595, 0.70037621]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 2.866264626488057}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8184328004975353
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.57242987, 3.48671352, 4.4171909 ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.6478474087358287}
episode index:2891
target Thresh 19.0
target distance 9.0
model initialize at round 2891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([16.        ,  9.        ,  1.00989264]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.219544457397827}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.818450197804683
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.72283667, 10.71762858,  3.87715141]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.776032519967749}
episode index:2892
target Thresh 19.0
target distance 1.0
model initialize at round 2892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.3832133 ,  6.95067693,  4.18209004]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.2171707898461697}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8185060739893341
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.3137313 ,  8.05777182,  1.89890474]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.319006126143127}
episode index:2893
target Thresh 19.0
target distance 12.0
model initialize at round 2893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.96917415,  8.52103085,  2.60105997]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.476329108917797}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.818478843270398
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.96274019, 5.13894226, 6.05239221]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.97271456438219}
episode index:2894
target Thresh 19.0
target distance 5.0
model initialize at round 2894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 9.41298425, 11.55741722,  3.81259871]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.45820408145299}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8185213307681978
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.8654346 , 11.47822306,  3.24622809]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9887741608858235}
episode index:2895
target Thresh 19.0
target distance 14.0
model initialize at round 2895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([15.1090653 , 11.33592565,  3.79102719]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 15.534968605979525}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.818486527315791
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.76792215, 3.90119849, 0.67598881]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.1840030217471904}
episode index:2896
target Thresh 19.0
target distance 2.0
model initialize at round 2896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.47437635,  6.58826079,  2.90039396]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.6281417791420681}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8185491829846498
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.47437635,  6.58826079,  2.90039396]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.6281417791420681}
episode index:2897
target Thresh 19.0
target distance 13.0
model initialize at round 2897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.33612239, 4.02331082, 1.66358775]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.838066880617014}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.8184776053644112
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.3767161 ,  2.80522245,  5.41580814]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.0182661810904992}
episode index:2898
target Thresh 19.0
target distance 2.0
model initialize at round 2898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.58732709, 5.8801941 , 0.93466633]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.159600883436969}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8185367714198218
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.0078619 , 7.44877345, 2.93466633]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1349840197780308}
episode index:2899
target Thresh 19.0
target distance 2.0
model initialize at round 2899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4.03368718, 4.02115163, 1.5706703 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.837556064215318}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8185857573641597
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.57789055, 6.20474276, 3.287485  ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.4691438870794528}
episode index:2900
target Thresh 19.0
target distance 5.0
model initialize at round 2900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([14.        ,  3.        ,  4.76435685]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 5.38516481082071}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8186216618581493
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.86272169,  7.50354796,  1.91480093]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.515082482033224}
episode index:2901
target Thresh 19.0
target distance 8.0
model initialize at round 2901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.2833531 , 4.52276239, 3.02066731]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.483432424264297}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8186450123784311
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.39344339, 10.557253  ,  1.88792609]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.5923028000650177}
episode index:2902
target Thresh 19.0
target distance 9.0
model initialize at round 2902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 5.        , 11.        ,  3.26165903]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.486832982515716}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8186447581328986
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.15123266,  8.74213813,  5.56254718]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1274639669638207}
episode index:2903
target Thresh 19.0
target distance 13.0
model initialize at round 2903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([3.06258792, 3.97996278, 0.70016545]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 13.547595009063652}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8186125045439953
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.54679839,  8.68116577,  1.86831237]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8181555531320173}
episode index:2904
target Thresh 19.0
target distance 13.0
model initialize at round 2904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([3.        , 9.        , 4.06178951]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 14.7648230602334}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8185704393851332
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.46072239,  2.83971997,  0.66356583]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9578072632572749}
episode index:2905
target Thresh 19.0
target distance 7.0
model initialize at round 2905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([2.50054746, 8.60681061, 2.27880803]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.92605507086618}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.818593775390753
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.18443157, 11.44853488,  1.1460668 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9307714034583491}
episode index:2906
target Thresh 19.0
target distance 5.0
model initialize at round 2906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([2.00000059, 5.99999867, 6.13994312]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.0710683355946475}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8186140462017637
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.34767215, 10.14997435,  0.72401659]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0714827243126768}
episode index:2907
target Thresh 19.0
target distance 8.0
model initialize at round 2907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([11.00000001, 10.99999999,  0.32496422]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 8.24621125616723}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8186282980272244
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.30130594, 8.44330536, 5.19222299]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.6330040984025663}
episode index:2908
target Thresh 19.0
target distance 12.0
model initialize at round 2908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([16.        ,  8.        ,  1.67233389]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8186169710567122
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.86317691, 7.98412083, 5.69003674]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.3090333004436796}
episode index:2909
target Thresh 19.0
target distance 10.0
model initialize at round 2909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([12.49070197, 10.25541452,  4.60989189]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.497398204580511}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8186083790679105
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.86644416, 5.97334967, 0.34440943]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8668539253897303}
episode index:2910
target Thresh 19.0
target distance 5.0
model initialize at round 2910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.48125009,  6.46555089,  4.48995972]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 4.495580740418389}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8186473543234375
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.24557732,  2.10075604,  5.9235891 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.2654430236067479}
episode index:2911
target Thresh 19.0
target distance 12.0
model initialize at round 2911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.99999997,  1.99999995,  5.17449546]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 12.649110626464855}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.8185869024854435
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.64088327, 6.27900336, 5.20990116]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.6989808612092722}
episode index:2912
target Thresh 19.0
target distance 1.0
model initialize at round 2912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.68775762,  5.3167875 ,  2.09092718]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.757208600163216}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8186491795529047
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.68775762,  5.3167875 ,  2.09092718]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.757208600163216}
episode index:2913
target Thresh 19.0
target distance 11.0
model initialize at round 2913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.       , 11.       ,  2.4235149]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.704699910720104}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8186461180184738
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.82879469, 7.84344681, 0.44121775]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.1824986995733942}
episode index:2914
target Thresh 19.0
target distance 14.0
model initialize at round 2914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([1.99818724, 7.00145963, 3.47368944]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 14.037581059944426}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8186114986066658
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.20192408,  6.28131461,  0.35865106]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.3462820339163295}
episode index:2915
target Thresh 19.0
target distance 12.0
model initialize at round 2915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.6754001 , 3.84056609, 0.91512411]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.487375583025393}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.818546756539655
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.32220856,  2.12729546,  4.95052981]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.689641483263325}
episode index:2916
target Thresh 19.0
target distance 5.0
model initialize at round 2916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.87225536,  7.19084439,  0.47865122]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 4.590045402323474}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8185889003150616
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.62755324,  3.88724458,  6.19546591]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.086750204798574}
episode index:2917
target Thresh 19.0
target distance 7.0
model initialize at round 2917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 7.99999982, 11.00000002,  4.06094217]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 7.000000184457349}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8186152023554809
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.32235133, 11.44800946,  0.92820094]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8123547239964674}
episode index:2918
target Thresh 19.0
target distance 10.0
model initialize at round 2918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.        ,  3.        ,  0.29042834]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8185806409749543
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.95914511, 10.47449396,  3.45857526]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.47624955738123326}
episode index:2919
target Thresh 19.0
target distance 11.0
model initialize at round 2919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.       , 3.       , 5.2953403]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.180339888644502}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8185271140571491
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.24416113,  4.75752171,  5.61393131]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.7937809066749173}
episode index:2920
target Thresh 19.0
target distance 5.0
model initialize at round 2920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.00020784, 7.00014374, 1.61504715]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.000143741366479}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8185627927905865
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.23655775, 2.57809614, 5.04867654]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9576216440948883}
episode index:2921
target Thresh 19.0
target distance 6.0
model initialize at round 2921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.00022077,  5.00002377,  1.11727017]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.810372747363061}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.81857996699318
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.78526635, 10.47929039,  3.98452894]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9422217012604196}
episode index:2922
target Thresh 19.0
target distance 10.0
model initialize at round 2922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([12.32906416, 11.20089997,  4.03193498]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.38385623359347}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8185633618542173
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.6428455 , 5.0702803 , 5.76645252]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6466758543360527}
episode index:2923
target Thresh 19.0
target distance 13.0
model initialize at round 2923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.13451949,  7.55662494,  5.18224692]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.961843776109196}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8185168472280738
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.48217887, 2.73805014, 5.78402324]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5487387307489185}
episode index:2924
target Thresh 19.0
target distance 9.0
model initialize at round 2924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([11.33045441, 11.21213716,  4.02520704]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 7.429995849421652}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8185370195958588
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.98372207, 9.48745351, 4.89246581]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.109239835021364}
episode index:2925
target Thresh 19.0
target distance 4.0
model initialize at round 2925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 6.00016403, 11.00001692,  1.11278885]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 4.472224446663411}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8185790370701599
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.87542114, 7.7911591 , 0.54641824]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.1799554660450318}
episode index:2926
target Thresh 19.0
target distance 1.0
model initialize at round 2926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.33532888, 11.34394173,  1.26209133]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.3851443383995088}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8186342201801462
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.60848987,  9.94810087,  5.26209133]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.394935061591594}
episode index:2927
target Thresh 19.0
target distance 1.0
model initialize at round 2927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([13.992515  ,  8.99680571,  4.55495238]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0074900665133373}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8186927467442923
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.0223722 ,  8.05978559,  0.27176708]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9404805429273938}
episode index:2928
target Thresh 19.0
target distance 14.0
model initialize at round 2928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([16.        ,  4.        ,  0.06278056]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 14.142135623731601}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.818613656281743
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.10757266, 1.8540299 , 5.53181564]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.18132607787514962}
episode index:2929
target Thresh 19.0
target distance 13.0
model initialize at round 2929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.63293366,  3.98158903,  3.52887094]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.800501709932282}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.8185326162673526
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.34676839, 2.43506807, 4.71472072]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7848539730688969}
episode index:2930
target Thresh 19.0
target distance 6.0
model initialize at round 2930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.59431243, 9.00937293, 6.26276112]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.7605602349860225}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8185619064272781
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.96906671, 11.45327088,  1.13001989]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.4543251658948908}
episode index:2931
target Thresh 19.0
target distance 11.0
model initialize at round 2931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 4.00001472, 10.00564188,  2.57818681]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.08536829053654}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8185350095879059
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.3857821 ,  5.5434367 ,  6.02951905]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6664467575949747}
episode index:2932
target Thresh 19.0
target distance 13.0
model initialize at round 2932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([2.        , 3.        , 4.25404716]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 13.038404810405543}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.8184728379126506
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([1.52741005e+01, 2.51754234e+00, 6.26754769e-03]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5856459333110802}
episode index:2933
target Thresh 19.0
target distance 1.0
model initialize at round 2933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.04417468,  5.01032252,  1.23955601]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0112877924818944}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8185279255616238
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.35913042,  3.72233475,  5.23955601]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.698435257923308}
episode index:2934
target Thresh 19.0
target distance 1.0
model initialize at round 2934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([3.00645989, 2.99862858, 0.8008086 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.4197556955066681}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8185863487556402
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.66966524, 4.50464823, 2.8008086 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8385233223194777}
episode index:2935
target Thresh 19.0
target distance 11.0
model initialize at round 2935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.        ,  7.        ,  0.21388357]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.180339887577125}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8185495527333366
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.88827362, 4.56448388, 5.3820305 ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.44961881219753497}
episode index:2936
target Thresh 19.0
target distance 13.0
model initialize at round 2936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.8240373 , 4.46742994, 2.06912778]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.184931614939675}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8184918588537434
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.9586584 ,  4.09845603,  0.10453348]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.10678350654529359}
episode index:2937
target Thresh 19.0
target distance 2.0
model initialize at round 2937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.99405771,  8.00447239,  3.50640535]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 2.004481198055825}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8185502346676121
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.64160611,  6.39808217,  5.50640535]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.5356450255838048}
episode index:2938
target Thresh 19.0
target distance 1.0
model initialize at round 2938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.9730364 , 1.98748755, 4.58606863]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0128714096412577}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8186052022638464
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.41274807, 3.445017  , 2.30288332]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.6069605426317922}
episode index:2939
target Thresh 19.0
target distance 1.0
model initialize at round 2939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.26416155,  7.33782985,  4.88434434]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7129169859894695}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.818666901174641
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.26416155,  7.33782985,  4.88434434]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7129169859894695}
episode index:2940
target Thresh 19.0
target distance 13.0
model initialize at round 2940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([3.        , 6.        , 5.24917102]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.601470508736087}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8186325807500995
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.32707419, 10.52099303,  2.13413264]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8510363649566578}
episode index:2941
target Thresh 19.0
target distance 12.0
model initialize at round 2941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.72405908,  3.65999344,  2.74552023]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.742621197922361}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.8185579116221988
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.84766004, 3.40672614, 4.21455532]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.4343197145591135}
episode index:2942
target Thresh 19.0
target distance 4.0
model initialize at round 2942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([15.99893751, 10.00303933,  2.91709858]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 4.474379848646495}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8185964802379939
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.47637165,  6.61483421,  4.35072797]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8075938019612766}
episode index:2943
target Thresh 19.0
target distance 12.0
model initialize at round 2943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.534909  ,  3.17181746,  4.6660893 ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 13.124966651848927}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8185747828218817
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.89223506, 10.41669731,  4.11742153]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.0659856667422005}
episode index:2944
target Thresh 19.0
target distance 12.0
model initialize at round 2944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.00000138, 8.999991  , 5.87443638]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 12.369313356633121}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8185531001408567
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.51822225,  5.82984317,  5.32576861]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.5109433864057589}
episode index:2945
target Thresh 19.0
target distance 8.0
model initialize at round 2945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 8.07566454, 11.86783216,  1.90404242]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 6.9785065905181325}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8185761251821246
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.4644223 , 11.33170935,  0.77130119]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6299798171581742}
episode index:2946
target Thresh 19.0
target distance 9.0
model initialize at round 2946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([5.00086227, 9.31703022, 5.72290134]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 7.585669450186625}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8185961268440917
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.83808573, 2.76960101, 0.30697481]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.1378371652801107}
episode index:2947
target Thresh 19.0
target distance 3.0
model initialize at round 2947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.72249769,  9.09560929,  3.4426924 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.567820526250174}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8186475864347145
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.8385829 , 10.50551761,  3.15950709]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.973516367492851}
episode index:2948
target Thresh 19.0
target distance 11.0
model initialize at round 2948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([5.61111594, 9.68819344, 0.80174893]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.49429852724355}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.818633742342807
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.67581077,  5.64119832,  0.53626647]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9315876149317679}
episode index:2949
target Thresh 19.0
target distance 2.0
model initialize at round 2949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.22810161, 6.65134703, 2.44353428]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.41664042205609325}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8186952224301484
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.22810161, 6.65134703, 2.44353428]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.41664042205609325}
episode index:2950
target Thresh 19.0
target distance 2.0
model initialize at round 2950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([15.00445541,  9.99873189,  0.73270958]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 2.2406202953055954}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8187499173734116
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.40046997, 10.401027  ,  4.73270958]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7205170722639395}
episode index:2951
target Thresh 19.0
target distance 7.0
model initialize at round 2951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.39599404, 7.42916121, 5.35530543]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.4435836322358595}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8187789255568925
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.95158444, 2.29583626, 0.2225642 ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9965099293135625}
episode index:2952
target Thresh 19.0
target distance 8.0
model initialize at round 2952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([2.        , 3.        , 5.55792522]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.630145812734781}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8187704038359602
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.9851002 , 11.74023192,  1.29244277]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.740381858532291}
episode index:2953
target Thresh 19.0
target distance 2.0
model initialize at round 2953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.55307655, 8.35676448, 0.61733931]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.5718579105713794}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8188317544101525
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.55307655, 8.35676448, 0.61733931]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.5718579105713794}
episode index:2954
target Thresh 19.0
target distance 13.0
model initialize at round 2954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 3.        , 11.        ,  2.62146223]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 14.317821064518544}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8187951119305796
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.19543486,  5.77909151,  1.50642385]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8032299584064075}
episode index:2955
target Thresh 19.0
target distance 1.0
model initialize at round 2955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.99478526, 10.02095   ,  2.82475209]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.020963321335065}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8188530296870307
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.1861253 ,  9.37611678,  4.82475209]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8965800907316371}
episode index:2956
target Thresh 19.0
target distance 3.0
model initialize at round 2956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.15151348,  5.97134507,  0.8230831 ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.0324424136277592}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8189009644115192
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.88336992,  9.22787666,  2.53989779]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.25598896344605054}
episode index:2957
target Thresh 19.0
target distance 1.0
model initialize at round 2957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.04247149, 3.25954751, 0.39240425]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7416695447666315}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.818962187885349
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.04247149, 3.25954751, 0.39240425]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7416695447666315}
episode index:2958
target Thresh 19.0
target distance 3.0
model initialize at round 2958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.25626906,  3.88010416,  0.2819311 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.0218623523175645}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8190133324653134
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.02438788,  2.44939117,  6.2819311 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.4500524337515055}
episode index:2959
target Thresh 19.0
target distance 5.0
model initialize at round 2959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4.610631  , 6.56149737, 2.20154178]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.317255443487749}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8190515257137735
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.75944091, 9.59777153, 3.63517117]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.46867517931465397}
episode index:2960
target Thresh 19.0
target distance 8.0
model initialize at round 2960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([16.00453717, 10.01653021,  2.31291619]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.064727402200392}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8190712722511884
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.60865281, 11.4401219 ,  3.18017496]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7511095347380085}
episode index:2961
target Thresh 19.0
target distance 8.0
model initialize at round 2961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.00047268e+01, 1.09925623e+01, 5.32739958e-03]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.627664314915362}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.81905734587953
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.5719583 , 2.51752864, 6.02303025]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7482746202801651}
episode index:2962
target Thresh 19.0
target distance 1.0
model initialize at round 2962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.23483865,  2.72577398,  1.54136198]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.7628217753802082}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8191184132619534
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.23483865,  2.72577398,  1.54136198]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.7628217753802082}
episode index:2963
target Thresh 19.0
target distance 3.0
model initialize at round 2963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([1.66168931, 9.36099411, 5.51883388]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.9087641217168392}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8191694188580189
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.41062224, 8.27052445, 5.23564857]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6484979733300037}
episode index:2964
target Thresh 19.0
target distance 5.0
model initialize at round 2964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([14.99999895,  1.99999947,  4.62178063]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 5.099020239360674}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8192043514973342
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.56764248,  6.25944845,  1.77222471]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9330780186443882}
episode index:2965
target Thresh 19.0
target distance 9.0
model initialize at round 2965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 5.59073775, 10.08001776,  4.72994161]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 8.485199412416925}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8192152258802059
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.26832894, 2.24377992, 5.31401507]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.362531476928909}
episode index:2966
target Thresh 19.0
target distance 4.0
model initialize at round 2966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.25722965,  8.66214665,  2.42725685]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.654465048412151}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8192564341456324
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.58405415, 11.11419115,  1.86088624]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5951124858729605}
episode index:2967
target Thresh 19.0
target distance 5.0
model initialize at round 2967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([3.99999689, 5.99999648, 4.99872923]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 5.830956508413557}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8192820681820586
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.38305188, 11.20509192,  1.865988  ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.650144346949604}
episode index:2968
target Thresh 19.0
target distance 5.0
model initialize at round 2968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.96875066,  3.99985845,  4.15612245]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.00023920242473}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8193138078854273
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.42675581,  9.09351925,  3.30656652]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5808224818496437}
episode index:2969
target Thresh 19.0
target distance 2.0
model initialize at round 2969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([1.99455443, 3.9999931 , 4.1528604 ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.832285217876699}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.819367944650449
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.11967423, 5.21495195, 1.8696751 ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1795227436313263}
episode index:2970
target Thresh 19.0
target distance 5.0
model initialize at round 2970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([ 6.22108543, 10.83766589,  0.37664908]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 5.323178731264213}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8193996340825705
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.6833236 , 6.24988464, 5.81027846]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7275805619400258}
episode index:2971
target Thresh 19.0
target distance 7.0
model initialize at round 2971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 3.00000001, 11.00000002,  2.24246983]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 7.071067831841956}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8194251854352543
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.35685979, 4.65068644, 5.39291391]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.7421197674679993}
episode index:2972
target Thresh 19.0
target distance 1.0
model initialize at round 2972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.5301636 , 5.51510112, 0.70311755]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7184708518404341}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8194859236843511
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.5301636 , 5.51510112, 0.70311755]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7184708518404341}
episode index:2973
target Thresh 19.0
target distance 2.0
model initialize at round 2973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.0967326 ,  4.60215532,  5.14869976]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1666630958488526}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8195399297624667
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.76670735,  5.76875907,  2.86551445]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0857396844498348}
episode index:2974
target Thresh 19.0
target distance 10.0
model initialize at round 2974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.15396908,  9.63556705,  2.4869349 ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.173840646011985}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8195393808575373
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.98234178, 8.5526503 , 4.78782306]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.44769807524626276}
episode index:2975
target Thresh 19.0
target distance 14.0
model initialize at round 2975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([2.        , 4.        , 4.57194924]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 14.866068747318506}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8194980078189594
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.97482832,  9.54705606,  1.17372556]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5476348628771461}
episode index:2976
target Thresh 19.0
target distance 6.0
model initialize at round 2976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.52394115,  8.44553319,  4.51092434]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 5.466302567707868}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8195265211099201
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.2869721 ,  2.60342076,  5.66136842]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.48951821271883456}
episode index:2977
target Thresh 19.0
target distance 4.0
model initialize at round 2977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 9.99843378, 10.99647564,  5.3042028 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.001567774348984}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8195738916568945
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.27450917, 10.56698718,  0.73783218]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8448887787178375}
episode index:2978
target Thresh 19.0
target distance 12.0
model initialize at round 2978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.0000001 ,  7.00000008,  1.68915242]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.165525168343756}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8195397045608694
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.84173625, 4.66069793, 4.85729934]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.3743972652683636}
episode index:2979
target Thresh 19.0
target distance 8.0
model initialize at round 2979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([15.        ,  3.        ,  6.26844883]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.246211251840673}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8195562166777177
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.69405272, 10.87904378,  2.85252229]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.3289895809577708}
episode index:2980
target Thresh 19.0
target distance 10.0
model initialize at round 2980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.10298784,  3.38179936,  3.89205587]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.79570128139711}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.819507968718101
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.8494042 , 6.81372595, 0.21064688]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8695892781830079}
episode index:2981
target Thresh 19.0
target distance 12.0
model initialize at round 2981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.67898852, 7.88432523, 0.94121331]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.535623270865493}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8195019736211261
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.67500682, 10.44136493,  0.95891616]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5481090882690219}
episode index:2982
target Thresh 19.0
target distance 12.0
model initialize at round 2982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.12317178,  6.25608255,  4.01339722]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.148020614516962}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8194702868502032
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.33047003, 6.57668231, 5.18154415]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5370365985230269}
episode index:2983
target Thresh 19.0
target distance 12.0
model initialize at round 2983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 2.70125852, 11.88339661,  2.11963034]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 12.738756398099834}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8194511422213482
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.43365664,  5.76274414,  5.85414788]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.49431612196163516}
episode index:2984
target Thresh 19.0
target distance 4.0
model initialize at round 2984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 3.00121272, 10.00054107,  1.42965906]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 4.123924802153412}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8194920229607718
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.92837176, 6.81469529, 0.86328845]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.2351528389876683}
episode index:2985
target Thresh 19.0
target distance 11.0
model initialize at round 2985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([3.92747481, 5.40434275, 1.99712151]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.090122460232061}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8194508043388994
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.27382084,  6.6561591 ,  4.88208313]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.978713924092994}
episode index:2986
target Thresh 19.0
target distance 12.0
model initialize at round 2986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.71664498,  8.08875525,  3.44804776]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.717012511698137}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8194421546834942
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.8232606 , 8.08669224, 5.46575061]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8278125194208127}
episode index:2987
target Thresh 19.0
target distance 13.0
model initialize at round 2987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([16.        ,  7.        ,  0.51664275]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8194154673403566
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.40479679, 8.51151399, 3.96797498]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6523089743204701}
episode index:2988
target Thresh 19.0
target distance 11.0
model initialize at round 2988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.60996399, 9.50968984, 0.71437662]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.887123518684986}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8193938225828669
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.39606014,  4.83813833,  0.16570886]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9270056642974799}
episode index:2989
target Thresh 19.0
target distance 11.0
model initialize at round 2989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.68995971,  9.62580677,  2.76923317]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.647889374002906}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8193696681587026
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.07103854, 5.26442431, 4.2205654 ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.27380045424930244}
episode index:2990
target Thresh 19.0
target distance 12.0
model initialize at round 2990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([15.        ,  8.        ,  1.00131148]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8193405587309177
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.86317512, 3.83149702, 0.16945841]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8794683316396555}
episode index:2991
target Thresh 19.0
target distance 10.0
model initialize at round 2991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([3.00000003, 2.99999998, 0.51563853]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 12.806248461663673}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8193164382548486
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.87502835, 11.83681394,  1.96697076]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8460942493628595}
episode index:2992
target Thresh 19.0
target distance 2.0
model initialize at round 2992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.83090435,  6.38005639,  5.61838198]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.2293199312366752}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8193734658397951
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.30561069,  6.87727836,  1.33519667]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9289861214804238}
episode index:2993
target Thresh 19.0
target distance 2.0
model initialize at round 2993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.36747889, 10.59351677,  4.39562154]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.5479683818535656}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8194337953435226
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.36747889, 10.59351677,  4.39562154]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.5479683818535656}
episode index:2994
target Thresh 19.0
target distance 7.0
model initialize at round 2994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.31705316, 3.00883217, 4.14634466]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 7.024446169189077}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8194561496261177
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.56586007, 9.8135798 , 3.01360343]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.47247219463302076}
episode index:2995
target Thresh 19.0
target distance 8.0
model initialize at round 2995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.86758247, 4.67775257, 2.65955859]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.143250103499497}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.819461175531266
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.89039027, 10.71309181,  0.96044675]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.3071328724200019}
episode index:2996
target Thresh 19.0
target distance 12.0
model initialize at round 2996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.17711907, 7.09427491, 2.42032826]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.87369994909846}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8194065202243066
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.69709718,  2.66764173,  4.73891927]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.733140916586708}
episode index:2997
target Thresh 19.0
target distance 3.0
model initialize at round 2997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.99967175, 1.99948496, 5.15498018]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.000515061681328}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8194504106611565
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.66373748, 5.22209109, 2.58860957]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.4029850255134783}
episode index:2998
target Thresh 19.0
target distance 14.0
model initialize at round 2998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([16.36606271,  9.35732347,  5.94165158]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 15.70984784793115}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.819400234385034
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.75604087, 3.59724497, 4.26024259]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.6451492879332029}
episode index:2999
target Thresh 19.0
target distance 10.0
model initialize at round 2999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([3.11224694, 5.42978411, 3.13644195]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 11.458167748313954}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8193663444844382
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.30689436,  9.19936397,  0.02140357]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7212083088150377}
episode index:3000
target Thresh 19.0
target distance 7.0
model initialize at round 3000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([2.        , 7.        , 4.83340764]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.062257754124511}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8193827988224204
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.05480171, 11.46164668,  1.4174811 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0519113372602724}
episode index:3001
target Thresh 19.0
target distance 12.0
model initialize at round 3001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        ,  5.99999998,  5.82110167]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.369316873384323}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8193260765804304
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9342825 , 3.96849946, 3.85650736]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9707265314385987}
episode index:3002
target Thresh 19.0
target distance 5.0
model initialize at round 3002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([10.99999451, 11.00000041,  4.07653832]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 7.071071984194734}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8193454555169668
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.21335091,  6.50517153,  4.9437971 ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9348877309483974}
episode index:3003
target Thresh 19.0
target distance 8.0
model initialize at round 3003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.        ,  3.        ,  0.11214989]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 11.313708498990195}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8193342480220568
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.59780277, 10.46354491,  4.12985274]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.803213682367279}
episode index:3004
target Thresh 19.0
target distance 12.0
model initialize at round 3004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 5.58862456, 10.55557186,  1.34642428]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 10.52694363875984}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8193310512234542
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.2293611 ,  8.91535666,  5.64731243]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7752733813256654}
episode index:3005
target Thresh 19.0
target distance 6.0
model initialize at round 3005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.78493026,  5.4886738 ,  2.09557293]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.67209359851717}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8193623839567409
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.29012781,  9.25140508,  1.246017  ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.802850232263574}
episode index:3006
target Thresh 19.0
target distance 13.0
model initialize at round 3006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([1.10252417, 3.56037974, 3.62970018]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 15.763495303089671}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8193191803325015
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.43913352, 11.70305438,  2.2314765 ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8993645929205517}
episode index:3007
target Thresh 19.0
target distance 12.0
model initialize at round 3007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([16.00000001,  8.00000002,  2.15435627]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 12.16552507368214}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8192927113142354
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.4261134 , 5.82498164, 5.60568851]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.46065611763171915}
episode index:3008
target Thresh 19.0
target distance 1.0
model initialize at round 3008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.10670087, 8.58352539, 5.15973735]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.42992577673909566}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8193527669103424
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.10670087, 8.58352539, 5.15973735]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.42992577673909566}
episode index:3009
target Thresh 19.0
target distance 12.0
model initialize at round 3009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([15.79196237,  6.88726551,  1.85209149]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 12.965259285046535}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8193363849764205
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.20284361, 9.60613882, 3.58660903]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.6391790049730603}
episode index:3010
target Thresh 19.0
target distance 8.0
model initialize at round 3010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([10.89333412, 10.14307372,  5.69567561]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 8.179088493561016}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8193470529890802
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.91691775, 7.68913801, 6.27974908]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9681804294465615}
episode index:3011
target Thresh 19.0
target distance 7.0
model initialize at round 3011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([10.        , 11.        ,  0.69309538]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 8.60232526798569}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8193605693574906
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.88357837, 6.44998622, 5.56035415]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9915635802896838}
episode index:3012
target Thresh 19.0
target distance 2.0
model initialize at round 3012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.23627726, 2.22520936, 3.86483634]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.9321372254646847}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.819413917990296
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.58037018, 3.08092954, 1.58165103]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0869774889720234}
episode index:3013
target Thresh 19.0
target distance 7.0
model initialize at round 3013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 9.83263682, 11.87078705,  2.04647242]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 8.90596455298163}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8194134179968013
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.5449807 ,  4.17116344,  4.34736057]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.486147600820794}
episode index:3014
target Thresh 19.0
target distance 8.0
model initialize at round 3014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([16.0978869 ,  8.66115478,  2.52193737]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.42887710314237}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.819429780316792
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.97884371, 10.23143903,  5.38919614]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.2445163602226268}
episode index:3015
target Thresh 19.0
target distance 10.0
model initialize at round 3015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 4.9999851 , 10.99970463,  5.67197895]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.049861063503407}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8194375765412839
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.7026847 , 10.73129021,  1.97286711]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7894186204832522}
episode index:3016
target Thresh 19.0
target distance 5.0
model initialize at round 3016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.00002843,  1.99993321,  6.12482262]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 5.099079434693322}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.819471818211117
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.13426829,  7.19731955,  3.2752667 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8879337843164273}
episode index:3017
target Thresh 19.0
target distance 2.0
model initialize at round 3017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([14.99903599,  2.99903626,  4.93684912]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.2373611324357094}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8195250415980584
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.47930046,  4.45303132,  2.65366381]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.7551839210905419}
episode index:3018
target Thresh 19.0
target distance 8.0
model initialize at round 3018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([3.99999984, 8.99999972, 5.18968081]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.246211476897084}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8195413452652233
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.07679404, 11.86603637,  1.77375428]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.2658310476466734}
episode index:3019
target Thresh 19.0
target distance 5.0
model initialize at round 3019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.35249754,  9.44680254,  5.32741308]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.507093839970348}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8195817223526853
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.39450616,  6.91145348,  4.76104247]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0942441350163652}
episode index:3020
target Thresh 19.0
target distance 14.0
model initialize at round 3020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([15.18773572,  9.47397925,  3.0844599 ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.196250595476718}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8195653242803426
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.95260702, 8.67519094, 4.81897744]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.32824841021067763}
episode index:3021
target Thresh 19.0
target distance 2.0
model initialize at round 3021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([1.99834394, 9.00059097, 3.80882752]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.8300161140721807}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8196119922769407
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.22816043, 6.63361456, 5.52564221]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.854385515837659}
episode index:3022
target Thresh 19.0
target distance 9.0
model initialize at round 3022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 7.        , 11.        ,  5.30957055]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.727922061343007}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8195981680517083
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.32272335,  2.47451081,  5.04408809]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8269608007908067}
episode index:3023
target Thresh 19.0
target distance 8.0
model initialize at round 3023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 3.0000026 , 10.00000125,  1.45854729]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 8.000001250082788}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8196115477430408
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.98386785, 1.87182255, 0.04262075]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9921821420958742}
episode index:3024
target Thresh 19.0
target distance 13.0
model initialize at round 3024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([2.25219532, 7.50770562, 3.04122961]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 13.757176256441676}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8195778680686127
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.07120697,  7.42172602,  6.20937653]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0200535879373926}
episode index:3025
target Thresh 19.0
target distance 3.0
model initialize at round 3025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.99933314,  7.0166153 ,  2.62090987]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 3.1782545587236557}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8196212957559331
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.65908225,  4.09959046,  0.05453926]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.3551664019307646}
episode index:3026
target Thresh 19.0
target distance 13.0
model initialize at round 3026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 4.55163101, 10.34821069,  0.61231488]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.453663313913463}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8196153523445631
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.02844136, 10.27256185,  0.63001773]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.2740417363658735}
episode index:3027
target Thresh 19.0
target distance 6.0
model initialize at round 3027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.04658936,  4.99096769,  0.81850499]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 6.009212912532504}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8196433466387059
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.62888268, 10.96460397,  1.96894906]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.62987801124097}
episode index:3028
target Thresh 19.0
target distance 13.0
model initialize at round 3028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([15.67572221,  6.80540081,  1.88272971]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.677106669427964}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8196169541087454
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.69013595, 6.48510899, 5.33406194]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8610460944432075}
episode index:3029
target Thresh 19.0
target distance 5.0
model initialize at round 3029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([16.10559424,  3.06747324,  1.77785748]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 4.460750366843836}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8196540657898669
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.35896225,  6.50501028,  3.21148686]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8099038340947242}
episode index:3030
target Thresh 19.0
target distance 5.0
model initialize at round 3030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.32869715,  4.79573353,  1.54959744]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 4.409228124777898}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8196911529829112
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.59119532,  8.75963   ,  2.98322682]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.47423517804828513}
episode index:3031
target Thresh 19.0
target distance 14.0
model initialize at round 3031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([2.        , 8.        , 5.80731297]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 14.317821063276396}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8196747783103592
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.37724006, 11.63510015,  1.2586452 ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8894841994280851}
episode index:3032
target Thresh 19.0
target distance 12.0
model initialize at round 3032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.90581676, 6.41839829, 2.01246313]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.102070031322286}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8196341381652024
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.45830756,  6.42886258,  4.89742475]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.6909079590435641}
episode index:3033
target Thresh 19.0
target distance 8.0
model initialize at round 3033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([15.00056975,  9.67804597,  2.58045679]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.109049194324239}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8196532175603355
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.99689078, 11.75872171,  3.44771556]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.252776858171134}
episode index:3034
target Thresh 19.0
target distance 12.0
model initialize at round 3034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.13324168,  3.04500224,  3.49489808]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.182177783417348}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.8195746387124138
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.79060554, 2.68202022, 4.68074786]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7134406955921232}
episode index:3035
target Thresh 19.0
target distance 5.0
model initialize at round 3035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.99998466, 2.99997301, 5.20559001]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 5.00002699371417}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8196086209442041
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.78864906, 7.87491581, 2.35603409]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.24559168286254207}
episode index:3036
target Thresh 19.0
target distance 14.0
model initialize at round 3036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.60777653, 2.50478628, 0.71120947]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.482101853816395}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8195568176580795
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.43932858,  4.38825283,  1.02980048]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.5863018512096232}
episode index:3037
target Thresh 19.0
target distance 4.0
model initialize at round 3037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([13.47305339, 10.18609811,  0.50521868]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.7914669778563113}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8195969504203385
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.3809613 ,  8.36746004,  6.22203337]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8850512489486443}
episode index:3038
target Thresh 19.0
target distance 8.0
model initialize at round 3038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 7.5224733 , 11.71725989,  1.45027417]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 7.02437565800202}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8196189273605476
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.52476675,  9.49145474,  0.31753294]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6836478683831987}
episode index:3039
target Thresh 19.0
target distance 12.0
model initialize at round 3039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.79311209, 7.17605107, 1.78143233]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 11.841322642761526}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8195975787947065
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.9918099 , 11.57799479,  1.23276456]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.5780528104975559}
episode index:3040
target Thresh 19.0
target distance 2.0
model initialize at round 3040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.02023394, 7.69154793, 0.71640271]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.3085407445605974}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8196536137901702
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.79278119, 9.35855524, 2.71640271]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6740853193174813}
episode index:3041
target Thresh 19.0
target distance 2.0
model initialize at round 3041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.01636459, 4.01695309, 1.8130595 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 2.2440228100973707}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.819706357506873
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.16126407, 2.34628658, 5.8130595 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9074096973546145}
episode index:3042
target Thresh 19.0
target distance 7.0
model initialize at round 3042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([10.        , 11.        ,  1.03952711]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 8.602325267050489}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8197112235614058
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.73053944, 5.55951132, 5.62360058]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.85306397560128}
episode index:3043
target Thresh 19.0
target distance 12.0
model initialize at round 3043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.00000001,  3.        ,  1.15147942]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.000000007365983}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.8196406896729617
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.95178841, 3.72268572, 0.6205145 ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1950631077661182}
episode index:3044
target Thresh 19.0
target distance 4.0
model initialize at round 3044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.54909783, 7.97974399, 4.19648576]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 4.005205988658976}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8196869804185534
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.45983639, 4.87644603, 5.91330045]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9897510575084493}
episode index:3045
target Thresh 19.0
target distance 11.0
model initialize at round 3045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.35593283,  6.6402355 ,  4.36702275]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.721333373896869}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8196353044699537
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.83914504, 4.63142741, 4.68561376]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6515941192696746}
episode index:3046
target Thresh 19.0
target distance 2.0
model initialize at round 3046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.99671959,  1.99957908,  4.27920985]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 2.2379131949055227}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8196879676453821
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.92623306,  3.37399347,  1.99602455]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6303377934840693}
episode index:3047
target Thresh 19.0
target distance 5.0
model initialize at round 3047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.00270727,  7.00111615,  1.40104788]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.404541514211976}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8197127872932211
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.11538312, 10.16540593,  4.55149196]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.8425322126648451}
episode index:3048
target Thresh 19.0
target distance 11.0
model initialize at round 3048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.68008591, 5.90154643, 0.95146674]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.821466955640737}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8196914709599676
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.38065365,  9.95245343,  0.40279897]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.1361150596091596}
episode index:3049
target Thresh 19.0
target distance 13.0
model initialize at round 3049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.33962016,  8.2748215 ,  3.98756254]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.362784391838055}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8196829210624869
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.7713836 , 8.51008903, 6.00526539]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.913808189907539}
episode index:3050
target Thresh 19.0
target distance 11.0
model initialize at round 3050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.85323884, 3.10147869, 1.1230871 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 12.269733422940707}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8196471525623917
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([1.45694874e+01, 9.58093706e+00, 8.04872519e-03]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.600795209448602}
episode index:3051
target Thresh 19.0
target distance 9.0
model initialize at round 3051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([15.        ,  9.        ,  6.09325314]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.2195444573656}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8196603934542918
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.94774094, 10.35584914,  4.6773266 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.1459246120287325}
episode index:3052
target Thresh 19.0
target distance 5.0
model initialize at round 3052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([2.31077972, 9.65402213, 2.39506844]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 4.878569800069138}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8196911359547926
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.55011616, 11.7652719 ,  1.54551252]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.887714227666729}
episode index:3053
target Thresh 19.0
target distance 13.0
model initialize at round 3053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([2.        , 4.        , 0.38116473]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 14.764823060163254}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8196673903615959
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.13517657, 11.66621437,  1.83249696]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0916781370272113}
episode index:3054
target Thresh 19.0
target distance 5.0
model initialize at round 3054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.01338104, 7.36344091, 3.2072022 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.004581974684445}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8196806116264992
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.64950047, 11.66707022,  1.79127566]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7535466784022615}
episode index:3055
target Thresh 19.0
target distance 8.0
model initialize at round 3055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 8.        , 11.        ,  4.25716877]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.313708500681784}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8196720820689132
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.27984712,  3.27298087,  6.27487162]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.3909385658274937}
episode index:3056
target Thresh 19.0
target distance 5.0
model initialize at round 3056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.33942385, 7.64832145, 2.37771213]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.087377283266}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8197088479393215
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.45622954, 10.50094475,  3.81134152]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7380667015606067}
episode index:3057
target Thresh 19.0
target distance 4.0
model initialize at round 3057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([1.97195099, 9.00276568, 4.05330849]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 4.13267684446863}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.819754919607752
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.59106573, 5.74195335, 5.77012318]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8471847528002953}
episode index:3058
target Thresh 19.0
target distance 3.0
model initialize at round 3058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.00179067, 7.99926085, 0.61852663]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 3.6071597353711504}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8197978209252716
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.51817357, 10.36446395,  4.33534133]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.797535441846389}
episode index:3059
target Thresh 19.0
target distance 5.0
model initialize at round 3059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 8.00003389, 11.00006646,  2.10924138]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.385220959335129}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8198314636943901
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.5416056 , 8.4549706 , 5.54287077]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.7683707945027273}
episode index:3060
target Thresh 19.0
target distance 2.0
model initialize at round 3060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.99109683,  6.98861159,  5.0588603 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.8302208253182215}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8198743119747579
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.52053808,  9.22397047,  2.49248969]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.5666768612086347}
episode index:3061
target Thresh 19.0
target distance 11.0
model initialize at round 3061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.83915344, 3.86379513, 1.54002827]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 12.416449679780387}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8198386094650575
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.85079462, 10.38533531,  0.42498989]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6325147653213657}
episode index:3062
target Thresh 19.0
target distance 10.0
model initialize at round 3062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.        ,  7.        ,  5.93963814]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.77032961426901}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8198353085374559
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.50846214, 10.47604614,  3.95734099]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7301105360672768}
episode index:3063
target Thresh 19.0
target distance 2.0
model initialize at round 3063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.10954248, 9.26393578, 1.86035603]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.1552943941154157}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8198941090242255
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.10954248, 9.26393578, 1.86035603]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.1552943941154157}
episode index:3064
target Thresh 19.0
target distance 12.0
model initialize at round 3064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([16.        , 11.        ,  1.63617009]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8198881502902989
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.95683712, 8.88140981, 1.65387294]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.3009306359573016}
episode index:3065
target Thresh 19.0
target distance 2.0
model initialize at round 3065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([13.99535534,  8.00115116,  3.90864265]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.830900371077353}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8199372079712218
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.29892571,  9.00373656,  3.62545734]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.2182142687512236}
episode index:3066
target Thresh 19.0
target distance 8.0
model initialize at round 3066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 7.99999998, 11.        ,  4.29708076]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 10.630145827841886}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8199365459984883
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.52841883,  4.92710175,  0.3147836 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0671195370268098}
episode index:3067
target Thresh 19.0
target distance 13.0
model initialize at round 3067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([1.9961957 , 7.03135226, 2.70154649]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 13.61435739493187}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8198873397444372
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.38683208,  3.10328157,  1.0201375 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.4003824926488933}
episode index:3068
target Thresh 19.0
target distance 14.0
model initialize at round 3068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([1.17927187, 6.37057162, 3.1205132 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 15.45172736466571}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.8198274825096112
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.10556715,  2.15091302,  5.1559189 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.90707489878521}
episode index:3069
target Thresh 19.0
target distance 10.0
model initialize at round 3069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([10.55340764, 11.69953779,  3.63040519]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.932200727723204}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8198112661132906
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.53072067, 2.23344935, 5.36492273]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5797956833908111}
episode index:3070
target Thresh 19.0
target distance 9.0
model initialize at round 3070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([15.        ,  5.        ,  5.67503667]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 10.816653826391967}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8198079826883827
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.44882716, 11.22473798,  3.69273952]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.5019491847178962}
episode index:3071
target Thresh 19.0
target distance 10.0
model initialize at round 3071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([2.36156839, 2.38461663, 3.92102063]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 12.922291620933494}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8197610819938425
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.61193315,  8.29954508,  0.23961163]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.6813144878894808}
episode index:3072
target Thresh 19.0
target distance 13.0
model initialize at round 3072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([2.        , 4.        , 4.54193807]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 13.03840481041377}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.8197013437589157
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.85856197,  3.47236534,  0.29415846]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9799273781477522}
episode index:3073
target Thresh 19.0
target distance 12.0
model initialize at round 3073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.25072734, 11.49693214,  3.04487491]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.116157159511355}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8196928574023394
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.02494211, 6.53862719, 5.06257776]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.46204651520804707}
episode index:3074
target Thresh 19.0
target distance 2.0
model initialize at round 3074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.4915826 ,  6.74473393,  1.47308033]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.3543194502129123}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8197482418389566
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.1557303 ,  7.01549933,  3.47308033]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.2969321117164945}
episode index:3075
target Thresh 19.0
target distance 7.0
model initialize at round 3075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.21325251, 7.51225651, 5.23595262]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.56811848515892}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8197757560890118
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.87859475, 2.744136  , 0.10321139]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.15137618018182}
episode index:3076
target Thresh 19.0
target distance 1.0
model initialize at round 3076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.34306567, 11.32732472,  3.68933725]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.370943095392939}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8198310775852454
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.28780223, 9.65590463, 5.68933725]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.4485886131792462}
episode index:3077
target Thresh 19.0
target distance 5.0
model initialize at round 3077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.99879084, 10.00075755,  3.59191287]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 5.0007576916833765}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8198645128083911
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.47864009,  5.77048741,  0.74235694]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9070541212070836}
episode index:3078
target Thresh 19.0
target distance 2.0
model initialize at round 3078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([13.86322615,  8.21055829,  3.33477151]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.3840061353059607}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8199165542137798
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.24286974,  8.25687245,  1.05158621]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7818083284999297}
episode index:3079
target Thresh 19.0
target distance 13.0
model initialize at round 3079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([2.        , 9.        , 5.79426003]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.152946437970861}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8199080145155428
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.76526257, 11.67614504,  1.52877757]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7157330327085364}
episode index:3080
target Thresh 19.0
target distance 13.0
model initialize at round 3080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([16.        ,  3.        ,  5.91838431]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.8198442746195271
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.86633325, 4.72129734, 5.67060471]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.1272990530966491}
episode index:3081
target Thresh 19.0
target distance 2.0
model initialize at round 3081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.62015134, 6.28047641, 3.95105863]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.4227464708772093}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8198994841345759
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.98255613, 4.72335687, 5.95105863]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0207585311423473}
episode index:3082
target Thresh 19.0
target distance 12.0
model initialize at round 3082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([15.2690788,  7.5893546,  2.1524962]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 12.371594187947315}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8198663446757577
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.84196862, 5.74402646, 5.32064313]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.300826144739037}
episode index:3083
target Thresh 19.0
target distance 2.0
model initialize at round 3083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.00035603, 4.99919095, 6.13694763]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.236950837262728}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8199119768629575
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.69009265, 6.4560677 , 1.57057702]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8786866446119036}
episode index:3084
target Thresh 19.0
target distance 9.0
model initialize at round 3084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 9.28908743, 11.88680024,  1.7354576 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.951781423821876}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8198933158378308
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.28264205,  1.86789583,  5.46997514]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.3119904535643458}
episode index:3085
target Thresh 19.0
target distance 8.0
model initialize at round 3085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 3.        , 11.        ,  3.44623744]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 8.000000000037183}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8199091461997656
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.2957108 , 10.64248304,  0.03031091]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7898364708697164}
episode index:3086
target Thresh 19.0
target distance 6.0
model initialize at round 3086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.32466424,  2.11593396,  4.08250237]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 5.92269462029775}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8199394695237966
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.09017356,  7.26790457,  3.23294644]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.1677961574119071}
episode index:3087
target Thresh 19.0
target distance 7.0
model initialize at round 3087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 7.4293688 , 10.39542931,  4.51903367]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 5.605824520219823}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8199668149271273
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.95338466, 8.30434949, 5.66947775]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.1801999555165894}
episode index:3088
target Thresh 19.0
target distance 11.0
model initialize at round 3088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([3.58320935, 7.63054397, 2.83105218]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.660078495978624}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8199408303231975
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.54927306,  9.37515026,  6.28238442]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7704492027298335}
episode index:3089
target Thresh 19.0
target distance 12.0
model initialize at round 3089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([2.25026129, 5.50674482, 3.04251277]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 12.836886230468208}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8199077525569435
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.40978487,  7.60697955,  6.2106597 ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8466274662201848}
episode index:3090
target Thresh 19.0
target distance 4.0
model initialize at round 3090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([15.27648671,  8.64556206,  2.41433156]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 4.881773916723916}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8199440377705799
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.3058945 , 10.11693123,  3.84796094]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9345490387973251}
episode index:3091
target Thresh 19.0
target distance 2.0
model initialize at round 3091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([4.0043579 , 5.00910204, 2.13426599]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 2.24404490645131}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8199895267654794
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.63882264, 4.09065395, 3.85108068]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.3723804834666855}
episode index:3092
target Thresh 19.0
target distance 12.0
model initialize at round 3092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([3.99990897, 2.99988912, 5.03501248]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.649232059221262}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8199495732224094
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.10050142,  7.33893507,  1.6367888 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.35352159764802177}
episode index:3093
target Thresh 19.0
target distance 1.0
model initialize at round 3093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.93908248,  3.08226296,  0.23609703]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9197566049240169}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8200077666376574
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.93908248,  3.08226296,  0.23609703]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9197566049240169}
episode index:3094
target Thresh 19.0
target distance 12.0
model initialize at round 3094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.00000233, 5.99999655, 0.03226727]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.369313780542383}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8199655828958905
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.13987726,  3.94476692,  4.91722889]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.2776524074691125}
episode index:3095
target Thresh 19.0
target distance 12.0
model initialize at round 3095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([3.11267155, 3.67897705, 2.51378953]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.906703474356261}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.8199041675368166
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.76501333,  3.78088189,  0.26600993]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0931705771555926}
episode index:3096
target Thresh 19.0
target distance 11.0
model initialize at round 3096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([15.00000403,  6.00000044,  1.11960619]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.401758260051027}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8198554325000174
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.53179052, 3.58072908, 5.72138251]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.7874308988523887}
episode index:3097
target Thresh 19.0
target distance 3.0
model initialize at round 3097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([3.00956386, 5.07374504, 4.07727265]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 3.2352940753519763}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8199008619956596
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.7411244 , 2.18630884, 5.79408735]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7641834610349264}
episode index:3098
target Thresh 19.0
target distance 7.0
model initialize at round 3098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 7.99632784, 10.13402191,  5.09191775]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 6.102618557428655}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8199223153708517
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.72401292, 9.66786442, 3.95917652]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7226421999877676}
episode index:3099
target Thresh 19.0
target distance 12.0
model initialize at round 3099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([2.00020718, 2.99983392, 0.33427018]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 12.16534800537467}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8198714636694366
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.55329298,  5.8634556 ,  0.65286118]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0255187430127741}
episode index:3100
target Thresh 19.0
target distance 13.0
model initialize at round 3100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([2.54809159, 8.62115845, 2.85265154]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.579137911689063}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8198163947125514
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.60909685,  3.93608713,  0.88805724]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.11680709641694}
episode index:3101
target Thresh 19.0
target distance 12.0
model initialize at round 3101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([15.34848627,  6.55174674,  2.97830385]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 11.44052350800631}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8197929761760006
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.66267918, 8.83212471, 4.42963609]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8978957989930606}
episode index:3102
target Thresh 19.0
target distance 2.0
model initialize at round 3102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.24211547, 7.46136631, 4.76946831]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.6462018238976508}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8198478285845805
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.66492649, 6.63968159, 0.486283  ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9226699161435441}
episode index:3103
target Thresh 19.0
target distance 8.0
model initialize at round 3103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.        ,  5.        ,  5.82191467]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8198445682880717
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.89853116, 10.86715812,  3.83961752]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.16716127123787036}
episode index:3104
target Thresh 19.0
target distance 6.0
model initialize at round 3104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.95192085, 7.68327084, 5.05010915]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.68351762497357}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8198747366227562
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9480144 , 3.82892248, 4.20055323]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8305510072946046}
episode index:3105
target Thresh 19.0
target distance 11.0
model initialize at round 3105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([16.00000001, 11.00000006,  2.37581176]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 11.0000000124083}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8198821636854611
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.93184721, 10.24872752,  4.95988522]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.1969751722872621}
episode index:3106
target Thresh 19.0
target distance 11.0
model initialize at round 3106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.        , 4.        , 0.10090369]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 12.083045973547481}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8198469757432618
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.30271315,  9.20614337,  5.26905061]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7271203704432719}
episode index:3107
target Thresh 19.0
target distance 3.0
model initialize at round 3107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.00275222,  4.99806996,  0.39842385]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.164979620214696}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8198922617903198
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.54956365,  7.40791136,  2.11523854]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8078299059864742}
episode index:3108
target Thresh 19.0
target distance 11.0
model initialize at round 3108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.        ,  3.        ,  5.59317994]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 12.529964086141668}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8198570932362773
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.82322999, 8.79458528, 4.47814156]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.2710033985997258}
episode index:3109
target Thresh 19.0
target distance 12.0
model initialize at round 3109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([2.75181084, 8.12889854, 3.41633594]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.379668775984301}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8198385998668596
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.05831032, 10.70136714,  0.86766818]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.7037868698169587}
episode index:3110
target Thresh 19.0
target distance 13.0
model initialize at round 3110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([2.        , 4.        , 5.34343743]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 13.60147050873544}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8198011871684131
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.20395869,  7.9129838 ,  6.22839905]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8007831053355368}
episode index:3111
target Thresh 19.0
target distance 11.0
model initialize at round 3111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([3.0112493 , 8.12364173, 2.49006307]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.192072911961048}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8197754478323654
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.1050813 ,  6.86328087,  5.94139531]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8696527706589503}
episode index:3112
target Thresh 19.0
target distance 2.0
model initialize at round 3112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 3.60386524, 10.17984305,  0.07386607]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.9910834559090065}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8198238010453972
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.58193216, 8.75435979, 6.07386607]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.6316519205119647}
episode index:3113
target Thresh 19.0
target distance 3.0
model initialize at round 3113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([15.70130361,  8.36947901,  2.10751946]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.3564873572625813}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8198721232030577
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.7246307 ,  9.15655893,  1.82433415]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.1119723382778073}
episode index:3114
target Thresh 19.0
target distance 9.0
model initialize at round 3114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 9.99999994, 10.99999996,  4.71506095]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 10.81665382825899}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8198741445515754
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.06721626,  2.8902739 ,  5.01594911]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.2894468226885951}
episode index:3115
target Thresh 19.0
target distance 11.0
model initialize at round 3115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([15.        ,  8.        ,  0.28993719]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 11.000000000000103}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8198606487925405
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.86311544, 8.05865751, 0.02445474]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8651063304536947}
episode index:3116
target Thresh 19.0
target distance 5.0
model initialize at round 3116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 8.00002881, 10.99996451,  0.12106293]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.38517837538415}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8198906958245236
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.64007987, 8.5192933 , 5.55469232]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8004880789734183}
episode index:3117
target Thresh 19.0
target distance 13.0
model initialize at round 3117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.318687  ,  8.92533626,  4.19597149]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.369590032139232}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8198848394722831
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.99390932, 9.71503191, 4.21367434]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.285033173327563}
episode index:3118
target Thresh 19.0
target distance 6.0
model initialize at round 3118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.00000053,  3.99999939,  0.15934294]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 6.082763216725494}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8199119306026251
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.36170329,  9.28639612,  1.30978702]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8000373603572477}
episode index:3119
target Thresh 19.0
target distance 10.0
model initialize at round 3119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.00001906, 6.99996374, 6.20641804]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.19801322285709}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8198910355246125
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.80877885,  5.50756755,  5.65775027]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5423931638500853}
episode index:3120
target Thresh 19.0
target distance 2.0
model initialize at round 3120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.52625985, 8.68010235, 0.80339497]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.7605945999642085}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8199423680989398
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.45911967, 7.87284255, 4.80339497]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0268425628242732}
episode index:3121
target Thresh 19.0
target distance 7.0
model initialize at round 3121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 8.        , 11.        ,  1.90289563]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 8.602325267391851}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8199552175501066
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.7163539 , 4.70971127, 0.48696909]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0083912909334036}
episode index:3122
target Thresh 19.0
target distance 3.0
model initialize at round 3122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([16.72160481,  7.54413687,  6.18255472]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.1291358946510694}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8199941304325435
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.89268084,  5.49740174,  5.61618411]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.5139284135682477}
episode index:3123
target Thresh 19.0
target distance 14.0
model initialize at round 3123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([15.63506337,  7.35707294,  5.50381184]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 13.837295465576517}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8199590981331965
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.83499804, 5.87927108, 4.38877346]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8946190685674679}
episode index:3124
target Thresh 19.0
target distance 11.0
model initialize at round 3124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.89048864, 4.65708301, 0.78685826]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.115325599969573}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.8198982547837789
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.41916713,  5.19534509,  4.82226396]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6128021961751478}
episode index:3125
target Thresh 19.0
target distance 8.0
model initialize at round 3125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 7.63696413, 11.8920352 ,  2.1529704 ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.934190208776782}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.819887307850325
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.08986654,  3.64400688,  6.17067325]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.6502467696101073}
episode index:3126
target Thresh 19.0
target distance 10.0
model initialize at round 3126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.        , 10.        ,  1.40006798]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8198592989158204
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.90362434, 2.57924143, 0.56821491]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.0733394507308995}
episode index:3127
target Thresh 19.0
target distance 12.0
model initialize at round 3127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([15.00000017,  7.00000003,  1.19797963]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.36931703551793}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8198509085656696
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.80941385, 10.64744687,  3.21568247]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0365028893386399}
episode index:3128
target Thresh 19.0
target distance 12.0
model initialize at round 3128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.32808156,  3.80745932,  4.26624894]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 12.585782060363291}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8198300930906417
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.66370045, 11.17862094,  3.71758118]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.6873163198937131}
episode index:3129
target Thresh 19.0
target distance 6.0
model initialize at round 3129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 7.82557916, 10.12586437,  4.97528052]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 4.904113307527526}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8198600250888503
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.17363761, 10.64716236,  4.1257246 ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.39324854600821174}
episode index:3130
target Thresh 19.0
target distance 6.0
model initialize at round 3130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.95647145, 3.68240661, 2.60666335]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.317812805325569}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8198899379672899
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.77940969, 7.63471922, 1.75710742]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8607610113835941}
episode index:3131
target Thresh 19.0
target distance 12.0
model initialize at round 3131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.13335562, 4.2005497 , 0.39567011]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.087216921538488}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.8198213288131296
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.45397369,  1.60608335,  6.1478905 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.601051111776822}
episode index:3132
target Thresh 19.0
target distance 12.0
model initialize at round 3132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.        ,  8.        ,  0.67527884]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.000000001212562}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8198104308916467
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.62424865, 7.9070236 , 4.69298169]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.631134679515544}
episode index:3133
target Thresh 19.0
target distance 12.0
model initialize at round 3133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.77537567,  7.15441905,  3.39569581]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.441032022443142}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8197970329747698
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.53530543, 11.51753334,  3.13021335]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7445754932354911}
episode index:3134
target Thresh 19.0
target distance 4.0
model initialize at round 3134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.00000675,  5.99994017,  5.83476901]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.000059827466407}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8198388808270586
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.45792631,  9.17764164,  3.2683984 ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9849452553848349}
episode index:3135
target Thresh 19.0
target distance 13.0
model initialize at round 3135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.411381  ,  7.55558776,  3.81515837]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.516921015843279}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8198157090201404
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.00449521, 5.72647104, 5.2664906 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.2735658977675189}
episode index:3136
target Thresh 19.0
target distance 7.0
model initialize at round 3136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([11.99999977, 11.00000054,  2.98341709]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 7.280110471671548}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8198313067580265
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.48831425,  4.79447295,  5.85067587]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9325438730819787}
episode index:3137
target Thresh 19.0
target distance 6.0
model initialize at round 3137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.87593569,  7.32161309,  5.64860439]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 4.465407064961383}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8198611620609982
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.33564896,  3.35067491,  4.79904847]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.7512224640209243}
episode index:3138
target Thresh 19.0
target distance 12.0
model initialize at round 3138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.        , 11.        ,  1.16203564]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.000000000000002}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8198502722804141
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.37921225, 5.86273845, 5.17973849]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.4032897970384233}
episode index:3139
target Thresh 19.0
target distance 1.0
model initialize at round 3139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.01284249,  5.02700976,  2.13695998]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0270900575907174}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8199013072255478
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.18795857,  3.50862234,  6.13695998]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.5260992605924865}
episode index:3140
target Thresh 19.0
target distance 10.0
model initialize at round 3140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.36977399, 10.55899347,  2.96496272]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 9.498585430481846}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.819908643069597
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.66322662, 8.91423419, 5.54903618]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6687490695132696}
episode index:3141
target Thresh 19.0
target distance 1.0
model initialize at round 3141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([16.01354093,  4.08255036,  2.41821185]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0168971335343782}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8199627778108224
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.49050782,  3.66657687,  4.41821185]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.6088951163424559}
episode index:3142
target Thresh 19.0
target distance 13.0
model initialize at round 3142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.39088275,  4.49308189,  3.85424721]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.91850176748794}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8199325574984289
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.24118619, 7.60069924, 5.02239414]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.4664888798077711}
episode index:3143
target Thresh 19.0
target distance 6.0
model initialize at round 3143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.00078844, 11.00314394,  2.33508307]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.000789263466528}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8199594180319881
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.44909696, 11.72945308,  3.48552715]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8566153587182829}
episode index:3144
target Thresh 19.0
target distance 10.0
model initialize at round 3144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.74481494, 8.49081646, 6.18084145]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.761390053484785}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8199315464744754
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.37696346,  3.55368526,  5.34898837]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8335117851282122}
episode index:3145
target Thresh 19.0
target distance 3.0
model initialize at round 3145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.71151   , 7.00682285, 4.12794685]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 3.020630749219034}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8199762586370708
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.40207739, 3.86509303, 5.84476155]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.42410625451485157}
episode index:3146
target Thresh 19.0
target distance 3.0
model initialize at round 3146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.41324817,  8.5803138 ,  4.77249384]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.029158409557337}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.820020942383929
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.34116426,  6.30417268,  0.20612322]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.45707119024098897}
episode index:3147
target Thresh 19.0
target distance 11.0
model initialize at round 3147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([2.99999843, 3.99999899, 4.72010541]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 11.401756034738531}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8199907515940859
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.6671804 ,  7.96988947,  5.88825234]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.025404541741101}
episode index:3148
target Thresh 19.0
target distance 5.0
model initialize at round 3148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.31232646, 3.65373504, 2.39413369]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 3.36080897725999}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8200293319045993
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.62456165, 6.39356524, 1.82776307]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8705402828880325}
episode index:3149
target Thresh 19.0
target distance 12.0
model initialize at round 3149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([3.9978572 , 8.00019807, 4.05941844]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.043746425162546}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8200110186926763
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.03150155,  7.49365926,  1.51075068]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.49466333692873377}
episode index:3150
target Thresh 19.0
target distance 12.0
model initialize at round 3150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 4.56301858, 11.62398798,  1.38983363]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.455617736201406}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8200129728675868
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.49603425, 11.60677287,  1.69072178]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7887678946665014}
episode index:3151
target Thresh 19.0
target distance 1.0
model initialize at round 3151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.57714369, 10.61317847,  0.41953152]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5730954167465598}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8200700753508141
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.57714369, 10.61317847,  0.41953152]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5730954167465598}
episode index:3152
target Thresh 19.0
target distance 6.0
model initialize at round 3152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.47907281, 2.80257881, 1.50716179]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 5.22346169931851}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8200968155980891
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.81328895, 8.43342913, 2.65760587]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.47193413321935745}
episode index:3153
target Thresh 19.0
target distance 12.0
model initialize at round 3153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.13255716,  8.34062743,  4.69325519]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 11.152067117659573}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8200909607388439
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.73304065, 8.64308186, 4.71095804]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8153153740119108}
episode index:3154
target Thresh 19.0
target distance 4.0
model initialize at round 3154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.10020405, 8.27355135, 1.86829537]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.323062874211337}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8201264518282793
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.44319788, 11.04441119,  3.30192475]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.5585704521461551}
episode index:3155
target Thresh 19.0
target distance 8.0
model initialize at round 3155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 8.25196807, 11.88209684,  1.76997584]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.154711074533049}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8201081426592421
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.81496356,  2.81848523,  5.50449338]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.2592028117441005}
episode index:3156
target Thresh 19.0
target distance 2.0
model initialize at round 3156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.99880827,  9.00023099,  3.96014154]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.236807741148684}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.820161956994795
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.39269544,  7.37485462,  5.96014154]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7136769668641385}
episode index:3157
target Thresh 19.0
target distance 6.0
model initialize at round 3157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 8.44407068, 10.09648804,  5.33735394]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 6.761949019870909}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.820171867005593
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.76971078, 5.07797363, 5.92142741]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.7736501580940978}
episode index:3158
target Thresh 19.0
target distance 11.0
model initialize at round 3158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([15.        , 11.        ,  1.79245013]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 13.038404810405297}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8201463932817509
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.70080272, 4.40093971, 5.24378236]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.5002715855017607}
episode index:3159
target Thresh 19.0
target distance 12.0
model initialize at round 3159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([15.50301857,  4.75150152,  1.47364348]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 13.888277532332081}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.820113983199256
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.85231568, 7.63821342, 4.64179041]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.39076871469867636}
episode index:3160
target Thresh 19.0
target distance 3.0
model initialize at round 3160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([12.53252858,  9.9132385 ,  6.1780138 ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.6961946785636126}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8201584254728405
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.05954593, 11.43320338,  1.61164319]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.43727667055467895}
episode index:3161
target Thresh 19.0
target distance 12.0
model initialize at round 3161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([2.68641862, 4.05208937, 3.47628367]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 13.3472837541108}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8201043281303028
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.2612379 ,  5.73755374,  1.51168937]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.7824517586378926}
episode index:3162
target Thresh 19.0
target distance 2.0
model initialize at round 3162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([1.99564661, 2.00470946, 3.32692659]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 2.0043589256679053}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8201549116497051
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.64493278, 2.01109893, 1.04374128]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.3552406483348478}
episode index:3163
target Thresh 19.0
target distance 2.0
model initialize at round 3163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([13.55157248,  4.17110848,  2.94648248]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.5849291016891325}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8201962628312003
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.19862916,  4.3504318 ,  0.38011187]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6792587086263905}
episode index:3164
target Thresh 19.0
target distance 12.0
model initialize at round 3164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 4.46566277, 11.82700173,  1.5238263 ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 13.123803445092735}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8201708296907759
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.58276252,  4.90145565,  4.97515853]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9933324705430715}
episode index:3165
target Thresh 19.0
target distance 12.0
model initialize at round 3165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 3.9999999 , 10.00000074,  2.71358299]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.64911096748249}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.82014541261677
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.27547386,  5.63834152,  6.16491523]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.4546236945790142}
episode index:3166
target Thresh 19.0
target distance 11.0
model initialize at round 3166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.02119215, 6.66227226, 0.09121197]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.327841000252782}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8201176759439053
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.65917809,  4.86594062,  5.54254421]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9305980516569903}
episode index:3167
target Thresh 19.0
target distance 13.0
model initialize at round 3167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.05654884,  8.39343461,  3.17596591]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.207933481257054}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.82007862781957
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.666293  , 2.81229781, 6.06092753]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.6922271832236784}
episode index:3168
target Thresh 19.0
target distance 3.0
model initialize at round 3168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.61521137,  6.08071131,  4.7376411 ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 2.115991928678676}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8201260309032495
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.44706913,  4.87977962,  4.45445579]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0391076629502607}
episode index:3169
target Thresh 19.0
target distance 7.0
model initialize at round 3169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([1.37775942, 2.4469769 , 3.88273323]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 6.7508352206399165}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8201469327457772
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.94695629, 8.9677881 , 2.749992  ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.0620583682665048}
episode index:3170
target Thresh 19.0
target distance 11.0
model initialize at round 3170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([2.99999092, 7.99998173, 5.26093483]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.180345556785593}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8201239195517016
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.70402239,  6.16403254,  0.42908176]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7228791075996432}
episode index:3171
target Thresh 19.0
target distance 11.0
model initialize at round 3171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.33754653,  3.9812367 ,  0.95446986]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 12.028733091593262}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.820091639164894
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.27379277, 8.84984697, 4.12261678]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8928618880732113}
episode index:3172
target Thresh 19.0
target distance 3.0
model initialize at round 3172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([2.99940454, 7.99875761, 5.27545166]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.2439402642956034}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8201269287043652
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.86162633, 10.22710591,  0.42589574]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.7851831320223198}
episode index:3173
target Thresh 19.0
target distance 5.0
model initialize at round 3173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 2.6280635 , 10.0382301 ,  4.04916573]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 5.051940153426712}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8201621960072016
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.80615117, 5.17418538, 5.48279511]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.26061065669832034}
episode index:3174
target Thresh 19.0
target distance 12.0
model initialize at round 3174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([2.71113135, 3.08222257, 3.45312715]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 13.854349857502056}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.820108318978024
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.83283615,  7.86648169,  1.48853285]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8824592186549778}
episode index:3175
target Thresh 19.0
target distance 5.0
model initialize at round 3175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.10253979,  3.36248315,  3.22550178]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.74659360456209}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.82014653428987
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.3724745 ,  6.48831461,  2.65913116]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8096975911146477}
episode index:3176
target Thresh 19.0
target distance 7.0
model initialize at round 3176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.98510048,  4.00254278,  3.98255897]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 7.0706738372734}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8201673836249114
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.58429121, 10.65683662,  2.84981774]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.5390500041941402}
episode index:3177
target Thresh 19.0
target distance 5.0
model initialize at round 3177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.99972038, 7.99835934, 5.55358124]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.83203591103522}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8201854307109322
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.49047268, 10.52439105,  0.1376547 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.6970092942805682}
episode index:3178
target Thresh 19.0
target distance 3.0
model initialize at round 3178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.13634945,  6.80509932,  1.62640541]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.4716146661392853}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8202326510850402
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.82447586,  8.04018313,  1.34322011]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.2653097923738958}
episode index:3179
target Thresh 19.0
target distance 1.0
model initialize at round 3179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.00728842,  8.00927377,  1.91470164]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.0093000879914336}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8202829238362713
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.28841196,  6.42769288,  5.91470164]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.6408719827464081}
episode index:3180
target Thresh 19.0
target distance 3.0
model initialize at round 3180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.91442942, 3.41207397, 2.00612494]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.5902299810441674}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8203300838727894
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.81433564, 4.32784365, 1.72293963]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0559056300452367}
episode index:3181
target Thresh 19.0
target distance 14.0
model initialize at round 3181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.37018902,  2.58035221,  4.40360117]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.377305069037389}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8202783331300808
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.95321477, 3.86070332, 0.43900687]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.284300821626928}
episode index:3182
target Thresh 19.0
target distance 1.0
model initialize at round 3182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.26498289,  9.74165533,  0.23728579]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.4572856485901065}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8203316544203321
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.82186504, 11.29853317,  2.23728579]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.3476407912304141}
episode index:3183
target Thresh 19.0
target distance 10.0
model initialize at round 3183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.99778937,  6.99088561,  5.4844408 ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.435573246678912}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8202949446340819
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.76070271, 4.55971748, 0.08621712]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.944432249111583}
episode index:3184
target Thresh 19.0
target distance 10.0
model initialize at round 3184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 2.86509481, 11.86892523,  2.03165963]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 9.176138835871363}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8202994089407746
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.82350474, 10.29573979,  0.33254779]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7260392679770724}
episode index:3185
target Thresh 19.0
target distance 12.0
model initialize at round 3185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([15.        , 11.        ,  1.99796122]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 13.41640786499874}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8202764562368797
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.49291608, 5.97040975, 3.44929345]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0884214888983208}
episode index:3186
target Thresh 19.0
target distance 7.0
model initialize at round 3186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 4.30764662, 11.05945634,  1.69093006]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.692663880507042}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8203057128390907
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.17342633, 11.32294074,  0.84137414]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8874202808866262}
episode index:3187
target Thresh 19.0
target distance 13.0
model initialize at round 3187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([2.        , 2.        , 5.89670157]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 14.317821063276423}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.820247958690165
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.14699857,  8.57953224,  1.64892197]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5978847732299524}
episode index:3188
target Thresh 19.0
target distance 11.0
model initialize at round 3188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([2.64854025, 4.64586273, 2.79117787]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.431943241173393}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.8201902407620915
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.27671616,  6.74880067,  4.82658357]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0410773050255437}
episode index:3189
target Thresh 19.0
target distance 2.0
model initialize at round 3189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([15.95718089,  4.9625377 ,  4.87036419]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 2.8252096678716834}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8202312438370564
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.70797587,  6.8302421 ,  2.30399357]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7280436628840279}
episode index:3190
target Thresh 19.0
target distance 12.0
model initialize at round 3190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([15.78142098,  3.66868451,  2.7110436 ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 11.786078665946956}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.820171573635667
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.6909218 , 4.78866177, 0.46326399]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0485038470779882}
episode index:3191
target Thresh 19.0
target distance 10.0
model initialize at round 3191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([3.9999376 , 4.99999036, 4.30486393]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 11.180400015369205}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8201463633599001
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.21308964, 10.41546883,  1.47301086]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.4669277722834448}
episode index:3192
target Thresh 19.0
target distance 1.0
model initialize at round 3192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.87042154,  7.59190049,  2.66201586]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6059181176169317}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8202026908377079
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.87042154,  7.59190049,  2.66201586]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6059181176169317}
episode index:3193
target Thresh 19.0
target distance 11.0
model initialize at round 3193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.73181692, 4.66122112, 2.74085271]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.347434900198461}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8201491216259141
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.39523148,  6.90681567,  0.77625841]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9892030000530438}
episode index:3194
target Thresh 19.0
target distance 12.0
model initialize at round 3194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([2.99999236, 4.99997391, 5.43745947]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.04160002661426}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.8200689200825108
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.02932539,  3.92025237,  0.05693863]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.08496860352970288}
episode index:3195
target Thresh 19.0
target distance 13.0
model initialize at round 3195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.50506272, 8.24689025, 0.546045  ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.519581475718077}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8200557011961893
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.06445238,  9.01003005,  0.28056254]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.06522814289575056}
episode index:3196
target Thresh 19.0
target distance 4.0
model initialize at round 3196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4.58288554, 5.21204402, 1.35890072]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 4.105379046729526}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8200907370569059
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.75361693, 9.44351698, 2.79253011]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.5073577933529744}
episode index:3197
target Thresh 19.0
target distance 11.0
model initialize at round 3197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([15.29525045,  4.64966142,  2.40369505]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.415080622864057}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8200372698559402
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.82116772, 3.47109491, 0.43910075]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9467031407242779}
episode index:3198
target Thresh 19.0
target distance 6.0
model initialize at round 3198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([3.99998001, 9.00001028, 3.67683709]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.324571038147505}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8200608087694765
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.46462557, 10.30450071,  0.54409586]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8776930225879197}
episode index:3199
target Thresh 19.0
target distance 4.0
model initialize at round 3199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.08287249,  3.97784558,  0.74877709]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.023008083017567}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.820104726019861
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.02465282,  7.28293006,  2.46559178]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.717493599860363}
episode index:3200
target Thresh 19.0
target distance 5.0
model initialize at round 3200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.00004028, 10.00001196,  1.29852265]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 5.099039138613482}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8201367909896854
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.32599591,  5.92827275,  4.73215203]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1471581461209972}
episode index:3201
target Thresh 19.0
target distance 11.0
model initialize at round 3201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([1.60329656, 4.93841575, 3.55999017]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 12.769164194117005}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8201048090226674
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.28601892,  8.48761793,  0.44495179]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.5653123676022281}
episode index:3202
target Thresh 19.0
target distance 5.0
model initialize at round 3202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([3.99999923, 5.99999964, 4.5935185 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 5.830952603600443}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8201282974538994
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.16214864, 11.0843805 ,  1.46077727]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.18279017902150127}
episode index:3203
target Thresh 19.0
target distance 2.0
model initialize at round 3203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.99836666, 8.99913922, 4.63661218]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 2.0008614501671826}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8201782262000125
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.40194892, 10.61813147,  2.35342687]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.5544244837586138}
episode index:3204
target Thresh 19.0
target distance 9.0
model initialize at round 3204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([2.43775332, 6.62591803, 3.77052391]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 11.432132261219534}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8201674617427915
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.44052349, 11.77355103,  1.50504145]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9546701910986624}
episode index:3205
target Thresh 19.0
target distance 9.0
model initialize at round 3205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 5.99986918, 11.00013993,  3.33257401]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 9.486924358061467}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8201745658387496
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.45157118, 1.69336859, 5.91664747]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.545838206128626}
episode index:3206
target Thresh 19.0
target distance 8.0
model initialize at round 3206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.        , 2.        , 0.42105931]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 8.000000000130875}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.820192447490499
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.69485333, 9.09131565, 1.28831808]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1439092624233527}
episode index:3207
target Thresh 19.0
target distance 6.0
model initialize at round 3207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.2139893 , 5.48813217, 3.06675136]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.57982140633377}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8202215387623173
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.06876272, 10.20549646,  2.21719544]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.21669588582429924}
episode index:3208
target Thresh 19.0
target distance 9.0
model initialize at round 3208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([2.2482704 , 9.12293929, 5.99824309]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 8.950761298966485}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8202312727081984
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.11369071, 10.56258653,  0.29913125]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.451947029608112}
episode index:3209
target Thresh 19.0
target distance 10.0
model initialize at round 3209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([3.34007693, 6.54616691, 2.98420101]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.234289453419105}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.8201549871870983
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.07688773,  3.33037005,  6.17005079]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.3391991953767261}
episode index:3210
target Thresh 19.0
target distance 10.0
model initialize at round 3210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.85456576, 8.74102918, 5.21069455]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 12.180776256792187}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.8200918090973305
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.46510023,  2.58078637,  0.67972964]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7440638605189178}
episode index:3211
target Thresh 19.0
target distance 7.0
model initialize at round 3211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([4.65871851, 9.98346142, 4.80012178]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 6.547564021630795}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8201096886782463
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.57778531, 3.74935042, 5.66738056]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.629810352960609}
episode index:3212
target Thresh 19.0
target distance 3.0
model initialize at round 3212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([1.99989396, 8.99618623, 5.69459176]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.607756349722628}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8201504233067001
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.28085785, 11.12403219,  3.12822115]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7297598328079515}
episode index:3213
target Thresh 19.0
target distance 1.0
model initialize at round 3213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.92745297,  6.07612837,  3.34211087]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.420641146644461}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.820203270094719
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.31737463,  4.54992229,  5.34211087]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5507237062157845}
episode index:3214
target Thresh 19.0
target distance 3.0
model initialize at round 3214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.99855968, 6.9994572 , 4.51199722]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.000543149999264}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8202409922966806
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.72436847, 9.60689472, 3.94562661]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.4801088447655518}
episode index:3215
target Thresh 19.0
target distance 12.0
model initialize at round 3215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.99999998,  8.99999968,  5.65881133]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 12.165525093496866}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8202377232904382
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.8178374 , 11.31563538,  3.67651418]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8766320274700251}
episode index:3216
target Thresh 19.0
target distance 11.0
model initialize at round 3216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([2.42675423, 4.5977339 , 3.78850091]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 13.684699057377507}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8202126883666265
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.8819551 , 10.53882745,  0.95664783]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5516063953057734}
episode index:3217
target Thresh 19.0
target distance 2.0
model initialize at round 3217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([1.99931764, 7.99961166, 4.6689918 ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.829184227949251}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8202533276958787
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.03328055, 10.10133211,  2.10262119]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.10665735696137554}
episode index:3218
target Thresh 19.0
target distance 1.0
model initialize at round 3218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.93759782,  3.56399366,  2.62067443]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.5674353553477419}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8203091669851934
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.93759782,  3.56399366,  2.62067443]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.5674353553477419}
episode index:3219
target Thresh 19.0
target distance 9.0
model initialize at round 3219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.00389245, 1.99822236, 0.58159655]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.81597459696534}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8202959720138934
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.51330197, 10.92343896,  0.31611409]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.4926830320124658}
episode index:3220
target Thresh 19.0
target distance 9.0
model initialize at round 3220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 7.        , 11.        ,  2.86146104]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.401754250996229}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8202803703913512
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.22071017,  4.81117899,  4.59597859]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.1248573217862583}
episode index:3221
target Thresh 19.0
target distance 6.0
model initialize at round 3221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.        ,  2.        ,  5.50716925]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 6.082762530753143}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8203036658239604
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.74885301,  8.21156813,  2.37442802]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.32838374164138157}
episode index:3222
target Thresh 19.0
target distance 13.0
model initialize at round 3222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([3.64639858, 4.65105904, 0.80114859]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.372253207525747}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.8201951591456365
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([1.49170728e+01, 3.86316564e+00, 4.70121715e-03]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.16000176325765336}
episode index:3223
target Thresh 19.0
target distance 7.0
model initialize at round 3223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4.79172414, 3.26409658, 1.33121937]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 7.2915100343636405}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8202212717125915
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.81093721, 9.31849471, 2.48166345]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0592774002315113}
episode index:3224
target Thresh 19.0
target distance 13.0
model initialize at round 3224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 2.        , 10.        ,  4.84977388]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.03840481041669}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.820213021483733
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.06577878, 10.75593143,  0.58429143]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.25277720552789706}
episode index:3225
target Thresh 19.0
target distance 3.0
model initialize at round 3225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.10252929,  9.67979354,  2.51983511]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.5963705002000859}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8202595453456414
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.72754411, 11.16274604,  2.23664981]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.3173617583722989}
episode index:3226
target Thresh 19.0
target distance 13.0
model initialize at round 3226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.33104123, 11.21671326,  4.02246571]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.333113424820125}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8202537725052922
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.04110846, 10.81746968,  4.04016856]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.18710217192011058}
episode index:3227
target Thresh 19.0
target distance 1.0
model initialize at round 3227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.41755209, 11.54204888,  2.31636   ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7956522895953582}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8203094559710589
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.41755209, 11.54204888,  2.31636   ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7956522895953582}
episode index:3228
target Thresh 19.0
target distance 8.0
model initialize at round 3228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 7.58317945, 10.43220186,  0.66396206]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 6.86230222277536}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8203217659427746
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.37193965,  8.7527443 ,  5.53122083]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9803488087639936}
episode index:3229
target Thresh 19.0
target distance 10.0
model initialize at round 3229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.00013588, 2.99951306, 5.99451256]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.049691977127939}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.8200677963557954
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.28228463, 6.76374136, 2.93258273]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.822533148699879}
episode index:3230
target Thresh 19.0
target distance 11.0
model initialize at round 3230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([15.29942558,  6.36824088,  1.89810389]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 11.601860819398581}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8200499303446507
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.26011097, 8.65439262, 5.63262143]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.43255309561680855}
episode index:3231
target Thresh 19.0
target distance 11.0
model initialize at round 3231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([15.27196061, 10.28341696,  2.37198168]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.294715017756115}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8200492672899641
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.80608585, 10.40175254,  4.67286984]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6288900710728595}
episode index:3232
target Thresh 19.0
target distance 4.0
model initialize at round 3232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.96658987, 4.67178599, 5.0611794 ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 2.841256126319681}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8200868271050308
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.85304319, 2.52176787, 4.49480879]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5420682766829154}
episode index:3233
target Thresh 19.0
target distance 6.0
model initialize at round 3233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([11.        , 11.        ,  3.13732779]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 7.810249676516308}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8201018731735725
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.19916924,  5.59203045,  6.00458656]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6246346432544317}
episode index:3234
target Thresh 19.0
target distance 5.0
model initialize at round 3234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.52208763, 6.4171601 , 4.5272336 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 4.442938623839921}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8201364832121301
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.66602213, 2.23602397, 5.96086299]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7066065306677961}
episode index:3235
target Thresh 19.0
target distance 10.0
model initialize at round 3235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([3.99999988, 4.99999986, 4.99577284]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.770329776918446}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8201070777278116
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.5124959 ,  8.71537753,  6.16391977]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.5645088078400999}
episode index:3236
target Thresh 19.0
target distance 2.0
model initialize at round 3236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 1.99702004, 10.00027909,  4.05820894]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.0029799755677073}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8201565040244666
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.31076175, 10.64431267,  1.77502364]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.943497845946574}
episode index:3237
target Thresh 19.0
target distance 8.0
model initialize at round 3237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([11.38961991, 10.10313499,  5.30753326]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.584087812156353}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.820150782617893
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.88906165, 4.66668022, 5.32523611]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6758474908431396}
episode index:3238
target Thresh 19.0
target distance 13.0
model initialize at round 3238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.63685361, 9.39127782, 1.24463957]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.182135145301155}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8201259445786124
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.06264312,  5.91489482,  4.69597181]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.3098360388928016}
episode index:3239
target Thresh 19.0
target distance 11.0
model initialize at round 3239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.74426792, 3.34201331, 1.95102042]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 12.799406262118364}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8200988388456111
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.13039851, 11.57901086,  1.11916734]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0447297867977727}
episode index:3240
target Thresh 19.0
target distance 4.0
model initialize at round 3240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.99981691,  6.99986346,  4.79237914]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.472176206987639}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8201305098902215
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.98147306, 10.7128712 ,  1.94282322]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0226105399631706}
episode index:3241
target Thresh 19.0
target distance 12.0
model initialize at round 3241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.37544629,  8.56043963,  4.41583848]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.384753204535988}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8201248035606868
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.18295173, 8.95216407, 4.43354133]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.18910212226661144}
episode index:3242
target Thresh 19.0
target distance 6.0
model initialize at round 3242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.00077774, 10.00055429,  1.62919539]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 6.000554343084909}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8201479961140935
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.15427718,  4.46665371,  4.77963947]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9659258647173848}
episode index:3243
target Thresh 19.0
target distance 12.0
model initialize at round 3243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.31940863,  5.91059938,  4.20473838]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.376752298202756}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.82012550046003
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.43585653, 6.85679005, 5.65607061]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.45878100250001846}
episode index:3244
target Thresh 19.0
target distance 12.0
model initialize at round 3244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([2.99828381, 8.07565027, 2.60347831]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 12.001954612868596}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8201076937462818
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.25408749,  8.19622961,  0.05481055]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7712921150826468}
episode index:3245
target Thresh 19.0
target distance 12.0
model initialize at round 3245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.83365388,  8.78672971,  4.95670748]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.901379780570378}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8201044960181471
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.64488137, 10.80507884,  2.97441033]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0315153502865302}
episode index:3246
target Thresh 19.0
target distance 12.0
model initialize at round 3246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([16.01068525,  9.3170639 ,  5.72873807]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 12.232143942006227}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.820091473801757
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.45773756, 7.35374941, 5.46325561]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5785000596224035}
episode index:3247
target Thresh 19.0
target distance 7.0
model initialize at round 3247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([2.00000046, 3.99999726, 5.88809466]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.062259903559138}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8201064535859216
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.86300821, 11.41409203,  2.47216813]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.436163910504288}
episode index:3248
target Thresh 19.0
target distance 2.0
model initialize at round 3248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([16.00128544,  9.9996709 ,  0.75936573]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.2370707033972472}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8201556975214139
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.60570278,  9.13918153,  4.75936573]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6214880125795906}
episode index:3249
target Thresh 19.0
target distance 4.0
model initialize at round 3249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([12.00000348, 11.00009303,  2.54345864]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 4.472174451062537}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8201872633666158
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.26448752,  8.6742878 ,  5.97708803]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8044047778557799}
episode index:3250
target Thresh 19.0
target distance 4.0
model initialize at round 3250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.16310934,  5.76596542,  0.81143683]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 3.9414868856411354}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8202245727748084
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.71567187,  2.4072684 ,  0.24506621]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8234402094112834}
episode index:3251
target Thresh 19.0
target distance 13.0
model initialize at round 3251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([2.        , 2.        , 0.31964844]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 15.811388300788026}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8201886635258002
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.45717473, 11.75254711,  1.20461006]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9278935428836752}
episode index:3252
target Thresh 19.0
target distance 11.0
model initialize at round 3252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.09263299,  3.46016032,  3.82648981]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.197710964181924}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.8201206547163752
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.49618675, 2.21670927, 5.29552489]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.5414464000775462}
episode index:3253
target Thresh 19.0
target distance 6.0
model initialize at round 3253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([1.14981475, 9.6331937 , 4.53511143]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 6.313194694261008}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8201465494368093
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.46704088, 3.87642818, 5.68555551]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.5470972648591529}
episode index:3254
target Thresh 19.0
target distance 10.0
model initialize at round 3254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([3.93030285, 8.81182343, 5.3676734 ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.626525374304324}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.8200861435521563
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.21761655,  3.23875815,  5.11989379]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8180032474046837}
episode index:3255
target Thresh 19.0
target distance 11.0
model initialize at round 3255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([12.62114658,  9.39452833,  4.60134196]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 9.721685591332054}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8200880116357814
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.8388452 , 8.17450925, 4.90223012]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.856804963288503}
episode index:3256
target Thresh 19.0
target distance 12.0
model initialize at round 3256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([15.        , 11.        ,  0.73423022]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8200633301380081
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.99564099, 5.74477895, 4.18556245]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7447917081043157}
episode index:3257
target Thresh 19.0
target distance 5.0
model initialize at round 3257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([10.94149528, 11.4975882 ,  2.69783556]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.966484629810227}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8200977076756905
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.25136636, 10.39057326,  4.13146495]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.659231368928523}
episode index:3258
target Thresh 19.0
target distance 3.0
model initialize at round 3258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.99996211, 2.99986247, 5.45352864]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.16239615329504}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8201378710209573
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.67460261, 6.00105493, 2.88715803]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.32539910204289635}
episode index:3259
target Thresh 19.0
target distance 10.0
model initialize at round 3259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.4985025 ,  9.76019297,  3.68292582]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.588461228098227}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8201474783522683
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.58290517, 10.59961194,  4.26699928]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7071697354116587}
episode index:3260
target Thresh 19.0
target distance 13.0
model initialize at round 3260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.27130839,  7.48296453,  5.27459335]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.526790372909685}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8201344988616356
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.16824125, 9.63454902, 5.0091109 ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.40231770640150194}
episode index:3261
target Thresh 19.0
target distance 1.0
model initialize at round 3261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.82395511, 1.57815248, 5.32704234]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9256658949790323}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8201896385002433
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.82395511, 1.57815248, 5.32704234]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9256658949790323}
episode index:3262
target Thresh 19.0
target distance 10.0
model initialize at round 3262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.00000018,  6.99999997,  0.82426661]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.198039193603286}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8201627043081361
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.87856064, 5.77814459, 6.27559884]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.1736174033337927}
episode index:3263
target Thresh 19.0
target distance 11.0
model initialize at round 3263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.6427729 ,  4.64462059,  2.79468334]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 11.914222744331957}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8201403419889032
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.99909237, 9.72368629, 4.24601557]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.27631520190798764}
episode index:3264
target Thresh 19.0
target distance 13.0
model initialize at round 3264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 2.        , 11.        ,  4.75168443]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 14.317821063276943}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8201111965046669
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.51284683,  5.26147295,  5.91983136]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5756561219960349}
episode index:3265
target Thresh 19.0
target distance 2.0
model initialize at round 3265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.21258917, 5.06258587, 1.29630965]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 2.207776322094944}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8201571817476233
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.20779343, 3.67384229, 1.01312434]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.7051535593413886}
episode index:3266
target Thresh 19.0
target distance 6.0
model initialize at round 3266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([2.99979033, 8.99919135, 5.46869397]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.3250099832128885}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8201747403154995
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.52491177, 10.63626086,  0.05276744]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5983435353994074}
episode index:3267
target Thresh 19.0
target distance 14.0
model initialize at round 3267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([16.04546787,  6.67584062,  2.5536716 ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 14.04920805879145}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8201456110608001
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.96066986, 7.57763131, 3.72181853]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5789687305641438}
episode index:3268
target Thresh 19.0
target distance 10.0
model initialize at round 3268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 5.18080485, 10.19865688,  1.80290049]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.767646729197965}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8201350673256353
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.43787036,  5.52262768,  5.82060334]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6477767994000531}
episode index:3269
target Thresh 19.0
target distance 12.0
model initialize at round 3269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([3.67907463, 9.11443986, 1.07805127]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.996160860799009}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8201015560595639
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.13773992,  3.00132395,  6.24619819]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.1377462813691438}
episode index:3270
target Thresh 19.0
target distance 10.0
model initialize at round 3270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([3.99999804, 7.99999886, 4.67797923]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.049877683863933}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8200862523572544
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.30144763,  8.76881018,  0.12931147]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.3798939369194126}
episode index:3271
target Thresh 19.0
target distance 12.0
model initialize at round 3271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.80389467, 4.11598234, 1.72901839]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 13.143153108154635}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8200527764938421
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.30208156, 10.92225154,  0.61398001]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.311926426436898}
episode index:3272
target Thresh 19.0
target distance 5.0
model initialize at round 3272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.74084463, 7.34747604, 1.96081179]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 4.046167482223088}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8200869997053952
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.49770177, 10.78572734,  3.39444118]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.5460918241575123}
episode index:3273
target Thresh 19.0
target distance 1.0
model initialize at round 3273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.0167457 ,  6.04978178,  2.25630429]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.438343070886444}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8201358735600973
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.82199543,  4.63592152,  6.25630429]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.40526382272125144}
episode index:3274
target Thresh 19.0
target distance 9.0
model initialize at round 3274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 7.68146035, 10.92873167,  0.96764057]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.93829020562091}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8201158990299121
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.67322865,  2.99696345,  0.41897281]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.2029850112629583}
episode index:3275
target Thresh 19.0
target distance 2.0
model initialize at round 3275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([4.0012774 , 5.99942033, 0.58400029]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.2374697514918367}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8201647342255685
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.56261311, 6.37502177, 4.58400029]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8409109966986078}
episode index:3276
target Thresh 19.0
target distance 14.0
model initialize at round 3276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.33467461,  9.75693743,  4.29652429]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.63902296826041}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8201447630790863
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.77396241, 7.377346  , 3.74785652]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.4398670178414737}
episode index:3277
target Thresh 19.0
target distance 7.0
model initialize at round 3277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 8.        , 11.        ,  1.93142527]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 8.062257748315663}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8201517180608755
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.67210915, 4.75927039, 4.51549873]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.8270453096878022}
episode index:3278
target Thresh 19.0
target distance 13.0
model initialize at round 3278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([3.29677553, 7.07273544, 1.70112723]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.739901303339513}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8201249268597757
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.02117269,  8.59323277,  0.86927416]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.593610478725704}
episode index:3279
target Thresh 19.0
target distance 2.0
model initialize at round 3279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.52716025, 7.36409717, 1.61444109]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.237944709828166}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.820176718040611
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.08392216, 8.22363823, 3.61444109]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7808844554330561}
episode index:3280
target Thresh 19.0
target distance 12.0
model initialize at round 3280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.        ,  8.        ,  0.09794491]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.416407864998774}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8201368742027044
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.90495159, 2.66902316, 4.98290653]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.6757412149894234}
episode index:3281
target Thresh 19.0
target distance 5.0
model initialize at round 3281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 2.99970646, 11.00043873,  3.17045784]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 5.099392167321143}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8201681380114263
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.61332046, 6.27252107, 0.32090192]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.6711406073306455}
episode index:3282
target Thresh 19.0
target distance 7.0
model initialize at round 3282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 9.99999998, 10.99999999,  4.69800425]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 9.219544460962057}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8201750752808057
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.26412203,  4.3976238 ,  0.9988924 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.4773522091767648}
episode index:3283
target Thresh 19.0
target distance 6.0
model initialize at round 3283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([4.00000182, 9.00000083, 1.43685692]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 6.324556680422909}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8201979629723336
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.52518998, 3.75094957, 4.587301  ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8884648640655434}
episode index:3284
target Thresh 19.0
target distance 2.0
model initialize at round 3284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 3.03778279, 10.37660301,  3.19083703]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.210626665008764}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8202436558298764
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.60177529, 11.5969097 ,  2.90765172]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8476052693133879}
episode index:3285
target Thresh 19.0
target distance 14.0
model initialize at round 3285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([16.        , 11.        ,  1.28006571]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8202331368052195
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.70075781, 9.57798348, 5.29776856]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.818021667829675}
episode index:3286
target Thresh 19.0
target distance 1.0
model initialize at round 3286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([1.99439527, 7.00382855, 3.55230439]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.4208844489286536}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8202817729059785
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.24426656, 5.95467136, 1.26911908]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.2484367926262867}
episode index:3287
target Thresh 19.0
target distance 2.0
model initialize at round 3287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.31905475,  2.96070625,  0.34267586]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.682078021185165}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.82033643173417
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.31905475,  2.96070625,  0.34267586]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.682078021185165}
episode index:3288
target Thresh 19.0
target distance 12.0
model initialize at round 3288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.        ,  5.        ,  5.08286858]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.999999999904997}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8203096658290074
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.54634462, 9.57845224, 4.2510155 ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.6900688063561088}
episode index:3289
target Thresh 19.0
target distance 9.0
model initialize at round 3289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 7.        , 11.        ,  3.68961251]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 9.219544457315557}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8203165453206656
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.35602557,  8.3061477 ,  6.27368597]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9466435881345906}
episode index:3290
target Thresh 19.0
target distance 5.0
model initialize at round 3290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.99965027, 9.00090113, 2.9510051 ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.099971735057633}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.82034766903659
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.7205978 , 4.49698505, 0.10144918]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.8753601156976893}
episode index:3291
target Thresh 19.0
target distance 5.0
model initialize at round 3291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.00025484, 3.99978473, 0.30857628]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 5.000215277551346}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.820381605148033
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.7734817 , 8.10606979, 1.74220567]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.1821104684597072}
episode index:3292
target Thresh 19.0
target distance 1.0
model initialize at round 3292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.0178282 ,  6.09353367,  0.28240889]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.90664163689174}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8204361506672714
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.0178282 ,  6.09353367,  0.28240889]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.90664163689174}
episode index:3293
target Thresh 19.0
target distance 5.0
model initialize at round 3293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.00000322,  6.99999718,  0.29042738]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.40312851003756}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.820458889617967
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.81489454, 11.35564531,  3.44087146]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.8891213047287266}
episode index:3294
target Thresh 19.0
target distance 12.0
model initialize at round 3294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.00000014, 1.99999972, 6.1901412 ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.041594462834071}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.8203991222447571
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.08762147,  3.13975978,  5.94236159]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.16495550570399675}
episode index:3295
target Thresh 19.0
target distance 11.0
model initialize at round 3295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.9858664 ,  8.65672758,  2.58932716]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 12.356691604947892}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8203679727940147
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.7200446 , 3.58740064, 5.75747408]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9292490155708178}
episode index:3296
target Thresh 19.0
target distance 8.0
model initialize at round 3296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([1.9984773 , 2.99988528, 4.22679186]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 8.062560591200207}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8203800111263918
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.88730457, 11.29271302,  2.81086532]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.3136577354344855}
episode index:3297
target Thresh 19.0
target distance 13.0
model initialize at round 3297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([3.        , 9.        , 3.92099798]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8203694890310859
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.91085386, 11.2458719 ,  1.65551552]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.26153398114290066}
episode index:3298
target Thresh 19.0
target distance 2.0
model initialize at round 3298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([4.00330549, 3.0103392 , 2.27136177]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 2.2436617792328257}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8204209077976723
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.40883787, 2.81845284, 4.27136177]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9148843941223028}
episode index:3299
target Thresh 19.0
target distance 13.0
model initialize at round 3299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([15.83333494,  7.15605171,  1.95621651]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.881556510310965}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8203897895021574
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.27265039, 5.66628165, 5.12436344]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.430936393650382}
episode index:3300
target Thresh 19.0
target distance 4.0
model initialize at round 3300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.13226082, 5.07349482, 3.47608244]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.052442280493974}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8204264724345713
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.83628126, 8.14917113, 2.90971183]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.22148556110285167}
episode index:3301
target Thresh 19.0
target distance 1.0
model initialize at round 3301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.86487499, 11.348869  ,  3.85341585]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.3741234416722355}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8204808556954937
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.86487499, 11.348869  ,  3.85341585]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.3741234416722355}
episode index:3302
target Thresh 19.0
target distance 8.0
model initialize at round 3302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.87103799, 4.44002809, 2.03679947]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 6.6564094421033975}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8204981248954039
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.06894747, 11.03398442,  2.90405824]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9316725613149042}
episode index:3303
target Thresh 19.0
target distance 5.0
model initialize at round 3303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 8.0000001 , 10.99999997,  0.75720423]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 5.099019604989764}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8205290711936886
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.32952076, 9.78290011, 4.19083362]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.39460903553017157}
episode index:3304
target Thresh 19.0
target distance 6.0
model initialize at round 3304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.00814906, 1.99817926, 0.79018038]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 6.001826276102698}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.820554442753088
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.97924313, 7.83095296, 1.94062446]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9937273336328606}
episode index:3305
target Thresh 19.0
target distance 8.0
model initialize at round 3305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 8.49924787, 11.76461134,  1.48161763]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 6.7359952053431575}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8205743551635426
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.63515466,  9.94570693,  0.3488764 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.3688629316413514}
episode index:3306
target Thresh 19.0
target distance 10.0
model initialize at round 3306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.73715932, 5.32456314, 5.35127807]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.787892237694345}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.8205147697506995
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.21811276,  2.17072444,  5.10349846]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8003089956445009}
episode index:3307
target Thresh 19.0
target distance 12.0
model initialize at round 3307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.44481005, 10.35674866,  4.54378152]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 10.464598854506825}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8205164788964325
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.57749882, 10.22401751,  4.84466968]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9672919446092196}
episode index:3308
target Thresh 19.0
target distance 13.0
model initialize at round 3308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([16.00000001,  2.        ,  1.47062271]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 13.92838828127446}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8204727232512721
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.21263279, 7.92314705, 4.07239903]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9473189457482207}
episode index:3309
target Thresh 19.0
target distance 6.0
model initialize at round 3309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.16104607,  4.99658227,  0.98878115]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 6.005577426073587}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8204980735086006
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.33926162, 11.2494221 ,  2.13922523]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.42108174096366013}
episode index:3310
target Thresh 19.0
target distance 13.0
model initialize at round 3310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([2.49414861, 8.55005149, 5.38671637]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 15.010369629322373}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8204564113284442
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.15276021,  2.36688055,  5.98849268]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.397412907231543}
episode index:3311
target Thresh 19.0
target distance 5.0
model initialize at round 3311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([13.82625488,  5.8099132 ,  4.94390392]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 3.9866171391542196}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8204901096788605
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.22941583,  1.93343022,  0.094348  ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.2388789623549015}
episode index:3312
target Thresh 19.0
target distance 11.0
model initialize at round 3312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([15.00000006,  6.99999999,  0.84403437]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 11.000000062658826}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8204702573328069
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.88476981, 7.29503249, 0.29536661]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9326638159115833}
episode index:3313
target Thresh 19.0
target distance 1.0
model initialize at round 3313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.13278604, 6.84754066, 3.4781245 ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8805134279598409}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8205244304597433
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.13278604, 6.84754066, 3.4781245 ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8805134279598409}
episode index:3314
target Thresh 19.0
target distance 12.0
model initialize at round 3314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.83397457,  3.45122268,  2.05920986]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.915763640328572}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.8204079016200244
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.55411403, 2.59133074, 4.69639188]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.740598640729512}
episode index:3315
target Thresh 19.0
target distance 12.0
model initialize at round 3315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([14.05200078,  7.03822536,  1.64389151]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 12.686464484480151}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8203998215181015
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.83570325, 11.20689413,  3.66159436]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.860932691193251}
episode index:3316
target Thresh 19.0
target distance 10.0
model initialize at round 3316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 4.        , 11.        ,  3.26133454]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8203846401265692
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.22074564,  6.81952384,  4.99585208]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1308654595433707}
episode index:3317
target Thresh 19.0
target distance 12.0
model initialize at round 3317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([3.00000257, 1.99999055, 5.98801422]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.165524078243735}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.8203179045528319
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.08158783,  4.62041115,  1.173864  ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.6257528044974497}
episode index:3318
target Thresh 19.0
target distance 3.0
model initialize at round 3318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 7.02968823, 10.72037322,  5.82816362]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 2.9834448499644792}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.820360169724705
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.04304193, 11.66744884,  1.26179301]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6688352231375683}
episode index:3319
target Thresh 19.0
target distance 13.0
model initialize at round 3319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([3.33582908, 7.02369368, 1.66387432]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.338678782446333}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8203145708056824
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.06995161,  3.41942416,  6.26565064]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.42521741555881465}
episode index:3320
target Thresh 19.0
target distance 2.0
model initialize at round 3320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([13.99902036,  8.00118728,  3.27065957]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.0009799879791164}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.820362684454943
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.64727364,  7.94220161,  0.98747426]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.3574304652745476}
episode index:3321
target Thresh 19.0
target distance 13.0
model initialize at round 3321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([2.        , 4.        , 5.06554413]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 13.92838827718412}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8203384032053744
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.01468808,  9.29212657,  2.23369106]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0277049693178768}
episode index:3322
target Thresh 19.0
target distance 12.0
model initialize at round 3322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([2.87216056, 5.43934848, 2.03601974]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 11.236745250335654}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8202990143045811
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.24493036,  7.8139486 ,  0.63779605]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.850001885140965}
episode index:3323
target Thresh 19.0
target distance 4.0
model initialize at round 3323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([5.49590536, 9.51204243, 5.51445889]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.541172227506422}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8203354707230818
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.46087126, 7.06311303, 4.94808828]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.4651726282261355}
episode index:3324
target Thresh 19.0
target distance 13.0
model initialize at round 3324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([2.91541395, 9.58776582, 0.01431435]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 14.269247003411323}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8202961063968098
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.09609058,  2.4268939 ,  4.89927597]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.9996452595077704}
episode index:3325
target Thresh 19.0
target distance 12.0
model initialize at round 3325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.12474778,  4.24493031,  4.02435875]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.262339134806215}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8202609745834012
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.15375281, 5.76011778, 4.90932037]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.28492702236906536}
episode index:3326
target Thresh 19.0
target distance 3.0
model initialize at round 3326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([15.00070001, 10.00081053,  1.86843699]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.1632679852966468}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8203002679634183
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.40275651,  6.93014036,  5.58525168]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.4087703173145776}
episode index:3327
target Thresh 19.0
target distance 3.0
model initialize at round 3327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.39925949,  6.01569608,  1.04929274]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.0108932293173836}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8203424241359052
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.70413433,  9.01602763,  2.76610743]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.2962994725775964}
episode index:3328
target Thresh 19.0
target distance 4.0
model initialize at round 3328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.81912625,  6.67319483,  2.68847901]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.3338246776410476}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.820387469667856
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.11185884,  8.30452008,  2.4052937 ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7044180006511687}
episode index:3329
target Thresh 19.0
target distance 11.0
model initialize at round 3329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([3.99999692, 3.99999398, 5.2490828 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 12.52996967093621}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8203588188769521
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.31730894,  9.00779991,  0.13404442]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.2043787212054438}
episode index:3330
target Thresh 19.0
target distance 14.0
model initialize at round 3330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 2.        , 10.        ,  3.34531927]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 14.000000000000002}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8203414018536768
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.32673278, 10.39668567,  0.79665151]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5139200592922302}
episode index:3331
target Thresh 19.0
target distance 6.0
model initialize at round 3331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 2.99999978, 10.0000045 ,  2.62964559]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 6.000004496520059}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8203639099126219
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.32979312, 3.84110672, 5.78008966]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.3660745457494601}
episode index:3332
target Thresh 19.0
target distance 10.0
model initialize at round 3332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.86985625, 6.36724526, 1.25015181]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.238258711479679}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8203511458710637
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.26322703, 11.4299173 ,  0.98466935]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5041005390882085}
episode index:3333
target Thresh 19.0
target distance 9.0
model initialize at round 3333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.        , 11.        ,  2.38448831]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.055385138137583}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8203579221300659
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.60105924,  1.9314792 ,  4.96856178]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.4047824450657876}
episode index:3334
target Thresh 19.0
target distance 1.0
model initialize at round 3334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.0023576 , 4.97977993, 5.83846188]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0025615230654261}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8204058208040899
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.99403789, 5.68037307, 3.55527657]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.6803991952623818}
episode index:3335
target Thresh 19.0
target distance 2.0
model initialize at round 3335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.63387871, 9.1400008 , 4.71343493]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0683636324874097}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8204596559897002
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.63387871, 9.1400008 , 4.71343493]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0683636324874097}
episode index:3336
target Thresh 19.0
target distance 1.0
model initialize at round 3336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([3.06745712, 6.93935978, 0.27776831]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.4219217684269847}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8205074954694755
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.67347046, 6.77052104, 4.27776831]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8368537621154557}
episode index:3337
target Thresh 19.0
target distance 12.0
model initialize at round 3337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.        ,  9.        ,  6.00119638]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.041594578792363}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8204970612709548
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.33950996, 8.43898365, 3.73571393]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.5549537445043435}
episode index:3338
target Thresh 19.0
target distance 12.0
model initialize at round 3338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([15.31011084,  4.65406336,  2.39546359]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 13.054730718615568}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8204684548842183
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.8760053 , 9.57821321, 3.56361051]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.591358770716861}
episode index:3339
target Thresh 19.0
target distance 11.0
model initialize at round 3339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.99996433,  1.99997373,  4.78634548]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 13.601457109772143}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8204420581521136
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.73920526, 10.47546172,  3.95449241]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8789131182925596}
episode index:3340
target Thresh 19.0
target distance 3.0
model initialize at round 3340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 4.99876906, 10.9990052 ,  4.83129096]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.0012311072119164}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.820484007853355
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.56943831, 10.67876133,  0.26492035]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.5371942434046363}
episode index:3341
target Thresh 19.0
target distance 7.0
model initialize at round 3341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.11274631, 4.6791892 , 2.51375365]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 5.322005199510077}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8205091120027134
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.61721568, 9.46518587, 3.66419773]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.657685326737661}
episode index:3342
target Thresh 19.0
target distance 5.0
model initialize at round 3342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.00010126, 11.00008626,  1.71556443]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.000101263510167}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8205396939896786
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.39805957, 10.31294222,  5.14919381]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7940401813624021}
episode index:3343
target Thresh 19.0
target distance 3.0
model initialize at round 3343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.00120261, 8.00087597, 1.63952797]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 3.0008762134398936}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8205787042635754
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.36057722, 5.04940784, 5.35634267]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.6413288009527627}
episode index:3344
target Thresh 19.0
target distance 12.0
model initialize at round 3344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.92978906,  4.70114026,  5.03320694]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.06138179093514}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.8204873913826068
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.36238267, 3.46539999, 5.08631549]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.7894004069021378}
episode index:3345
target Thresh 19.0
target distance 13.0
model initialize at round 3345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([15.        , 11.        ,  1.06342285]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.820476988139757
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.28195168, 8.47463649, 5.0811257 ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5962412012809954}
episode index:3346
target Thresh 19.0
target distance 10.0
model initialize at round 3346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.8844597 ,  9.67532655,  2.64965314]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.972828248163411}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8204786886583396
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.7879301 , 11.49122191,  2.95054129]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9285110695028743}
episode index:3347
target Thresh 19.0
target distance 2.0
model initialize at round 3347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.12243984, 10.24773061,  1.84821051]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.155863778765573}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8205323091217033
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.12243984, 10.24773061,  1.84821051]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.155863778765573}
episode index:3348
target Thresh 19.0
target distance 1.0
model initialize at round 3348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.89137211,  2.33401595,  2.89522511]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.9518985670982635}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8205858975632914
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.89137211,  2.33401595,  2.89522511]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.9518985670982635}
episode index:3349
target Thresh 19.0
target distance 12.0
model initialize at round 3349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        ,  8.        ,  6.28214836]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.000000000000192}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8205467522463676
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.5755249 , 3.52531128, 4.88392468]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.675374754361828}
episode index:3350
target Thresh 19.0
target distance 7.0
model initialize at round 3350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 6.99999756, 11.0000049 ,  3.04228687]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 7.000002436712881}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8205663995514914
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.53114297, 11.71201077,  1.90954564]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8525175936774071}
episode index:3351
target Thresh 19.0
target distance 10.0
model initialize at round 3351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.71547957, 8.39516073, 5.28269506]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 12.110715964888673}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8205252265787764
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.24537955,  1.71984444,  5.88447137]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.3724221492823909}
episode index:3352
target Thresh 19.0
target distance 10.0
model initialize at round 3352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 5.       , 11.       ,  3.7117939]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.20655561573398}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8205078742041173
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.25729256,  3.99361357,  1.16312613]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.2573718082853784}
episode index:3353
target Thresh 19.0
target distance 2.0
model initialize at round 3353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.99984994,  3.99910056,  5.55707502]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 2.2369395822717775}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8205554568295783
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.1328658 ,  5.60260823,  3.27388972]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9538563521379244}
episode index:3354
target Thresh 19.0
target distance 1.0
model initialize at round 3354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 3.21604617, 10.33236448,  6.03535318]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.2606484218834844}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8206030110898378
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.03209994, 10.83863222,  3.75216788]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8392463350284671}
episode index:3355
target Thresh 19.0
target distance 13.0
model initialize at round 3355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 3.        , 10.        ,  2.92120689]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 13.601470508735456}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8205789042252067
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.29711206,  6.17662913,  0.08935382]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.34564928163084097}
episode index:3356
target Thresh 19.0
target distance 13.0
model initialize at round 3356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.99725539,  8.01720614,  2.73897696]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 14.322549266010842}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8205357556237278
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.32318863, 1.66345634, 5.34075327]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.4665967437119361}
episode index:3357
target Thresh 19.0
target distance 7.0
model initialize at round 3357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.38573632, 2.95140954, 1.61165779]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 6.060877746615224}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8205607247480236
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.49497279, 8.63731354, 2.76210187]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.621766798194096}
episode index:3358
target Thresh 19.0
target distance 4.0
model initialize at round 3358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([3.36855889, 6.21093379, 3.82919598]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 4.427743985974537}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8205939205274696
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.36852037, 1.6197817 , 5.26282536]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5295027967856935}
episode index:3359
target Thresh 19.0
target distance 2.0
model initialize at round 3359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.00384081, 4.99618183, 0.22755831]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.0038218513509727}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8206443390035031
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.5778724 , 6.55307897, 2.22755831]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7305305727776563}
episode index:3360
target Thresh 19.0
target distance 1.0
model initialize at round 3360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.02238728, 7.87137042, 5.89470744]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9860386417483683}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8206977027824368
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.02238728, 7.87137042, 5.89470744]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9860386417483683}
episode index:3361
target Thresh 19.0
target distance 11.0
model initialize at round 3361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.91647216,  7.41133383,  2.00486695]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.706776732147054}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.82061964461342
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.45058264, 3.40768762, 4.90753142]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6841554170305172}
episode index:3362
target Thresh 19.0
target distance 7.0
model initialize at round 3362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.47500512,  4.62738046,  1.41215962]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 6.390298090837697}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8206445516697375
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.11312939, 10.39366743,  2.5626037 ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.074327072833788}
episode index:3363
target Thresh 19.0
target distance 12.0
model initialize at round 3363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([3.62966047, 5.56074206, 2.19733088]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 12.604370191397308}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8206182909140848
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.30117482, 10.96307748,  1.36547781]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.3034296409521111}
episode index:3364
target Thresh 19.0
target distance 11.0
model initialize at round 3364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.96689219, 10.32856176,  3.24173725]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.989482934489306}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8206199403443736
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.1204078 , 11.54453374,  3.5426254 ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5576872128149355}
episode index:3365
target Thresh 19.0
target distance 11.0
model initialize at round 3365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([15.13269652,  7.32233891,  5.80132079]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.942342542762784}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8205851298139681
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.74916727, 3.71057964, 0.40309711]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0325575156327675}
episode index:3366
target Thresh 19.0
target distance 2.0
model initialize at round 3366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([13.99886731, 10.00060174,  3.66325927]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.8296535825971496}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8206267130869667
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.11756815,  8.06280679,  5.38007396]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.884664154463528}
episode index:3367
target Thresh 19.0
target distance 2.0
model initialize at round 3367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.11834652, 7.40561123, 3.14100373]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6060560247132945}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8206799711887817
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.11834652, 7.40561123, 3.14100373]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6060560247132945}
episode index:3368
target Thresh 19.0
target distance 10.0
model initialize at round 3368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.00000026, 1.99999949, 6.19014835]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.19803887595802}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.8206159540014705
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.74552486,  4.19507864,  5.65918344]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.7706250650928627}
episode index:3369
target Thresh 19.0
target distance 2.0
model initialize at round 3369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([1.99910739, 6.99897397, 5.0064199 ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.2373252640737618}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8206632786442001
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.23729803, 8.12335877, 2.72323459]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.7726135434939577}
episode index:3370
target Thresh 19.0
target distance 1.0
model initialize at round 3370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.70978329, 11.59975146,  3.03147697]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9292438449714568}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8207164785022113
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.70978329, 11.59975146,  3.03147697]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9292438449714568}
episode index:3371
target Thresh 19.0
target distance 1.0
model initialize at round 3371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.32471719,  7.5903671 ,  2.07793324]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.386605478033634}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8207666812072817
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.74357825,  7.70704397,  4.07793324]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.3893267887244834}
episode index:3372
target Thresh 19.0
target distance 14.0
model initialize at round 3372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.39461896,  3.49488634,  4.456424  ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.880698778270085}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8207318994147506
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.97246241, 7.19710529, 5.34138562]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9922366847085475}
episode index:3373
target Thresh 19.0
target distance 5.0
model initialize at round 3373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.99999355, 3.99998662, 5.2729044 ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.099031367171727}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.820762134386598
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.42668899, 8.83340992, 2.42334847]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.4580564921366728}
episode index:3374
target Thresh 19.0
target distance 4.0
model initialize at round 3374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.18761741,  8.04288786,  4.86685443]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.149465349217066}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8208035666460389
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.4879713 ,  5.66605545,  0.30048381]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8256790273563962}
episode index:3375
target Thresh 19.0
target distance 11.0
model initialize at round 3375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([3.90770662, 4.41720033, 2.0111336 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 11.084062363146524}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8207709095549921
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.47274216,  9.06591169,  0.89609523]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.4773148881446195}
episode index:3376
target Thresh 19.0
target distance 11.0
model initialize at round 3376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 5.68239801, 10.95612481,  0.98392695]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.31770529381914}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8207725079305564
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.25309513, 11.20741533,  1.2848151 ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.32722815179641584}
episode index:3377
target Thresh 19.0
target distance 6.0
model initialize at round 3377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.19927679,  5.48025281,  3.07668304]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 4.590127704715262}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8207999634484822
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.89962083, 10.08752186,  2.22712712]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.13317677167148084}
episode index:3378
target Thresh 19.0
target distance 3.0
model initialize at round 3378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 2.99722146, 10.99648657,  5.05326271]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.002780593935351}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8208413354658101
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.69864861, 10.03735942,  0.4868921 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.008706867866673}
episode index:3379
target Thresh 19.0
target distance 12.0
model initialize at round 3379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.        , 7.        , 5.84391761]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.04159457879229}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8208195398323385
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.16119393,  8.77349308,  1.01206454]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7901107711338264}
episode index:3380
target Thresh 19.0
target distance 1.0
model initialize at round 3380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([14.68116001,  9.58598519,  5.0661366 ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.4431623446681727}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8208695784186051
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.2264857 ,  9.22896487,  0.78295129]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.3220569588201876}
episode index:3381
target Thresh 19.0
target distance 3.0
model initialize at round 3381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([10.00089608, 10.99682283,  5.99728513]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.000897760783705}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8209080528335908
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.57181436, 11.74550785,  3.43091451]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9395496884412095}
episode index:3382
target Thresh 19.0
target distance 6.0
model initialize at round 3382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 6.93568994, 10.13261289,  5.05318666]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 6.437353841504782}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8209247873799005
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.63161992, 5.48610549, 5.92044543]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.814267336710134}
episode index:3383
target Thresh 19.0
target distance 2.0
model initialize at round 3383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.68947092, 3.15462925, 1.23061722]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.969965313773661}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8209747505041971
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.69153498, 4.46753942, 3.23061722]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6153575708462952}
episode index:3384
target Thresh 19.0
target distance 4.0
model initialize at round 3384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([13.31764104,  3.53818543,  2.99832618]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.640821413499261}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8210103503266186
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.36068511,  5.96103402,  2.43195557]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.6405012743833717}
episode index:3385
target Thresh 19.0
target distance 11.0
model initialize at round 3385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.12545948,  3.35046051,  4.5437417 ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.13152270323941}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.8209576631593882
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.70351741, 3.89285994, 4.5791474 ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9407979638326283}
episode index:3386
target Thresh 19.0
target distance 12.0
model initialize at round 3386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.        ,  9.        ,  1.35052794]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.000000000172022}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8209495901214445
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.2357539 , 9.66824647, 3.36823079]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7086136063983074}
episode index:3387
target Thresh 19.0
target distance 10.0
model initialize at round 3387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.        ,  5.        ,  6.06301069]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8209322917519716
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.420891  , 9.81578206, 3.51434293]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9179594830897125}
episode index:3388
target Thresh 19.0
target distance 10.0
model initialize at round 3388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4.        , 3.        , 5.12286448]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 12.206555616359264}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8209105271614079
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.59644439,  9.46611888,  0.29101141]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6692429863450354}
episode index:3389
target Thresh 19.0
target distance 7.0
model initialize at round 3389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.84650522,  9.2254944 ,  3.33593893]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.109868525951746}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8209351500368791
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.83941886, 10.12782722,  4.48638301]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.210499638519416}
episode index:3390
target Thresh 19.0
target distance 12.0
model initialize at round 3390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 4.63814862, 11.89186605,  2.15237924]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 11.39680182683411}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8209318450289712
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.14367724, 10.04733856,  0.17008209]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.2809576426464808}
episode index:3391
target Thresh 19.0
target distance 8.0
model initialize at round 3391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([2.13845447, 4.44572729, 3.11821437]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 9.488903932504787}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8209358491316897
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.15458371, 11.59196226,  1.41910253]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0320600865831775}
episode index:3392
target Thresh 19.0
target distance 8.0
model initialize at round 3392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.15720125,  2.16010951,  1.14748638]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 7.841466384928498}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8209525261649545
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.35492909,  9.82077282,  2.01474516]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.3976141842896679}
episode index:3393
target Thresh 19.0
target distance 8.0
model initialize at round 3393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 5.        , 11.        ,  1.20237749]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 8.54400374531892}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8209640482122369
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.8279513 , 3.05973956, 6.06963627]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8301037094527796}
episode index:3394
target Thresh 19.0
target distance 5.0
model initialize at round 3394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.47719417, 6.98540254, 4.17950678]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.012740211519379}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8209967731900557
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.20829747, 2.19686139, 5.61313616]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.28660468062197897}
episode index:3395
target Thresh 19.0
target distance 6.0
model initialize at round 3395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.99999573, 10.00003145,  2.71587342]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 6.00003145240443}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8210186640855411
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.2063533 ,  3.78392498,  5.8663175 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.29878101846704297}
episode index:3396
target Thresh 19.0
target distance 5.0
model initialize at round 3396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.9996378 , 10.00243522,  2.72844702]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 5.002435235338119}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8210486099290332
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.09383886,  5.28498937,  6.16207641]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.30004112136894207}
episode index:3397
target Thresh 19.0
target distance 11.0
model initialize at round 3397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.43914781,  6.21764423,  1.16009396]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.848565387799617}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.8209923341848525
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.37523066, 3.30797601, 5.19549966]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.485435139573924}
episode index:3398
target Thresh 19.0
target distance 14.0
model initialize at round 3398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([15.08389739,  9.41178349,  3.15639603]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.304325254282555}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.820970615961889
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.92433614, 6.84921669, 4.60772827]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.16870277792441193}
episode index:3399
target Thresh 19.0
target distance 1.0
model initialize at round 3399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.02765202,  1.99369127,  0.78569191]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.006688583383496}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.821020330486606
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.73031093,  3.42450971,  2.78569191]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5029320888795372}
episode index:3400
target Thresh 19.0
target distance 12.0
model initialize at round 3400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.38413214,  5.08791138,  1.23498267]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.987346385618581}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8209964198846953
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.90198616, 8.76849015, 4.6863149 ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9312227633366742}
episode index:3401
target Thresh 19.0
target distance 13.0
model initialize at round 3401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([15.87194454,  5.4148361 ,  2.67105943]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 12.949467843832249}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8209557256387005
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.94343221, 4.91317638, 3.27283574]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.312994832801536}
episode index:3402
target Thresh 19.0
target distance 4.0
model initialize at round 3402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.60837709, 4.9464529 , 3.55434263]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.113562684522491}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8209911427482398
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.76857666, 7.90467149, 2.98797201]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.2502884067006951}
episode index:3403
target Thresh 19.0
target distance 3.0
model initialize at round 3403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.00020535,  9.00028777,  1.96101111]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.162615602316775}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8210293327914688
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.43865717,  6.05709519,  5.6778258 ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.4423572927338915}
episode index:3404
target Thresh 19.0
target distance 11.0
model initialize at round 3404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.10432988,  6.66124824,  2.5180766 ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.184739527098364}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.821007641972538
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.12731633, 8.4275538 , 3.96940883]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.4461072743129314}
episode index:3405
target Thresh 19.0
target distance 11.0
model initialize at round 3405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 4.4827098 , 11.79617673,  1.50281018]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.95961179050644}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.820977319861741
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.158348  ,  3.64539373,  4.6709571 ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0606182891747478}
episode index:3406
target Thresh 19.0
target distance 5.0
model initialize at round 3406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([16.00050635,  3.9997922 ,  0.62057942]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 5.385545809369216}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.821007189945265
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.79959939,  8.30503184,  4.0542088 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.723284971079976}
episode index:3407
target Thresh 19.0
target distance 2.0
model initialize at round 3407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.43993247,  3.59312803,  4.40671372]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.4740520032206474}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8210538721078398
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.96925754,  4.65908615,  2.12352842]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0274640752085749}
episode index:3408
target Thresh 19.0
target distance 11.0
model initialize at round 3408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        , 10.        ,  2.12221873]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.03840481040677}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8210256897857071
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.79359531, 3.76426642, 3.29036566]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1017697944477642}
episode index:3409
target Thresh 19.0
target distance 12.0
model initialize at round 3409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([2.99999999, 6.99999998, 5.26752543]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 12.000000007715688}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8209975239927957
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.28543537,  6.94219017,  0.15248706]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.29123071195039457}
episode index:3410
target Thresh 19.0
target distance 6.0
model initialize at round 3410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.90118219, 5.05413579, 1.06999987]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.013770074852965}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8210219697714578
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.53981377, 11.11862349,  2.22044395]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.5526937993834787}
episode index:3411
target Thresh 19.0
target distance 3.0
model initialize at round 3411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 3.69901433, 10.76105502,  4.33323288]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.7774119596744007}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8210628765827791
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.62860249, 7.90758867, 6.05004757]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.6353589085019432}
episode index:3412
target Thresh 19.0
target distance 6.0
model initialize at round 3412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 3.10680352, 10.67954609,  2.51729113]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.901902695961639}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.821089965469653
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.05751897, 11.43617443,  1.66773521]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.038517511473707}
episode index:3413
target Thresh 19.0
target distance 5.0
model initialize at round 3413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([10.14001067, 10.202621  ,  0.19239491]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.9414884097713703}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8211224714399041
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.09132216, 11.30451065,  1.62602429]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.3179095318028737}
episode index:3414
target Thresh 19.0
target distance 12.0
model initialize at round 3414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.57696869, 1.45782435, 0.6782152 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.437122984294348}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.8210390587762928
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.33987495,  1.94268733,  5.29769437]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.3446733549134524}
episode index:3415
target Thresh 19.0
target distance 2.0
model initialize at round 3415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([4.00198681, 9.0014547 , 1.64199704]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.8308606625066295}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8210771006355211
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.27411977, 6.65828577, 5.35881173]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.4380756391416625}
episode index:3416
target Thresh 19.0
target distance 8.0
model initialize at round 3416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.        ,  5.        ,  4.97075295]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.999999999999813}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8210714095289666
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.55538927, 10.33909525,  4.9884558 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8632799854201072}
episode index:3417
target Thresh 19.0
target distance 4.0
model initialize at round 3417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.17538604, 11.84137035,  3.12189889]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.284962797324461}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.821112230067431
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.66522522, 10.45463721,  4.83871358]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8602006526678243}
episode index:3418
target Thresh 19.0
target distance 8.0
model initialize at round 3418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([16.        ,  3.        ,  0.71097964]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 9.433981133015838}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8211186152570527
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.16413274, 11.04484665,  3.2950531 ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.17014928303146215}
episode index:3419
target Thresh 19.0
target distance 7.0
model initialize at round 3419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([16.66780179, 10.74398836,  0.64391678]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 6.947153506982333}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8211376989577717
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.81518852,  4.25521973,  5.79436086]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8542068978658321}
episode index:3420
target Thresh 19.0
target distance 1.0
model initialize at round 3420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.35454192, 9.8150477 , 1.55167931]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.39988414238465675}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8211899825885937
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.35454192, 9.8150477 , 1.55167931]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.39988414238465675}
episode index:3421
target Thresh 19.0
target distance 7.0
model initialize at round 3421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.04490909, 7.61437311, 5.118891  ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.614552720663207}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8212142935448825
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.83734227, 2.25711141, 6.26933508]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8759271373427919}
episode index:3422
target Thresh 19.0
target distance 13.0
model initialize at round 3422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.3357624 ,  1.74960354,  4.30092955]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.492644722555424}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8211737853069233
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.56301401, 6.87212064, 4.90270586]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5773542288288122}
episode index:3423
target Thresh 19.0
target distance 6.0
model initialize at round 3423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 9.63334567, 11.4056723 ,  1.25344294]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.385457794121346}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.821200754776017
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.93626585, 10.19001908,  0.40388702]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8124845392744144}
episode index:3424
target Thresh 19.0
target distance 12.0
model initialize at round 3424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([2.00000008, 6.99999984, 6.20606565]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 12.041594482357972}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8211813441285505
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.5708804 ,  6.81162454,  5.65739789]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9180838882059491}
episode index:3425
target Thresh 19.0
target distance 10.0
model initialize at round 3425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.21752707,  7.84302898,  4.97394514]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.255998221566985}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.821175637545191
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.86593842, 7.72111042, 4.99164799]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.7334662501157918}
episode index:3426
target Thresh 19.0
target distance 6.0
model initialize at round 3426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 8.68601263, 10.10975465,  4.85184336]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 4.687297775896742}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8212025828646944
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.00421322, 10.26032774,  4.00228744]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.2603618286191954}
episode index:3427
target Thresh 19.0
target distance 3.0
model initialize at round 3427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.40157701, 3.43099373, 5.35801697]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.5510812802736234}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8212460765686429
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.19518608, 2.46720922, 5.07483166]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9305965296947349}
episode index:3428
target Thresh 19.0
target distance 11.0
model initialize at round 3428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([15.        ,  3.        ,  5.38331628]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 11.704699910719622}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8212117230015479
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.6695935 , 7.62915983, 6.2682779 ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.918802236705194}
episode index:3429
target Thresh 19.0
target distance 4.0
model initialize at round 3429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([15.82163859, 11.12034212,  3.55804265]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 4.5050623024905585}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8212523598199147
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.79450083,  7.97648449,  5.27485734]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.2588699473861864}
episode index:3430
target Thresh 19.0
target distance 5.0
model initialize at round 3430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.00042404, 10.99669199,  5.84987807]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.000425129454447}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8212792513639728
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.2353571 , 10.40160759,  5.00032215]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.6430135663026054}
episode index:3431
target Thresh 19.0
target distance 11.0
model initialize at round 3431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([15.00000069,  5.99999983,  0.76549166]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 11.180340592968841}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8212620787133269
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.783795 , 8.8139933, 4.5000092]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8422171279140487}
episode index:3432
target Thresh 19.0
target distance 12.0
model initialize at round 3432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([2.        , 3.        , 5.94046378]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 13.416407865149296}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.82122776051242
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.22016551,  8.84451526,  0.5422401 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.26953359017085066}
episode index:3433
target Thresh 19.0
target distance 1.0
model initialize at round 3433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([4.00642339, 2.00208403, 1.32372826]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0064255503005428}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8212769079321891
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.33970779, 2.72797391, 3.32372826]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8033351724390375}
episode index:3434
target Thresh 19.0
target distance 12.0
model initialize at round 3434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 4.6785476 , 10.12192633,  1.08251065]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.522435027631229}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.821257531623389
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.72184747,  5.50510651,  0.53384288]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8810200700882581}
episode index:3435
target Thresh 19.0
target distance 3.0
model initialize at round 3435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.27170808,  7.99377502,  0.34062403]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 2.364821401972507}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8213009080693658
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.77585738,  6.38868027,  0.05743872]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8677712955883745}
episode index:3436
target Thresh 19.0
target distance 6.0
model initialize at round 3436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.35069681,  8.33494297,  3.95123589]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.102067395835624}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.821325080652124
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.287845  , 10.3167213 ,  5.10167997]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7414340977646934}
episode index:3437
target Thresh 19.0
target distance 8.0
model initialize at round 3437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4.67977139, 4.03696299, 1.03200119]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 9.40367875125076}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8213193521788508
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.47309992, 11.74413787,  1.04970404]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9117921159124447}
episode index:3438
target Thresh 19.0
target distance 2.0
model initialize at round 3438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4.00806802, 7.99765278, 0.72688597]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.835794695130189}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8213598513523956
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.68826842, 10.2166391 ,  2.44370066]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.7215579831194266}
episode index:3439
target Thresh 19.0
target distance 12.0
model initialize at round 3439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([1.99885863, 6.97352564, 5.67930341]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 12.364030448048656}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8213255745627583
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.74693126,  4.73185516,  0.28107972]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0457142422558987}
episode index:3440
target Thresh 19.0
target distance 13.0
model initialize at round 3440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([15.28892118,  8.34201549,  5.89491701]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 14.724693086277863}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8212812985336989
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.6047121 , 2.28059738, 4.21350802]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.4847550049159298}
episode index:3441
target Thresh 19.0
target distance 6.0
model initialize at round 3441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.20688431,  7.91640867,  4.93804622]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 5.233195777676274}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8213054416994384
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.22299044,  2.57074284,  6.0884903 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.4837214513938395}
episode index:3442
target Thresh 19.0
target distance 1.0
model initialize at round 3442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.0557313 , 5.07033007, 1.91069144]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.4273226106228847}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8213515626864558
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.34554993, 3.49579142, 5.91069144]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8261544613665084}
episode index:3443
target Thresh 19.0
target distance 13.0
model initialize at round 3443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([3.35468618, 2.99860575, 1.6452257 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.64531390573334}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.8212905190386179
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.34479089,  2.94131715,  5.39744609]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.6578317825631917}
episode index:3444
target Thresh 19.0
target distance 12.0
model initialize at round 3444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.13437533,  6.16686606,  4.09752083]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.775705068367415}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8212779009951813
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.62236889, 10.45884339,  3.83203837]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.7732271890499012}
episode index:3445
target Thresh 19.0
target distance 1.0
model initialize at round 3445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.29848704,  6.42415702,  5.90958548]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.648605930945911}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8213297646338943
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.29848704,  6.42415702,  5.90958548]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.648605930945911}
episode index:3446
target Thresh 19.0
target distance 2.0
model initialize at round 3446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([13.99113249,  2.0102853 ,  3.29230416]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.008893838868088}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8213758250445025
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.62974132,  1.97605641,  1.00911885]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.37103205860962335}
episode index:3447
target Thresh 19.0
target distance 5.0
model initialize at round 3447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.99955663,  2.99982284,  4.53174043]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.099106306904018}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8214025481948617
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.23459719,  7.67104963,  3.68218451]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8330965218795194}
episode index:3448
target Thresh 19.0
target distance 8.0
model initialize at round 3448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 3.        , 11.        ,  3.66501474]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 8.000000001855552}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8214162748589887
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.4855633 , 10.41287246,  0.24908821]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7806176155681376}
episode index:3449
target Thresh 19.0
target distance 3.0
model initialize at round 3449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.09294829,  7.32116663,  5.77769732]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.3244321955221683}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.821459429272073
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.75936975,  5.83173601,  5.49451202]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.2936250772604705}
episode index:3450
target Thresh 19.0
target distance 12.0
model initialize at round 3450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([16.        , 10.        ,  1.21535748]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 14.422205101855955}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8214211764921823
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.74162478, 2.51967279, 6.1003191 ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9055755745211062}
episode index:3451
target Thresh 19.0
target distance 6.0
model initialize at round 3451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 1.99999998, 10.00000061,  2.61635959]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 6.000000612699576}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8214425893188817
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.42727465, 3.77342381, 5.76680367]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.48363249950616155}
episode index:3452
target Thresh 19.0
target distance 10.0
model initialize at round 3452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.13965616, 11.44641098,  3.11739302]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.15055176016614}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.82145128181288
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.20212703, 11.17427366,  3.70146648]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.2668832027056935}
episode index:3453
target Thresh 19.0
target distance 5.0
model initialize at round 3453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.00007788,  5.99986628,  6.24978161]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.385260038279161}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8214806082207013
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.13343823, 10.79097476,  3.40022569]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8914150836496634}
episode index:3454
target Thresh 19.0
target distance 14.0
model initialize at round 3454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([2.51638121, 4.60179229, 2.26893854]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 14.924645305723635}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8214485018875758
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.04509318, 11.61906231,  1.15390016]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.1380180927599253}
episode index:3455
target Thresh 19.0
target distance 8.0
model initialize at round 3455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.69312343, 7.46027839, 2.07324356]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.119117829406122}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8214522823446252
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.12338028, 10.24338843,  0.37413172]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.157982383536407}
episode index:3456
target Thresh 19.0
target distance 7.0
model initialize at round 3456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.        ,  4.        ,  5.24588156]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 7.000000000010379}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8214736552031482
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.40982472, 10.30571579,  2.11314033]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8062176245343565}
episode index:3457
target Thresh 19.0
target distance 10.0
model initialize at round 3457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([16.        ,  2.        ,  5.09056401]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 13.453624047224345}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8214543508746347
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.77143456, 10.12136154,  4.54189625]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.1692377093626634}
episode index:3458
target Thresh 19.0
target distance 4.0
model initialize at round 3458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.76559523,  5.14395466,  3.40420663]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.9568785945693645}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8214890504405572
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.64390386,  8.18565879,  2.83783602]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.4015889027778163}
episode index:3459
target Thresh 19.0
target distance 11.0
model initialize at round 3459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 4.       , 10.       ,  4.5538733]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.180339888076396}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8214809941495755
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.18295213,  8.42197859,  0.28839084]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.4599319661876822}
episode index:3460
target Thresh 19.0
target distance 6.0
model initialize at round 3460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.76184843,  4.27238794,  1.35318535]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.992466089433159}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8215049470767236
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.8415947 , 10.17630212,  2.50362943]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.2370119756376201}
episode index:3461
target Thresh 19.0
target distance 11.0
model initialize at round 3461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 4.      , 11.      ,  2.667009]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 13.038404810431759}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8214813177948955
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.3473871 ,  4.45314983,  6.11834123]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.5709838579489063}
episode index:3462
target Thresh 19.0
target distance 11.0
model initialize at round 3462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.03192145, 9.37656373, 3.19370508]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 14.058755162139349}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8214452022593064
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.91601493,  1.86688247,  6.0786667 ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.15739685305696705}
episode index:3463
target Thresh 19.0
target distance 2.0
model initialize at round 3463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.89507834,  2.78312207,  0.07339447]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.6436677814957572}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8214938612655825
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.70070914,  4.23222187,  2.07339447]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.3788165993254195}
episode index:3464
target Thresh 19.0
target distance 2.0
model initialize at round 3464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.31693997, 6.20201887, 1.57746952]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 2.7715660192891463}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8215312339029951
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.10092739, 3.86928279, 5.29428422]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9085254760909331}
episode index:3465
target Thresh 19.0
target distance 13.0
model initialize at round 3465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.89442585,  5.26889369,  3.29752111]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.916873870549205}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.821497164791944
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.67507236, 6.09858838, 4.18248274]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.3395550583909957}
episode index:3466
target Thresh 19.0
target distance 12.0
model initialize at round 3466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.31721741,  5.02495564,  4.13676381]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.504559741891384}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8214757269291058
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.83178898, 6.91702311, 5.58809605]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8359175013039283}
episode index:3467
target Thresh 19.0
target distance 11.0
model initialize at round 3467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.        , 11.        ,  1.69388026]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8214723394265948
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.99986651, 6.86687869, 5.99476841]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.0086894041100427}
episode index:3468
target Thresh 19.0
target distance 4.0
model initialize at round 3468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.50198735,  5.25733375,  0.55079764]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 3.2957873806886777}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8215069337794269
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.95162757,  2.00663548,  6.26761233]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.048825416007381106}
episode index:3469
target Thresh 19.0
target distance 12.0
model initialize at round 3469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.        , 3.99999994, 5.77153277]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.041594580842288}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.8214553789287944
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.78154191,  5.22134305,  5.80693847]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.3109930598933716}
episode index:3470
target Thresh 19.0
target distance 1.0
model initialize at round 3470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.03400619, 8.95465507, 0.08266943]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.4233376468254935}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8215039368720588
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.81992567, 10.39969775,  2.08266943]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.4383891639412691}
episode index:3471
target Thresh 19.0
target distance 2.0
model initialize at round 3471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([16.00227933, 10.99806093,  0.30508917]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.002280269794811}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8215496154616695
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.36276074, 10.81390268,  4.30508917]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.4077101470898997}
episode index:3472
target Thresh 19.0
target distance 12.0
model initialize at round 3472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([2.        , 7.        , 6.08224106]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 12.369316876847591}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.821537024544289
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.37379749,  9.64397951,  5.8167586 ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7203333772392644}
episode index:3473
target Thresh 19.0
target distance 4.0
model initialize at round 3473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([11.99994905, 11.00013961,  2.93076098]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 4.472283610023945}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8215661574371743
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.60119356,  7.45667252,  0.08120506]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7549725060813222}
episode index:3474
target Thresh 19.0
target distance 13.0
model initialize at round 3474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.3148526 , 6.67424463, 0.20758408]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.963971184630664}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.821538362956173
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.3436019 ,  4.81305601,  1.37573101]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8826790647767352}
episode index:3475
target Thresh 19.0
target distance 1.0
model initialize at round 3475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.96678419, 10.69135857,  1.63079613]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.0148552640238122}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8215897040485332
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.96678419, 10.69135857,  1.63079613]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.0148552640238122}
episode index:3476
target Thresh 19.0
target distance 9.0
model initialize at round 3476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.44661413,  2.52467049,  1.3579337 ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 10.074562807233875}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8215958454029582
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.71439148, 10.23211361,  3.94200716]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.048811088539654}
episode index:3477
target Thresh 19.0
target distance 12.0
model initialize at round 3477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([12.46446142, 11.64598757,  3.73185706]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 10.793803922741427}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8215971601753654
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.45930024, 9.74438661, 4.03274522]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8746817343039653}
episode index:3478
target Thresh 19.0
target distance 1.0
model initialize at round 3478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.06240688,  5.1756696 ,  4.87279558]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9539081060245014}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8216484400948322
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.06240688,  5.1756696 ,  4.87279558]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9539081060245014}
episode index:3479
target Thresh 19.0
target distance 10.0
model initialize at round 3479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 6.        , 11.        ,  5.08316612]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 10.000000000041059}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8216545592768119
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.96018387, 11.16879111,  1.38405428]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.17342364812268968}
episode index:3480
target Thresh 19.0
target distance 8.0
model initialize at round 3480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 5.99999969, 11.0000014 ,  2.80066121]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 8.944273024712954}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8216680873588262
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.94618188, 3.63549103, 5.66791998]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.1397846255060453}
episode index:3481
target Thresh 19.0
target distance 7.0
model initialize at round 3481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.        , 4.        , 0.16718739]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 9.219544457405993}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8216623327643919
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.59574572, 10.52546577,  0.18489024]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.6233813110520428}
episode index:3482
target Thresh 19.0
target distance 8.0
model initialize at round 3482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 7.21865814, 11.87172948,  2.47908056]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 8.294342124554095}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8216733566007046
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.13339125,  9.01855058,  1.06315402]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.1346749766701254}
episode index:3483
target Thresh 19.0
target distance 14.0
model initialize at round 3483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.95822269,  5.67822539,  5.05490589]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.47014763274101}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.8216201166837882
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.34789858, 2.99666382, 0.80712628]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.055638282594104}
episode index:3484
target Thresh 19.0
target distance 5.0
model initialize at round 3484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 6.32959639, 11.20464402,  4.02968836]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.6212504160743}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8216518082852869
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.92426158, 8.24652009, 5.46331774]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9565728555558412}
episode index:3485
target Thresh 19.0
target distance 10.0
model initialize at round 3485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([2.73895421, 6.66260133, 2.73653531]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 11.140111661661399}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8216414888167046
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.14618981, 10.59132606,  0.47105285]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.43403438166518504}
episode index:3486
target Thresh 19.0
target distance 9.0
model initialize at round 3486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 8.32824953, 10.25033975,  0.36505478]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 9.29633383643883}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8216380722349451
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.28371977,  5.39535158,  4.66594294]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8181443882125038}
episode index:3487
target Thresh 19.0
target distance 8.0
model initialize at round 3487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.91762361, 10.0135605 ,  3.98843956]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 8.065927408309628}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8216540937231229
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.15293037,  2.6280454 ,  4.85569833]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.054498929447071}
episode index:3488
target Thresh 19.0
target distance 7.0
model initialize at round 3488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 8.4931835 , 10.11723271,  6.02599239]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 6.8426117275301195}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8216701060273006
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.92620916,  8.81164442,  0.61006586]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8149918729676319}
episode index:3489
target Thresh 19.0
target distance 8.0
model initialize at round 3489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([1.13198415, 3.88405062, 3.59180391]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.608600712758957}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8216597931432834
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.39295572, 11.37563258,  1.32632145]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5436120216343265}
episode index:3490
target Thresh 19.0
target distance 3.0
model initialize at round 3490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.01818477, 8.32770004, 1.92658013]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.957878285254751}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8217023709739499
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.84649807, 9.51247137, 1.64339482]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9768536973601053}
episode index:3491
target Thresh 19.0
target distance 13.0
model initialize at round 3491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([1.99693435, 5.9135848 , 5.68692803]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 13.325490334713097}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8216625233863316
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([1.50063779e+01, 3.27897922e+00, 5.51904049e-03]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.27905211316756157}
episode index:3492
target Thresh 19.0
target distance 8.0
model initialize at round 3492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.31714839, 10.98005333,  4.163445  ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.177800105058457}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8216544935438631
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7075475 , 2.83923722, 6.18114785]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7255812439424264}
episode index:3493
target Thresh 19.0
target distance 14.0
model initialize at round 3493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([2.        , 6.99999999, 5.88430762]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 14.035668846696902}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8216331763145522
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.56775846,  8.78530555,  1.05245455]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8964025657314152}
episode index:3494
target Thresh 19.0
target distance 6.0
model initialize at round 3494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.99999644, 4.99999261, 5.27322769]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 6.082770401796659}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.821654265034994
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.90506223, 11.20945808,  2.14048646]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.22996927070053239}
episode index:3495
target Thresh 19.0
target distance 14.0
model initialize at round 3495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.48451186, 8.79285094, 1.50053995]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.401823142309066}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8216224855619497
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.11021579,  4.73128313,  0.38550158]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7395421105074476}
episode index:3496
target Thresh 19.0
target distance 4.0
model initialize at round 3496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([3.99952242, 6.00068422, 3.19017017]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 4.472534422681554}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8216540677359118
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.51651527, 2.48250881, 4.62379955]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.683060925625288}
episode index:3497
target Thresh 19.0
target distance 14.0
model initialize at round 3497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([16.        , 10.        ,  1.02093619]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8216371119459207
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.83185046, 8.68410047, 4.75545373]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.357864197264488}
episode index:3498
target Thresh 19.0
target distance 8.0
model initialize at round 3498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.99998783, 11.00001765,  3.18457258]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 8.000017651247529}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8216505754214345
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.5918609 ,  2.75731253,  6.05183135]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.6396847112794862}
episode index:3499
target Thresh 19.0
target distance 10.0
model initialize at round 3499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.07965029,  8.54544391,  4.59053302]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.210235950265817}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8216425650523551
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.86858572, 7.89218667, 0.32505056]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.245165939744351}
episode index:3500
target Thresh 19.0
target distance 8.0
model initialize at round 3500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([12.00000158, 11.00000285,  2.07352931]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 8.544005857657012}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8216486492078341
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.25887352,  3.72071047,  4.65760277]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.033775629270567}
episode index:3501
target Thresh 19.0
target distance 1.0
model initialize at round 3501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([15.20836717,  7.23097298,  5.98698521]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.2302437724201825}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8216938951675121
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.82298239,  7.90782059,  3.7037999 ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9249180847537646}
episode index:3502
target Thresh 19.0
target distance 2.0
model initialize at round 3502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.02214693,  9.00423521,  1.19895094]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.0043575647948235}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8217391152944984
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.22363992,  7.63013953,  5.19895094]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9999053916663783}
episode index:3503
target Thresh 19.0
target distance 12.0
model initialize at round 3503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 4.        , 11.        ,  4.68989968]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.892443989449873}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8217178347519861
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.09715183,  4.52976309,  6.14123192]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.5385976365712976}
episode index:3504
target Thresh 19.0
target distance 12.0
model initialize at round 3504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.66963908, 7.78859633, 0.88405388]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.003173133026136}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.821686118744146
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.27052161,  3.70924784,  6.05220081]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.3971382118509245}
episode index:3505
target Thresh 19.0
target distance 9.0
model initialize at round 3505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.12861253,  3.13916712,  3.43071425]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.879084132734544}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.82168039839925
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.20562014, 11.61788162,  3.4484171 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.6511968500233037}
episode index:3506
target Thresh 19.0
target distance 3.0
model initialize at round 3506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([1.80147143, 7.09492424, 3.70558631]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.643211907014411}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8217145585791763
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.17641171, 10.06691555,  3.1392157 ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8263022255698926}
episode index:3507
target Thresh 19.0
target distance 6.0
model initialize at round 3507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([12.79802809,  9.171822  ,  4.75571704]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 5.62106956622977}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8217381240057526
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.16033164,  3.74668693,  5.90616112]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.29978949916560826}
episode index:3508
target Thresh 19.0
target distance 4.0
model initialize at round 3508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.99930989,  9.00060954,  3.42810297]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.123864377177694}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8217695652208855
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.36713039,  5.06326122,  4.86173236]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.636023521929397}
episode index:3509
target Thresh 19.0
target distance 2.0
model initialize at round 3509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 3.85718408, 11.32870915,  2.99066496]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.2835500261405417}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.821817494119683
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.73396807, 10.07794952,  4.99066496]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.7380956914569203}
episode index:3510
target Thresh 19.0
target distance 13.0
model initialize at round 3510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([15.04238755,  3.03226832,  1.66067427]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 13.956440483715072}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8217817765816396
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.96497594, 7.74302878, 4.54563589]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.2593470509313434}
episode index:3511
target Thresh 19.0
target distance 9.0
model initialize at round 3511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([10.72452186, 10.12032218,  5.56505775]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 9.177109877012118}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8217830256839329
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.15844853,  2.26401945,  5.86594591]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.30791590815658854}
episode index:3512
target Thresh 19.0
target distance 8.0
model initialize at round 3512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([16.        ,  3.        ,  5.72775817]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.821796393969468
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.63445987, 10.72434956,  2.31183163]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.691753205608461}
episode index:3513
target Thresh 19.0
target distance 13.0
model initialize at round 3513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([3.        , 7.        , 6.25175619]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.928388276196902}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8217627147722656
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.22023799,  2.70596972,  0.85353251]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.7395255372659193}
episode index:3514
target Thresh 19.0
target distance 8.0
model initialize at round 3514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.96499752,  9.67291373,  5.06002116]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.090299601701991}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8217810994541841
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.46612309, 10.81612148,  3.92727993]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.5010808736533421}
episode index:3515
target Thresh 19.0
target distance 1.0
model initialize at round 3515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.98342971,  3.00257865,  3.99721217]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.0165735609002535}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8218289432825532
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.25654914,  2.03392486,  5.99721217]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.219024342758206}
episode index:3516
target Thresh 19.0
target distance 3.0
model initialize at round 3516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([1.39309899, 4.56544852, 2.95064151]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.1540818942622693}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8218711582546082
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.16335229, 5.90458695, 2.66745621]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8420706845981534}
episode index:3517
target Thresh 19.0
target distance 13.0
model initialize at round 3517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([3.00000003, 1.99999996, 0.13168209]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 13.038404783142271}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.8218147779921399
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.02424122,  3.98338106,  6.16708779]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9836798012224706}
episode index:3518
target Thresh 19.0
target distance 10.0
model initialize at round 3518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([3.99810195, 2.00036689, 3.96064878]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.199828289942037}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.8217566578906764
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.5693743 ,  4.80770719,  5.71286917]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9882196123652537}
episode index:3519
target Thresh 19.0
target distance 8.0
model initialize at round 3519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([15.        ,  3.        ,  5.13860154]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.246211251236241}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8217700070824032
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.84160217, 10.00880119,  1.72267501]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.300295852380528}
episode index:3520
target Thresh 19.0
target distance 4.0
model initialize at round 3520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 2.40565699, 10.23035559,  3.03080523]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 4.658362395190839}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8217986849260117
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.50819513, 10.46377755,  0.18124931]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.7276032888626424}
episode index:3521
target Thresh 19.0
target distance 6.0
model initialize at round 3521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.76226832,  9.97357475,  5.49479079]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.852972319877432}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8218247265394578
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.70994504, 10.36788407,  4.64523487]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.950574828660984}
episode index:3522
target Thresh 19.0
target distance 11.0
model initialize at round 3522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([16.        ,  8.        ,  0.23521775]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8218212928584138
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.04013391, 10.30108869,  4.5361059 ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7000626772695354}
episode index:3523
target Thresh 19.0
target distance 12.0
model initialize at round 3523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.00015392,  4.70896982,  5.06341887]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 11.328808579018073}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.8217632533714909
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.84782338, 2.763728  , 0.53245395]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.1410893717931425}
episode index:3524
target Thresh 19.0
target distance 13.0
model initialize at round 3524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([1.97766531, 4.01086468, 3.69884479]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 14.333560015687993}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8217337405429027
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.47912052, 10.52679501,  0.58380641]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7408295458693781}
episode index:3525
target Thresh 19.0
target distance 4.0
model initialize at round 3525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([15.00278993,  1.99838031,  0.48400563]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 4.125354263800481}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8217650314128302
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.70973744,  5.62090767,  1.91763501]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8046354648626576}
episode index:3526
target Thresh 19.0
target distance 1.0
model initialize at round 3526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.44319304,  5.95603651,  6.12386084]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.1831710573190652}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8218127305816952
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.36814305,  6.96878019,  1.84067553]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.3694644523780631}
episode index:3527
target Thresh 19.0
target distance 14.0
model initialize at round 3527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([1.12983538, 8.13933003, 3.34681439]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 14.895051145651298}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8217894561040326
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.97978555,  9.4852391 ,  0.51496132]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.4856599692977221}
episode index:3528
target Thresh 19.0
target distance 2.0
model initialize at round 3528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.11600483,  4.2551337 ,  1.85400694]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.1559728632940498}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8218399549830058
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.11600483,  4.2551337 ,  1.85400694]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.1559728632940498}
episode index:3529
target Thresh 19.0
target distance 12.0
model initialize at round 3529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.37150057,  5.42470886,  3.89647758]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.686446950768136}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8218252816659581
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.82323833, 8.22859267, 5.63099512]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.854386307562769}
episode index:3530
target Thresh 19.0
target distance 8.0
model initialize at round 3530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.99999999,  2.99999999,  4.83925152]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 8.062257757824238}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8218361094974437
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.50417946, 11.01991417,  3.42332498]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.4962203000031466}
episode index:3531
target Thresh 19.0
target distance 14.0
model initialize at round 3531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([16.        ,  9.        ,  0.47824257]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 15.231546211727983}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8218046024526461
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.94569676, 3.49017497, 5.6463895 ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0651825480768498}
episode index:3532
target Thresh 19.0
target distance 10.0
model initialize at round 3532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.        , 10.        ,  2.09552687]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.180339887544246}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8217943770177054
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.75470815, 5.47836621, 6.11322972]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8935427401629462}
episode index:3533
target Thresh 19.0
target distance 12.0
model initialize at round 3533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.81961257,  6.15121875,  2.73622507]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.850049403808928}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8217669819862793
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.00344303, 7.67203993, 3.904372  ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.6720487461275491}
episode index:3534
target Thresh 19.0
target distance 12.0
model initialize at round 3534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([2.        , 4.        , 4.49837708]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 13.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8217396024541637
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.31872032,  9.22793137,  5.66652401]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7183973222608706}
episode index:3535
target Thresh 19.0
target distance 9.0
model initialize at round 3535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([10.9507078 , 10.13311555,  5.06299186]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.041357515501495}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8217339155161221
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.16228203, 4.36689114, 5.08069471]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.4011789741384631}
episode index:3536
target Thresh 19.0
target distance 12.0
model initialize at round 3536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.23155694, 9.85297969, 0.26012772]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.139966746602697}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8217215003178985
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.44692918,  6.7494686 ,  6.27783057]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.5123589330817113}
episode index:3537
target Thresh 19.0
target distance 2.0
model initialize at round 3537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4.28397369, 5.65790025, 2.17331806]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.309451890234139}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8217690634890917
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.61843735, 5.62171011, 4.17331806]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.7249606847148498}
episode index:3538
target Thresh 19.0
target distance 8.0
model initialize at round 3538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.99997002, 10.0000673 ,  2.99994034]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 8.062320808579484}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8217823375069724
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.11182258,  1.75470709,  5.86719911]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.269579121551333}
episode index:3539
target Thresh 19.0
target distance 2.0
model initialize at round 3539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4.00049121, 8.00061991, 1.91071528]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.8292128071444758}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8218188368607557
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.37941453, 5.51635774, 5.62752997]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.6147074270976736}
episode index:3540
target Thresh 19.0
target distance 5.0
model initialize at round 3540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([4.00000267, 7.00000171, 1.57953661]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.385167384746476}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8218473389385776
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.97841882, 2.53883934, 5.013166  ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5392713425431985}
episode index:3541
target Thresh 19.0
target distance 14.0
model initialize at round 3541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([3.5964367 , 8.53270812, 1.33206766]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.659502080971746}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8218220583148385
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.94620659,  6.50890197,  0.50021459]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.5117371896743879}
episode index:3542
target Thresh 19.0
target distance 8.0
model initialize at round 3542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 7.63986891, 10.62156972,  0.78319567]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 8.48842232680866}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8218118568139895
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.15314117,  5.33110595,  4.80089852]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.909286003959325}
episode index:3543
target Thresh 19.0
target distance 1.0
model initialize at round 3543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.99874103,  4.99883791,  4.89699578]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.4159255068696337}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8218565205112769
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.77896655,  6.31906506,  2.61381048]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.3881472668022613}
episode index:3544
target Thresh 19.0
target distance 7.0
model initialize at round 3544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 7.62768606, 11.42781557,  1.26702326]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 5.389321217148757}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8218823768517486
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.27278677, 10.13822661,  0.41746734]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.1276047469982249}
episode index:3545
target Thresh 19.0
target distance 5.0
model initialize at round 3545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.97079337, 10.96204091,  2.61114603]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.970938306937799}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8219134493196153
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.26427302, 10.55383107,  4.04477542]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.518562377618984}
episode index:3546
target Thresh 19.0
target distance 6.0
model initialize at round 3546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([10.        , 11.        ,  4.45355463]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 7.810249675919936}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8219217787026926
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.20999042,  6.33950752,  5.03762809]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8598723735389824}
episode index:3547
target Thresh 19.0
target distance 2.0
model initialize at round 3547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([16.00070353,  3.00898091,  2.50261945]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.2407269142544552}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.821969151369349
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.43611561,  2.43604049,  4.50261945]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6167074910903196}
episode index:3548
target Thresh 19.0
target distance 2.0
model initialize at round 3548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.53119773, 6.39000964, 5.43904042]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.6098221726475049}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8220193150347845
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.53119773, 6.39000964, 5.43904042]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.6098221726475049}
episode index:3549
target Thresh 19.0
target distance 12.0
model initialize at round 3549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([16.        , 11.        ,  1.30487984]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8220158526554004
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.81766695, 8.40399162, 5.605768  ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9120243744035601}
episode index:3550
target Thresh 19.0
target distance 13.0
model initialize at round 3550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([15.00033103,  3.00021836,  1.59310406]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.000331036553197}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.8219447699603348
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.78857615, 3.66825507, 0.49576853]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0336426803351}
episode index:3551
target Thresh 19.0
target distance 12.0
model initialize at round 3551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([15.75077157,  7.65532349,  2.73023564]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 11.866790985163302}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8219216155694079
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.77400495, 6.82610057, 6.18156787]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.132044966632518}
episode index:3552
target Thresh 19.0
target distance 4.0
model initialize at round 3552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.09986861, 9.03595481, 1.35557431]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.460877777198029}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8219499924562242
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.4468839 , 5.78501008, 4.78920369]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9603011206581996}
episode index:3553
target Thresh 19.0
target distance 12.0
model initialize at round 3553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.11095877,  5.30551366,  3.28925848]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.132641916381273}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8219206679036475
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.96090195, 6.10009274, 0.1742201 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9661009902761382}
episode index:3554
target Thresh 19.0
target distance 10.0
model initialize at round 3554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.        , 11.        ,  1.33003348]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.19803902718557}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8219242102647013
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.47127927, 8.58893499, 5.91410694]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.6253627695257391}
episode index:3555
target Thresh 19.0
target distance 5.0
model initialize at round 3555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([16.37547117,  7.57145679,  0.70797079]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 5.151803610742142}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8219499675867538
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.47799413,  2.84923301,  6.14160018]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5012076137045972}
episode index:3556
target Thresh 19.0
target distance 6.0
model initialize at round 3556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([10.99992215, 11.00002662,  3.82206917]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 6.708262559154878}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8219631235735916
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.73823749,  5.22015705,  0.40614263]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.7703659672472725}
episode index:3557
target Thresh 19.0
target distance 12.0
model initialize at round 3557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([12.31842405, 11.06848659,  4.11088753]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.496061552487804}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8219507172317776
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.76821018, 6.5367497 , 3.84540507]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5846595241679693}
episode index:3558
target Thresh 19.0
target distance 12.0
model initialize at round 3558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4.        , 9.        , 4.14265347]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.369316876852983}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8219339687061005
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.048877  ,  6.55435944,  1.5939857 ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.5565099765344063}
episode index:3559
target Thresh 19.0
target distance 14.0
model initialize at round 3559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([2.00000433, 4.99999182, 6.20867682]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 14.035663950440343}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8218891226589875
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.30458015,  4.18197104,  0.24408251]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.35479927711204967}
episode index:3560
target Thresh 19.0
target distance 10.0
model initialize at round 3560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 6.67786719, 10.86904278,  0.9321081 ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 9.177547964990273}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8218857075355847
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.23799166,  7.29604914,  5.23299626]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8174972762229862}
episode index:3561
target Thresh 19.0
target distance 4.0
model initialize at round 3561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.29225642, 5.92607433, 0.76224869]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 4.134945146911006}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8219246492263382
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.24944882, 9.20184728, 2.47906338]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.09561619420014}
episode index:3562
target Thresh 19.0
target distance 12.0
model initialize at round 3562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([16.7089849 ,  9.47403079,  6.15732861]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 14.262937002502731}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.821891397204383
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.66126336, 3.81267141, 0.75910492]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0477137227642495}
episode index:3563
target Thresh 19.0
target distance 10.0
model initialize at round 3563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.        ,  7.        ,  5.31163216]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.049875620548164}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8218704321923536
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.76659202, 6.75343295, 0.47977908]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0748602439011057}
episode index:3564
target Thresh 19.0
target distance 12.0
model initialize at round 3564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([2.        , 8.        , 4.11369872]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.821860280076958
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.191956  ,  9.79980489,  1.84821626]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.136935784490569}
episode index:3565
target Thresh 19.0
target distance 13.0
model initialize at round 3565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([3.        , 9.        , 4.42401648]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8218270740800211
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.48804555,  2.79945215,  5.3089781 ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5498334316390893}
episode index:3566
target Thresh 19.0
target distance 8.0
model initialize at round 3566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([15.00000014,  6.00000007,  1.45503872]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.433981213302928}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8218329939340452
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.9350374 , 10.39814498,  4.03911219]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.6053508149219944}
episode index:3567
target Thresh 19.0
target distance 7.0
model initialize at round 3567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.44322229, 3.80067579, 1.51650828]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 6.3651010412906155}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8218561298872613
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.50338365, 9.44825944, 2.66695236]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.7423243536976027}
episode index:3568
target Thresh 19.0
target distance 1.0
model initialize at round 3568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.50220228, 10.16067259,  4.04472876]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9758447991043102}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8219060441125661
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.50220228, 10.16067259,  4.04472876]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9758447991043102}
episode index:3569
target Thresh 19.0
target distance 13.0
model initialize at round 3569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.06516181,  6.39942826,  3.16973138]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.071771714831169}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8218768632970158
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.84732514, 6.16408404, 0.054693  ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8630663144477835}
episode index:3570
target Thresh 19.0
target distance 4.0
model initialize at round 3570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.87067289, 7.85660672, 0.21727246]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 3.414618428679738}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8219103562362776
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.52700509, 4.87720873, 5.93408715]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5411211151592785}
episode index:3571
target Thresh 19.0
target distance 5.0
model initialize at round 3571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.00026944,  5.99974658,  0.25522583]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.000253426530126}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8219411946997912
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.82001117, 10.09520985,  1.68885522]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.2210911244431606}
episode index:3572
target Thresh 19.0
target distance 11.0
model initialize at round 3572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.59785424, 7.528441  , 1.32939595]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.736188107296899}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8219161071473017
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.9109136 ,  5.70306048,  0.49754287]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1506770297375317}
episode index:3573
target Thresh 19.0
target distance 1.0
model initialize at round 3573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.06333789, 10.98838958,  0.82870358]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9904169089815351}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8219659347614182
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.06333789, 10.98838958,  0.82870358]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9904169089815351}
episode index:3574
target Thresh 19.0
target distance 4.0
model initialize at round 3574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([10.59969729, 11.89674466,  2.15164484]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 4.466899163745434}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.821994124624262
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.18579833,  8.92631406,  5.58527423]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8175291906365256}
episode index:3575
target Thresh 19.0
target distance 12.0
model initialize at round 3575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.        ,  3.        ,  1.04221505]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.16552506231816}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8219513331348731
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.52518664, 5.03881775, 5.64399136]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.5266192409117153}
episode index:3576
target Thresh 19.0
target distance 9.0
model initialize at round 3576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.97129842, 9.66806137, 5.06474662]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 7.7367559166061595}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8219668684130013
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.75551139, 2.7973477 , 5.93200539]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.098435621467141}
episode index:3577
target Thresh 19.0
target distance 4.0
model initialize at round 3577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([4.77206136, 4.5133844 , 6.20139694]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 3.7418478549977494}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8220002706715221
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.66865216, 2.08968345, 5.63502633]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.6746397760035455}
episode index:3578
target Thresh 19.0
target distance 4.0
model initialize at round 3578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([4.00067566, 4.99981697, 0.74545401]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 4.472601857156714}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8220310236967346
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.98514648, 9.10762131, 2.1790834 ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9910075334097266}
episode index:3579
target Thresh 19.0
target distance 8.0
model initialize at round 3579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([1.12385536, 3.7674421 , 3.65565515]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.693154260252005}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8220208692601734
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.26419616, 11.50015662,  1.39017269]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5656467563977848}
episode index:3580
target Thresh 19.0
target distance 13.0
model initialize at round 3580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 3.00199561, 11.02057254,  2.48409545]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 13.34436123091507}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8220042040395888
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.28908387,  7.69281931,  6.218613  ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.42181686183095546}
episode index:3581
target Thresh 19.0
target distance 8.0
model initialize at round 3581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.        ,  9.        ,  1.61130684]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.24621125124758}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.822017253064918
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.86988228, 10.34532403,  4.47856561]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.6674812756652823}
episode index:3582
target Thresh 19.0
target distance 12.0
model initialize at round 3582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([16.        , 11.        ,  0.96443528]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.822000598155982
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.54715661, 7.97119383, 4.69895282]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.071580420498879}
episode index:3583
target Thresh 19.0
target distance 12.0
model initialize at round 3583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([15.99999992,  1.99999989,  5.0960784 ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 12.649110599284583}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8219616902309973
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.53689234, 5.91738495, 5.69785471]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.5432114096925883}
episode index:3584
target Thresh 19.0
target distance 8.0
model initialize at round 3584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.        ,  3.        ,  6.23372984]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8219582777283446
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.52198106, 10.67448127,  4.25143269]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5783291013498663}
episode index:3585
target Thresh 19.0
target distance 2.0
model initialize at round 3585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.28014874, 9.90737946, 1.62661284]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.1582414794326765}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8220079268421961
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.28014874, 9.90737946, 1.62661284]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.1582414794326765}
episode index:3586
target Thresh 19.0
target distance 2.0
model initialize at round 3586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 2.99277433, 10.00529822,  3.51889813]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.0053112417336942}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8220547604282451
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.65317737, 8.35954089, 5.51889813]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.49955538928920984}
episode index:3587
target Thresh 19.0
target distance 6.0
model initialize at round 3587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([2.32651687, 2.17844415, 4.04536366]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 6.057314445478521}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.822075185036336
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.36484558, 7.2683149 , 0.91262244]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8176034370160923}
episode index:3588
target Thresh 19.0
target distance 5.0
model initialize at round 3588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([3.63754296, 9.38821402, 1.24277371]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.728802941892385}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.822108454739419
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.2461547 , 10.09051796,  0.67640309]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.181287568698286}
episode index:3589
target Thresh 19.0
target distance 1.0
model initialize at round 3589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.98580564,  1.99615343,  4.41622949]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.0039469161616301}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8221524635263997
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.64250681,  3.33659325,  2.13304418]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.725334417921144}
episode index:3590
target Thresh 19.0
target distance 2.0
model initialize at round 3590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([16.00192202,  6.99772858,  0.14147394]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 2.001923311392218}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8221964478027778
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.37961702,  7.12478578,  4.14147394]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.3996005162022898}
episode index:3591
target Thresh 19.0
target distance 6.0
model initialize at round 3591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 8.       , 11.       ,  1.2498781]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 7.211102551495037}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8222094069801068
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.8387962 , 7.24867404, 4.11713688]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.2963535776838445}
episode index:3592
target Thresh 19.0
target distance 13.0
model initialize at round 3592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 4.14736984, 11.87197766,  1.85393732]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 13.227432228064066}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8221970529451553
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.6505567 ,  6.85387908,  1.58845487]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9226159061659707}
episode index:3593
target Thresh 19.0
target distance 3.0
model initialize at round 3593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.59713159, 10.47087583,  0.69008368]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.503503610056378}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.822238261055076
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.96940215,  8.94375751,  0.40689837]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9442533943460666}
episode index:3594
target Thresh 19.0
target distance 8.0
model initialize at round 3594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.        ,  3.        ,  5.51751852]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.822241675658802
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.93313535, 10.74031056,  3.81840668]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.26815944208949954}
episode index:3595
target Thresh 19.0
target distance 4.0
model initialize at round 3595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.00003738, 2.9999061 , 6.10121489]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 4.123187658337402}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.822272216168326
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.21884815, 6.89347261, 1.25165896]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.24339802340802016}
episode index:3596
target Thresh 19.0
target distance 13.0
model initialize at round 3596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([15.        ,  5.        ,  1.10695237]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 13.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8222372261771893
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.93415451, 5.8735388 , 3.99191399]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8760169284350394}
episode index:3597
target Thresh 19.0
target distance 2.0
model initialize at round 3597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4.19772345, 4.18457814, 1.76102703]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.850569225405682}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8222756805362285
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.60079948, 5.98526948, 3.47784173]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.39947220839847025}
episode index:3598
target Thresh 19.0
target distance 9.0
model initialize at round 3598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.        ,  2.        ,  5.28448391]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.295630140987}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8222699441953012
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.62376034, 10.19083238,  5.30218676]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0216796032609912}
episode index:3599
target Thresh 19.0
target distance 14.0
model initialize at round 3599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([16.        , 10.        ,  1.44806069]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8222554367513039
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.46054518, 9.39336522, 3.18257823]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.6056715775243522}
episode index:3600
target Thresh 19.0
target distance 6.0
model initialize at round 3600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 7.0591109 , 10.92631725,  0.11530608]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 7.18314815500684}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8222732455363538
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.65798621, 5.43116065, 5.26575015]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7866672478324296}
episode index:3601
target Thresh 19.0
target distance 11.0
model initialize at round 3601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([3.        , 7.        , 4.72776484]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.045361017187291}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8222462347341387
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.55079567,  5.89046831,  5.89591177]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.5615808607829526}
episode index:3602
target Thresh 19.0
target distance 12.0
model initialize at round 3602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 3.00175303, 11.0067956 ,  2.32833561]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 13.000997371672524}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8222296087223744
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.32837105,  6.91835517,  6.06285316]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9752967564580296}
episode index:3603
target Thresh 19.0
target distance 10.0
model initialize at round 3603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 2.99999988, 10.99999974,  5.29649758]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.000000116756757}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8222353561099054
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.91286635, 11.5170836 ,  1.59738574]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.5243736474357903}
episode index:3604
target Thresh 19.0
target distance 4.0
model initialize at round 3604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 4.57654771, 11.50498636,  3.81065357]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 5.189749535880837}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8222606770229077
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.90160274, 7.67748535, 2.96109765]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.1277738669857555}
episode index:3605
target Thresh 19.0
target distance 1.0
model initialize at round 3605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.46787354, 2.52868807, 6.03027439]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7108400045272514}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8223099669072608
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.46787354, 2.52868807, 6.03027439]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7108400045272514}
episode index:3606
target Thresh 19.0
target distance 3.0
model initialize at round 3606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.57353976,  5.40385597,  0.64785355]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.441391382154608}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.822350995194783
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.15831424,  3.98591085,  0.36466824]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9985407339427591}
episode index:3607
target Thresh 19.0
target distance 12.0
model initialize at round 3607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.        ,  6.        ,  5.98937392]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.369316876853166}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8223122489641334
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.68049511, 3.31266533, 0.30796493]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7488879771812728}
episode index:3608
target Thresh 19.0
target distance 13.0
model initialize at round 3608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.97483162,  9.66530233,  5.06741023]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.979508121706417}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8223065183851849
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.44341739, 9.5747534 , 5.08511308]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6143725649432601}
episode index:3609
target Thresh 19.0
target distance 2.0
model initialize at round 3609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([4.00244168, 7.00114183, 1.44742697]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 2.8309611989363046}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8223421647928068
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.2225475 , 4.53337272, 5.16424166]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.5169800828052395}
episode index:3610
target Thresh 19.0
target distance 3.0
model initialize at round 3610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.99995923,  7.99992265,  5.23729944]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.162338147496001}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8223777914571954
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.30460275, 11.12941296,  2.67092883]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.3309539989175551}
episode index:3611
target Thresh 19.0
target distance 6.0
model initialize at round 3611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.65249802,  6.55133137,  2.18265401]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.648270019342939}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8224004947472152
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.41773129, 11.10367447,  3.33309809]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.591426445630375}
episode index:3612
target Thresh 19.0
target distance 1.0
model initialize at round 3612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.94622579,  5.99945143,  4.16179371]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.001992570714496}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8224441425482816
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.54446828,  6.69745073,  1.8786084 ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.6228818216825762}
episode index:3613
target Thresh 19.0
target distance 5.0
model initialize at round 3613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.00017962, 7.00020525, 1.86191624]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.000205257143811}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8224718958830574
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.33102348, 2.24602305, 5.29554563]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7127811229995427}
episode index:3614
target Thresh 19.0
target distance 2.0
model initialize at round 3614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([2.0000155 , 8.99850757, 5.7327733 ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.237396019434934}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8225154997846112
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.29969859, 10.60638453,  3.449588  ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8033400276347643}
episode index:3615
target Thresh 19.0
target distance 3.0
model initialize at round 3615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([15.79811953,  7.85874835,  4.76210356]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 4.2067724952284715}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8225483993005449
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.20827767, 10.12909654,  4.19573294]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.895462130377592}
episode index:3616
target Thresh 19.0
target distance 3.0
model initialize at round 3616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.86765757,  5.38247519,  1.42518824]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.7575833259350717}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8225865656291872
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.6477849 ,  8.03445864,  3.14200293]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.3538966992625886}
episode index:3617
target Thresh 19.0
target distance 3.0
model initialize at round 3617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.86526939,  6.96739715,  4.88990426]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.271181430987773}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8226273927254755
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.15921533,  5.7284576 ,  4.60671895]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.1124609370115905}
episode index:3618
target Thresh 19.0
target distance 4.0
model initialize at round 3618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.08324081, 8.89143289, 4.26946855]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 3.8923230804565807}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.822665516134504
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.92141305, 5.91628982, 5.98628324]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.2994572071548776}
episode index:3619
target Thresh 19.0
target distance 3.0
model initialize at round 3619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([15.02477214,  6.99359794,  0.75709551]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 3.1762574370281773}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8227036184808756
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.86678411, 10.15456192,  2.4739102 ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.880456749292463}
episode index:3620
target Thresh 19.0
target distance 2.0
model initialize at round 3620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([14.37022348,  9.26404495,  1.62953299]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.7896363593330236}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8227390469347334
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.21976822,  6.91861011,  5.34634768]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7844653843574708}
episode index:3621
target Thresh 19.0
target distance 8.0
model initialize at round 3621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 3.        , 11.        ,  2.82861257]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 8.062257748528301}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8227517489683707
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.43085357, 2.80733141, 5.69587134]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.4719703194266305}
episode index:3622
target Thresh 19.0
target distance 1.0
model initialize at round 3622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.64054581, 10.27034362,  0.15960091]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8133915072709904}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8228006720296547
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.64054581, 10.27034362,  0.15960091]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8133915072709904}
episode index:3623
target Thresh 19.0
target distance 1.0
model initialize at round 3623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.02859388, 11.1784615 ,  2.4219223 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9876630760855026}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8228495680914566
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.02859388, 11.1784615 ,  2.4219223 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9876630760855026}
episode index:3624
target Thresh 19.0
target distance 11.0
model initialize at round 3624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        , 11.        ,  1.63242405]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8228328765455961
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.4954296 , 3.67582377, 5.3669416 ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8379667386113945}
episode index:3625
target Thresh 19.0
target distance 6.0
model initialize at round 3625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([13.59005533,  3.63227861,  2.82685584]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 4.988469051456381}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8228578860246193
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.64532432,  7.95967945,  1.97729992]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.3569601998005028}
episode index:3626
target Thresh 19.0
target distance 11.0
model initialize at round 3626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([15.        ,  7.        ,  1.15335387]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 11.180339887498945}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.822854265948026
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.77443716, 9.14789723, 5.45424202]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7884329439640301}
episode index:3627
target Thresh 19.0
target distance 13.0
model initialize at round 3627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.48919207, 11.66474855,  3.70034409]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.609174046280636}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8228484159820919
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.13625274, 10.86275312,  3.71804694]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8734459115458414}
episode index:3628
target Thresh 19.0
target distance 2.0
model initialize at round 3628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([13.99195351, 10.02478255,  2.89474219]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.25442453256385}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8228917479148607
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.50542537,  9.34338397,  0.61155688]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.6020935311812478}
episode index:3629
target Thresh 19.0
target distance 13.0
model initialize at round 3629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.57109002, 10.11084064,  4.70822215]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.981963689255682}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8228793318298703
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.70266273, 7.26492198, 4.44273969]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.39823750173509964}
episode index:3630
target Thresh 19.0
target distance 1.0
model initialize at round 3630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.99630056,  6.00097097,  3.89491761]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.4175173519867352}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8229253579020734
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.27329355,  4.41144241,  5.89491761]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.935148279290694}
episode index:3631
target Thresh 19.0
target distance 10.0
model initialize at round 3631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 3.93840678, 10.68184255,  2.61740243]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.08466992302766}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8229285485418169
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.77323942, 10.58639295,  0.91829059]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6287106249845474}
episode index:3632
target Thresh 19.0
target distance 13.0
model initialize at round 3632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([15.        ,  9.        ,  0.83496683]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.341664064126332}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8229118720116229
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.04273657, 6.75960256, 4.56948438]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.7608038246450753}
episode index:3633
target Thresh 19.0
target distance 13.0
model initialize at round 3633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([16.5160112 ,  4.49383728,  2.24810454]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 13.746396265367975}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8228770622554419
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.5694246 , 6.82168563, 5.13306616]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5966912053639333}
episode index:3634
target Thresh 19.0
target distance 9.0
model initialize at round 3634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.        ,  7.        ,  1.07892626]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.848857803495042}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8228825825115983
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.62616231, 11.63163147,  3.66299973]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8894029165853087}
episode index:3635
target Thresh 19.0
target distance 10.0
model initialize at round 3635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 4.56632624, 10.5848226 ,  2.23759435]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.78138582084229}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8228723502669051
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.15982346,  7.79230418,  6.2552972 ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.26207077839027226}
episode index:3636
target Thresh 19.0
target distance 9.0
model initialize at round 3636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([15.        ,  6.        ,  5.00363898]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 10.295630140579286}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8228687361668101
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.83627228, 11.61102654,  3.02134183]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0357146084045115}
episode index:3637
target Thresh 19.0
target distance 1.0
model initialize at round 3637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.91529791, 11.10341708,  3.3576417 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.1336769864398339}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8229174253542298
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.91529791, 11.10341708,  3.3576417 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.1336769864398339}
episode index:3638
target Thresh 19.0
target distance 4.0
model initialize at round 3638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([16.00115454,  8.00778589,  2.43358332]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.479616856173252}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8229526198099996
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.78897374,  4.79580545,  6.15039801]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.1206185275101945}
episode index:3639
target Thresh 19.0
target distance 6.0
model initialize at round 3639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([2.99992208, 8.99985383, 5.23263884]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.324675460249713}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8229749905394498
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.17818431, 10.41459548,  0.09989761]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0089992488393238}
episode index:3640
target Thresh 19.0
target distance 12.0
model initialize at round 3640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 2.        , 11.        ,  2.78265768]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8229583378956178
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.25979874,  7.99030986,  0.23398992]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.0238207871976992}
episode index:3641
target Thresh 19.0
target distance 2.0
model initialize at round 3641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.03870498,  7.01416249,  1.3607766 ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.0145343456626335}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.823001484974724
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.47045911,  5.52967444,  5.3607766 ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.7489783422846714}
episode index:3642
target Thresh 19.0
target distance 12.0
model initialize at round 3642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.62500934,  9.97042983,  3.5370034 ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.73013197203505}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8229869479615015
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.27726782, 4.86159986, 5.27152094]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.3098903715224323}
episode index:3643
target Thresh 19.0
target distance 8.0
model initialize at round 3643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 1.99999952, 11.00000023,  3.70843911]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 8.000000228670682}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8230019134047061
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.4204906 , 3.94039773, 4.57569788]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.1046171465479435}
episode index:3644
target Thresh 19.0
target distance 14.0
model initialize at round 3644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([16.        ,  8.        ,  0.06717366]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 14.866068747328223}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8229652733971518
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.91432061, 2.59694039, 4.95213528]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.41206553772860005}
episode index:3645
target Thresh 19.0
target distance 11.0
model initialize at round 3645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.86009271, 6.25958723, 1.19858616]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.807793833543846}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8229528916324788
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.12760302, 10.75120815,  0.9331037 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.1512559077050308}
episode index:3646
target Thresh 19.0
target distance 2.0
model initialize at round 3646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([16.11329325,  4.7603446 ,  0.17094963]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.348163852405806}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8229959810507315
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.8237665 ,  4.79463746,  4.17094963]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8139452929273179}
episode index:3647
target Thresh 19.0
target distance 12.0
model initialize at round 3647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 4.        , 10.        ,  4.09961319]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8229731478249467
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.4102612 ,  4.32718351,  1.26776012]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.5247506993310924}
episode index:3648
target Thresh 19.0
target distance 12.0
model initialize at round 3648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.34516564,  2.70938501,  4.32543564]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 12.107612852387637}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8229423394060504
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.77518091, 9.82956473, 3.21039726]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.135377948771666}
episode index:3649
target Thresh 19.0
target distance 7.0
model initialize at round 3649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.70759941, 7.47301261, 6.15632677]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.4808179253651055}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8229671544493593
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.12030374, 2.75522382, 5.30677085]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.1594086978665552}
episode index:3650
target Thresh 19.0
target distance 5.0
model initialize at round 3650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([16.00001063, 10.00077946,  2.56716019]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 5.099785921247768}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8229944832743328
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.51177935,  5.19584025,  6.00078957]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5479703487874658}
episode index:3651
target Thresh 19.0
target distance 2.0
model initialize at round 3651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.38093476,  5.08995971,  4.09608698]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.9476563333388748}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8230348185746411
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.6138396 ,  6.46635504,  3.81290167]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.6587084284425204}
episode index:3652
target Thresh 19.0
target distance 2.0
model initialize at round 3652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.41828508,  3.24125001,  4.00023484]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.3098335735723465}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8230805248931259
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.81745299,  1.84175837,  6.00023484]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.832628250710134}
episode index:3653
target Thresh 19.0
target distance 11.0
model initialize at round 3653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.        ,  3.        ,  4.67510319]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 13.038404808640527}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8230556816650912
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.3134044 , 10.67178384,  3.84325012]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7412933636834581}
episode index:3654
target Thresh 19.0
target distance 7.0
model initialize at round 3654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 9.0000005 , 10.99999938,  0.12054842]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 9.219544435439861}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8230588165706412
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.74796757, 5.24985868, 4.70462189]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.35489393293241944}
episode index:3655
target Thresh 19.0
target distance 14.0
model initialize at round 3655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([2.81939386, 9.52997218, 6.230896  ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 13.93732493228286}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8230380598085408
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.18542026,  5.58927066,  1.39904292]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6177544685339339}
episode index:3656
target Thresh 19.0
target distance 11.0
model initialize at round 3656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.64360245,  5.64480056,  2.79417896]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.500175445520295}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8230173143982381
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.77198084, 9.78954272, 4.2455112 ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.31029825528642413}
episode index:3657
target Thresh 19.0
target distance 14.0
model initialize at round 3657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([16.        ,  3.        ,  0.53370589]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8229752024365573
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.97278245, 4.8090307 , 5.1354822 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.2652416237995092}
episode index:3658
target Thresh 19.0
target distance 8.0
model initialize at round 3658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4.68160606, 4.05207366, 1.04095667]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 9.391260960982148}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8229628619492555
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.51152755, 11.29946973,  0.77547421]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5927415603454594}
episode index:3659
target Thresh 19.0
target distance 6.0
model initialize at round 3659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 3.00661942, 11.0004374 ,  1.07598275]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 6.00044105350075}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8229876035846473
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.50535942, 5.85315573, 0.22642682]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9915961094063426}
episode index:3660
target Thresh 19.0
target distance 6.0
model initialize at round 3660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([1.4404444 , 6.4136778 , 5.38197398]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.1021443145718255}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8230098364367162
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.68987262, 2.05474757, 0.24923263]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.6920415679496833}
episode index:3661
target Thresh 19.0
target distance 6.0
model initialize at round 3661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 2.00014737, 11.00022015,  1.99088496]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 6.082955455134582}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8230295875065747
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.36970211, 5.03855269, 5.14132903]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.6314758404467974}
episode index:3662
target Thresh 19.0
target distance 11.0
model initialize at round 3662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.        ,  8.        ,  1.62072008]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.000000000003725}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8230259561335784
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.91130338, 8.02849083, 5.92160824]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9117486386475675}
episode index:3663
target Thresh 19.0
target distance 8.0
model initialize at round 3663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([10.00000007, 10.9999998 ,  6.08302689]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 8.246211275667834}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.823036063229241
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.35673681, 8.66471135, 4.66710035]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.48957086591980586}
episode index:3664
target Thresh 19.0
target distance 4.0
model initialize at round 3664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 9.99262145, 10.98924669,  5.12100863]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 5.654482467136253}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8230582585939832
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.40545978,  7.36368939,  6.27145271]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.5446720142746867}
episode index:3665
target Thresh 19.0
target distance 7.0
model initialize at round 3665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 2.99999995, 11.00000004,  3.49544215]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 7.00000003692074}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8230779749048572
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.53961481, 4.68699433, 0.36270092]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8735819061713567}
episode index:3666
target Thresh 19.0
target distance 11.0
model initialize at round 3666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.        , 5.        , 4.26480937]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.045361017205778}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8230323193950863
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.3815811 ,  4.69492686,  0.30021507]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7927972511016169}
episode index:3667
target Thresh 19.0
target distance 9.0
model initialize at round 3667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 6.38451235, 11.46396169,  3.87185228]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 10.430269383869627}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8230309220718044
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.4429631 , 1.74007623, 6.17274044]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5135919346618236}
episode index:3668
target Thresh 19.0
target distance 9.0
model initialize at round 3668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([11.32750925, 11.18750473,  4.03994775]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 7.990780824226647}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8230386699183628
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.76203712, 8.53221645, 4.62402121]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.5829928696639163}
episode index:3669
target Thresh 19.0
target distance 2.0
model initialize at round 3669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.2497679 , 6.43432871, 6.13818741]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.8393739166602026}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8230841634687938
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.30726027, 7.62604155, 1.8550021 ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9337110640375691}
episode index:3670
target Thresh 19.0
target distance 12.0
model initialize at round 3670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([3.3037005 , 9.53217332, 3.00734651]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.70744779433495}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8230739738685046
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.84131536,  9.46788756,  0.74186405]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.49406435043781705}
episode index:3671
target Thresh 19.0
target distance 8.0
model initialize at round 3671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([5.31713264, 9.95253592, 0.33815544]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 6.764458379145578}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8230888014962635
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.28610436, 11.27888807,  1.20541422]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.3995425647874985}
episode index:3672
target Thresh 19.0
target distance 9.0
model initialize at round 3672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.        , 2.        , 0.70819157]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 9.219544457292912}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8230965251471207
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.70865383, 10.1003125 ,  1.29226503]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.1452632260566482}
episode index:3673
target Thresh 19.0
target distance 5.0
model initialize at round 3673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.00012659,  9.0000228 ,  1.18823593]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.3852738700930685}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8231236476755042
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.28101963, 10.3664098 ,  4.62186532]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6931151250953139}
episode index:3674
target Thresh 19.0
target distance 9.0
model initialize at round 3674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.        ,  8.        ,  0.13553923]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8231267470261912
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.82804031, 10.62706361,  4.71961269]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.4106722307311606}
episode index:3675
target Thresh 19.0
target distance 8.0
model initialize at round 3675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([11.00000001, 11.00000012,  2.46849951]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 8.544003857051612}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8231321377896182
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.20632241,  3.19044391,  5.05257298]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8162064677103539}
episode index:3676
target Thresh 19.0
target distance 3.0
model initialize at round 3676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([15.54999412,  8.55167014,  2.24016   ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.121353608146086}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8231721614127377
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.77141709,  9.38905247,  1.9569747 ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.9840433006608175}
episode index:3677
target Thresh 19.0
target distance 1.0
model initialize at round 3677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 8.97638621, 11.02432471,  3.35136569]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0239027716935947}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8232148280355184
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.52749687, 11.09423747,  1.06818038]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5358485306740634}
episode index:3678
target Thresh 19.0
target distance 12.0
model initialize at round 3678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.99997627,  8.99980636,  5.60045409]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.649026897549277}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8231962100575809
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.08523408, 5.4257891 , 5.05178633]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.4342363455822082}
episode index:3679
target Thresh 19.0
target distance 3.0
model initialize at round 3679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.99976174, 6.0003892 , 3.13013327]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 3.16257155947389}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8232335469597392
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.1895585 , 3.71263677, 4.84694796]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0791971934566116}
episode index:3680
target Thresh 19.0
target distance 6.0
model initialize at round 3680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.98194873,  5.34013371,  3.23045373]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 4.659901256891982}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.82325807390908
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.90338327, 10.05206919,  2.38089781]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.10975424395923702}
episode index:3681
target Thresh 19.0
target distance 2.0
model initialize at round 3681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([13.97826531,  4.99166607,  4.51774144]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.837982536821859}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8233033596032926
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.1799881 ,  3.81363287,  0.23455613]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.1551701040979285}
episode index:3682
target Thresh 19.0
target distance 1.0
model initialize at round 3682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.02980898,  8.51417795,  5.74244857]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.4867356961299328}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8233513358836066
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.02980898,  8.51417795,  5.74244857]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.4867356961299328}
episode index:3683
target Thresh 19.0
target distance 5.0
model initialize at round 3683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.60808466, 6.63669758, 2.81582588]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.386059774069283}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8233834012510108
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.23906521, 9.8516727 , 2.24945527]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.28134171756183335}
episode index:3684
target Thresh 19.0
target distance 13.0
model initialize at round 3684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 3.        , 10.        ,  3.20506084]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8233731691586245
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.48594301, 10.43711548,  0.93957838]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6747773935719839}
episode index:3685
target Thresh 19.0
target distance 13.0
model initialize at round 3685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([2.10359752, 5.67973825, 2.51919955]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 13.41110251185173}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.823310660552342
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.02872415,  2.27959078,  5.98823464]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.2810624193599653}
episode index:3686
target Thresh 19.0
target distance 3.0
model initialize at round 3686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([16.02323938, 10.03460408,  1.98940056]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.1736360225420754}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.823347895526426
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.90014698, 11.03226358,  3.70621525]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.10493600500531441}
episode index:3687
target Thresh 19.0
target distance 12.0
model initialize at round 3687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.85068562,  6.77058366,  4.97065234]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.48715280385877}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8233153846260662
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.53709656, 2.97626547, 5.85561396]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5376207211538396}
episode index:3688
target Thresh 19.0
target distance 1.0
model initialize at round 3688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 8.0047944 , 11.12613218,  2.5428037 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0126801631583422}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8233605688536005
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.51641079, 10.51221666,  4.5428037 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.6868705172802045}
episode index:3689
target Thresh 19.0
target distance 13.0
model initialize at round 3689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.36329385, 10.39189419,  3.9165765 ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.37955358386276}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8233546799703174
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.02735964, 10.7162152 ,  3.93427935]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.2851006183427781}
episode index:3690
target Thresh 19.0
target distance 6.0
model initialize at round 3690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.22829599, 6.73724384, 5.00805593]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 4.7996881491794365}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8233841328741204
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.86032686, 2.99812422, 0.1585    ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.317730725870679}
episode index:3691
target Thresh 19.0
target distance 11.0
model initialize at round 3691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([14.        ,  6.        ,  0.18398684]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.180339888199649}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8233594631116015
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.61206402, 4.43434358, 5.63531907]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7505176286304707}
episode index:3692
target Thresh 19.0
target distance 2.0
model initialize at round 3692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([4.02008893, 3.98995032, 0.54614466]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 2.835606027531466}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.823394023248831
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.56969237, 2.66698918, 4.26295935]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7937500975876632}
episode index:3693
target Thresh 19.0
target distance 11.0
model initialize at round 3693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([2.00004068, 9.99945495, 5.79688191]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 11.04536986662521}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8233947743588979
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.12922008, 10.52158   ,  6.09777007]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9935508866230341}
episode index:3694
target Thresh 19.0
target distance 2.0
model initialize at round 3694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.73699672,  6.74659624,  0.67884606]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.8957221839113743}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8234371844334962
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.40989199,  5.98202905,  4.67884606]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.1456912828685477}
episode index:3695
target Thresh 19.0
target distance 12.0
model initialize at round 3695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 3.57448755, 11.59445526,  1.37100857]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 12.898335573540098}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8234125270160777
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.62880657,  4.91277088,  0.5391555 ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.1083990176639114}
episode index:3696
target Thresh 19.0
target distance 6.0
model initialize at round 3696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([2.        , 5.        , 4.56563544]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.211102551802612}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8234319821762731
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.7108971 , 10.07576387,  1.43289421]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9683970822339949}
episode index:3697
target Thresh 19.0
target distance 2.0
model initialize at round 3697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([4.01159152, 4.00058638, 1.06054419]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 2.0115916057395014}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8234743477841217
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.55529387, 3.62530037, 5.06054419]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6698888663840208}
episode index:3698
target Thresh 19.0
target distance 12.0
model initialize at round 3698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([2.        , 6.        , 5.15467548]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 12.369316876852983}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8234537199783762
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.35506769,  9.48952793,  0.32282241]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8096760322912717}
episode index:3699
target Thresh 19.0
target distance 13.0
model initialize at round 3699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.50367132,  9.60631298,  2.88047689]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 13.325161005311273}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8234331033227961
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.66166539, 5.21427393, 4.33180913]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.40047924126842593}
episode index:3700
target Thresh 19.0
target distance 6.0
model initialize at round 3700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.99999971, 3.9999997 , 4.95251513]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.082762776652397}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8234525318964074
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.71260074, 9.56644477, 1.8197739 ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8341282599270188}
episode index:3701
target Thresh 19.0
target distance 13.0
model initialize at round 3701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([15.       ,  8.       ,  1.3973357]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8234423281170749
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.60258106, 9.35865452, 5.41503855]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8800158896021115}
episode index:3702
target Thresh 19.0
target distance 12.0
model initialize at round 3702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.       , 11.       ,  2.3820402]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 13.892443989449806}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8234217312405463
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.85414826, 4.61199852, 3.83337244]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6291382394462915}
episode index:3703
target Thresh 19.0
target distance 13.0
model initialize at round 3703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.16761489, 10.46271042,  3.09816289]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.17640969400125}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8234158481029379
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.75743631, 10.77247724,  3.11586574]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0818645231801642}
episode index:3704
target Thresh 19.0
target distance 11.0
model initialize at round 3704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([3.        , 3.        , 4.68255639]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.8233602136340686
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.35751667,  5.53622745,  0.15159148]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6444827712991686}
episode index:3705
target Thresh 19.0
target distance 12.0
model initialize at round 3705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.        , 2.        , 0.13819712]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.165525060573309}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.8232964458435407
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.37473573,  3.81858353,  5.60723221]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6510509535417665}
episode index:3706
target Thresh 19.0
target distance 2.0
model initialize at round 3706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.99239382, 8.98871924, 5.12914705]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.2427875064685074}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8233387451567742
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.5768807 , 10.60439555,  2.84596175]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.6994956977230227}
episode index:3707
target Thresh 19.0
target distance 4.0
model initialize at round 3707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.54536626, 6.84382168, 0.7310912 ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 4.18096999203998}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8233757616791159
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.57758103, 10.09744385,  2.44790589]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9965166325475765}
episode index:3708
target Thresh 19.0
target distance 10.0
model initialize at round 3708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.        , 5.        , 4.98068094]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.440306508912286}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8233552160691543
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.05940559,  8.64056457,  0.14882787]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.6433132939089861}
episode index:3709
target Thresh 19.0
target distance 13.0
model initialize at round 3709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([15.        ,  8.        ,  5.28447652]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.152946437963966}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8233367158187862
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.25564345, 6.19173376, 4.73580875]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.31955501470529857}
episode index:3710
target Thresh 19.0
target distance 11.0
model initialize at round 3710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 3.        , 11.        ,  3.64273202]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 11.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8233352526610871
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.17896483, 10.31735637,  5.94362018]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0677551569691424}
episode index:3711
target Thresh 19.0
target distance 13.0
model initialize at round 3711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([15.        ,  8.        ,  1.59538668]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8233188214815844
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.03560081, 6.57173964, 5.32990422]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.4297375417253828}
episode index:3712
target Thresh 19.0
target distance 3.0
model initialize at round 3712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([1.99950301, 1.99900727, 5.25823593]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 3.163376617506805}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8233532064070944
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.62504739, 5.0059941 , 2.69186532]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.3750005163272678}
episode index:3713
target Thresh 19.0
target distance 11.0
model initialize at round 3713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([15.91465181,  3.40562673,  2.00392393]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 13.617807700334351}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8233326945298528
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.86764612, 10.97862122,  3.45525616]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.3078643981962519}
episode index:3714
target Thresh 19.0
target distance 12.0
model initialize at round 3714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([1.99913671, 9.99791444, 5.32992506]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 13.416247515831914}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8233004240050803
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.04873479,  3.62567446,  6.21488668]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.37748468545277003}
episode index:3715
target Thresh 19.0
target distance 2.0
model initialize at round 3715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.07678272, 9.70766621, 0.13386792]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.1632375541716362}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8233479750212253
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.07678272, 9.70766621, 0.13386792]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.1632375541716362}
episode index:3716
target Thresh 19.0
target distance 2.0
model initialize at round 3716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([1.50816548, 7.44657867, 2.90853345]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.5911773650982262}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8233875098678701
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.79993504, 8.46323219, 2.62534815]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5045889935689977}
episode index:3717
target Thresh 19.0
target distance 2.0
model initialize at round 3717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([4.00370543, 1.99663943, 0.27336806]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 2.240885509142207}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.823429659542462
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.61221624, 2.64064399, 4.27336806]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7098911673980727}
episode index:3718
target Thresh 19.0
target distance 11.0
model initialize at round 3718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([15.        , 11.        ,  1.10341948]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 12.529964086141668}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8234132339051413
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.47758548, 5.43812586, 4.83793702]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6818146425590381}
episode index:3719
target Thresh 19.0
target distance 13.0
model initialize at round 3719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([3.82203124, 9.46855464, 2.07049431]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.273884799572654}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8234030900629107
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.2963846 , 10.16220569,  6.08819716]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.09406303870008}
episode index:3720
target Thresh 19.0
target distance 12.0
model initialize at round 3720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([2.99999912, 2.99998416, 5.66692233]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.000000879238833}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.8233493739395106
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.14475706,  3.2351232 ,  5.41914272]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8869743028413618}
episode index:3721
target Thresh 19.0
target distance 2.0
model initialize at round 3721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.99812329, 4.99683599, 5.18702722]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.2397372623762353}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8233914885623104
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.54537601, 6.47174256, 2.90384191]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6969497038619533}
episode index:3722
target Thresh 19.0
target distance 14.0
model initialize at round 3722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.36507585,  5.39926323,  3.9120717 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.443996081466778}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8233536596894788
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.43920455, 4.69944316, 4.51384802]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8965000114987374}
episode index:3723
target Thresh 19.0
target distance 5.0
model initialize at round 3723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.93549457,  9.70012008,  5.03621387]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.144611600413984}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8233853800143208
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.77579789, 10.53542516,  4.46984326]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9042633179956368}
episode index:3724
target Thresh 19.0
target distance 8.0
model initialize at round 3724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([16.        ,  3.        ,  5.69813585]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.630145812734648}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8233817135682018
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.83583014, 10.98491347,  3.7158387 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.16486159465739336}
episode index:3725
target Thresh 19.0
target distance 11.0
model initialize at round 3725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.        , 4.        , 4.30778456]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.045361018891944}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8233366994799051
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.29182752,  4.92822656,  0.34319026]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.30052408693764243}
episode index:3726
target Thresh 19.0
target distance 6.0
model initialize at round 3726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.20214926,  6.16797251,  1.78097933]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.979322499067143}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8233584449522766
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.83824725, 11.30903699,  2.93142341]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.34880913658188717}
episode index:3727
target Thresh 19.0
target distance 9.0
model initialize at round 3727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 8.44756364, 10.30072191,  0.48929137]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 7.663626587126471}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.823365982325169
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.28800584,  8.98780821,  1.07336483]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.2882637765963645}
episode index:3728
target Thresh 19.0
target distance 14.0
model initialize at round 3728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.3281686 ,  5.19330765,  4.03647757]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.902505479480329}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8233455495313923
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.98282426, 8.5782351 , 5.4878098 ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0694994890504232}
episode index:3729
target Thresh 19.0
target distance 5.0
model initialize at round 3729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.99999912,  5.99999913,  4.93525767]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 5.385165293232284}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8233697242493418
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.6977979 , 10.31429622,  4.08570175]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7493435680997634}
episode index:3730
target Thresh 19.0
target distance 1.0
model initialize at round 3730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.02445403, 9.99717975, 0.89517849]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.4296410392115142}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8234117318279403
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.85468741, 8.97462597, 4.89517849]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.14751131829081346}
episode index:3731
target Thresh 19.0
target distance 7.0
model initialize at round 3731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 8.34847019, 10.13208921,  5.95346308]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 7.830517866346256}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8234124705449841
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.18570393,  6.00079129,  6.25435123]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.1857056166064183}
episode index:3732
target Thresh 19.0
target distance 6.0
model initialize at round 3732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 9.        , 11.        ,  2.14921191]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 6.324555320866614}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8234341607685213
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.67383862, 9.99551718, 3.29965599]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.2021284983254776}
episode index:3733
target Thresh 19.0
target distance 14.0
model initialize at round 3733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([16.        ,  5.        ,  0.09072703]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 14.000000000004786}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8233982783167538
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.92982581, 5.95497265, 4.97568865]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.3328723844895105}
episode index:3734
target Thresh 19.0
target distance 4.0
model initialize at round 3734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([11.4722919 , 11.8152202 ,  1.51574486]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 3.1119660386805577}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8234298932755447
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.14240089, 10.26577778,  0.94937425]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.3015225368484579}
episode index:3735
target Thresh 19.0
target distance 4.0
model initialize at round 3735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 5.46895999, 10.30127972,  4.57973099]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.565925458817228}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8234692051349464
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.94972017, 10.53950243,  4.29654568]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0554744923858637}
episode index:3736
target Thresh 19.0
target distance 11.0
model initialize at round 3736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([15.        ,  6.        ,  4.95478129]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 11.70469991034167}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8234569900303877
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.17016397, 9.5566617 , 4.68929884]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.4748732701072812}
episode index:3737
target Thresh 19.0
target distance 13.0
model initialize at round 3737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.50261828, 8.24202481, 0.54280966]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.082532634742115}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8234192954356794
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.34432559,  2.8914924 ,  1.14458598]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9556771510719708}
episode index:3738
target Thresh 19.0
target distance 9.0
model initialize at round 3738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([15.40595228,  8.63245898,  2.01016231]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.699339604484509}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8234245171254222
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.65830475, 10.71158424,  4.59423577]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.4471457195090033}
episode index:3739
target Thresh 19.0
target distance 14.0
model initialize at round 3739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([2.35817897, 6.64441334, 2.36633047]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 14.410754878786475}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.8233727521826623
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.34428261,  2.498092  ,  0.11855087]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.6054966200541365}
episode index:3740
target Thresh 19.0
target distance 12.0
model initialize at round 3740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.00000001,  6.99999999,  6.12454128]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.649110641761458}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8233425945978158
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.96885593, 3.51829835, 5.0095029 ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.5192332145242408}
episode index:3741
target Thresh 19.0
target distance 12.0
model initialize at round 3741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([2.84904325, 6.45310483, 2.05200397]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 11.257740500838354}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8233202423206353
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.15335512,  8.6946618 ,  5.5033362 ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.095154136084356}
episode index:3742
target Thresh 19.0
target distance 12.0
model initialize at round 3742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([2.        , 8.        , 4.59284329]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 12.369316876852983}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8233101856544548
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.53222102, 11.35858727,  2.32736084]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5894081775826345}
episode index:3743
target Thresh 19.0
target distance 9.0
model initialize at round 3743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 7.41132226, 11.90169176,  2.2949973 ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 10.420909313058102}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8233065578987301
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.31597142,  5.98203607,  0.31270015]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.31648166109041054}
episode index:3744
target Thresh 19.0
target distance 1.0
model initialize at round 3744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.03314405, 4.12937682, 2.33000758]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.97547362654363}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8233537390581698
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.03314405, 4.12937682, 2.33000758]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.97547362654363}
episode index:3745
target Thresh 19.0
target distance 13.0
model initialize at round 3745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.01157217,  5.94810054,  4.96808934]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.368070290355385}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8233107407404777
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.22816427, 3.84439633, 5.28668035]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.87467942704895}
episode index:3746
target Thresh 19.0
target distance 11.0
model initialize at round 3746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.14396049,  7.67673048,  2.49514851]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.506523267239832}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8232768743079475
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.11859455, 1.86667047, 5.38011013]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.17844167645504003}
episode index:3747
target Thresh 19.0
target distance 1.0
model initialize at round 3747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.5499207 , 7.81717886, 0.68904465]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9849842977666753}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8233240256221662
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.5499207 , 7.81717886, 0.68904465]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9849842977666753}
episode index:3748
target Thresh 19.0
target distance 12.0
model initialize at round 3748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.66890701, 7.21709938, 1.13935822]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 10.483807375214862}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8233118883412319
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.90451528,  9.2777958 ,  0.87387576]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.2937479196224991}
episode index:3749
target Thresh 19.0
target distance 12.0
model initialize at round 3749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([3.79677183, 5.48241112, 2.08761331]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.742476438058432}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8232876194029154
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.055262  ,  9.61552797,  1.25576024]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.6180036990932255}
episode index:3750
target Thresh 19.0
target distance 14.0
model initialize at round 3750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([16.        ,  9.        ,  1.97708767]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8232713717609383
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.6000758 , 6.2843652 , 5.71160522]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.6640440779521154}
episode index:3751
target Thresh 19.0
target distance 5.0
model initialize at round 3751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([2.99999935, 5.9999994 , 4.89435482]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 5.830952744547633}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8232905793522224
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.05949717, 11.30689744,  1.76161359]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.312611504134477}
episode index:3752
target Thresh 19.0
target distance 10.0
model initialize at round 3752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.        , 9.        , 4.26003385]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.049875621132044}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8232826719992492
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.33648871,  7.96286447,  6.2777367 ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.33853168224400976}
episode index:3753
target Thresh 19.0
target distance 1.0
model initialize at round 3753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.50197199, 5.38915513, 2.92503351]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.6351516285468184}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8233297464073475
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.50197199, 5.38915513, 2.92503351]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.6351516285468184}
episode index:3754
target Thresh 19.0
target distance 9.0
model initialize at round 3754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.98206909,  8.96544345,  5.243747  ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.209613754358903}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8233395276612047
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.89951864, 11.44746948,  3.82782046]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0046704508756346}
episode index:3755
target Thresh 19.0
target distance 12.0
model initialize at round 3755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.13783905, 6.76664567, 0.18434208]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.004888792985007}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8233212581616153
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.03335609,  5.8171679 ,  5.91885962]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8178483951787202}
episode index:3756
target Thresh 19.0
target distance 6.0
model initialize at round 3756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([3.99998829, 8.99999715, 4.39014053]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.324567331567333}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8233333488069725
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.30199174, 11.52060662,  0.974214  ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6018556796782449}
episode index:3757
target Thresh 19.0
target distance 12.0
model initialize at round 3757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([2.        , 4.        , 0.20510786]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 13.892443988755014}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8233071771697056
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.08908702, 11.14505858,  1.37325479]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.17023069663733473}
episode index:3758
target Thresh 19.0
target distance 3.0
model initialize at round 3758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.56922456,  6.53561502,  0.72227877]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 2.5987227911176918}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8233462811395993
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.61144617,  4.78990509,  0.43909346]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9989076393336063}
episode index:3759
target Thresh 19.0
target distance 1.0
model initialize at round 3759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.02513805, 7.99520643, 0.82157248]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.4287560557055716}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8233879709584452
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.84222213, 7.14484535, 4.82157248]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.21418223937529202}
episode index:3760
target Thresh 19.0
target distance 6.0
model initialize at round 3760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([1.37394037, 4.43349861, 3.89105761]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 5.7991385278833345}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8234119351372606
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.38082532, 9.50368588, 3.04150169]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7935395327514733}
episode index:3761
target Thresh 19.0
target distance 2.0
model initialize at round 3761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.81133501,  5.77320842,  4.34012198]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 2.134758200069088}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8234562169195208
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.78330681,  4.40518227,  0.05693668]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.4594873304457175}
episode index:3762
target Thresh 19.0
target distance 5.0
model initialize at round 3762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([2.        , 6.        , 4.99872589]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.071067813470831}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8234682524220053
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.24341351, 11.50947212,  1.58279936]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.5646343768912424}
episode index:3763
target Thresh 19.0
target distance 13.0
model initialize at round 3763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([15.99999847,  1.9999986 ,  4.89007926]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 14.317820255585985}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8234308152122786
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.70299359, 7.71573735, 5.49185557]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.41111805787352695}
episode index:3764
target Thresh 19.0
target distance 2.0
model initialize at round 3764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([1.36098806, 7.46193767, 4.85143614]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.1962745487587623}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8234750566956218
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.87422614, 6.74629859, 0.56825083]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7568227299446314}
episode index:3765
target Thresh 19.0
target distance 5.0
model initialize at round 3765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([11.18044705, 10.12822414,  5.19837451]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 4.2703782840249565}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8235063910271951
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.9335058 , 10.58418788,  4.6320039 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0219260266943506}
episode index:3766
target Thresh 19.0
target distance 3.0
model initialize at round 3766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.50844223, 10.23179321,  4.62719703]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.6927904876094562}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8235453590678039
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.14972504, 10.9223897 ,  4.34401172]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.16864443348672747}
episode index:3767
target Thresh 19.0
target distance 12.0
model initialize at round 3767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([16.01391653,  7.9398371 ,  5.94970512]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 13.402083986239164}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8235043363500495
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.69347923, 2.9989874 , 4.26829613]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.0449549324077687}
episode index:3768
target Thresh 19.0
target distance 12.0
model initialize at round 3768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.57149171, 5.39766962, 0.64398497]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.968041939271192}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8234580636761356
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.2032135 ,  2.11283609,  4.96257598]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8047364200742956}
episode index:3769
target Thresh 19.0
target distance 10.0
model initialize at round 3769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.        ,  6.        ,  6.12454081]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8234459584495369
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.4754341 , 7.83536819, 5.85905835]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.5031314106157203}
episode index:3770
target Thresh 19.0
target distance 8.0
model initialize at round 3770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.60024043,  3.93437745,  3.5629946 ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.43120596168517}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8234533866682178
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.781413  , 10.28601902,  4.14706806]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0584777397225815}
episode index:3771
target Thresh 19.0
target distance 6.0
model initialize at round 3771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.15578472,  3.0854898 ,  1.08383244]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 5.916561480694459}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8234748417817758
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.00107491,  9.13744632,  2.23427652]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.13745052352669243}
episode index:3772
target Thresh 19.0
target distance 10.0
model initialize at round 3772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.43869744,  8.62383838,  2.31693465]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.049823930499498}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8234586392566141
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.73832465, 5.39867313, 6.05145219]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8390849540446094}
episode index:3773
target Thresh 19.0
target distance 12.0
model initialize at round 3773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.99999823,  9.99996337,  5.67400861]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 14.422183304873087}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.823428722613269
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.88706547, 1.87998497, 4.55897023]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.1647962895079073}
episode index:3774
target Thresh 19.0
target distance 8.0
model initialize at round 3774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.99918226,  3.00004327,  4.09872866]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 7.999956772666419}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8234384258270513
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.64099216, 11.07130994,  2.68280212]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.36602149391768235}
episode index:3775
target Thresh 19.0
target distance 3.0
model initialize at round 3775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.20026805, 8.32951649, 5.8417058 ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.3445152949130916}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8234773189875844
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.9498633 , 6.81066574, 5.55852049]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.19586002370211342}
episode index:3776
target Thresh 19.0
target distance 10.0
model initialize at round 3776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([3.99999986, 7.99999985, 4.97023392]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.198039133986954}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8234512408877619
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.50001076,  6.50864469,  6.13838085]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7132532424922409}
episode index:3777
target Thresh 19.0
target distance 10.0
model initialize at round 3777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([12.63444817, 10.98364442,  3.527354  ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 11.105178278846425}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8234371042823933
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.6942466 , 3.82413571, 5.26187154]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.35272281874904665}
episode index:3778
target Thresh 19.0
target distance 8.0
model initialize at round 3778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.99999958, 11.00000041,  3.37287033]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 8.06225810439994}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8234490938850623
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.34835419,  2.67983601,  6.2401291 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.4731338352133929}
episode index:3779
target Thresh 19.0
target distance 1.0
model initialize at round 3779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.02390459, 5.01272082, 1.49903553]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0239836048202495}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.823493154971336
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.07633651, 5.73689705, 3.49903553]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7408404207354722}
episode index:3780
target Thresh 19.0
target distance 2.0
model initialize at round 3780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([16.00005359,  1.99983526,  6.03687334]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 2.8285815090855446}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8235268753878737
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.59347542,  4.03419831,  3.47050273]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.40796048279105}
episode index:3781
target Thresh 19.0
target distance 6.0
model initialize at round 3781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 9.        , 11.        ,  4.84732056]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 6.00000000338076}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8235482543407084
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.21899006, 10.20651369,  5.99776464]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.1133719343773694}
episode index:3782
target Thresh 19.0
target distance 2.0
model initialize at round 3782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([13.37952159,  4.23554144,  3.04619563]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.729707674171625}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8235819423649113
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.94358529,  4.50630626,  0.47982501]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.49690655538782685}
episode index:3783
target Thresh 19.0
target distance 11.0
model initialize at round 3783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.8892004 , 5.34315388, 0.57095879]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.131688296155453}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8235637439888116
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.28928553,  9.2799211 ,  0.02229102]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.011745360450616}
episode index:3784
target Thresh 19.0
target distance 11.0
model initialize at round 3784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.05517949, 9.13315188, 5.78595853]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 13.064109297162805}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.823506000877358
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.01124536,  1.96810024,  5.25499361]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.03382384711217096}
episode index:3785
target Thresh 19.0
target distance 12.0
model initialize at round 3785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.        ,  6.        ,  0.97288388]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8234838652124111
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.91880676, 9.72055516, 4.42421611]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.2910013118391822}
episode index:3786
target Thresh 19.0
target distance 12.0
model initialize at round 3786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 4.        , 10.        ,  4.26409721]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.000000000000002}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8234759778130003
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.22482788,  9.15604573,  6.28180006]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.1459278424495216}
episode index:3787
target Thresh 19.0
target distance 12.0
model initialize at round 3787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.85753618, 2.7473697 , 1.46334046]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.16750023000649}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.8234199169000437
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.35918811,  2.31881621,  5.21556085]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.7157399350335225}
episode index:3788
target Thresh 19.0
target distance 4.0
model initialize at round 3788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.00002416, 11.00001978,  1.696181  ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.000024159260814}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8234535854492651
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.9166656 , 10.4915397 ,  5.41299569]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0482402881839603}
episode index:3789
target Thresh 19.0
target distance 5.0
model initialize at round 3789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.35594522,  4.5548574 ,  2.97350186]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.4634815643341024}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8234847270228671
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.45966923,  7.73820144,  2.40713125]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.5289936556818272}
episode index:3790
target Thresh 19.0
target distance 3.0
model initialize at round 3790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.02238417, 8.6825858 , 2.56749368]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.6405221393874496}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8235234540798381
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.73130899, 10.02805389,  2.28430837]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.27015158341311957}
episode index:3791
target Thresh 19.0
target distance 6.0
model initialize at round 3791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 9.6158301 , 10.52954488,  0.72658985]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 5.62837745288056}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8235400314579068
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.75355655,  7.48911262,  5.87703393]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.5476910844520939}
episode index:3792
target Thresh 19.0
target distance 6.0
model initialize at round 3792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.72818729, 4.38133976, 1.99314993]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.6757116241064365}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8235686434580252
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.10575034, 8.21777529, 3.42677932]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.188090047413994}
episode index:3793
target Thresh 19.0
target distance 9.0
model initialize at round 3793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([2.0012874 , 1.99956937, 0.68720644]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.84872854243506}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8235737500868934
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.32514316, 10.41253026,  3.27127991]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8947359626622833}
episode index:3794
target Thresh 19.0
target distance 6.0
model initialize at round 3794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([2.99988384, 9.99973273, 5.31240082]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.082921052419268}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8235950434531443
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.39669835, 10.64230409,  0.17965959]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7013695437884149}
episode index:3795
target Thresh 19.0
target distance 1.0
model initialize at round 3795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([14.56154419,  5.36879011,  5.11527777]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.4849785362117176}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8236388803753115
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.16174623,  5.08106002,  0.83209246]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.18092144217646544}
episode index:3796
target Thresh 19.0
target distance 6.0
model initialize at round 3796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.00000069, 5.99999911, 0.09646862]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.810249718056596}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8236462069201415
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.88086205, 10.37873186,  0.68054209]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6325882939881037}
episode index:3797
target Thresh 19.0
target distance 10.0
model initialize at round 3797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.00000019,  4.99999992,  0.63256234]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.198039202470518}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8236108732916393
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.14124416, 3.14549642, 5.51752396]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.20277850357107374}
episode index:3798
target Thresh 19.0
target distance 14.0
model initialize at round 3798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.37104715,  8.57703347,  4.40563798]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.877809438752875}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8235868386763097
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.24261504, 5.93481015, 3.57378491]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9657805509023853}
episode index:3799
target Thresh 19.0
target distance 4.0
model initialize at round 3799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.99359361, 8.00258516, 3.76804805]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 4.002590287525762}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8236228937213949
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.68953115, 4.71050235, 5.48486274]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.7753737816829621}
episode index:3800
target Thresh 19.0
target distance 6.0
model initialize at round 3800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([13.09957257,  8.62028609,  5.13010502]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.455228947282165}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8236441405462535
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.29413405,  3.84450904,  6.2805491 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.3327044867065724}
episode index:3801
target Thresh 19.0
target distance 7.0
model initialize at round 3801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.99999915, 3.99999952, 4.66791534]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.071068408361675}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8236606425797015
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.09305266, 10.3197807 ,  3.53517412]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.133689454698688}
episode index:3802
target Thresh 19.0
target distance 12.0
model initialize at round 3802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([15.02688906,  4.37311441,  3.19733   ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 12.379584957290575}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8236405298928101
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.2341951 , 9.45952801, 4.64866223]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.5890308295062995}
episode index:3803
target Thresh 19.0
target distance 2.0
model initialize at round 3803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([2.99813139, 8.99660152, 5.21967101]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.832151729362675}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8236816601425755
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.03918813, 10.42532276,  2.9364857 ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.1195594626174803}
episode index:3804
target Thresh 19.0
target distance 9.0
model initialize at round 3804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 6.99999998, 11.        ,  3.94227898]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 9.00000002123334}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8236912203776606
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.02826804, 11.57851264,  2.52635245]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.130902239714854}
episode index:3805
target Thresh 19.0
target distance 5.0
model initialize at round 3805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.99999996, 5.99999998, 4.74540734]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.071067853797634}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8237030581581102
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.29082536, 11.55283965,  1.32948081]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.8991998365882018}
episode index:3806
target Thresh 19.0
target distance 3.0
model initialize at round 3806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 1.99983578, 10.00059948,  2.84817696]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 3.60614117549058}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8237364931441207
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.84654304, 7.41867874, 0.28180634]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.4459158301013776}
episode index:3807
target Thresh 19.0
target distance 1.0
model initialize at round 3807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.45546921,  9.71938388,  0.81953591]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8514489806987042}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8237827808297447
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.45546921,  9.71938388,  0.81953591]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8514489806987042}
episode index:3808
target Thresh 19.0
target distance 3.0
model initialize at round 3808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.99961142, 6.0005859 , 3.16641259]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.000585925747184}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8238186992411833
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.50377832, 3.52340356, 4.88322728]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7212400691217492}
episode index:3809
target Thresh 19.0
target distance 6.0
model initialize at round 3809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.00000043,  9.00000105,  2.19360022]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 6.000001045863317}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8238374707779333
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.15647803,  2.80447381,  5.3440443 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.2504313573214321}
episode index:3810
target Thresh 19.0
target distance 14.0
model initialize at round 3810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([3.61561846, 7.52865609, 0.72613638]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.471477820289206}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8238193342826369
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.68118241,  8.78493819,  0.17746862]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.3845727978146511}
episode index:3811
target Thresh 19.0
target distance 11.0
model initialize at round 3811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([5.67581716, 9.85236456, 0.92174977]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.511223875982623}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8238032071525384
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.39869628,  5.49131087,  4.65626731]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.7765001828105025}
episode index:3812
target Thresh 19.0
target distance 13.0
model initialize at round 3812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([2.        , 3.        , 5.47736263]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 14.317821063276352}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8237716426332222
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.26452813,  8.34786326,  0.07913895]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9829553447337233}
episode index:3813
target Thresh 19.0
target distance 14.0
model initialize at round 3813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([16.        ,  9.        ,  0.88796967]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 15.652475842498527}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8237310553537089
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.35404763, 1.6021758 , 5.48974598]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5325540566303723}
episode index:3814
target Thresh 19.0
target distance 11.0
model initialize at round 3814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 4.65364463, 10.31280587,  1.19695252]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.438104569873419}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8237273847935169
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.45917274,  8.76874883,  5.49784067]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.5881931866833213}
episode index:3815
target Thresh 19.0
target distance 12.0
model initialize at round 3815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([3.58604321, 8.42236421, 6.07806063]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.24073995984459}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8237093009105007
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.22427161,  4.0443712 ,  5.52939286]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7769963541015259}
episode index:3816
target Thresh 19.0
target distance 12.0
model initialize at round 3816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([15.00012661,  9.99972629,  6.15565205]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 14.422158626134847}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8236796556200531
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.18789827, 2.52566432, 5.04061367]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9673841990181954}
episode index:3817
target Thresh 19.0
target distance 4.0
model initialize at round 3817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.34445217,  8.55004137,  2.98090625]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.7946106943557045}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8237105095995662
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.76947633, 11.27358419,  2.41453563]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8166652532106085}
episode index:3818
target Thresh 19.0
target distance 10.0
model initialize at round 3818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.75832145, 9.50234666, 2.11302876]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.280065554120519}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.823694440525135
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.37382814,  5.86891562,  5.8475463 ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9459185163567979}
episode index:3819
target Thresh 19.0
target distance 2.0
model initialize at round 3819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.9955923 , 11.00753148,  3.11028993]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.007536314289347}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8237379760119085
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.07557056,  9.69704855,  5.11028993]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.157776519293746}
episode index:3820
target Thresh 19.0
target distance 12.0
model initialize at round 3820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([2.05237268, 6.67508796, 2.54954088]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.500082800586174}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.823704653646569
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.80601045,  3.6049685 ,  5.4345025 ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.6353100255231718}
episode index:3821
target Thresh 19.0
target distance 5.0
model initialize at round 3821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([1.99999802, 3.99998868, 5.54930592]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.385176051469242}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8237305667917236
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.81828739, 8.68700089, 2.69974999]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.3619225285040125}
episode index:3822
target Thresh 19.0
target distance 6.0
model initialize at round 3822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([14.43792466,  9.41368927,  5.38186312]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.681958174156962}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8237540527139553
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.26428199,  5.64978847,  4.5323072 ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9815834382031445}
episode index:3823
target Thresh 19.0
target distance 10.0
model initialize at round 3823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 4.        , 11.        ,  3.49669766]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8237461709751818
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.36352143,  8.09957002,  5.51440051]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.6442198037627485}
episode index:3824
target Thresh 19.0
target distance 8.0
model initialize at round 3824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.99997118, 10.00003842,  3.22443497]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 8.000038415696723}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8237579355874154
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.00485951,  1.70657265,  6.09169374]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.29346758650478594}
episode index:3825
target Thresh 19.0
target distance 11.0
model initialize at round 3825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([15.        , 10.        ,  6.02316141]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.045361017187602}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8237542685546485
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.75050649, 10.91247675,  4.04086426]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.2643999447042212}
episode index:3826
target Thresh 19.0
target distance 8.0
model initialize at round 3826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.        ,  3.        ,  5.40807986]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.000000000000066}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8237660249027577
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.7540498 , 10.59718683,  1.99215333]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8548973950754584}
episode index:3827
target Thresh 19.0
target distance 10.0
model initialize at round 3827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.85403901, 2.1412614 , 1.14235276]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.86155699991425}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8237309368831565
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.70329682,  8.18092005,  6.02731438]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.3475123634274401}
episode index:3828
target Thresh 19.0
target distance 12.0
model initialize at round 3828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([15.        ,  9.        ,  1.77140826]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8237149044406034
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.77092285, 5.4410202 , 5.5059258 ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8881558782260566}
episode index:3829
target Thresh 19.0
target distance 7.0
model initialize at round 3829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([16.00000616,  6.00000242,  1.38424748]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.602328871477772}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8237266618579214
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.51790156, 10.35478   ,  4.25150625]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.8273638126924677}
episode index:3830
target Thresh 19.0
target distance 7.0
model initialize at round 3830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([15.        ,  5.        ,  0.29758566]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 9.219544458383046}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8237294775978306
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.08628329, 10.3171677 ,  4.88165913]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6882621235157361}
episode index:3831
target Thresh 19.0
target distance 13.0
model initialize at round 3831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([15.1485885 ,  7.54828152,  5.19196606]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 13.355584240206877}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8236980888236661
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.75445373, 2.23570216, 6.07692768]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7904150431674271}
episode index:3832
target Thresh 19.0
target distance 3.0
model initialize at round 3832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.35432918, 4.82059165, 0.87829655]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.244307058453056}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8237338044305476
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.49558157, 7.79263774, 2.59511124]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.5453779086104746}
episode index:3833
target Thresh 19.0
target distance 13.0
model initialize at round 3833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([3.        , 8.99999997, 5.8019321 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 13.15294642995493}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8237158037739938
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.23881737,  6.91705695,  5.25326433]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7656882789552807}
episode index:3834
target Thresh 19.0
target distance 4.0
model initialize at round 3834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([12.56838805, 10.39004909,  0.63909214]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.506945705357526}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8237514961354607
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.15758101, 11.38613092,  2.35590684]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9266967410051782}
episode index:3835
target Thresh 19.0
target distance 5.0
model initialize at round 3835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 5.49470788, 10.24735336,  4.61524057]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 4.154946484033392}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8237748970091177
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.1200132 , 8.64571827, 3.76568465]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6567764115191452}
episode index:3836
target Thresh 19.0
target distance 13.0
model initialize at round 3836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([3.        , 9.        , 5.12187505]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 13.152946437966278}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.823760893425275
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.7834331 ,  7.49938324,  0.57320728]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5443205280098787}
episode index:3837
target Thresh 19.0
target distance 10.0
model initialize at round 3837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.       ,  3.       ,  6.0083952]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.049875621461048}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.8237087215913683
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.34198449, 2.89259857, 5.76061559]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.955869030059469}
episode index:3838
target Thresh 19.0
target distance 12.0
model initialize at round 3838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.        ,  9.        ,  5.86414933]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.369316876848737}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8236967686447176
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.5353525 , 5.99018912, 5.59866687]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5354423897564702}
episode index:3839
target Thresh 19.0
target distance 11.0
model initialize at round 3839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.00000001, 10.99999999,  0.03804081]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.180339891278228}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8236931309102323
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.82746236, 9.01149814, 4.33892897]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.17292033811747862}
episode index:3840
target Thresh 19.0
target distance 2.0
model initialize at round 3840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([16.00913946,  5.00039987,  1.05372446]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.009139500379002}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8237338512614664
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.26143055,  5.14737647,  5.05372446]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.3001095720367887}
episode index:3841
target Thresh 19.0
target distance 3.0
model initialize at round 3841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.00004829, 7.99987598, 6.09366941]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 3.162410587802117}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.823766973645287
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.72729915, 11.06961764,  3.5272988 ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.28144692341091454}
episode index:3842
target Thresh 19.0
target distance 5.0
model initialize at round 3842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 9.99996506, 11.00002622,  3.50778782]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.099058919811349}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8237927289720584
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.7346073 , 10.25855675,  0.6582319 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.3705197365340535}
episode index:3843
target Thresh 19.0
target distance 14.0
model initialize at round 3843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([16.        , 10.        ,  0.32467478]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.823782813626542
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.99704943, 8.9630392 , 4.34237763]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.3862005816091283}
episode index:3844
target Thresh 19.0
target distance 11.0
model initialize at round 3844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.98966839,  8.6829379 ,  2.58693528]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.23127722385354}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8237812854403186
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.22385732, 10.38401437,  4.88782344]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6554009427131416}
episode index:3845
target Thresh 19.0
target distance 3.0
model initialize at round 3845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.00244437,  9.0012492 ,  1.48245018]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.164236314518749}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8238143610421023
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.79581099,  6.04112652,  5.19926488]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.20828956218672065}
episode index:3846
target Thresh 19.0
target distance 3.0
model initialize at round 3846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.9960233 , 5.99582429, 4.96139717]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.6068241875421654}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8238449474180728
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.91149784, 8.4551558 , 4.39502656]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5519853604074013}
episode index:3847
target Thresh 19.0
target distance 12.0
model initialize at round 3847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 5.42924401, 11.88486854,  1.56691235]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.187571196374956}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8238269833691605
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.43520693,  4.41646475,  1.01824458]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6023686243704994}
episode index:3848
target Thresh 19.0
target distance 14.0
model initialize at round 3848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([3.42402487, 6.89670774, 1.57197159]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.495662160498975}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.8237701320009526
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.3939831 ,  2.75294946,  1.04100668]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8497973720825021}
episode index:3849
target Thresh 19.0
target distance 10.0
model initialize at round 3849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.96700514, 9.08876198, 1.67418974]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.546488708919147}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8237602379772659
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.63376892,  6.50175004,  5.69189259]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6211910361124079}
episode index:3850
target Thresh 19.0
target distance 2.0
model initialize at round 3850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.47348384,  2.47155645,  3.85198152]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.616588679386446}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.823800835162938
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.69647077,  3.23124149,  1.56879621]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0373336871098107}
episode index:3851
target Thresh 19.0
target distance 4.0
model initialize at round 3851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.99979158, 6.00034703, 3.1216532 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 4.123391752211107}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8238338541698791
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.55514149, 2.85604422, 0.55528258]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0202910309440558}
episode index:3852
target Thresh 19.0
target distance 9.0
model initialize at round 3852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.30049142,  6.53071089,  3.00944197]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.427231988973928}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8238323159096734
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.5726753 , 10.34912199,  5.31033013]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.866948201114342}
episode index:3853
target Thresh 19.0
target distance 1.0
model initialize at round 3853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.67622976,  4.94962027,  3.52931023]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.32766642874549046}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8238780262584254
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.67622976,  4.94962027,  3.52931023]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.32766642874549046}
episode index:3854
target Thresh 19.0
target distance 14.0
model initialize at round 3854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([15.78038013,  5.6685788 ,  2.71166497]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.784364935059708}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8238449614573337
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.93169884, 5.62733777, 5.5966266 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.3788696197585224}
episode index:3855
target Thresh 19.0
target distance 12.0
model initialize at round 3855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.00018379,  4.00005508,  1.30116719]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.000148469383669}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8238231397280626
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.8206475 , 8.92866791, 4.75249942]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8237418213718354}
episode index:3856
target Thresh 19.0
target distance 11.0
model initialize at round 3856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([15.1700765 ,  4.67435402,  2.47956616]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 11.24846416915379}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8237956332185656
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.59398781, 6.76215786, 5.64771309]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9662847022236185}
episode index:3857
target Thresh 19.0
target distance 7.0
model initialize at round 3857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([2.        , 2.        , 5.03764844]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 7.280109889569083}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8238141771846206
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.73525823, 8.13654787, 1.90490721]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9031266718161464}
episode index:3858
target Thresh 19.0
target distance 2.0
model initialize at round 3858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([2.00077762, 8.99839008, 6.17235804]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.2371606199947878}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8238572416631941
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.06267997, 10.28077347,  1.88917273]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.721952619324221}
episode index:3859
target Thresh 19.0
target distance 7.0
model initialize at round 3859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.32133082,  9.95048817,  4.22441792]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 6.9579119952469455}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8238781030189831
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.67409945,  3.65119741,  5.374862  ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9372663056239755}
episode index:3860
target Thresh 19.0
target distance 1.0
model initialize at round 3860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.39804343, 11.17091438,  1.41557997]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.3165835303888482}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8239160260692243
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.37480345, 10.02698783,  1.13239466]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.3757738247300292}
episode index:3861
target Thresh 19.0
target distance 2.0
model initialize at round 3861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([16.00592003,  8.99842789,  0.75043362]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.2420664217772117}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8239564672846389
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.37845455,  9.55841961,  4.75043362]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.5815677839415501}
episode index:3862
target Thresh 19.0
target distance 11.0
model initialize at round 3862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.90909329, 9.38628691, 1.9568116 ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.219122666661821}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8239570399371242
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.15040416, 11.17717216,  2.25769976]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8678727213881561}
episode index:3863
target Thresh 19.0
target distance 12.0
model initialize at round 3863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([16.00000003,  6.00000003,  1.71189469]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 12.000000031286982}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8239352343815991
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.66101573, 5.94972327, 5.16322692]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.34269240341039225}
episode index:3864
target Thresh 19.0
target distance 10.0
model initialize at round 3864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.05876505, 6.94095273, 0.22220629]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.101403793869832}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8239004384828895
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.23515145,  2.25845331,  5.10716791]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8073359961343484}
episode index:3865
target Thresh 19.0
target distance 4.0
model initialize at round 3865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([11.99997488, 11.00001023,  3.76480603]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 5.000026230999411}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8239260060607335
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.06699962,  8.12367687,  0.91525011]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.14065886413060222}
episode index:3866
target Thresh 19.0
target distance 12.0
model initialize at round 3866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([2.        , 4.        , 4.03654647]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.8238725703573153
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.53381176,  5.05940295,  5.78876686]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5371067950837328}
episode index:3867
target Thresh 19.0
target distance 6.0
model initialize at round 3867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.00000018,  5.00000001,  1.03913754]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.485281496281058}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8238841715058187
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.6874569 , 10.62676284,  3.90639631]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7822422628927659}
episode index:3868
target Thresh 19.0
target distance 12.0
model initialize at round 3868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.27476495,  4.51869028,  3.02631557]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.53654979739113}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8238586083536998
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.70303372, 9.83586464, 4.1944625 ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.33930722059615664}
episode index:3869
target Thresh 19.0
target distance 1.0
model initialize at round 3869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([2.99532276, 6.99426118, 5.03855801]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.4215790268643098}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8238989808063216
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.86091048, 8.31094866, 2.7553727 ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.3406390525041009}
episode index:3870
target Thresh 19.0
target distance 10.0
model initialize at round 3870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([2.76194333, 8.50061002, 2.1109634 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 9.570195465049814}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.823901722935137
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.43186024, 10.25788313,  0.41185156]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9346230431075361}
episode index:3871
target Thresh 19.0
target distance 1.0
model initialize at round 3871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.81458831, 6.89381801, 4.24092817]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9128461670411985}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8239472028620648
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.81458831, 6.89381801, 4.24092817]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9128461670411985}
episode index:3872
target Thresh 19.0
target distance 13.0
model initialize at round 3872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([15.        , 10.        ,  0.39778441]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.341664064126421}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.823931296719923
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.75004343, 7.21463911, 4.13230195]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.32946659042414933}
episode index:3873
target Thresh 19.0
target distance 10.0
model initialize at round 3873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.       , 3.       , 4.7940402]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.049875624036128}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.8238686315463035
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.38997957,  4.45092989,  5.97988998]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.5961726476423788}
episode index:3874
target Thresh 19.0
target distance 8.0
model initialize at round 3874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.99997799,  9.99970366,  5.64824057]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.062272666709669}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.823877970829683
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.82438545, 10.6629179 ,  4.23231403]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.38008527166871964}
episode index:3875
target Thresh 19.0
target distance 9.0
model initialize at round 3875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.00152647, 10.98306672,  5.81229234]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 11.392572128226528}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.823866088318994
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.17343115, 4.57728982, 5.54680988]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6027784828156824}
episode index:3876
target Thresh 19.0
target distance 12.0
model initialize at round 3876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([15.01622201,  3.3654922 ,  3.20512009]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.022283422972338}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8238227024877469
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.95410755, 3.13444177, 5.52371109]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.14205881855879365}
episode index:3877
target Thresh 19.0
target distance 7.0
model initialize at round 3877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.62987268,  4.07820414,  1.1335265 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 7.111099976505595}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8238434759205786
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.77749892, 10.31621729,  2.28397058]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.035404930168246}
episode index:3878
target Thresh 19.0
target distance 12.0
model initialize at round 3878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 3.99999631, 10.99999092,  5.33693814]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.649111264670415}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8238316114924988
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.07586964,  7.09317803,  5.07145569]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9288159466896366}
episode index:3879
target Thresh 19.0
target distance 7.0
model initialize at round 3879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([11.99924604, 10.99988895,  4.29782438]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 7.280210284571983}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8238523719212402
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.41328985,  4.88788212,  5.44826846]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0642196524651808}
episode index:3880
target Thresh 19.0
target distance 1.0
model initialize at round 3880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.99892459,  9.00133398,  3.25928688]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.4159172687826425}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8238926315522834
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.11103646,  7.90507836,  0.97610157]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.14607947441881858}
episode index:3881
target Thresh 19.0
target distance 4.0
model initialize at round 3881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([16.00005297, 11.00025419,  2.37535012]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 4.123365076987107}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8239253717424813
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.72147577,  7.85034469,  6.09216481]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.1151741436686586}
episode index:3882
target Thresh 19.0
target distance 1.0
model initialize at round 3882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.02222786,  3.0107325 ,  1.45982569]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0222842004573403}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8239681414123905
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.08289001,  3.7887964 ,  3.45982569]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7931396602807073}
episode index:3883
target Thresh 19.0
target distance 13.0
model initialize at round 3883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([15.2140614 ,  8.48818276,  3.0666858 ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 12.304388801540247}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8239542575309262
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.38852068, 7.51000041, 4.80120334]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.641130828801951}
episode index:3884
target Thresh 19.0
target distance 13.0
model initialize at round 3884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 3.        , 10.        ,  4.18563914]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8239444052486292
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.8849066 , 11.34440133,  1.92015669]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.3631236261270925}
episode index:3885
target Thresh 19.0
target distance 8.0
model initialize at round 3885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 3.01652593, 11.00351779,  1.21973437]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 8.063716155398906}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8239536985963886
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.62524728, 2.99137703, 6.08699314]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.6253067427721867}
episode index:3886
target Thresh 19.0
target distance 11.0
model initialize at round 3886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.50284244,  6.76868653,  3.67725837]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.632233335549914}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8239208865355326
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.72875552, 2.47021511, 0.27903469]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8672870685137597}
episode index:3887
target Thresh 19.0
target distance 13.0
model initialize at round 3887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([1.76594974, 2.60080118, 5.55664897]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 13.240069713977562}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.8238742534891205
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.84991427,  3.75517484,  5.59205467]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7699446515948632}
episode index:3888
target Thresh 19.0
target distance 8.0
model initialize at round 3888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.        ,  3.        ,  5.75710368]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 8.000000000004807}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8238857915604189
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.91632643, 10.9572771 ,  2.34117715]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.09394951820926233}
episode index:3889
target Thresh 19.0
target distance 1.0
model initialize at round 3889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.00714592, 11.010089  ,  1.96452921]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0101142767311477}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8239259494546194
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.28140653,  9.75650263,  5.96452921]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.37212982181229115}
episode index:3890
target Thresh 19.0
target distance 3.0
model initialize at round 3890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.63638092, 9.44201598, 6.11017084]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.4871546356581842}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8239635678176482
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.80465017, 7.83106514, 5.82698553]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.25826448567442206}
episode index:3891
target Thresh 19.0
target distance 2.0
model initialize at round 3891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([2.99969442, 6.99879522, 5.47398734]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 2.2372822419674594}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8240036850921042
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.57667454, 8.64832791, 3.19080203]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5503432590443434}
episode index:3892
target Thresh 19.0
target distance 11.0
model initialize at round 3892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.13532488,  9.55614231,  5.18280482]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.45268740551379}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8239979380857972
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.13267714, 7.50413305, 5.20050767]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5212996792303356}
episode index:3893
target Thresh 19.0
target distance 14.0
model initialize at round 3893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([2.        , 4.        , 3.83790958]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.8239386079646662
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.43486665,  2.62379734,  1.02375936]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.7604157594681171}
episode index:3894
target Thresh 19.0
target distance 2.0
model initialize at round 3894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.57147301, 3.53822813, 1.33997696]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.523289982720285}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8239761844452914
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.55726355, 4.05590937, 1.05679166]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0962891019061123}
episode index:3895
target Thresh 19.0
target distance 1.0
model initialize at round 3895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.99575946, 11.02404058,  2.75539142]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.428354301616692}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8240187983609881
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.69761759, 10.12867564,  4.75539142]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.32862218639783564}
episode index:3896
target Thresh 19.0
target distance 7.0
model initialize at round 3896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([16.        ,  4.99999999,  5.18154454]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 9.219544458552896}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8240214914487709
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.12328959, 11.61699365,  3.4824327 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.6291911385581329}
episode index:3897
target Thresh 19.0
target distance 10.0
model initialize at round 3897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 5.        , 11.        ,  4.56475496]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.806248474876554}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8239905592280298
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.97388651,  2.57832382,  5.44971658]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.4224839831370964}
episode index:3898
target Thresh 19.0
target distance 6.0
model initialize at round 3898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.98062231,  4.99294054,  4.50095749]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 6.325158349163035}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8240065618729356
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.67275675, 11.08490094,  3.36821626]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.33807737710061536}
episode index:3899
target Thresh 19.0
target distance 6.0
model initialize at round 3899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.1016323, 6.6798985, 2.5203709]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.818981671647168}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8240225563113569
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.89115609, 11.42604607,  1.38762967]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.43972974394059505}
episode index:3900
target Thresh 19.0
target distance 8.0
model initialize at round 3900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 2.        , 10.        ,  4.54519272]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.062257749464226}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8240340208733814
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.99962945, 11.55522777,  1.12926618]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5552278969383441}
episode index:3901
target Thresh 19.0
target distance 1.0
model initialize at round 3901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.00900908,  1.999362  ,  0.93930024]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.0090092834679696}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8240765544405588
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.64156253,  2.9800151 ,  2.93930024]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.1713377244512913}
episode index:3902
target Thresh 19.0
target distance 14.0
model initialize at round 3902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([16.        ,  7.        ,  1.34155958]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8240493071892541
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.67984293, 4.86229024, 4.50970651]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9198070505312954}
episode index:3903
target Thresh 19.0
target distance 8.0
model initialize at round 3903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 4.        , 11.        ,  4.77450347]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.00000000080323}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8240607560892488
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.36727303, 11.32051779,  1.35857693]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.4874639820822552}
episode index:3904
target Thresh 19.0
target distance 11.0
model initialize at round 3904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4.31395626, 8.94837886, 0.33504742]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 9.742964144963667}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8240612958761236
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.96508012, 10.58632905,  0.63593558]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.5873679889815313}
episode index:3905
target Thresh 19.0
target distance 10.0
model initialize at round 3905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.00000346,  3.00000027,  1.08773821]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.440309744878853}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8240322359507258
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.55794302, 6.9609903 , 6.25588514]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1112167970052227}
episode index:3906
target Thresh 19.0
target distance 2.0
model initialize at round 3906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([13.99875831, 11.00036066,  3.86891127]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.8295602267009268}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8240671895657883
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.20242459,  8.71244138,  5.58572596]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8478304594166588}
episode index:3907
target Thresh 19.0
target distance 12.0
model initialize at round 3907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.        ,  2.        ,  1.04319971]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.165525061109014}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8240224186954717
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.14287505, 3.6506852 , 5.36179072]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.3774044390956777}
episode index:3908
target Thresh 19.0
target distance 3.0
model initialize at round 3908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([10.09975608, 11.82800591,  3.19208419]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 2.2571152761396904}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8240598391562812
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.83086924, 11.70506407,  2.90889889]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0897059422544595}
episode index:3909
target Thresh 19.0
target distance 7.0
model initialize at round 3909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.00000001, 1.99999998, 6.20117426]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 7.071067830284063}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8240780689299647
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.61355928, 8.56400835, 3.06843303]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5826020497260942}
episode index:3910
target Thresh 19.0
target distance 13.0
model initialize at round 3910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.1433288 , 6.23498476, 1.83391732]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.174808471006758}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8240546031413492
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.56076301,  9.20545757,  1.00206424]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.4849143692575158}
episode index:3911
target Thresh 19.0
target distance 6.0
model initialize at round 3911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([3.26879793, 8.89431841, 0.2931897 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 5.056105306403469}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8240774719154652
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.5562428 , 4.12768332, 5.72681909]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.5707092749573787}
episode index:3912
target Thresh 19.0
target distance 4.0
model initialize at round 3912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([15.89527752,  8.06534028,  3.59375775]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.485428461822233}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8241123603739585
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.99103776,  4.87695376,  5.31057244]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.323330545756202}
episode index:3913
target Thresh 19.0
target distance 14.0
model initialize at round 3913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([2.        , 8.        , 3.03726006]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 14.035668847706567}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.824090793693584
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.5679685 ,  7.87231809,  0.20540698]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9734423785961974}
episode index:3914
target Thresh 19.0
target distance 12.0
model initialize at round 3914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.        , 4.        , 4.67997837]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.165525060596758}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.8240427974464244
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.3208987 ,  2.72056273,  0.43219876]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.7887880689538295}
episode index:3915
target Thresh 19.0
target distance 8.0
model initialize at round 3915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 8.99998417, 10.99999188,  4.62581229]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.433982631351865}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8240350271415718
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.80486143,  3.81955353,  0.36032983]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.1486818142038075}
episode index:3916
target Thresh 19.0
target distance 4.0
model initialize at round 3916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 3.89312734, 10.60964509,  2.64709419]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.125382405646576}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8240650105784519
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.18315462, 11.55648585,  2.08072358]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9883890304617148}
episode index:3917
target Thresh 19.0
target distance 4.0
model initialize at round 3917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.17599018,  7.04665788,  4.76834416]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 3.156123635848459}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8240998576941797
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.36127184,  4.51899493,  0.20197355]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6323551899792006}
episode index:3918
target Thresh 19.0
target distance 4.0
model initialize at round 3918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.00112345, 3.99885189, 0.21374529]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 4.001148266662423}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8241322359009177
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.5964139 , 7.15961176, 3.93055998]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9322736371970308}
episode index:3919
target Thresh 19.0
target distance 2.0
model initialize at round 3919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.573176  ,  2.43497671,  6.07345867]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7195383641747443}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8241771001264531
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.573176  ,  2.43497671,  6.07345867]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7195383641747443}
episode index:3920
target Thresh 19.0
target distance 10.0
model initialize at round 3920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([2.00110093, 5.99906516, 0.30600661]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 11.179773340786747}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8241713499324752
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.0858133 , 10.38843859,  0.32370946]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6175526536665232}
episode index:3921
target Thresh 19.0
target distance 7.0
model initialize at round 3921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.99999974, 2.99999953, 5.2269783 ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 7.071068243553571}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8241872126356327
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.69674999, 9.38935607, 4.09423707]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6817965847780063}
episode index:3922
target Thresh 19.0
target distance 3.0
model initialize at round 3922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.99666065,  2.99712557,  4.86230898]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.163951216616257}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8242171114214509
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.70096694,  5.7861297 ,  4.29593836]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.3676428631491173}
episode index:3923
target Thresh 19.0
target distance 6.0
model initialize at round 3923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.00000005, 4.99999993, 0.07261771]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 6.324555370196225}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8242306954967765
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.9892685 , 10.25714901,  0.93987648]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.7429285028934317}
episode index:3924
target Thresh 19.0
target distance 3.0
model initialize at round 3924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([3.99625409, 9.99875564, 4.47231412]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.166224842556221}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8242629908736946
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.66161231, 11.37015994,  1.9059435 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.5015222862080574}
episode index:3925
target Thresh 19.0
target distance 7.0
model initialize at round 3925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.49691956,  3.34852162,  1.23875063]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 6.6700145170567}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.824283398179893
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.57114923,  9.4376463 ,  2.38919471]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7072161431778339}
episode index:3926
target Thresh 19.0
target distance 6.0
model initialize at round 3926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 9.96613125, 10.14455895,  5.76393223]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 6.744106540146621}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8242969549980289
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.89464727, 7.64028866, 0.34800569]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.1001651236661905}
episode index:3927
target Thresh 19.0
target distance 2.0
model initialize at round 3927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([16.00304933, 10.00012908,  1.05230683]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.0030493382453862}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8243366197243532
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.45521337,  9.71889797,  5.05230683]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.5350117447920945}
episode index:3928
target Thresh 19.0
target distance 12.0
model initialize at round 3928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.        , 11.        ,  0.79223126]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.000000000000139}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8243267804576399
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.63639157, 6.5899521 , 4.80993411]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8677774521462371}
episode index:3929
target Thresh 19.0
target distance 11.0
model initialize at round 3929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 4.63649639, 10.39276922,  1.24555081]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.95922099678333}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8243110084306398
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.63593461,  7.16156374,  4.98006835]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.398304474184251}
episode index:3930
target Thresh 19.0
target distance 1.0
model initialize at round 3930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.99357261, 2.99064882, 5.12021375]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.0093716396436727}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8243506393112221
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.71529583, 4.29034792, 2.83702844]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.4066428174433834}
episode index:3931
target Thresh 19.0
target distance 12.0
model initialize at round 3931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([2.00000007, 2.99999971, 5.97253895]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 12.649110662106457}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8243163306760639
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.19418947,  7.48739556,  4.57431526]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9417457424014353}
episode index:3932
target Thresh 19.0
target distance 12.0
model initialize at round 3932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([12.42994353,  9.59045724,  3.8001585 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 10.52475808822247}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8243168016379656
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.49950179, 10.78370321,  4.10104666]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.544321908879363}
episode index:3933
target Thresh 19.0
target distance 10.0
model initialize at round 3933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([3.99999999, 9.        , 4.53666449]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.049875631373355}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8243030055892029
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.4748291 ,  8.0438932 ,  6.27118203]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.4768535261772596}
episode index:3934
target Thresh 19.0
target distance 1.0
model initialize at round 3934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.6633018 , 10.79830038,  3.61322117]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.3924900198656911}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8243476553971852
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.6633018 , 10.79830038,  3.61322117]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.3924900198656911}
episode index:3935
target Thresh 19.0
target distance 12.0
model initialize at round 3935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([2.98878658, 4.01165375, 3.34694111]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.178505600274338}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.8242998499679848
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.31465538,  2.70242082,  5.38234681]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.981372637421537}
episode index:3936
target Thresh 19.0
target distance 8.0
model initialize at round 3936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([16.00000008,  9.00000006,  1.6860544 ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.24621131007344}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8243111392651148
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.02098193, 10.55458909,  4.55331318]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.4459048366398594}
episode index:3937
target Thresh 19.0
target distance 7.0
model initialize at round 3937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.55054892,  9.14497422,  4.68454981]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 6.169587695382534}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8243314721589045
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.20030199,  3.52370245,  5.83499389]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5607005855308247}
episode index:3938
target Thresh 19.0
target distance 13.0
model initialize at round 3938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([15.2018654 ,  8.33235945,  5.8428514 ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 14.23809352625118}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8243025871005427
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.32796752, 3.41894216, 4.72781302]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7919218336029824}
episode index:3939
target Thresh 19.0
target distance 4.0
model initialize at round 3939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([2.00208992, 9.9636887 , 5.77988124]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.1300394792596045}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8243371793398573
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.12793156, 11.0711025 ,  1.21351062]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8749622485410952}
episode index:3940
target Thresh 19.0
target distance 4.0
model initialize at round 3940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([3.99996066, 6.99939579, 5.65737915]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 5.000506982016048}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8243574901481976
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.62007125, 10.19883665,  0.52463792]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8866840327285395}
episode index:3941
target Thresh 19.0
target distance 12.0
model initialize at round 3941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.4411962 ,  2.52443989,  4.4477036 ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.730655896176339}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8243163751179682
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.30885984, 5.50745669, 4.76629461]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8574304695135425}
episode index:3942
target Thresh 19.0
target distance 1.0
model initialize at round 3942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.99960925, 5.00403284, 2.6773867 ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.4167923622541732}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8243583948047248
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.74886701, 4.27825086, 4.6773867 ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.37482171807848635}
episode index:3943
target Thresh 19.0
target distance 4.0
model initialize at round 3943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.00026357, 9.00049862, 2.09452719]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.12352544845685}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8243905022223454
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.66798574, 5.78053433, 5.81134189]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8482141927460743}
episode index:3944
target Thresh 19.0
target distance 3.0
model initialize at round 3944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.00248204, 7.00574362, 2.17289439]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 3.6089572870577267}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8244225933624412
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.04652876, 3.96404316, 5.88970908]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.954148993265385}
episode index:3945
target Thresh 19.0
target distance 8.0
model initialize at round 3945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 8.64714534, 10.10400859,  4.80816054]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 8.488986858480262}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8244230358435545
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.02407905, 3.66379511, 5.1090487 ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.1802735500782466}
episode index:3946
target Thresh 19.0
target distance 13.0
model initialize at round 3946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([15.        ,  8.        ,  0.40928524]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8244132195539583
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.59046571, 9.78100127, 4.42698809]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.6297699536172542}
episode index:3947
target Thresh 19.0
target distance 8.0
model initialize at round 3947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 8.      , 11.      ,  2.748954]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.313708498984761}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8243974975414946
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.34264369,  2.88578475,  0.20028623]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.3611783767335942}
episode index:3948
target Thresh 19.0
target distance 3.0
model initialize at round 3948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.06931742, 6.0483913 , 4.09964395]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.049179299184032}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8244319869090454
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.50245003, 2.99220686, 5.81645864]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5025104599785278}
episode index:3949
target Thresh 19.0
target distance 6.0
model initialize at round 3949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 8.3311893 , 11.21772499,  4.02185822]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.865953645395123}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8244522274376783
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.11592722, 8.72588277, 5.17230229]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.29762287104518775}
episode index:3950
target Thresh 19.0
target distance 10.0
model initialize at round 3950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([3.28429427, 4.52320498, 3.02004933]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 12.034186010353668}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8244365074900472
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.19802701, 10.47213483,  0.47138157]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.930629878362339}
episode index:3951
target Thresh 19.0
target distance 13.0
model initialize at round 3951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([2.26190839, 7.66246566, 2.42453849]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 12.755306292136126}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8244207954978552
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.27756615,  7.56949826,  6.15905604]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9199124642143833}
episode index:3952
target Thresh 19.0
target distance 7.0
model initialize at round 3952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 7.99999999, 11.        ,  4.3467536 ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 8.06225775463285}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8244320085050069
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.23638776,  7.98367708,  0.93082706]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.0116816554911237}
episode index:3953
target Thresh 19.0
target distance 7.0
model initialize at round 3953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([11.        , 11.        ,  2.20443167]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.899494936612005}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8244282897037212
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.48337708, 4.15043052, 0.22213452]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.5062437555532336}
episode index:3954
target Thresh 19.0
target distance 11.0
model initialize at round 3954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([15.        ,  7.        ,  0.04032486]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.704699910719658}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8243994970204263
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.21781948, 3.02307262, 5.20847179]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.7825207387306287}
episode index:3955
target Thresh 19.0
target distance 3.0
model initialize at round 3955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.3288257 , 10.19573544,  4.03499937]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.5532608369898708}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8244363775823523
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.90259976, 10.85725223,  3.75181406]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.17281126655354734}
episode index:3956
target Thresh 19.0
target distance 9.0
model initialize at round 3956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([2.        , 5.        , 5.24343085]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 10.816653826391967}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8244285883243441
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.52933277, 11.64789101,  0.9779484 ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.800806093133894}
episode index:3957
target Thresh 19.0
target distance 4.0
model initialize at round 3957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([3.00043463, 7.00012006, 1.27951878]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 4.1233275338218665}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8244581617354296
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.73390606, 3.79090452, 0.71314816]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0789569343862198}
episode index:3958
target Thresh 19.0
target distance 9.0
model initialize at round 3958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 7.22085327, 11.87201423,  2.4769668 ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 9.236984520154026}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8244671539538954
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.29792945,  9.37607806,  1.06104026]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7964532385643204}
episode index:3959
target Thresh 19.0
target distance 1.0
model initialize at round 3959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.73375073, 11.09426794,  4.07728314]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.28244489318594385}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8245114804301696
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.73375073, 11.09426794,  4.07728314]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.28244489318594385}
episode index:3960
target Thresh 19.0
target distance 12.0
model initialize at round 3960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.        , 4.        , 5.13428235]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 12.165525060597302}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.82448091648535
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.67284022,  6.55885631,  6.01924397]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.6475753958042147}
episode index:3961
target Thresh 19.0
target distance 4.0
model initialize at round 3961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 3.4508148 , 11.84980276,  1.5402252 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.687100650968778}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8245104468318709
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.33226863, 11.32286851,  0.97385459]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.4632995979752552}
episode index:3962
target Thresh 19.0
target distance 10.0
model initialize at round 3962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.41289066, 9.96103205, 5.20803499]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 13.246317053511317}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8244798985725138
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.94945725,  1.65791039,  6.09299661]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.3458032224557082}
episode index:3963
target Thresh 19.0
target distance 3.0
model initialize at round 3963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.76119828,  9.50072352,  6.19218588]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.6827340612139614}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8245166844205026
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.0121887 ,  8.14790205,  5.90900057]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.14840343669243045}
episode index:3964
target Thresh 19.0
target distance 7.0
model initialize at round 3964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([2.99999999, 3.99999998, 5.2128551 ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.2801099157720754}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8245345461026812
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.72744218, 10.25055081,  2.08011387]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.797472163475615}
episode index:3965
target Thresh 19.0
target distance 12.0
model initialize at round 3965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([2.        , 3.        , 4.50594115]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 12.649110640673516}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8244936352340179
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.20884061,  7.76171433,  4.82453216]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.098244917834384}
episode index:3966
target Thresh 19.0
target distance 11.0
model initialize at round 3966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([15.70306233,  9.25422955,  2.069887  ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 11.705823361553117}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8244919748615357
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.09299452, 9.46745659, 4.37077516]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.4766168696802363}
episode index:3967
target Thresh 19.0
target distance 2.0
model initialize at round 3967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 4.00286942, 11.00284295,  1.79076451]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.2399062255563438}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8245311905936775
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.73902579, 9.87861865, 5.79076451]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.7489276015692464}
episode index:3968
target Thresh 19.0
target distance 1.0
model initialize at round 3968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.11356148, 5.99079428, 0.92911309]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.997281058105217}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8245754004222001
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.11356148, 5.99079428, 0.92911309]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.997281058105217}
episode index:3969
target Thresh 19.0
target distance 3.0
model initialize at round 3969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.06714829, 10.04178615,  1.56665295]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 3.181615206052147}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.824607242903177
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.02934688,  7.21390335,  5.28346764]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9939427170143356}
episode index:3970
target Thresh 19.0
target distance 3.0
model initialize at round 3970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.00520171,  7.00435593,  1.70713633]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 3.0043604380008126}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8246390693466413
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.52387878,  3.76454838,  5.42395102]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.5311580519312801}
episode index:3971
target Thresh 19.0
target distance 11.0
model initialize at round 3971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.50955864,  3.78164739,  3.66858459]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.403182412027283}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8246139790310852
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.66725592, 8.92855714, 4.83673151]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1434372875525503}
episode index:3972
target Thresh 19.0
target distance 4.0
model initialize at round 3972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([11.99999492, 10.99999819,  4.49421573]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.000005080970425}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8246457877577071
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.47617719, 11.29035335,  1.92784511]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.5989118522113432}
episode index:3973
target Thresh 19.0
target distance 9.0
model initialize at round 3973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([1.62138245, 3.63982829, 2.80770844]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 11.152280465314618}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8246420338776024
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.03837941, 10.48740399,  0.82541129]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0897104296713256}
episode index:3974
target Thresh 19.0
target distance 6.0
model initialize at round 3974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 9.98285259, 11.20918583,  2.66258562]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 6.1038230186454125}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8246620942653083
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.98335463, 10.67356917,  3.81302969]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6737748129182058}
episode index:3975
target Thresh 19.0
target distance 2.0
model initialize at round 3975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.293219  , 2.53194689, 0.66273802]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.5672603223024284}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8247036782456239
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.15547878, 4.20913699, 2.66273802]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.15701356454738}
episode index:3976
target Thresh 19.0
target distance 5.0
model initialize at round 3976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 3.04430085, 10.24817389,  3.89752638]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 5.351064699022945}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8247306738879828
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.40716313, 5.45814608, 5.33115577]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6129271094678505}
episode index:3977
target Thresh 19.0
target distance 7.0
model initialize at round 3977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 8.5235961 , 10.19221966,  4.65226793]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 5.942721677333458}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8247506968646346
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.97249458, 8.18779374, 5.80271201]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9904605997132576}
episode index:3978
target Thresh 19.0
target distance 6.0
model initialize at round 3978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.        ,  5.        ,  6.05248356]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.708203933102445}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8247661867301413
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.32881588, 10.11162343,  4.91974233]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.947276526608871}
episode index:3979
target Thresh 19.0
target distance 6.0
model initialize at round 3979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.88251397, 5.59131197, 1.36737507]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.797617035655288}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8247728931583738
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.24928298, 11.51244477,  1.95144854]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9089420672229609}
episode index:3980
target Thresh 19.0
target distance 13.0
model initialize at round 3980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 2.        , 10.        ,  4.03544617]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8247534003679123
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.20388567,  5.8013278 ,  5.4867784 ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8205295032663423}
episode index:3981
target Thresh 19.0
target distance 4.0
model initialize at round 3981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.45335594, 6.59164953, 2.9116196 ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.4696096321965015}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8247899512467753
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.51746293, 8.21117807, 2.6284343 ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9247064693117826}
episode index:3982
target Thresh 19.0
target distance 5.0
model initialize at round 3982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 3.99991842, 10.99994724,  4.72569966]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.000081576490268}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8248168845625323
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.04210211, 10.38689738,  6.15932905]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.1373052328683924}
episode index:3983
target Thresh 19.0
target distance 2.0
model initialize at round 3983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([13.99421532,  2.00409864,  3.53516245]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.239417713628618}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8248558612481341
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.59377065,  2.39196799,  1.25197714]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.7312490710432068}
episode index:3984
target Thresh 19.0
target distance 7.0
model initialize at round 3984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.1942896 , 8.81422581, 0.22817963]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.935617021005797}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8248781100276161
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.51856944, 3.99599847, 5.66180902]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1229101563491874}
episode index:3985
target Thresh 19.0
target distance 1.0
model initialize at round 3985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.01205981, 2.97467303, 6.16678715]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0123766666850453}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8249170517962996
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.85542833, 3.31291043, 3.88360184]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3446939333662527}
episode index:3986
target Thresh 19.0
target distance 8.0
model initialize at round 3986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.00006823, 11.0000712 ,  1.81671303]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 8.246263780994209}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8249215730256921
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.6300704 ,  3.13392031,  4.4007865 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.3934241438765321}
episode index:3987
target Thresh 19.0
target distance 4.0
model initialize at round 3987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.00002308, 1.99952016, 5.77045441]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 4.123565545808}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.824953184980776
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.7610634 , 5.41386628, 3.20408379]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.6329640096058649}
episode index:3988
target Thresh 19.0
target distance 6.0
model initialize at round 3988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([10.99999507, 11.00000907,  3.07819068]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 7.21111283155451}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8249641633281783
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.37629133,  5.2458567 ,  5.94544946]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.4494893558011664}
episode index:3989
target Thresh 19.0
target distance 1.0
model initialize at round 3989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.1622169 ,  9.74124095,  6.28235579]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.3784724407337414}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8250030444902515
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.61773173,  9.98433918,  3.99917048]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0559605394668496}
episode index:3990
target Thresh 19.0
target distance 13.0
model initialize at round 3990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([15.        ,  5.        ,  0.48358935]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.152946437965907}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8249761658854177
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.21998766, 7.89675889, 3.65173628]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.923347751558921}
episode index:3991
target Thresh 19.0
target distance 11.0
model initialize at round 3991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([15.68565087,  8.33453734,  2.10619017]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 11.916564141379574}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8249604761430483
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.40093662, 6.27460599, 5.84070771]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.4859615473684619}
episode index:3992
target Thresh 19.0
target distance 5.0
model initialize at round 3992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.99998032,  3.99998941,  4.64541507]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.000010595022525}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8249849650532123
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.60614147,  8.24551642,  1.79585915]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9678083250615621}
episode index:3993
target Thresh 19.0
target distance 13.0
model initialize at round 3993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([3.00000021, 2.99999968, 0.02228039]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 13.341663931312132}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.8249345244998417
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.60565038,  6.57007241,  6.05768609]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.6931768725041522}
episode index:3994
target Thresh 19.0
target distance 9.0
model initialize at round 3994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 7.       , 11.       ,  0.7555291]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 9.848857802354832}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8249369223063374
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.17888322, 1.96843376, 5.33960257]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.18164700829874086}
episode index:3995
target Thresh 19.0
target distance 8.0
model initialize at round 3995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.29271437, 10.13728867,  5.92672253]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.885590298655192}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8249372305399534
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.61862799, 2.95039774, 6.22761068]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6206133851243029}
episode index:3996
target Thresh 19.0
target distance 11.0
model initialize at round 3996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.41514793,  7.56624377,  3.80844271]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.724617216526987}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8249375386193368
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.88706746, 10.27082641,  4.10933087]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9274888780917574}
episode index:3997
target Thresh 19.0
target distance 10.0
model initialize at round 3997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.26839196,  9.34802281,  5.88344884]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.626690486187199}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8249089282363086
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.47497088, 2.28021353, 4.76841046]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.5951261985780293}
episode index:3998
target Thresh 19.0
target distance 12.0
model initialize at round 3998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.26912036,  3.66130002,  2.42019761]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.286929318801112}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8248649656707001
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.86347388, 3.46669243, 0.45560331]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9815237957768012}
episode index:3999
target Thresh 19.0
target distance 11.0
model initialize at round 3999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.        , 6.        , 0.19401902]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.180339886049232}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8248512602157337
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.3813309 ,  8.61034799,  1.92853656]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8690662387050385}

Process finished with exit code 0
