/home/yangyutu123/anaconda3/bin/python3.7 /home/yangyutu123/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/StaticObstacle/FullControl/SingleObstacle/DDPGHER_CNN.py
episode index:0
model initialize at round 0
at step 0:
{'currentState': array([15.00720917,  6.85342807,  4.7414024 ]), 'targetState': array([18,  8], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([27.75838156, 29.78508262,  1.37600838]), 'targetState': array([18,  8], dtype=int32)}
episode index:1
model initialize at round 1
at step 0:
{'currentState': array([ 4.90697818, 10.7277745 ,  4.88763237]), 'targetState': array([ 3, 16], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([ 3.27843372, 14.25228622,  0.56876302]), 'targetState': array([ 3, 16], dtype=int32)}
episode index:2
model initialize at round 2
at step 0:
{'currentState': array([2.88931761, 7.26767988, 2.42459691]), 'targetState': array([7, 5], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([3.47027633, 4.44230847, 4.4087244 ]), 'targetState': array([7, 5], dtype=int32)}
episode index:3
model initialize at round 3
at step 0:
{'currentState': array([ 9.06051679, 28.28128505,  1.86252636]), 'targetState': array([ 8, 24], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([19.43460054, 29.81071127,  1.20295116]), 'targetState': array([ 8, 24], dtype=int32)}
episode index:4
model initialize at round 4
at step 0:
{'currentState': array([ 7.97946488, 14.29411551,  1.31491572]), 'targetState': array([ 6, 12], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([ 1.21604229, 29.76248744,  2.84206795]), 'targetState': array([ 6, 12], dtype=int32)}
episode index:5
model initialize at round 5
at step 0:
{'currentState': array([20.95865111, 29.2900293 ,  1.33211333]), 'targetState': array([18, 29], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([27.78025014, 29.77083272,  0.61733907]), 'targetState': array([18, 29], dtype=int32)}
episode index:6
model initialize at round 6
at step 0:
{'currentState': array([12.96209323, 21.71482005,  4.07589883]), 'targetState': array([19, 16], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([1.22495617, 7.92274994, 3.170697  ]), 'targetState': array([19, 16], dtype=int32)}
episode index:7
model initialize at round 7
at step 0:
{'currentState': array([16.71195458, 12.99676872,  2.65630966]), 'targetState': array([14, 10], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([12.30329533, 29.79384876,  1.57447729]), 'targetState': array([14, 10], dtype=int32)}
episode index:8
model initialize at round 8
at step 0:
{'currentState': array([19.74682171,  3.13710724,  3.14478868]), 'targetState': array([26, 10], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([10.18371801, 29.67416894,  6.0024854 ]), 'targetState': array([26, 10], dtype=int32)}
episode index:9
model initialize at round 9
at step 0:
{'currentState': array([6.06956291, 6.71992197, 4.47048014]), 'targetState': array([9, 3], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([11.4299969 ,  2.46759907,  1.13808835]), 'targetState': array([9, 3], dtype=int32)}
episode index:10
model initialize at round 10
at step 0:
{'currentState': array([18.77784438, 18.18323956,  2.9502486 ]), 'targetState': array([26, 24], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([ 1.23867356, 29.78347851,  2.07207939]), 'targetState': array([26, 24], dtype=int32)}
episode index:11
model initialize at round 11
at step 0:
{'currentState': array([ 8.95757335, 28.29637292,  1.82418984]), 'targetState': array([ 2, 23], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([ 1.18371617, 29.52347536,  3.138056  ]), 'targetState': array([ 2, 23], dtype=int32)}
episode index:12
model initialize at round 12
at step 0:
{'currentState': array([5.78424654, 9.796976  , 4.173473  ]), 'targetState': array([ 3, 17], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([13.14296109,  7.68272872,  1.0779042 ]), 'targetState': array([ 3, 17], dtype=int32)}
episode index:13
model initialize at round 13
at step 0:
{'currentState': array([26.80892535, 19.2162969 ,  1.80942279]), 'targetState': array([23, 23], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([23.95583368, 29.30606389,  2.87078499]), 'targetState': array([23, 23], dtype=int32)}
episode index:14
model initialize at round 14
at step 0:
{'currentState': array([11.76775072, 29.18875464,  2.33782402]), 'targetState': array([ 9, 26], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([ 3.35545591, 26.61856072,  4.38317545]), 'targetState': array([ 9, 26], dtype=int32)}
episode index:15
model initialize at round 15
at step 0:
{'currentState': array([17.95959499, 27.70505998,  4.3591795 ]), 'targetState': array([20, 28], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([24.46432553, 29.81549287,  1.7082621 ]), 'targetState': array([20, 28], dtype=int32)}
episode index:16
model initialize at round 16
at step 0:
{'currentState': array([ 8.27781378, 22.91533202,  5.54331521]), 'targetState': array([12, 19], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([ 1.21752295, 24.4692102 ,  4.67571136]), 'targetState': array([12, 19], dtype=int32)}
episode index:17
model initialize at round 17
at step 0:
{'currentState': array([12.21669925, 29.20256115,  1.01434252]), 'targetState': array([18, 25], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([18.53834344, 25.30546265,  5.26203067]), 'targetState': array([18, 25], dtype=int32)}
episode index:18
model initialize at round 18
at step 0:
{'currentState': array([12.70063436,  5.9896707 ,  3.27258559]), 'targetState': array([19, 11], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([18.93616487, 11.43366809,  0.1547615 ]), 'targetState': array([19, 11], dtype=int32)}
episode index:19
model initialize at round 19
at step 0:
{'currentState': array([11.27359601, 23.87966941,  6.01952827]), 'targetState': array([18, 24], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([ 4.79785238, 25.69268825,  5.26965727]), 'targetState': array([18, 24], dtype=int32)}
episode index:20
model initialize at round 20
at step 0:
{'currentState': array([19.89647922, 21.28121876,  2.00590323]), 'targetState': array([24, 18], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([20.22630851, 22.04445896,  3.87598314]), 'targetState': array([24, 18], dtype=int32)}
episode index:21
model initialize at round 21
at step 0:
{'currentState': array([27.26194112, 14.88111492,  5.35214382]), 'targetState': array([25,  7], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([20.43751428,  2.08111087,  5.5193691 ]), 'targetState': array([25,  7], dtype=int32)}
episode index:22
model initialize at round 22
at step 0:
{'currentState': array([13.71248514, 10.99048369,  2.67000616]), 'targetState': array([ 9, 12], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([13.08119929, 10.73353337,  1.04243602]), 'targetState': array([ 9, 12], dtype=int32)}
episode index:23
model initialize at round 23
at step 0:
{'currentState': array([13.74384416,  8.15156407,  2.38744569]), 'targetState': array([12,  5], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([12.49698529, 10.5233702 ,  2.82451027]), 'targetState': array([12,  5], dtype=int32)}
episode index:24
model initialize at round 24
at step 0:
{'currentState': array([9.7135899 , 5.9732301 , 2.72982848]), 'targetState': array([9, 9], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([ 7.30537615, 13.96616235,  3.16472367]), 'targetState': array([9, 9], dtype=int32)}
episode index:25
model initialize at round 25
at step 0:
{'currentState': array([ 9.87925081, 16.26108602,  1.49898672]), 'targetState': array([ 2, 22], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([27.7844615 , 29.76076352,  0.24068405]), 'targetState': array([ 2, 22], dtype=int32)}
episode index:26
model initialize at round 26
at step 0:
{'currentState': array([17.23544732, 18.17824947,  0.9580667 ]), 'targetState': array([16, 24], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([21.48101923, 24.3078454 ,  1.81346802]), 'targetState': array([16, 24], dtype=int32)}
episode index:27
model initialize at round 27
at step 0:
{'currentState': array([23.70925934, 16.04547197,  3.32921094]), 'targetState': array([22,  9], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([18.05229588, 19.1562037 ,  1.78718158]), 'targetState': array([22,  9], dtype=int32)}
episode index:28
model initialize at round 28
at step 0:
{'currentState': array([20.72532573, 14.91377217,  2.94565594]), 'targetState': array([24, 12], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([ 1.22599398, 29.77748105,  2.14133513]), 'targetState': array([24, 12], dtype=int32)}
episode index:29
model initialize at round 29
at step 0:
{'currentState': array([ 9.87508499, 11.74086719,  4.76794124]), 'targetState': array([ 4, 17], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([27.77485006,  1.22395648,  5.65424973]), 'targetState': array([ 4, 17], dtype=int32)}
episode index:30
model initialize at round 30
at step 0:
{'currentState': array([16.04200961,  1.71542719,  4.35396159]), 'targetState': array([10,  7], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([1.19260663, 1.59015533, 3.5098005 ]), 'targetState': array([10,  7], dtype=int32)}
episode index:31
model initialize at round 31
at step 0:
{'currentState': array([1.72926463, 1.90279588, 2.981296  ]), 'targetState': array([7, 6], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([27.76101785, 13.10406832,  5.0238057 ]), 'targetState': array([7, 6], dtype=int32)}
episode index:32
model initialize at round 32
at step 0:
{'currentState': array([3.73570609, 5.88643931, 3.04245692]), 'targetState': array([6, 5], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([5.45704745, 5.68384574, 4.13178958]), 'targetState': array([6, 5], dtype=int32)}
episode index:33
model initialize at round 33
at step 0:
{'currentState': array([20.26831992, 17.10393945,  0.8726713 ]), 'targetState': array([21, 14], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([4.29677562, 5.09336516, 2.13558316]), 'targetState': array([21, 14], dtype=int32)}
episode index:34
model initialize at round 34
at step 0:
{'currentState': array([20.28075412,  3.93734066,  5.55870235]), 'targetState': array([25, 11], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([ 5.98454516, 29.7687514 ,  0.79927022]), 'targetState': array([25, 11], dtype=int32)}
episode index:35
model initialize at round 35
at step 0:
{'currentState': array([ 8.25797404, 13.86836851,  5.34865987]), 'targetState': array([ 8, 16], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([12.18336623,  3.75935507,  4.11159866]), 'targetState': array([ 8, 16], dtype=int32)}
episode index:36
model initialize at round 36
at step 0:
{'currentState': array([26.24387033, 19.15255668,  0.05400538]), 'targetState': array([20, 28], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([15.62254646, 29.80451469,  1.64404202]), 'targetState': array([20, 28], dtype=int32)}
episode index:37
model initialize at round 37
at step 0:
{'currentState': array([ 8.74964449, 19.85782004,  4.15777582]), 'targetState': array([ 7, 23], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([ 7.82440725, 23.0673607 ,  5.37115235]), 'targetState': array([ 7, 23], dtype=int32)}
episode index:38
model initialize at round 38
at step 0:
{'currentState': array([6.0089591 , 3.287517  , 1.03464675]), 'targetState': array([2, 8], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([1.42386894, 8.39529316, 1.72474603]), 'targetState': array([2, 8], dtype=int32)}
episode index:39
model initialize at round 39
at step 0:
{'currentState': array([25.88580816,  5.26402015,  1.47401738]), 'targetState': array([19,  3], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([19.43695232,  3.68579518,  5.65895145]), 'targetState': array([19,  3], dtype=int32)}
episode index:40
model initialize at round 40
at step 0:
{'currentState': array([12.74480756,  8.85967338,  3.21944809]), 'targetState': array([11,  7], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([10.45021859,  7.0499318 ,  1.3016225 ]), 'targetState': array([11,  7], dtype=int32)}
episode index:41
model initialize at round 41
at step 0:
{'currentState': array([13.72446152, 10.08261248,  2.34529924]), 'targetState': array([22, 17], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([21.84568342, 17.28760921,  0.48743646]), 'targetState': array([22, 17], dtype=int32)}
episode index:42
model initialize at round 42
at step 0:
{'currentState': array([ 3.80864244, 12.7852244 ,  3.47958803]), 'targetState': array([ 2, 16], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([ 1.99695053, 15.67207017,  3.11419848]), 'targetState': array([ 2, 16], dtype=int32)}
episode index:43
model initialize at round 43
at step 0:
{'currentState': array([9.12198073, 9.26051294, 0.62788439]), 'targetState': array([18, 17], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([18.0061733 , 17.41546003,  5.80856854]), 'targetState': array([18, 17], dtype=int32)}
episode index:44
model initialize at round 44
at step 0:
{'currentState': array([26.89254122, 11.26953714,  2.40028328]), 'targetState': array([18, 16], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([17.79527388, 16.22812211,  0.74484726]), 'targetState': array([18, 16], dtype=int32)}
episode index:45
model initialize at round 45
at step 0:
{'currentState': array([ 2.05456127, 26.71229732,  4.51595271]), 'targetState': array([ 4, 20], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([ 4.26039418, 20.14762456,  5.30004853]), 'targetState': array([ 4, 20], dtype=int32)}
episode index:46
model initialize at round 46
at step 0:
{'currentState': array([24.24101701,  8.15702638,  1.08243252]), 'targetState': array([23, 13], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([23.10450373, 12.60094103,  3.72534822]), 'targetState': array([23, 13], dtype=int32)}
episode index:47
model initialize at round 47
at step 0:
{'currentState': array([10.19521189, 27.78329083,  5.85968709]), 'targetState': array([14, 26], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([14.30580154, 26.07765603,  1.8408921 ]), 'targetState': array([14, 26], dtype=int32)}
episode index:48
model initialize at round 48
at step 0:
{'currentState': array([ 2.02579452, 12.71350232,  5.30718082]), 'targetState': array([ 8, 11], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([ 8.12025573, 11.29126099,  2.75510298]), 'targetState': array([ 8, 11], dtype=int32)}
episode index:49
model initialize at round 49
at step 0:
{'currentState': array([23.03721299, 17.71476068,  4.33711863]), 'targetState': array([19,  9], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.0
{'currentState': array([18.52116541,  8.80766351,  1.85837491]), 'targetState': array([19,  9], dtype=int32)}
episode index:50
model initialize at round 50
at step 0:
{'currentState': array([13.20477775, 11.20202066,  0.27362072]), 'targetState': array([21,  4], dtype=int32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.013117093305287849
{'currentState': array([21.00838193,  4.09744869,  2.07253324]), 'targetState': array([21,  4], dtype=int32)}
episode index:51
model initialize at round 51
at step 0:
{'currentState': array([17.89835089, 17.269098  ,  1.42696714]), 'targetState': array([21, 15], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.029075287537569366
{'currentState': array([21.08496428, 14.92849347,  1.52097802]), 'targetState': array([21, 15], dtype=int32)}
episode index:52
model initialize at round 52
at step 0:
{'currentState': array([ 7.26656245, 18.89187615,  0.11965459]), 'targetState': array([ 6, 29], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.04127630190592245
{'currentState': array([ 5.90355704, 29.03636295,  4.41970083]), 'targetState': array([ 6, 29], dtype=int32)}
episode index:53
model initialize at round 53
at step 0:
{'currentState': array([ 7.05655274, 21.29227108,  1.59691787]), 'targetState': array([ 6, 28], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.05462938361779618
{'currentState': array([ 5.93966573, 27.97762626,  4.42704016]), 'targetState': array([ 6, 28], dtype=int32)}
episode index:54
model initialize at round 54
at step 0:
{'currentState': array([20.76829348, 12.82953487,  3.27087498]), 'targetState': array([24, 21], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.06655527168424155
{'currentState': array([23.96377708, 21.06475882,  3.62977938]), 'targetState': array([24, 21], dtype=int32)}
episode index:55
model initialize at round 55
at step 0:
{'currentState': array([13.76042311, 19.84078563,  4.23313767]), 'targetState': array([13, 12], dtype=int32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.07870914351723002
{'currentState': array([12.92014945, 12.04108722,  6.27014503]), 'targetState': array([13, 12], dtype=int32)}
episode index:56
model initialize at round 56
at step 0:
{'currentState': array([20.72238541, 17.075342  ,  2.37158513]), 'targetState': array([26, 24], dtype=int32)}
done in step count: 140
reward sum = 0.24486529903492948
running average episode reward sum: 0.08162416378947035
{'currentState': array([26.05897018, 23.9496063 ,  3.54449815]), 'targetState': array([26, 24], dtype=int32)}
episode index:57
model initialize at round 57
at step 0:
{'currentState': array([ 9.78157126, 12.8128207 ,  4.35506576]), 'targetState': array([6, 4], dtype=int32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.09175084645809466
{'currentState': array([5.97531179, 4.06180801, 5.91526267]), 'targetState': array([6, 4], dtype=int32)}
episode index:58
model initialize at round 58
at step 0:
{'currentState': array([ 3.14503317, 21.7515817 ,  4.73583126]), 'targetState': array([ 6, 12], dtype=int32)}
done in step count: 147
reward sum = 0.22823046013534068
running average episode reward sum: 0.09406406024923442
{'currentState': array([ 6.00989285, 11.9863324 ,  1.11019742]), 'targetState': array([ 6, 12], dtype=int32)}
episode index:59
model initialize at round 59
at step 0:
{'currentState': array([ 7.2593932 , 20.12434405,  6.22518915]), 'targetState': array([ 5, 15], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.1019896792863665
{'currentState': array([ 5.03357667, 14.95656217,  4.05268064]), 'targetState': array([ 5, 15], dtype=int32)}
episode index:60
model initialize at round 60
at step 0:
{'currentState': array([ 6.74548848, 12.15686583,  2.44419318]), 'targetState': array([ 3, 13], dtype=int32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.11441703462005373
{'currentState': array([ 3.05397155, 13.03400723,  5.3396919 ]), 'targetState': array([ 3, 13], dtype=int32)}
episode index:61
model initialize at round 61
at step 0:
{'currentState': array([20.01541518, 20.71275682,  4.26100349]), 'targetState': array([14, 26], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.12391752914230397
{'currentState': array([13.90301066, 26.08802262,  6.13413241]), 'targetState': array([14, 26], dtype=int32)}
episode index:62
model initialize at round 62
at step 0:
{'currentState': array([20.03737376, 17.29402401,  1.17246246]), 'targetState': array([20, 27], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.1316508578786621
{'currentState': array([19.91615243, 26.99158318,  3.23741113]), 'targetState': array([20, 27], dtype=int32)}
episode index:63
model initialize at round 63
at step 0:
{'currentState': array([13.08940628, 27.72659043,  4.52343321]), 'targetState': array([ 5, 27], dtype=int32)}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.13624365890225573
{'currentState': array([ 4.95417053, 26.92403897,  2.52106681]), 'targetState': array([ 5, 27], dtype=int32)}
episode index:64
model initialize at round 64
at step 0:
{'currentState': array([19.28538847, 25.03605125,  5.90384317]), 'targetState': array([10, 19], dtype=int32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.14265042533702693
{'currentState': array([ 9.90817743, 19.07635605,  1.62954567]), 'targetState': array([10, 19], dtype=int32)}
episode index:65
model initialize at round 65
at step 0:
{'currentState': array([ 5.16215981, 22.23759308,  0.46690488]), 'targetState': array([ 8, 25], dtype=int32)}
done in step count: 108
reward sum = 0.337754400898902
running average episode reward sum: 0.14560654617887353
{'currentState': array([ 7.91827777, 25.07302116,  0.85748034]), 'targetState': array([ 8, 25], dtype=int32)}
episode index:66
model initialize at round 66
at step 0:
{'currentState': array([13.05785448,  7.28177852,  0.86329162]), 'targetState': array([20, 17], dtype=int32)}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.1502483850082025
{'currentState': array([20.02197744, 16.91049608,  4.67763337]), 'targetState': array([20, 17], dtype=int32)}
episode index:67
model initialize at round 67
at step 0:
{'currentState': array([ 7.73908507, 13.12111844,  2.2019875 ]), 'targetState': array([ 2, 23], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.14803884993455246
{'currentState': array([ 2.1675847 , 23.38690717,  5.93667901]), 'targetState': array([ 2, 23], dtype=int32)}
episode index:68
model initialize at round 68
at step 0:
{'currentState': array([11.93309618,  8.70770019,  4.54105606]), 'targetState': array([12,  4], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.14589335935579084
{'currentState': array([11.91451752,  3.73757287,  2.49741041]), 'targetState': array([12,  4], dtype=int32)}
episode index:69
model initialize at round 69
at step 0:
{'currentState': array([23.8553154 , 11.24862148,  2.60283601]), 'targetState': array([17,  4], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.1531757288017673
{'currentState': array([17.06999334,  3.93012033,  4.7390175 ]), 'targetState': array([17,  4], dtype=int32)}
episode index:70
model initialize at round 70
at step 0:
{'currentState': array([25.85843181,  6.26303726,  2.22661984]), 'targetState': array([23, 15], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.1510183241707565
{'currentState': array([22.759439 , 14.9184772,  1.5601448]), 'targetState': array([23, 15], dtype=int32)}
episode index:71
model initialize at round 71
at step 0:
{'currentState': array([11.71297334,  7.98097447,  3.71278071]), 'targetState': array([3, 2], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.14892084744616266
{'currentState': array([3.22784817, 2.3233385 , 5.68166919]), 'targetState': array([3, 2], dtype=int32)}
episode index:72
model initialize at round 72
at step 0:
{'currentState': array([ 9.14273975, 27.75025685,  5.73662376]), 'targetState': array([18, 29], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.15516859018166093
{'currentState': array([17.91643733, 28.9408704 ,  1.72531643]), 'targetState': array([18, 29], dtype=int32)}
episode index:73
model initialize at round 73
at step 0:
{'currentState': array([12.96446491,  3.2854532 ,  2.19964576]), 'targetState': array([5, 2], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.1530717173413682
{'currentState': array([5.16051555, 1.7071592 , 3.66738499]), 'targetState': array([5, 2], dtype=int32)}
episode index:74
model initialize at round 74
at step 0:
{'currentState': array([ 4.1354359 , 19.25377823,  0.57557058]), 'targetState': array([15, 11], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.15103076111014999
{'currentState': array([14.78372482, 11.13777309,  0.86876833]), 'targetState': array([15, 11], dtype=int32)}
episode index:75
model initialize at round 75
at step 0:
{'currentState': array([21.93740519, 22.70670408,  4.45718573]), 'targetState': array([23, 20], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.16013487206112076
{'currentState': array([23.06373683, 20.04950624,  0.55909508]), 'targetState': array([23, 20], dtype=int32)}
episode index:76
model initialize at round 76
at step 0:
{'currentState': array([10.74278172, 22.87121712,  4.11078048]), 'targetState': array([22, 14], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.15805519839798932
{'currentState': array([21.84305302, 13.62053363,  2.68661847]), 'targetState': array([22, 14], dtype=int32)}
episode index:77
model initialize at round 77
at step 0:
{'currentState': array([4.10758641, 9.73322024, 5.60071802]), 'targetState': array([ 5, 10], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.16410351163908216
{'currentState': array([ 4.96663681, 10.09573933,  0.65859636]), 'targetState': array([ 5, 10], dtype=int32)}
episode index:78
model initialize at round 78
at step 0:
{'currentState': array([11.25831586,  9.87343307,  5.32259413]), 'targetState': array([22,  5], dtype=int32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.1693827888856521
{'currentState': array([21.90631262,  5.09418137,  0.69510587]), 'targetState': array([22,  5], dtype=int32)}
episode index:79
model initialize at round 79
at step 0:
{'currentState': array([23.77239263, 15.17590099,  1.97864199]), 'targetState': array([16, 20], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.1757974364622113
{'currentState': array([15.9223296 , 20.05054353,  5.50886084]), 'targetState': array([16, 20], dtype=int32)}
episode index:80
model initialize at round 80
at step 0:
{'currentState': array([ 2.01185722, 20.71258796,  5.25862074]), 'targetState': array([10, 26], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.1736270977404556
{'currentState': array([ 9.67284374, 26.22232872,  0.99257115]), 'targetState': array([10, 26], dtype=int32)}
episode index:81
model initialize at round 81
at step 0:
{'currentState': array([12.01513618, 10.28725802,  2.02315307]), 'targetState': array([ 4, 11], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.17150969410947442
{'currentState': array([ 4.05005441, 10.69751313,  3.32787244]), 'targetState': array([ 4, 11], dtype=int32)}
episode index:82
model initialize at round 82
at step 0:
{'currentState': array([26.72260906, 18.07616128,  3.37863278]), 'targetState': array([23,  8], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.16944331225273376
{'currentState': array([22.55540331,  8.39451592,  3.74276972]), 'targetState': array([23,  8], dtype=int32)}
episode index:83
model initialize at round 83
at step 0:
{'currentState': array([ 6.28348451, 18.95118601,  5.60766471]), 'targetState': array([ 3, 10], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.16742612996401074
{'currentState': array([3.04868272, 9.69927223, 6.04672157]), 'targetState': array([ 3, 10], dtype=int32)}
episode index:84
model initialize at round 84
at step 0:
{'currentState': array([ 4.7132789 , 13.02317945,  3.56592488]), 'targetState': array([9, 6], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.16545641078796355
{'currentState': array([8.69766223, 5.75909839, 5.45551284]), 'targetState': array([9, 6], dtype=int32)}
episode index:85
model initialize at round 85
at step 0:
{'currentState': array([24.07224423, 24.72156321,  4.4612546 ]), 'targetState': array([13, 19], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.16353249903461514
{'currentState': array([12.85864778, 19.41193276,  3.53774984]), 'targetState': array([13, 19], dtype=int32)}
episode index:86
model initialize at round 86
at step 0:
{'currentState': array([11.10093226, 14.73063231,  5.57589722]), 'targetState': array([17, 18], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.17068359836533473
{'currentState': array([17.03739877, 17.9037575 ,  0.87940533]), 'targetState': array([17, 18], dtype=int32)}
episode index:87
model initialize at round 87
at step 0:
{'currentState': array([10.84353932, 24.75861581,  4.64229345]), 'targetState': array([21, 24], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.1765786607233318
{'currentState': array([21.07160491, 24.01062071,  0.94760508]), 'targetState': array([21, 24], dtype=int32)}
episode index:88
model initialize at round 88
at step 0:
{'currentState': array([26.81435421, 10.21973146,  2.77730799]), 'targetState': array([23,  6], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.18307462281861608
{'currentState': array([22.93421158,  6.03038751,  3.66606247]), 'targetState': array([23,  6], dtype=int32)}
episode index:89
model initialize at round 89
at step 0:
{'currentState': array([ 6.83828256, 19.7621056 ,  4.62035817]), 'targetState': array([ 4, 13], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.18909579740905202
{'currentState': array([ 3.90023859, 13.04500294,  3.63624797]), 'targetState': array([ 4, 13], dtype=int32)}
episode index:90
model initialize at round 90
at step 0:
{'currentState': array([18.92705771, 13.72174527,  3.95101547]), 'targetState': array([11,  8], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.19474801606389286
{'currentState': array([10.96885754,  8.08786259,  4.14915054]), 'targetState': array([11,  8], dtype=int32)}
episode index:91
model initialize at round 91
at step 0:
{'currentState': array([24.27682359, 13.07819829,  0.78031081]), 'targetState': array([13, 18], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.19927376849290343
{'currentState': array([13.09609071, 17.97082835,  2.29889006]), 'targetState': array([13, 18], dtype=int32)}
episode index:92
model initialize at round 92
at step 0:
{'currentState': array([ 9.77820135, 28.18317105,  2.95629072]), 'targetState': array([ 4, 23], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.20454447083028168
{'currentState': array([ 3.91264045, 22.97999446,  4.23428148]), 'targetState': array([ 4, 23], dtype=int32)}
episode index:93
model initialize at round 93
at step 0:
{'currentState': array([14.75554196, 21.84838687,  4.20173359]), 'targetState': array([12, 17], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.2104784946974819
{'currentState': array([11.9274401 , 16.95599056,  4.6963571 ]), 'targetState': array([12, 17], dtype=int32)}
episode index:94
model initialize at round 94
at step 0:
{'currentState': array([12.28755179, 19.00776146,  0.53198498]), 'targetState': array([12, 29], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.21516460760144676
{'currentState': array([12.08781544, 28.98966348,  1.71214292]), 'targetState': array([12, 29], dtype=int32)}
episode index:95
model initialize at round 95
at step 0:
{'currentState': array([24.72976445,  8.0985851 ,  2.28678393]), 'targetState': array([26, 19], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.21996215386664297
{'currentState': array([26.00186072, 18.96640729,  1.31408855]), 'targetState': array([26, 19], dtype=int32)}
episode index:96
model initialize at round 96
at step 0:
{'currentState': array([13.71467457, 10.96345314,  2.7639873 ]), 'targetState': array([ 5, 20], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.22431936467301486
{'currentState': array([ 5.00093674, 20.05808651,  3.12902958]), 'targetState': array([ 5, 20], dtype=int32)}
episode index:97
model initialize at round 97
at step 0:
{'currentState': array([ 5.8218923 , 11.22588475,  1.73347902]), 'targetState': array([15, 23], dtype=int32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.2276700188820901
{'currentState': array([15.0383064 , 23.07827248,  0.97734809]), 'targetState': array([15, 23], dtype=int32)}
episode index:98
model initialize at round 98
at step 0:
{'currentState': array([12.92663865, 11.72185546,  4.95950913]), 'targetState': array([15, 10], dtype=int32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.23397090526807782
{'currentState': array([15.04394297, 10.01820677,  0.4866178 ]), 'targetState': array([15, 10], dtype=int32)}
episode index:99
model initialize at round 99
at step 0:
{'currentState': array([13.76744904, 20.83068868,  3.26590443]), 'targetState': array([ 2, 18], dtype=int32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.2378040976248199
{'currentState': array([ 1.98962631, 18.09959155,  4.97871023]), 'targetState': array([ 2, 18], dtype=int32)}
episode index:100
model initialize at round 100
at step 0:
{'currentState': array([17.27275409, 21.90861355,  5.45488993]), 'targetState': array([22, 19], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.24213998823309182
{'currentState': array([22.05020376, 18.90855566,  0.42053439]), 'targetState': array([22, 19], dtype=int32)}
episode index:101
model initialize at round 101
at step 0:
{'currentState': array([ 5.24381828, 23.84736016,  6.22883892]), 'targetState': array([13, 26], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.24716527547790104
{'currentState': array([12.99189989, 25.92447665,  6.14839017]), 'targetState': array([13, 26], dtype=int32)}
episode index:102
model initialize at round 102
at step 0:
{'currentState': array([ 6.05594832, 23.28216318,  0.87005174]), 'targetState': array([14, 15], dtype=int32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.25119553533718336
{'currentState': array([14.08506145, 14.90133761,  6.28207702]), 'targetState': array([14, 15], dtype=int32)}
episode index:103
model initialize at round 103
at step 0:
{'currentState': array([12.2808095 , 23.06238828,  0.72362202]), 'targetState': array([21, 28], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.2554764745940333
{'currentState': array([21.08586252, 27.99655283,  1.46418959]), 'targetState': array([21, 28], dtype=int32)}
episode index:104
model initialize at round 104
at step 0:
{'currentState': array([18.17051995, 12.23166618,  0.43127739]), 'targetState': array([18,  7], dtype=int32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.26037710955794874
{'currentState': array([17.96433976,  6.97180673,  4.86342542]), 'targetState': array([18,  7], dtype=int32)}
episode index:105
model initialize at round 105
at step 0:
{'currentState': array([17.71406293, 14.96859516,  2.74598551]), 'targetState': array([14, 19], dtype=int32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.26518527971122424
{'currentState': array([13.91040306, 18.9973185 ,  3.79651931]), 'targetState': array([14, 19], dtype=int32)}
episode index:106
model initialize at round 106
at step 0:
{'currentState': array([20.74862848, 28.13985225,  3.13888156]), 'targetState': array([18, 29], dtype=int32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.27115908153643525
{'currentState': array([18.02980347, 29.0605156 ,  2.49804449]), 'targetState': array([18, 29], dtype=int32)}
episode index:107
model initialize at round 107
at step 0:
{'currentState': array([19.87793833,  6.260475  ,  2.51401961]), 'targetState': array([15,  5], dtype=int32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.2763753285726728
{'currentState': array([14.91353313,  5.00444054,  3.10932666]), 'targetState': array([15,  5], dtype=int32)}
episode index:108
model initialize at round 108
at step 0:
{'currentState': array([ 4.03200842, 24.71412986,  4.31889296]), 'targetState': array([ 3, 22], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.2815731988920421
{'currentState': array([ 2.93903449, 21.92083399,  5.09576416]), 'targetState': array([ 3, 22], dtype=int32)}
episode index:109
model initialize at round 109
at step 0:
{'currentState': array([ 5.73708445, 14.88328802,  4.06437421]), 'targetState': array([7, 4], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.28515643389357154
{'currentState': array([6.97385509, 3.9844309 , 4.74227071]), 'targetState': array([7, 4], dtype=int32)}
episode index:110
model initialize at round 110
at step 0:
{'currentState': array([7.28252447, 4.94590567, 0.31582182]), 'targetState': array([6, 9], dtype=int32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.2903357304768843
{'currentState': array([6.09716341, 8.93480129, 1.80081629]), 'targetState': array([6, 9], dtype=int32)}
episode index:111
model initialize at round 111
at step 0:
{'currentState': array([20.21929802, 15.81381351,  6.0838781 ]), 'targetState': array([25, 12], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.2949731602781734
{'currentState': array([24.93637334, 12.021664  ,  5.96708862]), 'targetState': array([25, 12], dtype=int32)}
episode index:112
model initialize at round 112
at step 0:
{'currentState': array([23.28764393, 22.99730825,  0.49564236]), 'targetState': array([23, 25], dtype=int32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.3006111442394903
{'currentState': array([23.06901197, 24.91729353,  2.48334792]), 'targetState': array([23, 25], dtype=int32)}
episode index:113
model initialize at round 113
at step 0:
{'currentState': array([27.28726299,  5.01504149,  0.55731362]), 'targetState': array([26, 11], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.30459454900233374
{'currentState': array([26.06050697, 11.06706637,  2.19950264]), 'targetState': array([26, 11], dtype=int32)}
episode index:114
model initialize at round 114
at step 0:
{'currentState': array([16.17802215, 24.24086949,  1.0336919 ]), 'targetState': array([18, 29], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.30850867716060587
{'currentState': array([17.95206497, 29.05617341,  1.98453261]), 'targetState': array([18, 29], dtype=int32)}
episode index:115
model initialize at round 115
at step 0:
{'currentState': array([20.1102099 , 19.2657067 ,  0.67261362]), 'targetState': array([22, 23], dtype=int32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.3131892727979703
{'currentState': array([22.08487922, 22.90163576,  1.84827641]), 'targetState': array([22, 23], dtype=int32)}
episode index:116
model initialize at round 116
at step 0:
{'currentState': array([17.72035048, 12.93260252,  3.88308954]), 'targetState': array([9, 2], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.3153321438890337
{'currentState': array([8.93794147, 2.03283435, 5.07026729]), 'targetState': array([9, 2], dtype=int32)}
episode index:117
model initialize at round 117
at step 0:
{'currentState': array([17.82741108,  5.23012904,  2.71927422]), 'targetState': array([10, 17], dtype=int32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.31720448356122144
{'currentState': array([10.05545632, 17.09405139,  2.15330582]), 'targetState': array([10, 17], dtype=int32)}
episode index:118
model initialize at round 118
at step 0:
{'currentState': array([14.2861718 , 23.02918861,  0.60664528]), 'targetState': array([27, 29], dtype=int32)}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.31900029078301884
{'currentState': array([27.0783262 , 28.9372764 ,  0.78052621]), 'targetState': array([27, 29], dtype=int32)}
episode index:119
model initialize at round 119
at step 0:
{'currentState': array([15.28629742, 26.03702996,  5.92850316]), 'targetState': array([22, 28], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.32108863171380336
{'currentState': array([22.07536565, 27.91816503,  0.6797388 ]), 'targetState': array([22, 28], dtype=int32)}
episode index:120
model initialize at round 120
at step 0:
{'currentState': array([16.15116704,  9.24473414,  0.51247919]), 'targetState': array([19,  5], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.32448710062240327
{'currentState': array([18.95777729,  4.90361652,  5.42891615]), 'targetState': array([19,  5], dtype=int32)}
episode index:121
model initialize at round 121
at step 0:
{'currentState': array([ 9.2581054 , 10.87300442,  0.04774874]), 'targetState': array([16, 17], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.3277698320595791
{'currentState': array([16.02285791, 16.93999379,  1.05731302]), 'targetState': array([16, 17], dtype=int32)}
episode index:122
model initialize at round 122
at step 0:
{'currentState': array([17.27170108, 11.09447115,  6.11281193]), 'targetState': array([27,  3], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.3303295212467754
{'currentState': array([26.95477106,  2.95311699,  6.17109394]), 'targetState': array([27,  3], dtype=int32)}
episode index:123
model initialize at round 123
at step 0:
{'currentState': array([21.93417009,  8.71997733,  3.97649384]), 'targetState': array([21, 21], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.33279610160820355
{'currentState': array([21.05721737, 21.00748363,  1.67770451]), 'targetState': array([21, 21], dtype=int32)}
episode index:124
model initialize at round 124
at step 0:
{'currentState': array([24.9308523, 28.7207781,  3.9646287]), 'targetState': array([24, 27], dtype=int32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.3375902555785939
{'currentState': array([23.91583285, 26.9951862 ,  4.82385376]), 'targetState': array([24, 27], dtype=int32)}
episode index:125
model initialize at round 125
at step 0:
{'currentState': array([18.13752004, 22.74734507,  4.70584536]), 'targetState': array([17, 10], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.339617050743855
{'currentState': array([16.94396649,  9.90996319,  5.26159387]), 'targetState': array([17, 10], dtype=int32)}
episode index:126
model initialize at round 126
at step 0:
{'currentState': array([21.75858259, 10.84359057,  3.21147585]), 'targetState': array([19, 18], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.3425378080393545
{'currentState': array([18.9806051 , 17.97994778,  4.15510427]), 'targetState': array([19, 18], dtype=int32)}
episode index:127
model initialize at round 127
at step 0:
{'currentState': array([17.16019404,  5.76107712,  4.7980299 ]), 'targetState': array([14,  3], dtype=int32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.34658093730968215
{'currentState': array([13.96808497,  2.9776364 ,  4.65643716]), 'targetState': array([14,  3], dtype=int32)}
episode index:128
model initialize at round 128
at step 0:
{'currentState': array([21.76414417,  9.83532364,  4.25611472]), 'targetState': array([23,  9], dtype=int32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.3511925591088397
{'currentState': array([23.0726389 ,  8.92229293,  0.08392095]), 'targetState': array([23,  9], dtype=int32)}
episode index:129
model initialize at round 129
at step 0:
{'currentState': array([12.7280305 ,  7.09369562,  3.31481892]), 'targetState': array([12,  7], dtype=int32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.35551813363479967
{'currentState': array([11.92202675,  7.04817514,  2.79013575]), 'targetState': array([12,  7], dtype=int32)}
episode index:130
model initialize at round 130
at step 0:
{'currentState': array([19.92786632,  7.72153455,  3.95392036]), 'targetState': array([15, 20], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.35737643800755814
{'currentState': array([15.01958053, 20.09339301,  2.35796034]), 'targetState': array([15, 20], dtype=int32)}
episode index:131
model initialize at round 131
at step 0:
{'currentState': array([6.73898539, 6.87909654, 3.07037401]), 'targetState': array([20, 18], dtype=int32)}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.3584178184741691
{'currentState': array([20.02412608, 18.04799695,  3.04795972]), 'targetState': array([20, 18], dtype=int32)}
episode index:132
model initialize at round 132
at step 0:
{'currentState': array([20.13820973, 21.74772169,  4.70857716]), 'targetState': array([10, 28], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.360317814121227
{'currentState': array([ 9.96817413, 27.92767676,  4.52502767]), 'targetState': array([10, 28], dtype=int32)}
episode index:133
model initialize at round 133
at step 0:
{'currentState': array([15.27948861,  5.93193831,  0.26612728]), 'targetState': array([24, 18], dtype=int32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.3619659380018007
{'currentState': array([23.98117298, 18.09561232,  1.83404693]), 'targetState': array([24, 18], dtype=int32)}
episode index:134
model initialize at round 134
at step 0:
{'currentState': array([15.74879796, 11.85984363,  3.14551353]), 'targetState': array([3, 9], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.36363312943836135
{'currentState': array([2.96073635, 9.09364287, 3.73919516]), 'targetState': array([3, 9], dtype=int32)}
episode index:135
model initialize at round 135
at step 0:
{'currentState': array([22.85103142, 14.24607852,  2.6201551 ]), 'targetState': array([12, 12], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.36563719088413715
{'currentState': array([11.94158166, 12.05328904,  3.94824576]), 'targetState': array([12, 12], dtype=int32)}
episode index:136
model initialize at round 136
at step 0:
{'currentState': array([17.24871154, 12.14452973,  0.0214169 ]), 'targetState': array([27,  8], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.36765890191479833
{'currentState': array([26.99113344,  7.99401371,  0.12935724]), 'targetState': array([27,  8], dtype=int32)}
episode index:137
model initialize at round 137
at step 0:
{'currentState': array([16.21735871, 18.81158167,  6.07398748]), 'targetState': array([27, 27], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.3692485966975714
{'currentState': array([27.07964123, 26.97332756,  0.520103  ]), 'targetState': array([27, 27], dtype=int32)}
episode index:138
model initialize at round 138
at step 0:
{'currentState': array([20.74260854, 16.8715636 ,  3.0994339 ]), 'targetState': array([15, 22], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.3720766119324601
{'currentState': array([15.04417125, 22.02412608,  2.87098759]), 'targetState': array([15, 22], dtype=int32)}
episode index:139
model initialize at round 139
at step 0:
{'currentState': array([17.19681737, 23.79021631,  4.96090841]), 'targetState': array([26, 14], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.3739176620701085
{'currentState': array([25.98162953, 14.01080348,  6.02310073]), 'targetState': array([26, 14], dtype=int32)}
episode index:140
model initialize at round 140
at step 0:
{'currentState': array([24.8905559 ,  7.73397695,  3.8170867 ]), 'targetState': array([18, 11], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.37667244967490987
{'currentState': array([18.02898333, 11.07405164,  3.61522789]), 'targetState': array([18, 11], dtype=int32)}
episode index:141
model initialize at round 141
at step 0:
{'currentState': array([21.2134452 , 11.19284041,  1.23972654]), 'targetState': array([ 9, 18], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.37803110286365815
{'currentState': array([ 8.94926715, 18.07042949,  4.16504848]), 'targetState': array([ 9, 18], dtype=int32)}
episode index:142
model initialize at round 142
at step 0:
{'currentState': array([23.71932426, 15.93701268,  2.85734844]), 'targetState': array([23, 28], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.3798813161449243
{'currentState': array([23.0827073 , 28.01811285,  2.59786853]), 'targetState': array([23, 28], dtype=int32)}
episode index:143
model initialize at round 143
at step 0:
{'currentState': array([ 4.2067929 , 22.19995742,  0.26359463]), 'targetState': array([ 4, 13], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.38217764886108657
{'currentState': array([ 4.01712769, 12.96508387,  5.25634461]), 'targetState': array([ 4, 13], dtype=int32)}
episode index:144
model initialize at round 144
at step 0:
{'currentState': array([16.01853831, 17.71294146,  4.27187967]), 'targetState': array([5, 8], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.38363136470619286
{'currentState': array([4.94046882, 8.07012043, 5.32293323]), 'targetState': array([5, 8], dtype=int32)}
episode index:145
model initialize at round 145
at step 0:
{'currentState': array([9.9200461 , 8.72367836, 4.93573046]), 'targetState': array([23,  8], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.3852744745019806
{'currentState': array([23.02898757,  7.98327463,  0.40081395]), 'targetState': array([23,  8], dtype=int32)}
episode index:146
model initialize at round 146
at step 0:
{'currentState': array([14.76601295,  6.83267895,  3.25737238]), 'targetState': array([2, 6], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.3868108198423268
{'currentState': array([2.03663161, 6.04417259, 3.96853569]), 'targetState': array([2, 6], dtype=int32)}
episode index:147
model initialize at round 147
at step 0:
{'currentState': array([ 2.28051504, 25.06369919,  6.00147808]), 'targetState': array([ 8, 17], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.38895025818798384
{'currentState': array([ 7.99350164, 16.96789305,  6.02689439]), 'targetState': array([ 8, 17], dtype=int32)}
episode index:148
model initialize at round 148
at step 0:
{'currentState': array([ 8.08328192, 16.27533688,  0.77207243]), 'targetState': array([ 9, 16], dtype=int32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.39278680685786316
{'currentState': array([ 9.08235426, 16.0471671 ,  0.66258645]), 'targetState': array([ 9, 16], dtype=int32)}
episode index:149
model initialize at round 149
at step 0:
{'currentState': array([19.05232821, 18.28285691,  0.88286567]), 'targetState': array([25,  7], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.39392759608182665
{'currentState': array([25.03872947,  6.92075342,  5.31113193]), 'targetState': array([25,  7], dtype=int32)}
episode index:150
model initialize at round 150
at step 0:
{'currentState': array([ 4.81214615, 14.78215325,  4.50578523]), 'targetState': array([12,  2], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.39524573416341385
{'currentState': array([11.98060662,  1.98233015,  0.38690706]), 'targetState': array([12,  2], dtype=int32)}
episode index:151
model initialize at round 151
at step 0:
{'currentState': array([15.94011636, 24.28135427,  1.27550769]), 'targetState': array([25, 29], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.3969158453094993
{'currentState': array([25.05127057, 29.00562099,  1.0723673 ]), 'targetState': array([25, 29], dtype=int32)}
episode index:152
model initialize at round 152
at step 0:
{'currentState': array([26.72276723, 27.92326498,  2.90662146]), 'targetState': array([26, 28], dtype=int32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.40029232506227147
{'currentState': array([25.9318491 , 28.03005929,  2.92187152]), 'targetState': array([26, 28], dtype=int32)}
episode index:153
model initialize at round 153
at step 0:
{'currentState': array([14.27528157, 24.91653545,  5.48379805]), 'targetState': array([16, 22], dtype=int32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.4034487701704134
{'currentState': array([15.9010527 , 21.9911134 ,  0.15814058]), 'targetState': array([16, 22], dtype=int32)}
episode index:154
model initialize at round 154
at step 0:
{'currentState': array([8.73463843, 5.88837182, 3.03953379]), 'targetState': array([3, 2], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.40586407719769557
{'currentState': array([2.98256398, 1.975442  , 5.28133427]), 'targetState': array([3, 2], dtype=int32)}
episode index:155
model initialize at round 155
at step 0:
{'currentState': array([16.99144314,  4.71247078,  5.18763781]), 'targetState': array([23,  8], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.408248418750269
{'currentState': array([23.02375958,  7.94441217,  1.25819841]), 'targetState': array([23,  8], dtype=int32)}
episode index:156
model initialize at round 156
at step 0:
{'currentState': array([4.28693996, 7.02029127, 5.84878385]), 'targetState': array([4, 2], dtype=int32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.41096348462733795
{'currentState': array([3.98298868, 1.90259749, 5.09237263]), 'targetState': array([4, 2], dtype=int32)}
episode index:157
model initialize at round 157
at step 0:
{'currentState': array([ 3.98692275, 20.71264089,  4.16191196]), 'targetState': array([ 2, 11], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.4128146505157698
{'currentState': array([ 1.90119489, 11.07718212,  5.5480287 ]), 'targetState': array([ 2, 11], dtype=int32)}
episode index:158
model initialize at round 158
at step 0:
{'currentState': array([16.90896301, 11.72712908,  3.88537455]), 'targetState': array([14,  2], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.4145982893052906
{'currentState': array([14.01949206,  1.95536786,  5.71308367]), 'targetState': array([14,  2], dtype=int32)}
episode index:159
model initialize at round 159
at step 0:
{'currentState': array([ 1.80589141, 19.21229256,  1.80648041]), 'targetState': array([ 8, 25], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.4165839460574725
{'currentState': array([ 8.03035505, 24.97803701,  1.7303212 ]), 'targetState': array([ 8, 25], dtype=int32)}
episode index:160
model initialize at round 160
at step 0:
{'currentState': array([21.28752528, 19.00868831,  5.80839366]), 'targetState': array([9, 8], dtype=int32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.4172610425883302
{'currentState': array([8.95599544, 8.05721517, 4.10693569]), 'targetState': array([9, 8], dtype=int32)}
episode index:161
model initialize at round 161
at step 0:
{'currentState': array([17.72931499, 24.90265576,  3.99181342]), 'targetState': array([11, 14], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.41857315733286665
{'currentState': array([10.95201826, 14.08451688,  4.49917153]), 'targetState': array([11, 14], dtype=int32)}
episode index:162
model initialize at round 162
at step 0:
{'currentState': array([19.23251181, 13.16936508,  0.12454295]), 'targetState': array([23,  2], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.41971691751571744
{'currentState': array([23.01772183,  1.99657195,  1.35049397]), 'targetState': array([23,  2], dtype=int32)}
episode index:163
model initialize at round 163
at step 0:
{'currentState': array([13.12771014, 13.25781132,  0.60695684]), 'targetState': array([13, 25], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.420998056013812
{'currentState': array([12.99078695, 25.04198568,  2.73020235]), 'targetState': array([13, 25], dtype=int32)}
episode index:164
model initialize at round 164
at step 0:
{'currentState': array([15.73729977, 21.88280417,  3.05621529]), 'targetState': array([ 3, 29], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.4220402886828284
{'currentState': array([ 3.03420189, 29.09383421,  4.30813974]), 'targetState': array([ 3, 29], dtype=int32)}
episode index:165
model initialize at round 165
at step 0:
{'currentState': array([ 5.23743791, 20.16238693,  0.09484768]), 'targetState': array([19,  6], dtype=int32)}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.4223043879990149
{'currentState': array([19.09212178,  5.94749722,  0.3030152 ]), 'targetState': array([19,  6], dtype=int32)}
episode index:166
model initialize at round 166
at step 0:
{'currentState': array([11.88598528, 21.26409642,  1.47334194]), 'targetState': array([15, 20], dtype=int32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.42503023611278706
{'currentState': array([14.96594125, 19.92305976,  6.22647697]), 'targetState': array([15, 20], dtype=int32)}
episode index:167
model initialize at round 167
at step 0:
{'currentState': array([ 7.24697895, 20.82978561,  5.64864272]), 'targetState': array([18, 14], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.42621175491503954
{'currentState': array([17.99476884, 13.98397998,  6.24411028]), 'targetState': array([18, 14], dtype=int32)}
episode index:168
model initialize at round 168
at step 0:
{'currentState': array([7.28033311, 3.93550486, 0.27886885]), 'targetState': array([15,  8], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.42802294790166295
{'currentState': array([14.98918204,  7.9405146 ,  1.12422793]), 'targetState': array([15,  8], dtype=int32)}
episode index:169
model initialize at round 169
at step 0:
{'currentState': array([22.95087897, 18.71656853,  4.0357852 ]), 'targetState': array([15, 25], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.42932341661029083
{'currentState': array([14.93088006, 25.07179785,  3.34287309]), 'targetState': array([15, 25], dtype=int32)}
episode index:170
model initialize at round 170
at step 0:
{'currentState': array([14.18147234,  7.77680936,  4.89005613]), 'targetState': array([3, 3], dtype=int32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.429886417025
{'currentState': array([2.92101892, 2.92816434, 5.31237661]), 'targetState': array([3, 3], dtype=int32)}
episode index:171
model initialize at round 171
at step 0:
{'currentState': array([11.79174323,  9.19843234,  1.8753469 ]), 'targetState': array([11, 15], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.43200111392394563
{'currentState': array([10.96360021, 15.03590557,  2.84692898]), 'targetState': array([11, 15], dtype=int32)}
episode index:172
model initialize at round 172
at step 0:
{'currentState': array([6.94830973, 4.28297419, 1.24647212]), 'targetState': array([21, 16], dtype=int32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.43233573334059455
{'currentState': array([20.99829227, 16.06939048,  1.16698503]), 'targetState': array([21, 16], dtype=int32)}
episode index:173
model initialize at round 173
at step 0:
{'currentState': array([1.12527459e+01, 5.86264725e+00, 7.20566114e-03]), 'targetState': array([14,  9], dtype=int32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.43474448068400995
{'currentState': array([14.05207706,  8.97880594,  1.74316185]), 'targetState': array([14,  9], dtype=int32)}
episode index:174
model initialize at round 174
at step 0:
{'currentState': array([10.71709275, 24.9479446 ,  2.81855893]), 'targetState': array([ 6, 24], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4370770447565809
{'currentState': array([ 6.01416835, 24.07651181,  3.97018656]), 'targetState': array([ 6, 24], dtype=int32)}
episode index:175
model initialize at round 175
at step 0:
{'currentState': array([21.75826276, 21.84408528,  3.20942861]), 'targetState': array([19, 19], dtype=int32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.43948034765365307
{'currentState': array([18.90354725, 18.95358655,  5.10406271]), 'targetState': array([19, 19], dtype=int32)}
episode index:176
model initialize at round 176
at step 0:
{'currentState': array([12.96136535,  5.28505024,  1.20051169]), 'targetState': array([20, 13], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.44081508607967923
{'currentState': array([19.9319564 , 13.01554419,  3.3283537 ]), 'targetState': array([20, 13], dtype=int32)}
episode index:177
model initialize at round 177
at step 0:
{'currentState': array([25.74180728, 12.87318204,  3.09315586]), 'targetState': array([25, 22], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.4423707908354017
{'currentState': array([24.97846294, 22.07621209,  2.50675815]), 'targetState': array([25, 22], dtype=int32)}
episode index:178
model initialize at round 178
at step 0:
{'currentState': array([14.89673209, 11.73151906,  3.84019566]), 'targetState': array([ 6, 14], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.44308157525798136
{'currentState': array([ 5.9052029 , 13.93761985,  5.01740139]), 'targetState': array([ 6, 14], dtype=int32)}
episode index:179
model initialize at round 179
at step 0:
{'currentState': array([19.72863358, 15.09542821,  2.29844141]), 'targetState': array([27, 22], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.4444889732734902
{'currentState': array([27.00214508, 22.01228621,  2.04714382]), 'targetState': array([27, 22], dtype=int32)}
episode index:180
model initialize at round 180
at step 0:
{'currentState': array([3.13039876, 7.25640288, 0.59531641]), 'targetState': array([5, 6], dtype=int32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.44708028970559044
{'currentState': array([5.09727338, 5.91928203, 5.75201929]), 'targetState': array([5, 6], dtype=int32)}
episode index:181
model initialize at round 181
at step 0:
{'currentState': array([16.73620391,  6.88529213,  3.04676437]), 'targetState': array([ 4, 11], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.4478492814211503
{'currentState': array([ 4.03774144, 10.92518663,  5.89292973]), 'targetState': array([ 4, 11], dtype=int32)}
episode index:182
model initialize at round 182
at step 0:
{'currentState': array([ 7.96299634, 14.28526655,  2.20479214]), 'targetState': array([ 3, 23], dtype=int32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.44905760096841
{'currentState': array([ 2.92460775, 23.08630271,  4.10231605]), 'targetState': array([ 3, 23], dtype=int32)}
episode index:183
model initialize at round 183
at step 0:
{'currentState': array([11.01758553, 27.71288152,  4.26856089]), 'targetState': array([ 9, 21], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.45044015582727503
{'currentState': array([ 9.08230245, 20.97316617,  0.51328235]), 'targetState': array([ 9, 21], dtype=int32)}
episode index:184
model initialize at round 184
at step 0:
{'currentState': array([27.23838139, 22.83900128,  5.18417263]), 'targetState': array([16, 13], dtype=int32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.4507071659573935
{'currentState': array([15.90580817, 12.92439657,  5.22393597]), 'targetState': array([16, 13], dtype=int32)}
episode index:185
model initialize at round 185
at step 0:
{'currentState': array([22.14206061, 27.74986992,  4.72390652]), 'targetState': array([25, 16], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.4516362962204785
{'currentState': array([25.07166548, 16.0187042 ,  1.09895145]), 'targetState': array([25, 16], dtype=int32)}
episode index:186
model initialize at round 186
at step 0:
{'currentState': array([18.82909778, 21.76861568,  4.58121896]), 'targetState': array([21,  8], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.4522671245961827
{'currentState': array([21.02486431,  7.97296471,  5.06025015]), 'targetState': array([21,  8], dtype=int32)}
episode index:187
model initialize at round 187
at step 0:
{'currentState': array([12.13320935, 26.74504597,  4.68886191]), 'targetState': array([18, 22], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.45375667909117323
{'currentState': array([18.08904906, 22.07762931,  0.4977343 ]), 'targetState': array([18, 22], dtype=int32)}
episode index:188
model initialize at round 188
at step 0:
{'currentState': array([15.84919718, 20.24495874,  2.62762594]), 'targetState': array([ 2, 17], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.4544000551276058
{'currentState': array([ 1.9206065 , 17.00216989,  5.73067504]), 'targetState': array([ 2, 17], dtype=int32)}
episode index:189
model initialize at round 189
at step 0:
{'currentState': array([5.85276519, 2.24711978, 1.60312438]), 'targetState': array([7, 3], dtype=int32)}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4570136866790394
{'currentState': array([6.96558957, 2.96614908, 1.48964827]), 'targetState': array([7, 3], dtype=int32)}
episode index:190
model initialize at round 190
at step 0:
{'currentState': array([ 8.12069652, 25.73881238,  4.64171958]), 'targetState': array([ 9, 25], dtype=int32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.45965024334564136
{'currentState': array([ 8.90013763, 24.98670081,  0.04662942]), 'targetState': array([ 9, 25], dtype=int32)}
episode index:191
model initialize at round 191
at step 0:
{'currentState': array([19.96970938, 18.29761341,  1.80390564]), 'targetState': array([18, 29], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.460603167089074
{'currentState': array([17.94975194, 29.09686884,  3.85899659]), 'targetState': array([18, 29], dtype=int32)}
episode index:192
model initialize at round 192
at step 0:
{'currentState': array([24.04162455, 19.29602104,  1.57871383]), 'targetState': array([25, 25], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.4623286132888386
{'currentState': array([24.99775956, 25.04921186,  3.73146084]), 'targetState': array([25, 25], dtype=int32)}
episode index:193
model initialize at round 193
at step 0:
{'currentState': array([ 7.17470437, 11.77147285,  5.87009287]), 'targetState': array([16, 18], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.4632579070455184
{'currentState': array([15.96732872, 18.0164452 ,  2.72838555]), 'targetState': array([16, 18], dtype=int32)}
episode index:194
model initialize at round 194
at step 0:
{'currentState': array([14.24848828, 24.85468517,  5.25323248]), 'targetState': array([20, 25], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.46491134414173224
{'currentState': array([19.98641851, 25.08955346,  2.66861417]), 'targetState': array([20, 25], dtype=int32)}
episode index:195
model initialize at round 195
at step 0:
{'currentState': array([8.07185083, 6.72146143, 5.46984196]), 'targetState': array([13,  6], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.4665078238114129
{'currentState': array([13.07377313,  6.0507576 ,  1.33237238]), 'targetState': array([13,  6], dtype=int32)}
episode index:196
model initialize at round 196
at step 0:
{'currentState': array([ 8.26167109, 26.88052399,  5.3498659 ]), 'targetState': array([20, 15], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.4669172086772979
{'currentState': array([2.00536158e+01, 1.49877108e+01, 1.78818777e-03]), 'targetState': array([20, 15], dtype=int32)}
episode index:197
model initialize at round 197
at step 0:
{'currentState': array([12.19504521, 19.21143235,  0.32069135]), 'targetState': array([25, 24], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.46746487302729606
{'currentState': array([25.09723439, 23.92343416,  3.18525344]), 'targetState': array([25, 24], dtype=int32)}
episode index:198
model initialize at round 198
at step 0:
{'currentState': array([16.92966045, 17.27892404,  2.32282734]), 'targetState': array([12, 23], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.46854572590158294
{'currentState': array([12.01512709, 22.9289389 ,  4.15745551]), 'targetState': array([12, 23], dtype=int32)}
episode index:199
model initialize at round 199
at step 0:
{'currentState': array([15.97259108, 19.28634774,  1.1612246 ]), 'targetState': array([25, 18], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.46620299727207504
{'currentState': array([25.12625603, 18.07100052,  2.13324808]), 'targetState': array([25, 18], dtype=int32)}
episode index:200
model initialize at round 200
at step 0:
{'currentState': array([15.82612858, 16.22916152,  1.71486044]), 'targetState': array([26, 25], dtype=int32)}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.4665785300018998
{'currentState': array([25.94879051, 25.05646726,  3.09403519]), 'targetState': array([26, 25], dtype=int32)}
episode index:201
model initialize at round 201
at step 0:
{'currentState': array([21.72902026, 23.90347924,  3.98877287]), 'targetState': array([20, 21], dtype=int32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4685694571443111
{'currentState': array([19.97115746, 20.90974968,  5.7864807 ]), 'targetState': array([20, 21], dtype=int32)}
episode index:202
model initialize at round 202
at step 0:
{'currentState': array([19.73664915,  7.11572641,  2.22255659]), 'targetState': array([ 8, 20], dtype=int32)}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.46865035326810345
{'currentState': array([ 8.01505518, 19.90206452,  5.12550675]), 'targetState': array([ 8, 20], dtype=int32)}
episode index:203
model initialize at round 203
at step 0:
{'currentState': array([15.28682412, 11.97813224,  5.70209152]), 'targetState': array([13,  8], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4704851220922006
{'currentState': array([12.99446894,  7.96996295,  4.68232924]), 'targetState': array([13,  8], dtype=int32)}
episode index:204
model initialize at round 204
at step 0:
{'currentState': array([ 4.89148409, 19.73359698,  4.83057332]), 'targetState': array([12, 12], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4681900727161411
{'currentState': array([11.80352491, 12.42132054,  3.59914537]), 'targetState': array([12, 12], dtype=int32)}
episode index:205
model initialize at round 205
at step 0:
{'currentState': array([25.11182805,  4.73497026,  4.60666943]), 'targetState': array([24,  2], dtype=int32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.4702636075779983
{'currentState': array([23.99445867,  1.97464812,  5.39871908]), 'targetState': array([24,  2], dtype=int32)}
episode index:206
model initialize at round 206
at step 0:
{'currentState': array([17.28754993, 25.00783032,  5.80540973]), 'targetState': array([ 9, 22], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.47142442699681125
{'currentState': array([ 8.96140761, 22.03722134,  4.60953078]), 'targetState': array([ 9, 22], dtype=int32)}
episode index:207
model initialize at round 207
at step 0:
{'currentState': array([ 8.2846132 , 23.95826752,  5.63259405]), 'targetState': array([19, 19], dtype=int32)}
done in step count: 86
reward sum = 0.421334222154768
running average episode reward sum: 0.4711836087043014
{'currentState': array([18.90053406, 18.91063026,  5.65679759]), 'targetState': array([19, 19], dtype=int32)}
episode index:208
model initialize at round 208
at step 0:
{'currentState': array([13.21284426, 21.19350351,  1.24283701]), 'targetState': array([24, 27], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.47200383833770054
{'currentState': array([24.012304  , 26.9196058 ,  0.84071007]), 'targetState': array([24, 27], dtype=int32)}
episode index:209
model initialize at round 209
at step 0:
{'currentState': array([8.05472503, 8.28240298, 1.88438529]), 'targetState': array([ 4, 20], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.4726662831052965
{'currentState': array([ 4.09825282, 20.06570272,  3.03367151]), 'targetState': array([ 4, 20], dtype=int32)}
episode index:210
model initialize at round 210
at step 0:
{'currentState': array([12.02313634, 11.71324564,  4.28851342]), 'targetState': array([18,  3], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.47353354821178395
{'currentState': array([18.03238047,  2.97164667,  0.17749492]), 'targetState': array([18,  3], dtype=int32)}
episode index:211
model initialize at round 211
at step 0:
{'currentState': array([11.17509225, 21.2282301 ,  0.411394  ]), 'targetState': array([25, 27], dtype=int32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.47372979146278404
{'currentState': array([24.95569401, 27.03267895,  2.9323157 ]), 'targetState': array([25, 27], dtype=int32)}
episode index:212
model initialize at round 212
at step 0:
{'currentState': array([12.92968492, 10.27893021,  1.31273961]), 'targetState': array([17, 16], dtype=int32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.4751209339714337
{'currentState': array([17.04082687, 16.06151007,  2.64213398]), 'targetState': array([17, 16], dtype=int32)}
episode index:213
model initialize at round 213
at step 0:
{'currentState': array([ 4.1083444 , 21.72702553,  4.73025262]), 'targetState': array([ 9, 18], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.4761224673915161
{'currentState': array([ 9.05394611, 17.94252841,  0.77552663]), 'targetState': array([ 9, 18], dtype=int32)}
episode index:214
model initialize at round 214
at step 0:
{'currentState': array([26.23763912, 22.83790767,  5.17957783]), 'targetState': array([14, 23], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.476721925994986
{'currentState': array([13.98016804, 22.93893272,  5.12196904]), 'targetState': array([14, 23], dtype=int32)}
episode index:215
model initialize at round 215
at step 0:
{'currentState': array([13.10986484, 10.26584957,  1.68391192]), 'targetState': array([10, 20], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.47734412652062425
{'currentState': array([ 9.92815131, 19.94154038,  2.55801851]), 'targetState': array([10, 20], dtype=int32)}
episode index:216
model initialize at round 216
at step 0:
{'currentState': array([11.17227032, 21.76963244,  5.85948467]), 'targetState': array([15, 28], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.47845189797720333
{'currentState': array([15.02938719, 28.08121819,  2.16205098]), 'targetState': array([15, 28], dtype=int32)}
episode index:217
model initialize at round 217
at step 0:
{'currentState': array([ 9.11688024, 16.26284079,  1.65737468]), 'targetState': array([ 5, 27], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4762571645002437
{'currentState': array([ 4.91352483, 27.20929554,  3.03347423]), 'targetState': array([ 5, 27], dtype=int32)}
episode index:218
model initialize at round 218
at step 0:
{'currentState': array([19.01771608, 13.28711046,  2.01416969]), 'targetState': array([22, 22], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.4773597826194128
{'currentState': array([21.97671593, 21.99593706,  1.28149477]), 'targetState': array([22, 22], dtype=int32)}
episode index:219
model initialize at round 219
at step 0:
{'currentState': array([ 7.08551919, 11.27465022,  0.76393676]), 'targetState': array([14, 22], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.47788526745478593
{'currentState': array([13.94535375, 21.93568624,  5.96751267]), 'targetState': array([14, 22], dtype=int32)}
episode index:220
model initialize at round 220
at step 0:
{'currentState': array([ 5.19598677, 27.78944012,  4.95695639]), 'targetState': array([10, 19], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.4784881270569492
{'currentState': array([ 9.94471962, 19.00067524,  4.24201337]), 'targetState': array([10, 19], dtype=int32)}
episode index:221
model initialize at round 221
at step 0:
{'currentState': array([21.16146391,  6.23806654,  1.47983089]), 'targetState': array([ 7, 19], dtype=int32)}
done in step count: 105
reward sum = 0.348093114492442
running average episode reward sum: 0.47790076213548743
{'currentState': array([ 7.06997231, 18.90872619,  0.47118824]), 'targetState': array([ 7, 19], dtype=int32)}
episode index:222
model initialize at round 222
at step 0:
{'currentState': array([ 5.96960736, 15.28604643,  2.18164992]), 'targetState': array([ 3, 27], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.4786684835087292
{'currentState': array([ 3.01039651, 27.08906927,  1.71044999]), 'targetState': array([ 3, 27], dtype=int32)}
episode index:223
model initialize at round 223
at step 0:
{'currentState': array([8.84218724, 3.24050241, 2.65650344]), 'targetState': array([ 3, 15], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.4793151661488295
{'currentState': array([ 2.9883713 , 15.05232875,  3.19640905]), 'targetState': array([ 3, 15], dtype=int32)}
episode index:224
model initialize at round 224
at step 0:
{'currentState': array([26.15975201,  6.23921866,  1.48700434]), 'targetState': array([22,  8], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.4771848765215014
{'currentState': array([21.83843274,  8.06047004,  3.53236667]), 'targetState': array([22,  8], dtype=int32)}
episode index:225
model initialize at round 225
at step 0:
{'currentState': array([17.71937005, 19.06319102,  2.41511106]), 'targetState': array([ 9, 23], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.4782174798434075
{'currentState': array([ 9.03294406, 22.93539816,  1.88952398]), 'targetState': array([ 9, 23], dtype=int32)}
episode index:226
model initialize at round 226
at step 0:
{'currentState': array([23.07601236, 24.72256822,  4.47481203]), 'targetState': array([ 9, 16], dtype=int32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.47847320999919507
{'currentState': array([ 8.97570826, 15.97275914,  3.48531395]), 'targetState': array([ 9, 16], dtype=int32)}
episode index:227
model initialize at round 227
at step 0:
{'currentState': array([ 3.02770024, 12.7136803 ,  5.31383467]), 'targetState': array([ 6, 14], dtype=int32)}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.47831783619054147
{'currentState': array([ 6.07373641, 13.9135139 ,  0.50053982]), 'targetState': array([ 6, 14], dtype=int32)}
episode index:228
model initialize at round 228
at step 0:
{'currentState': array([21.77931381, 27.81549016,  3.33294582]), 'targetState': array([16, 29], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.4795581195012688
{'currentState': array([15.97556039, 29.07644153,  2.8265872 ]), 'targetState': array([16, 29], dtype=int32)}
episode index:229
model initialize at round 229
at step 0:
{'currentState': array([24.28606632, 15.96979514,  0.39980286]), 'targetState': array([24, 24], dtype=int32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.4807216585222703
{'currentState': array([24.01078614, 23.95255898,  0.79877591]), 'targetState': array([24, 24], dtype=int32)}
episode index:230
model initialize at round 230
at step 0:
{'currentState': array([25.77417422, 19.17818247,  2.97857893]), 'targetState': array([17, 20], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.4813946621047014
{'currentState': array([17.08460832, 20.07661804,  1.76739848]), 'targetState': array([17, 20], dtype=int32)}
episode index:231
model initialize at round 231
at step 0:
{'currentState': array([4.28009966, 3.93449845, 0.27527731]), 'targetState': array([5, 5], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4829530609464222
{'currentState': array([4.92694671, 5.02372649, 1.28708938]), 'targetState': array([5, 5], dtype=int32)}
episode index:232
model initialize at round 232
at step 0:
{'currentState': array([13.75163287, 13.1451208 ,  3.11779761]), 'targetState': array([ 5, 22], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.48342522139901917
{'currentState': array([ 5.06453044, 22.01892873,  1.23560483]), 'targetState': array([ 5, 22], dtype=int32)}
episode index:233
model initialize at round 233
at step 0:
{'currentState': array([8.1089651 , 9.26621961, 1.68729395]), 'targetState': array([16, 19], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.4838680058457647
{'currentState': array([16.06046469, 18.99483627,  0.23560206]), 'targetState': array([16, 19], dtype=int32)}
episode index:234
model initialize at round 234
at step 0:
{'currentState': array([17.19967916, 22.2070616 ,  1.30854639]), 'targetState': array([26, 28], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.48471348069327375
{'currentState': array([25.99483811, 28.04427443,  0.8238484 ]), 'targetState': array([26, 28], dtype=int32)}
episode index:235
model initialize at round 235
at step 0:
{'currentState': array([26.09800111, 13.72955212,  4.5550375 ]), 'targetState': array([20, 24], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.48522319504261385
{'currentState': array([20.06266974, 24.09145015,  0.95183903]), 'targetState': array([20, 24], dtype=int32)}
episode index:236
model initialize at round 236
at step 0:
{'currentState': array([ 3.10224657, 25.26887156,  0.70240438]), 'targetState': array([ 6, 23], dtype=int32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.4868784601394761
{'currentState': array([ 5.98029179, 23.0632538 ,  6.17818249]), 'targetState': array([ 6, 23], dtype=int32)}
episode index:237
model initialize at round 237
at step 0:
{'currentState': array([19.25023   , 19.85811546,  5.26237154]), 'targetState': array([15, 25], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.48803587297228124
{'currentState': array([14.95616667, 24.96415816,  2.78159295]), 'targetState': array([15, 25], dtype=int32)}
episode index:238
model initialize at round 238
at step 0:
{'currentState': array([11.28253963, 27.94598489,  0.3161022 ]), 'targetState': array([22, 26], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.4888497588385495
{'currentState': array([22.04809164, 26.05184603,  0.38275903]), 'targetState': array([22, 26], dtype=int32)}
episode index:239
model initialize at round 239
at step 0:
{'currentState': array([11.76718827, 16.16895258,  2.00882268]), 'targetState': array([24, 28], dtype=int32)}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.4890249912723685
{'currentState': array([23.95481758, 27.95489424,  2.29714674]), 'targetState': array([24, 28], dtype=int32)}
episode index:240
model initialize at round 240
at step 0:
{'currentState': array([16.16541734, 10.76466327,  5.83005619]), 'targetState': array([22,  4], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.489828018673771
{'currentState': array([22.02760383,  3.98276311,  6.01944581]), 'targetState': array([22,  4], dtype=int32)}
episode index:241
model initialize at round 241
at step 0:
{'currentState': array([10.94913383, 23.71687652,  5.03962469]), 'targetState': array([12, 21], dtype=int32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4915410519644116
{'currentState': array([11.98210316, 21.07639304,  6.08534628]), 'targetState': array([12, 21], dtype=int32)}
episode index:242
model initialize at round 242
at step 0:
{'currentState': array([ 8.28764187, 13.00290283,  5.7882768 ]), 'targetState': array([5, 4], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.4924423366364605
{'currentState': array([5.05140712, 3.96878361, 4.19191993]), 'targetState': array([5, 4], dtype=int32)}
episode index:243
model initialize at round 243
at step 0:
{'currentState': array([18.99698638, 16.28764074,  1.07627296]), 'targetState': array([20,  2], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.4928300187893335
{'currentState': array([20.05921435,  1.93303952,  4.16977364]), 'targetState': array([20,  2], dtype=int32)}
episode index:244
model initialize at round 244
at step 0:
{'currentState': array([ 7.16137184, 26.23812896,  0.47021759]), 'targetState': array([11, 25], dtype=int32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.4943643689688423
{'currentState': array([10.97410173, 25.05425254,  0.37360686]), 'targetState': array([11, 25], dtype=int32)}
episode index:245
model initialize at round 245
at step 0:
{'currentState': array([21.2872011 , 10.98381968,  5.72190684]), 'targetState': array([10,  5], dtype=int32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.494864067228897
{'currentState': array([9.97015284, 5.06409782, 3.87651477]), 'targetState': array([10,  5], dtype=int32)}
episode index:246
model initialize at round 246
at step 0:
{'currentState': array([23.97654006, 18.71330172,  4.12574291]), 'targetState': array([15, 22], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.49573730269465965
{'currentState': array([15.07133014, 22.03630347,  3.89569515]), 'targetState': array([15, 22], dtype=int32)}
episode index:247
model initialize at round 247
at step 0:
{'currentState': array([ 9.96013672, 27.7090229 ,  4.21642417]), 'targetState': array([10, 26], dtype=int32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4975346528829917
{'currentState': array([10.00570383, 25.98577501,  5.52556567]), 'targetState': array([10, 26], dtype=int32)}
episode index:248
model initialize at round 248
at step 0:
{'currentState': array([11.22436976,  2.18001245,  0.17114341]), 'targetState': array([23,  5], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.4979179130979255
{'currentState': array([23.09372463,  4.98611408,  5.90609197]), 'targetState': array([23,  5], dtype=int32)}
episode index:249
model initialize at round 249
at step 0:
{'currentState': array([16.83920321, 12.76133759,  4.62201166]), 'targetState': array([2, 6], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.4981148680150968
{'currentState': array([2.02076672, 6.03626123, 5.19869566]), 'targetState': array([2, 6], dtype=int32)}
episode index:250
model initialize at round 250
at step 0:
{'currentState': array([14.20891014,  5.80218166,  5.021106  ]), 'targetState': array([14,  7], dtype=int32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.49980662031156226
{'currentState': array([14.05267117,  6.9521815 ,  1.06755647]), 'targetState': array([14,  7], dtype=int32)}
episode index:251
model initialize at round 251
at step 0:
{'currentState': array([12.24351793, 13.15311856,  0.05631101]), 'targetState': array([23,  8], dtype=int32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5002728247585095
{'currentState': array([23.04948977,  7.94866355,  5.47265894]), 'targetState': array([23,  8], dtype=int32)}
episode index:252
model initialize at round 252
at step 0:
{'currentState': array([18.00192452,  9.71234992,  4.21407938]), 'targetState': array([22, 24], dtype=int32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5003521437225087
{'currentState': array([22.02590424, 23.95531356,  1.94636223]), 'targetState': array([22, 24], dtype=int32)}
episode index:253
model initialize at round 253
at step 0:
{'currentState': array([11.83034891,  2.23230321,  2.70656991]), 'targetState': array([2, 7], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5008869206608605
{'currentState': array([2.09362355, 7.05567882, 3.69779513]), 'targetState': array([2, 7], dtype=int32)}
episode index:254
model initialize at round 254
at step 0:
{'currentState': array([10.15631368, 28.75852058,  4.78187561]), 'targetState': array([13, 24], dtype=int32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5019424352692695
{'currentState': array([13.01283133, 23.98363865,  1.0277888 ]), 'targetState': array([13, 24], dtype=int32)}
episode index:255
model initialize at round 255
at step 0:
{'currentState': array([17.22001441,  1.81468962,  6.08819914]), 'targetState': array([24,  2], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5030507778690271
{'currentState': array([23.90884026,  2.00948581,  0.69453576]), 'targetState': array([24,  2], dtype=int32)}
episode index:256
model initialize at round 256
at step 0:
{'currentState': array([ 9.87529834, 13.74077853,  3.75900602]), 'targetState': array([6, 3], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5035195506979071
{'currentState': array([6.07280509, 2.94231802, 4.8989127 ]), 'targetState': array([6, 3], dtype=int32)}
episode index:257
model initialize at round 257
at step 0:
{'currentState': array([20.09863204, 10.72978129,  5.56736541]), 'targetState': array([19,  2], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.504349825821552
{'currentState': array([18.97210324,  1.97565377,  5.00292435]), 'targetState': array([19,  2], dtype=int32)}
episode index:258
model initialize at round 258
at step 0:
{'currentState': array([10.26337423,  8.88432681,  5.36435132]), 'targetState': array([12,  2], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5054057004685697
{'currentState': array([11.98613881,  2.00487325,  6.145709  ]), 'targetState': array([12,  2], dtype=int32)}
episode index:259
model initialize at round 259
at step 0:
{'currentState': array([25.83351226, 28.7654193 ,  3.59016609]), 'targetState': array([13, 16], dtype=int32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5054431289953206
{'currentState': array([13.04906535, 16.0281869 ,  4.69233029]), 'targetState': array([13, 16], dtype=int32)}
episode index:260
model initialize at round 260
at step 0:
{'currentState': array([17.03777198, 27.28516583,  0.93410671]), 'targetState': array([22, 22], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5064274185943696
{'currentState': array([22.04010768, 21.97976836,  5.02378009]), 'targetState': array([22, 22], dtype=int32)}
episode index:261
model initialize at round 261
at step 0:
{'currentState': array([21.26126392, 22.87963622,  0.07328528]), 'targetState': array([27, 27], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5074632733302658
{'currentState': array([27.05912361, 27.08020661,  1.0025914 ]), 'targetState': array([27, 27], dtype=int32)}
episode index:262
model initialize at round 262
at step 0:
{'currentState': array([19.19510362, 18.78862154,  4.95277023]), 'targetState': array([18,  4], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5077658341994947
{'currentState': array([17.94766057,  4.07893215,  6.09796855]), 'targetState': array([18,  4], dtype=int32)}
episode index:263
model initialize at round 263
at step 0:
{'currentState': array([ 8.22573509, 10.82170265,  6.11966276]), 'targetState': array([23, 18], dtype=int32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5078134655951417
{'currentState': array([23.023845  , 18.00120774,  2.53457464]), 'targetState': array([23, 18], dtype=int32)}
episode index:264
model initialize at round 264
at step 0:
{'currentState': array([16.26841642, 19.10343549,  6.14600289]), 'targetState': array([ 2, 14], dtype=int32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5080038530402463
{'currentState': array([ 1.91539717, 14.00293348,  3.94482055]), 'targetState': array([ 2, 14], dtype=int32)}
episode index:265
model initialize at round 265
at step 0:
{'currentState': array([25.26907433, 14.10171175,  6.1395889 ]), 'targetState': array([27, 13], dtype=int32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5095980691863619
{'currentState': array([26.9732406 , 13.02783919,  0.20664427]), 'targetState': array([27, 13], dtype=int32)}
episode index:266
model initialize at round 266
at step 0:
{'currentState': array([13.28574309, 22.0331479 ,  0.62043136]), 'targetState': array([20, 19], dtype=int32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5105735189115258
{'currentState': array([19.94143139, 19.01691779,  6.03249422]), 'targetState': array([20, 19], dtype=int32)}
episode index:267
model initialize at round 267
at step 0:
{'currentState': array([11.10170656, 10.73092371,  4.56877327]), 'targetState': array([4, 4], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5112931986730483
{'currentState': array([3.98300411, 4.02612048, 4.59808577]), 'targetState': array([4, 4], dtype=int32)}
episode index:268
model initialize at round 268
at step 0:
{'currentState': array([11.28711279, 18.01767829,  5.83968032]), 'targetState': array([26,  9], dtype=int32)}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5114061796295309
{'currentState': array([26.00707043,  8.98906995,  5.66822642]), 'targetState': array([26,  9], dtype=int32)}
episode index:269
model initialize at round 269
at step 0:
{'currentState': array([24.26965697, 18.89984316,  5.4225542 ]), 'targetState': array([11, 11], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5116006204103563
{'currentState': array([11.0451774 , 11.02776124,  4.28325877]), 'targetState': array([11, 11], dtype=int32)}
episode index:270
model initialize at round 270
at step 0:
{'currentState': array([ 9.73556174, 20.88678037,  3.04112959]), 'targetState': array([17, 27], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5123085431948183
{'currentState': array([17.0420381 , 26.90549021,  0.85728846]), 'targetState': array([17, 27], dtype=int32)}
episode index:271
model initialize at round 271
at step 0:
{'currentState': array([20.84045108,  9.23935417,  2.66373956]), 'targetState': array([13, 11], dtype=int32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5131720121328212
{'currentState': array([12.97216316, 10.98196715,  3.39002982]), 'targetState': array([13, 11], dtype=int32)}
episode index:272
model initialize at round 272
at step 0:
{'currentState': array([12.21756414, 29.18818109,  0.20810688]), 'targetState': array([ 8, 25], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5141992732006264
{'currentState': array([ 7.96286894, 25.02511025,  3.57905513]), 'targetState': array([ 8, 25], dtype=int32)}
episode index:273
model initialize at round 273
at step 0:
{'currentState': array([16.73715134, 13.11686254,  3.22823834]), 'targetState': array([10, 15], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5151613976028109
{'currentState': array([ 9.9357167 , 14.98616959,  4.01269737]), 'targetState': array([10, 15], dtype=int32)}
episode index:274
model initialize at round 274
at step 0:
{'currentState': array([10.04151516,  4.71535502,  4.35221672]), 'targetState': array([10,  3], dtype=int32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5167116476093497
{'currentState': array([9.98486305, 3.00868182, 5.5645197 ]), 'targetState': array([10,  3], dtype=int32)}
episode index:275
model initialize at round 275
at step 0:
{'currentState': array([25.26149204,  3.11986738,  0.93481558]), 'targetState': array([16, 17], dtype=int32)}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5167630747663996
{'currentState': array([15.95791419, 17.09194225,  2.28622571]), 'targetState': array([16, 17], dtype=int32)}
episode index:276
model initialize at round 276
at step 0:
{'currentState': array([17.28745013, 21.98910506,  5.74030143]), 'targetState': array([10, 10], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5170816415258621
{'currentState': array([9.99559262, 9.99242746, 4.72829908]), 'targetState': array([10, 10], dtype=int32)}
episode index:277
model initialize at round 277
at step 0:
{'currentState': array([13.16668421, 12.23444114,  0.44773567]), 'targetState': array([25,  7], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.517464532725018
{'currentState': array([24.91127085,  7.06178911,  0.05340773]), 'targetState': array([25,  7], dtype=int32)}
episode index:278
model initialize at round 278
at step 0:
{'currentState': array([13.73596475, 11.88583656,  4.0546385 ]), 'targetState': array([ 9, 10], dtype=int32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5186009098889073
{'currentState': array([8.93300654, 9.95559427, 4.41943441]), 'targetState': array([ 9, 10], dtype=int32)}
episode index:279
model initialize at round 279
at step 0:
{'currentState': array([24.86489689, 10.25395555,  2.56471121]), 'targetState': array([21, 22], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5188878923766831
{'currentState': array([21.03659996, 21.99135215,  3.02199875]), 'targetState': array([21, 22], dtype=int32)}
episode index:280
model initialize at round 280
at step 0:
{'currentState': array([26.71561294, 23.04324666,  3.49567914]), 'targetState': array([12, 11], dtype=int32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5188200957130622
{'currentState': array([11.99955476, 11.05937074,  3.18482609]), 'targetState': array([12, 11], dtype=int32)}
episode index:281
model initialize at round 281
at step 0:
{'currentState': array([23.05727257, 10.28189737,  0.86535633]), 'targetState': array([27,  4], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5196566176687025
{'currentState': array([26.95657264,  4.07083656,  6.0512742 ]), 'targetState': array([27,  4], dtype=int32)}
episode index:282
model initialize at round 282
at step 0:
{'currentState': array([ 1.72109065, 17.0703978 ,  2.3893528 ]), 'targetState': array([13, 29], dtype=int32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5195514362387926
{'currentState': array([12.93963151, 29.04995184,  1.01520802]), 'targetState': array([13, 29], dtype=int32)}
episode index:283
model initialize at round 283
at step 0:
{'currentState': array([17.80560915, 26.78796587,  3.46537447]), 'targetState': array([ 8, 18], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.519939718615428
{'currentState': array([ 8.06308648, 18.04096016,  4.25587552]), 'targetState': array([ 8, 18], dtype=int32)}
episode index:284
model initialize at round 284
at step 0:
{'currentState': array([24.1080477 ,  3.73340674,  4.59244776]), 'targetState': array([13,  2], dtype=int32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5204391653605809
{'currentState': array([12.9533151 ,  1.9331067 ,  3.49355941]), 'targetState': array([13,  2], dtype=int32)}
episode index:285
model initialize at round 285
at step 0:
{'currentState': array([10.25103245, 18.1404599 ,  1.01512958]), 'targetState': array([21, 25], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5208438727756274
{'currentState': array([21.0504651 , 25.04681187,  1.8522736 ]), 'targetState': array([21, 25], dtype=int32)}
episode index:286
model initialize at round 286
at step 0:
{'currentState': array([25.80103017, 16.79225669,  4.4535591 ]), 'targetState': array([20, 13], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5217392647150821
{'currentState': array([19.95481164, 12.95130804,  4.31319849]), 'targetState': array([20, 13], dtype=int32)}
episode index:287
model initialize at round 287
at step 0:
{'currentState': array([21.95050762,  2.71663314,  4.03447485]), 'targetState': array([19, 18], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5218856741794478
{'currentState': array([18.97763513, 17.99837385,  2.8639904 ]), 'targetState': array([19, 18], dtype=int32)}
episode index:288
model initialize at round 288
at step 0:
{'currentState': array([11.26375878,  1.88520635,  0.0945031 ]), 'targetState': array([21, 15], dtype=int32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5219922409717762
{'currentState': array([21.01584709, 14.93238728,  1.11101058]), 'targetState': array([21, 15], dtype=int32)}
episode index:289
model initialize at round 289
at step 0:
{'currentState': array([9.12068895, 4.73888613, 5.65034866]), 'targetState': array([16,  5], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.522717451760337
{'currentState': array([16.06149654,  4.91898223,  1.7704502 ]), 'targetState': array([16,  5], dtype=int32)}
episode index:290
model initialize at round 290
at step 0:
{'currentState': array([22.14880107,  8.75382015,  4.75106716]), 'targetState': array([20,  7], dtype=int32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5240290140395414
{'currentState': array([19.94292489,  6.92624202,  4.53776966]), 'targetState': array([20,  7], dtype=int32)}
episode index:291
model initialize at round 291
at step 0:
{'currentState': array([17.2829156, 16.05201  ,  5.9599911]), 'targetState': array([ 2, 14], dtype=int32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5241271457625649
{'currentState': array([ 1.90829956, 13.96817625,  3.79098198]), 'targetState': array([ 2, 14], dtype=int32)}
episode index:292
model initialize at round 292
at step 0:
{'currentState': array([12.72829742, 19.90553318,  2.97120333]), 'targetState': array([26, 21], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.524362092181128
{'currentState': array([26.08246872, 21.01984978,  1.03329356]), 'targetState': array([26, 21], dtype=int32)}
episode index:293
model initialize at round 293
at step 0:
{'currentState': array([13.02584501, 17.71350688,  4.29735708]), 'targetState': array([7, 9], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5248769457759551
{'currentState': array([7.04281377, 8.91560847, 5.73466196]), 'targetState': array([7, 9], dtype=int32)}
episode index:294
model initialize at round 294
at step 0:
{'currentState': array([ 9.7265313 , 14.91077474,  2.95197487]), 'targetState': array([21, 25], dtype=int32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5249155602825016
{'currentState': array([21.06490741, 24.97492087,  2.41342181]), 'targetState': array([21, 25], dtype=int32)}
episode index:295
model initialize at round 295
at step 0:
{'currentState': array([ 7.81445796, 25.21981907,  1.76683593]), 'targetState': array([21, 24], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5252067821718609
{'currentState': array([21.04714941, 23.99240897,  0.43534321]), 'targetState': array([21, 24], dtype=int32)}
episode index:296
model initialize at round 296
at step 0:
{'currentState': array([ 5.71960665, 25.93576728,  3.87178779]), 'targetState': array([ 5, 13], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5254149639892536
{'currentState': array([ 5.04419558, 12.99221992,  5.61142386]), 'targetState': array([ 5, 13], dtype=int32)}
episode index:297
model initialize at round 297
at step 0:
{'currentState': array([ 9.18109965, 15.77650684,  5.89838743]), 'targetState': array([19,  3], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5255632399573339
{'currentState': array([18.91718376,  2.97477717,  5.16659251]), 'targetState': array([19,  3], dtype=int32)}
episode index:298
model initialize at round 298
at step 0:
{'currentState': array([ 5.16664112, 17.76552823,  4.82526588]), 'targetState': array([5, 8], dtype=int32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5260204934724733
{'currentState': array([4.99150713, 8.01496801, 5.68066588]), 'targetState': array([5, 8], dtype=int32)}
episode index:299
model initialize at round 299
at step 0:
{'currentState': array([ 4.88064836, 18.73827216,  4.78954482]), 'targetState': array([ 4, 11], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5267828227849105
{'currentState': array([ 3.92633733, 11.00356167,  4.62512502]), 'targetState': array([ 4, 11], dtype=int32)}
episode index:300
model initialize at round 300
at step 0:
{'currentState': array([27.20096565, 22.79418678,  4.98087072]), 'targetState': array([25, 29], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5275654137867783
{'currentState': array([24.98276668, 29.06106633,  2.65802068]), 'targetState': array([25, 29], dtype=int32)}
episode index:301
model initialize at round 301
at step 0:
{'currentState': array([15.27391814,  3.08783579,  6.08849079]), 'targetState': array([2, 2], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.52776233884688
{'currentState': array([1.91522456, 2.01680862, 3.7741552 ]), 'targetState': array([2, 2], dtype=int32)}
episode index:302
model initialize at round 302
at step 0:
{'currentState': array([ 3.04278476, 28.71554308,  4.35667849]), 'targetState': array([ 3, 16], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5279775339213177
{'currentState': array([ 2.92821922, 15.99782411,  5.8536135 ]), 'targetState': array([ 3, 16], dtype=int32)}
episode index:303
model initialize at round 303
at step 0:
{'currentState': array([22.77875493, 23.18383931,  1.94327402]), 'targetState': array([21, 29], dtype=int32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.528877708446376
{'currentState': array([20.99015628, 28.98953878,  3.14792547]), 'targetState': array([21, 29], dtype=int32)}
episode index:304
model initialize at round 304
at step 0:
{'currentState': array([14.94136211, 14.29253633,  1.58555245]), 'targetState': array([14, 15], dtype=int32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5302931782875354
{'currentState': array([14.04000705, 14.97943116,  3.26050846]), 'targetState': array([14, 15], dtype=int32)}
episode index:305
model initialize at round 305
at step 0:
{'currentState': array([12.95481551, 19.71591439,  4.04965782]), 'targetState': array([ 5, 25], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5308822634149366
{'currentState': array([ 4.9242344 , 25.06143551,  3.24168219]), 'targetState': array([ 5, 25], dtype=int32)}
episode index:306
model initialize at round 306
at step 0:
{'currentState': array([16.87773468,  4.26037946,  2.51480162]), 'targetState': array([11, 16], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5312045480005663
{'currentState': array([10.97969555, 16.09218692,  1.99459625]), 'targetState': array([11, 16], dtype=int32)}
episode index:307
model initialize at round 307
at step 0:
{'currentState': array([14.74671266, 20.13635175,  2.14275455]), 'targetState': array([23, 16], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5317637789973163
{'currentState': array([22.96237511, 15.99556147,  5.90255988]), 'targetState': array([23, 16], dtype=int32)}
episode index:308
model initialize at round 308
at step 0:
{'currentState': array([14.84800304, 12.24421957,  2.63250816]), 'targetState': array([11, 27], dtype=int32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5318495471512015
{'currentState': array([11.01203936, 27.04645293,  1.85664625]), 'targetState': array([11, 27], dtype=int32)}
episode index:309
model initialize at round 309
at step 0:
{'currentState': array([9.26682302, 8.89252092, 5.39525855]), 'targetState': array([20,  8], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5321861146960811
{'currentState': array([20.01113409,  8.0234412 ,  0.41802052]), 'targetState': array([20,  8], dtype=int32)}
episode index:310
model initialize at round 310
at step 0:
{'currentState': array([11.93271516,  8.72032336,  3.97129488]), 'targetState': array([4, 8], dtype=int32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5328773879424976
{'currentState': array([3.97932068, 7.96340635, 4.038983  ]), 'targetState': array([4, 8], dtype=int32)}
episode index:311
model initialize at round 311
at step 0:
{'currentState': array([15.86477862, 26.2538926 ,  2.56517696]), 'targetState': array([ 5, 26], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5333352458306957
{'currentState': array([ 4.94961655, 25.94549265,  4.65537279]), 'targetState': array([ 5, 26], dtype=int32)}
episode index:312
model initialize at round 312
at step 0:
{'currentState': array([ 3.82685337, 26.22970964,  1.71170145]), 'targetState': array([ 4, 25], dtype=int32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5345793654747762
{'currentState': array([ 4.00390432, 25.00326588,  5.00544923]), 'targetState': array([ 4, 25], dtype=int32)}
episode index:313
model initialize at round 313
at step 0:
{'currentState': array([12.78218913,  8.811067  ,  4.3468594 ]), 'targetState': array([14,  6], dtype=int32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5357282791333239
{'currentState': array([14.03097947,  5.99627509,  6.27586121]), 'targetState': array([14,  6], dtype=int32)}
episode index:314
model initialize at round 314
at step 0:
{'currentState': array([20.16365998, 11.23656222,  0.46057713]), 'targetState': array([18,  5], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5364968285944852
{'currentState': array([18.01918054,  4.95458643,  5.58674503]), 'targetState': array([18,  5], dtype=int32)}
episode index:315
model initialize at round 315
at step 0:
{'currentState': array([ 5.08379606, 14.72481916,  4.50298071]), 'targetState': array([ 2, 21], dtype=int32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5372358992185696
{'currentState': array([ 2.05112814, 20.91738602,  2.74576256]), 'targetState': array([ 2, 21], dtype=int32)}
episode index:316
model initialize at round 316
at step 0:
{'currentState': array([19.8193529 , 14.77614089,  3.52841377]), 'targetState': array([ 6, 18], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5374305998723474
{'currentState': array([ 5.90392246, 17.99803374,  3.44483102]), 'targetState': array([ 6, 18], dtype=int32)}
episode index:317
model initialize at round 317
at step 0:
{'currentState': array([17.02496283, 16.28657134,  1.98890704]), 'targetState': array([15, 23], dtype=int32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5381620858658468
{'currentState': array([14.99856608, 22.98925369,  2.7584525 ]), 'targetState': array([15, 23], dtype=int32)}
episode index:318
model initialize at round 318
at step 0:
{'currentState': array([23.93929352, 21.71882212,  5.00475216]), 'targetState': array([18, 11], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5384494261333621
{'currentState': array([17.91721349, 10.97396773,  3.83925024]), 'targetState': array([18, 11], dtype=int32)}
episode index:319
model initialize at round 319
at step 0:
{'currentState': array([27.21492722, 19.80881275,  5.05117702]), 'targetState': array([23, 22], dtype=int32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5393746271812269
{'currentState': array([23.09023039, 21.99873267,  2.25684232]), 'targetState': array([23, 22], dtype=int32)}
episode index:320
model initialize at round 320
at step 0:
{'currentState': array([18.78006992, 17.81458955,  3.33703375]), 'targetState': array([22, 23], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5401666510331348
{'currentState': array([21.99455993, 22.90666295,  2.1953142 ]), 'targetState': array([22, 23], dtype=int32)}
episode index:321
model initialize at round 321
at step 0:
{'currentState': array([16.18796337, 17.78224774,  4.91949558]), 'targetState': array([20,  5], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5403492266711255
{'currentState': array([19.90324326,  5.02354482,  5.13736334]), 'targetState': array([20,  5], dtype=int32)}
episode index:322
model initialize at round 322
at step 0:
{'currentState': array([14.7209895 , 26.06999582,  2.3907938 ]), 'targetState': array([13, 29], dtype=int32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5414205444576425
{'currentState': array([13.06414502, 29.02846718,  2.50473081]), 'targetState': array([13, 29], dtype=int32)}
episode index:323
model initialize at round 323
at step 0:
{'currentState': array([ 5.06509846, 23.28019362,  0.83751249]), 'targetState': array([ 2, 13], dtype=int32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.541793573767909
{'currentState': array([ 1.994349  , 13.0910412 ,  4.19665571]), 'targetState': array([ 2, 13], dtype=int32)}
episode index:324
model initialize at round 324
at step 0:
{'currentState': array([14.08258334, 20.72445279,  4.49857664]), 'targetState': array([13,  6], dtype=int32)}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.541760072134639
{'currentState': array([1.29721726e+01, 5.99559740e+00, 7.72693008e-03]), 'targetState': array([13,  6], dtype=int32)}
episode index:325
model initialize at round 325
at step 0:
{'currentState': array([14.82143526, 18.77447637,  4.54768133]), 'targetState': array([10,  4], dtype=int32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5417935795120248
{'currentState': array([9.95738316, 3.976     , 4.89335091]), 'targetState': array([10,  4], dtype=int32)}
episode index:326
model initialize at round 326
at step 0:
{'currentState': array([17.76947763, 20.8279369 ,  3.27778912]), 'targetState': array([10, 27], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.542245125403025
{'currentState': array([10.02175939, 27.03887689,  2.7507257 ]), 'targetState': array([10, 27], dtype=int32)}
episode index:327
model initialize at round 327
at step 0:
{'currentState': array([11.15259895, 14.24384388,  0.50661767]), 'targetState': array([26,  3], dtype=int32)}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5419840419345522
{'currentState': array([25.9831453 ,  2.97095252,  0.84784037]), 'targetState': array([26,  3], dtype=int32)}
episode index:328
model initialize at round 328
at step 0:
{'currentState': array([16.03819668, 22.71489074,  4.34056807]), 'targetState': array([ 8, 23], dtype=int32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5425850034283325
{'currentState': array([ 7.95093628, 22.92773271,  4.3654729 ]), 'targetState': array([ 8, 23], dtype=int32)}
episode index:329
model initialize at round 329
at step 0:
{'currentState': array([23.95143425,  3.28352715,  2.24544132]), 'targetState': array([14,  4], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5430300461023954
{'currentState': array([13.94822479,  3.9635625 ,  4.00315732]), 'targetState': array([14,  4], dtype=int32)}
episode index:330
model initialize at round 330
at step 0:
{'currentState': array([11.71371967, 16.02810417,  3.5487361 ]), 'targetState': array([ 3, 11], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5434515704193379
{'currentState': array([ 3.01019043, 11.06690163,  3.67972621]), 'targetState': array([ 3, 11], dtype=int32)}
episode index:331
model initialize at round 331
at step 0:
{'currentState': array([ 7.25629057, 27.13061936,  0.97634047]), 'targetState': array([19, 28], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5437697964974977
{'currentState': array([19.02957104, 28.06625027,  1.29909181]), 'targetState': array([19, 28], dtype=int32)}
episode index:332
model initialize at round 332
at step 0:
{'currentState': array([14.18103316, 15.22354702,  0.38509536]), 'targetState': array([26, 17], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5440666187364983
{'currentState': array([25.95092076, 16.90918376,  0.90035331]), 'targetState': array([26, 17], dtype=int32)}
episode index:333
model initialize at round 333
at step 0:
{'currentState': array([16.77458993,  7.17870807,  2.97624934]), 'targetState': array([5, 5], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5443810978072525
{'currentState': array([4.9829433 , 5.0102311 , 2.76880466]), 'targetState': array([5, 5], dtype=int32)}
episode index:334
model initialize at round 334
at step 0:
{'currentState': array([12.0728333 , 28.72171672,  4.4633708 ]), 'targetState': array([ 6, 22], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5447935560078588
{'currentState': array([ 5.97039701, 22.02708952,  5.88796169]), 'targetState': array([ 6, 22], dtype=int32)}
episode index:335
model initialize at round 335
at step 0:
{'currentState': array([19.75388895,  3.85108518,  4.19073296]), 'targetState': array([9, 3], dtype=int32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5451432241179068
{'currentState': array([9.0006895 , 2.94913615, 3.69378671]), 'targetState': array([9, 3], dtype=int32)}
episode index:336
model initialize at round 336
at step 0:
{'currentState': array([11.23677832, 23.83665283,  5.1742878 ]), 'targetState': array([ 2, 17], dtype=int32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5454908170462929
{'currentState': array([ 2.02322107, 16.98095945,  4.89333877]), 'targetState': array([ 2, 17], dtype=int32)}
episode index:337
model initialize at round 337
at step 0:
{'currentState': array([4.138123 , 9.2523258, 0.5649519]), 'targetState': array([12,  3], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5459167290842302
{'currentState': array([12.06041522,  3.02104011,  0.48817651]), 'targetState': array([12,  3], dtype=int32)}
episode index:338
model initialize at round 338
at step 0:
{'currentState': array([10.7258221 , 29.08702158,  2.3292582 ]), 'targetState': array([26, 29], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5459865947874541
{'currentState': array([25.96780386, 28.9233026 ,  0.222493  ]), 'targetState': array([26, 29], dtype=int32)}
episode index:339
model initialize at round 339
at step 0:
{'currentState': array([11.28582071,  2.96755315,  0.39196223]), 'targetState': array([24,  2], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5462707859853873
{'currentState': array([23.90659411,  1.90973286,  0.15242559]), 'targetState': array([24,  2], dtype=int32)}
episode index:340
model initialize at round 340
at step 0:
{'currentState': array([10.76431359, 18.83508125,  3.24714279]), 'targetState': array([ 5, 12], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5467736004915834
{'currentState': array([ 5.01875157, 11.91777741,  5.23719388]), 'targetState': array([ 5, 12], dtype=int32)}
episode index:341
model initialize at round 341
at step 0:
{'currentState': array([19.19038797, 26.21563557,  0.34250081]), 'targetState': array([11, 18], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5470350387534907
{'currentState': array([10.96525717, 17.94350369,  4.47371979]), 'targetState': array([11, 18], dtype=int32)}
episode index:342
model initialize at round 342
at step 0:
{'currentState': array([22.72435343,  8.08225109,  3.35661054]), 'targetState': array([11, 11], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5473517273302273
{'currentState': array([11.07167992, 10.98147739,  3.976536  ]), 'targetState': array([11, 11], dtype=int32)}
episode index:343
model initialize at round 343
at step 0:
{'currentState': array([27.14145788, 25.74952856,  4.72149849]), 'targetState': array([21, 16], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5475731624103464
{'currentState': array([20.97906198, 16.04328427,  5.46080513]), 'targetState': array([21, 16], dtype=int32)}
episode index:344
model initialize at round 344
at step 0:
{'currentState': array([12.2845997 ,  7.04182444,  0.65091437]), 'targetState': array([22, 18], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5476536887511191
{'currentState': array([21.94792069, 17.969109  ,  1.62623201]), 'targetState': array([22, 18], dtype=int32)}
episode index:345
model initialize at round 345
at step 0:
{'currentState': array([16.95697691, 18.2852803 ,  1.23338163]), 'targetState': array([14, 13], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5483189132327608
{'currentState': array([13.96768503, 12.97270412,  4.42350249]), 'targetState': array([14, 13], dtype=int32)}
episode index:346
model initialize at round 346
at step 0:
{'currentState': array([ 3.21292408, 16.19341565,  0.23242486]), 'targetState': array([18,  6], dtype=int32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5481937460153381
{'currentState': array([17.9778062 ,  6.08697606,  0.0296923 ]), 'targetState': array([18,  6], dtype=int32)}
episode index:347
model initialize at round 347
at step 0:
{'currentState': array([22.79123604, 13.19789867,  2.88790631]), 'targetState': array([12, 22], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5483745606518827
{'currentState': array([11.95681473, 21.99021428,  3.57689984]), 'targetState': array([12, 22], dtype=int32)}
episode index:348
model initialize at round 348
at step 0:
{'currentState': array([22.28328519,  7.04995772,  0.67955654]), 'targetState': array([25, 11], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5492185968488226
{'currentState': array([24.94233688, 11.06628401,  1.91051992]), 'targetState': array([25, 11], dtype=int32)}
episode index:349
model initialize at round 349
at step 0:
{'currentState': array([20.73568293, 16.88649774,  4.05219865]), 'targetState': array([25,  3], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5492932715720457
{'currentState': array([25.04006766,  2.9472972 ,  5.32408202]), 'targetState': array([25,  3], dtype=int32)}
episode index:350
model initialize at round 350
at step 0:
{'currentState': array([19.87054068, 24.74312151,  3.74056959]), 'targetState': array([ 7, 24], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5495408277386891
{'currentState': array([ 6.96662587, 23.93779219,  3.70682281]), 'targetState': array([ 7, 24], dtype=int32)}
episode index:351
model initialize at round 351
at step 0:
{'currentState': array([8.84022645, 9.23920428, 1.65467834]), 'targetState': array([17, 13], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5498993170038072
{'currentState': array([17.01369239, 12.9421207 ,  1.85833416]), 'targetState': array([17, 13], dtype=int32)}
episode index:352
model initialize at round 352
at step 0:
{'currentState': array([ 2.28218147, 27.05585601,  5.97360278]), 'targetState': array([10, 21], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5503342982445883
{'currentState': array([10.05979506, 21.04043418,  0.43439309]), 'targetState': array([10, 21], dtype=int32)}
episode index:353
model initialize at round 353
at step 0:
{'currentState': array([24.90013668, 24.73023419,  3.85284615]), 'targetState': array([22, 14], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5506885207045197
{'currentState': array([21.93994979, 14.02458985,  4.85505013]), 'targetState': array([22, 14], dtype=int32)}
episode index:354
model initialize at round 354
at step 0:
{'currentState': array([23.22591067, 23.82192517,  5.11064816]), 'targetState': array([ 8, 25], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5507257507601474
{'currentState': array([ 8.09142808, 25.03741799,  3.9824139 ]), 'targetState': array([ 8, 25], dtype=int32)}
episode index:355
model initialize at round 355
at step 0:
{'currentState': array([15.14656893, 13.7524847 ,  5.75202465]), 'targetState': array([25,  5], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5507949333422171
{'currentState': array([24.94079833,  5.06158041,  0.52119219]), 'targetState': array([25,  5], dtype=int32)}
episode index:356
model initialize at round 356
at step 0:
{'currentState': array([26.82681586,  5.22968136,  1.71186471]), 'targetState': array([26,  8], dtype=int32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5517853735149526
{'currentState': array([25.98605167,  7.97773946,  2.59024948]), 'targetState': array([26,  8], dtype=int32)}
episode index:357
model initialize at round 357
at step 0:
{'currentState': array([19.2670017 , 24.89296559,  5.39692451]), 'targetState': array([11, 26], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5522896137276326
{'currentState': array([10.95833689, 25.96035297,  4.02194342]), 'targetState': array([11, 26], dtype=int32)}
episode index:358
model initialize at round 358
at step 0:
{'currentState': array([25.84843871, 25.75550982,  3.6524601 ]), 'targetState': array([24, 29], dtype=int32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5530294948526175
{'currentState': array([24.06183117, 29.0296746 ,  3.55004448]), 'targetState': array([24, 29], dtype=int32)}
episode index:359
model initialize at round 359
at step 0:
{'currentState': array([10.96169654,  5.71490507,  4.07383537]), 'targetState': array([ 6, 16], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5532963646679392
{'currentState': array([ 5.93886358, 16.04023315,  2.9135202 ]), 'targetState': array([ 6, 16], dtype=int32)}
episode index:360
model initialize at round 360
at step 0:
{'currentState': array([21.19255516,  9.78629744,  5.95077991]), 'targetState': array([27,  2], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5537319792457907
{'currentState': array([26.99333697,  2.03407125,  6.20787612]), 'targetState': array([27,  2], dtype=int32)}
episode index:361
model initialize at round 361
at step 0:
{'currentState': array([20.26980702, 12.90024806,  0.15087002]), 'targetState': array([26, 18], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5543082519946894
{'currentState': array([25.95159188, 18.04779313,  1.73636788]), 'targetState': array([26, 18], dtype=int32)}
episode index:362
model initialize at round 362
at step 0:
{'currentState': array([11.28321305, 19.05036507,  0.68099469]), 'targetState': array([22, 18], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5546427445485891
{'currentState': array([22.02076914, 18.04684884,  0.55019927]), 'targetState': array([22, 18], dtype=int32)}
episode index:363
model initialize at round 363
at step 0:
{'currentState': array([14.24611996, 12.1489001 ,  0.0390805 ]), 'targetState': array([23, 21], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5547480294437894
{'currentState': array([23.04442245, 20.99093991,  2.74352469]), 'targetState': array([23, 21], dtype=int32)}
episode index:364
model initialize at round 364
at step 0:
{'currentState': array([ 5.7158472 , 23.95523993,  3.80282998]), 'targetState': array([ 4, 10], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5548527374354544
{'currentState': array([4.041952  , 9.913765  , 5.56594189]), 'targetState': array([ 4, 10], dtype=int32)}
episode index:365
model initialize at round 365
at step 0:
{'currentState': array([13.78169597, 10.18732225,  1.92742658]), 'targetState': array([22, 15], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.555278148609872
{'currentState': array([22.03757216, 15.01165798,  0.62025365]), 'targetState': array([22, 15], dtype=int32)}
episode index:366
model initialize at round 366
at step 0:
{'currentState': array([16.76777264, 22.8302451 ,  3.26781321]), 'targetState': array([ 3, 22], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.555380841519386
{'currentState': array([ 3.01590425, 21.9366495 ,  3.9789345 ]), 'targetState': array([ 3, 22], dtype=int32)}
episode index:367
model initialize at round 367
at step 0:
{'currentState': array([17.75655186,  8.1532295 ,  2.074826  ]), 'targetState': array([24, 12], dtype=int32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5559641629984234
{'currentState': array([23.96317338, 12.00962351,  1.32661791]), 'targetState': array([24, 12], dtype=int32)}
episode index:368
model initialize at round 368
at step 0:
{'currentState': array([22.75332151, 19.85202704,  4.18691039]), 'targetState': array([17,  6], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5560011197449783
{'currentState': array([17.02673907,  6.02746402,  4.41037505]), 'targetState': array([17,  6], dtype=int32)}
episode index:369
model initialize at round 369
at step 0:
{'currentState': array([24.17276616, 14.77000407,  4.85163879]), 'targetState': array([13, 19], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.556117214033414
{'currentState': array([13.0156211 , 19.01756156,  4.31027186]), 'targetState': array([13, 19], dtype=int32)}
episode index:370
model initialize at round 370
at step 0:
{'currentState': array([25.25792265,  8.87263368,  5.31949715]), 'targetState': array([22, 10], dtype=int32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5569598787200328
{'currentState': array([22.02592135, 10.02947625,  2.58636826]), 'targetState': array([22, 10], dtype=int32)}
episode index:371
model initialize at round 371
at step 0:
{'currentState': array([27.16032311, 20.23883629,  0.4746151 ]), 'targetState': array([20,  8], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.557056670568639
{'currentState': array([20.01843604,  8.01589433,  4.55766531]), 'targetState': array([20,  8], dtype=int32)}
episode index:372
model initialize at round 372
at step 0:
{'currentState': array([15.76637532, 19.83217336,  4.26953483]), 'targetState': array([20,  8], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5572517562539864
{'currentState': array([19.9112255 ,  8.04063082,  5.09205373]), 'targetState': array([20,  8], dtype=int32)}
episode index:373
model initialize at round 373
at step 0:
{'currentState': array([20.21511792,  1.80902734,  5.05217505]), 'targetState': array([8, 5], dtype=int32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5574122866943294
{'currentState': array([7.92410442, 4.91373317, 4.32798631]), 'targetState': array([8, 5], dtype=int32)}
episode index:374
model initialize at round 374
at step 0:
{'currentState': array([ 3.86846947, 21.74417099,  3.7325874 ]), 'targetState': array([ 4, 14], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5579384386956876
{'currentState': array([ 3.98478767, 14.02639436,  4.43285178]), 'targetState': array([ 4, 14], dtype=int32)}
episode index:375
model initialize at round 375
at step 0:
{'currentState': array([16.82675524, 28.77036436,  3.56105661]), 'targetState': array([ 9, 23], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5583254313986235
{'currentState': array([ 8.99398652, 22.95108075,  4.42063961]), 'targetState': array([ 9, 23], dtype=int32)}
episode index:376
model initialize at round 376
at step 0:
{'currentState': array([13.87952874, 16.73878562,  3.77526283]), 'targetState': array([ 6, 29], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5584173173800635
{'currentState': array([ 6.09506653, 28.90672314,  1.84428193]), 'targetState': array([ 6, 29], dtype=int32)}
episode index:377
model initialize at round 377
at step 0:
{'currentState': array([3.28194105, 4.05705714, 0.70467585]), 'targetState': array([14, 11], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5585895609713628
{'currentState': array([13.97965317, 10.94106247,  0.50234498]), 'targetState': array([14, 11], dtype=int32)}
episode index:378
model initialize at round 378
at step 0:
{'currentState': array([13.2637504 ,  3.88518711,  0.09443014]), 'targetState': array([21,  5], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5591070536527145
{'currentState': array([20.92879463,  4.96723909,  6.10565498]), 'targetState': array([21,  5], dtype=int32)}
episode index:379
model initialize at round 379
at step 0:
{'currentState': array([16.8657105 , 20.74561328,  4.73167515]), 'targetState': array([27,  9], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5591346698338315
{'currentState': array([27.07769496,  9.02564225,  0.17150752]), 'targetState': array([27,  9], dtype=int32)}
episode index:380
model initialize at round 380
at step 0:
{'currentState': array([25.24439527, 26.84828571,  5.2226305 ]), 'targetState': array([23, 26], dtype=int32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5600648078329123
{'currentState': array([22.9975199 , 26.03118896,  3.05108157]), 'targetState': array([23, 26], dtype=int32)}
episode index:381
model initialize at round 381
at step 0:
{'currentState': array([20.71390071,  3.97010903,  3.75069261]), 'targetState': array([18,  2], dtype=int32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5607832605910725
{'currentState': array([18.01370098,  2.02390845,  5.25501094]), 'targetState': array([18,  2], dtype=int32)}
episode index:382
model initialize at round 382
at step 0:
{'currentState': array([21.80092663, 16.79235591,  4.45306063]), 'targetState': array([10,  9], dtype=int32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5607766884708553
{'currentState': array([9.95964274, 8.93499389, 3.24584741]), 'targetState': array([10,  9], dtype=int32)}
episode index:383
model initialize at round 383
at step 0:
{'currentState': array([4.12812288, 3.74245233, 4.66901255]), 'targetState': array([ 2, 19], dtype=int32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.560644420392154
{'currentState': array([ 2.04308159, 19.01784483,  3.10439505]), 'targetState': array([ 2, 19], dtype=int32)}
episode index:384
model initialize at round 384
at step 0:
{'currentState': array([ 4.28753882, 13.0082282 ,  5.80679346]), 'targetState': array([ 5, 10], dtype=int32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5615137550255737
{'currentState': array([5.05572658, 9.98040651, 5.95526763]), 'targetState': array([ 5, 10], dtype=int32)}
episode index:385
model initialize at round 385
at step 0:
{'currentState': array([12.80073464, 24.79254015,  4.45213556]), 'targetState': array([22, 11], dtype=int32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5613670507088936
{'currentState': array([22.01404909, 10.95327282,  1.3355273 ]), 'targetState': array([22, 11], dtype=int32)}
episode index:386
model initialize at round 386
at step 0:
{'currentState': array([15.14285312, 13.24967832,  0.54610753]), 'targetState': array([22, 15], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5619263641680415
{'currentState': array([21.95969525, 14.93607917,  0.17352915]), 'targetState': array([22, 15], dtype=int32)}
episode index:387
model initialize at round 387
at step 0:
{'currentState': array([8.90444077, 9.7286797 , 3.86875486]), 'targetState': array([ 5, 10], dtype=int32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.562586107913993
{'currentState': array([5.00957598, 9.95845979, 4.12929356]), 'targetState': array([ 5, 10], dtype=int32)}
episode index:388
model initialize at round 388
at step 0:
{'currentState': array([13.71236916, 22.996156  ,  2.64995623]), 'targetState': array([10, 23], dtype=int32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5633957092381189
{'currentState': array([ 9.96574806, 23.03226645,  2.84306116]), 'targetState': array([10, 23], dtype=int32)}
episode index:389
model initialize at round 389
at step 0:
{'currentState': array([16.7287999 , 18.09589985,  3.30670285]), 'targetState': array([ 8, 13], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5636322823441087
{'currentState': array([ 8.03332491, 12.98280275,  5.06616784]), 'targetState': array([ 8, 13], dtype=int32)}
episode index:390
model initialize at round 390
at step 0:
{'currentState': array([ 7.00324868, 16.28763818,  1.05450249]), 'targetState': array([21, 20], dtype=int32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5636771266708964
{'currentState': array([21.0646147 , 19.97182142,  5.88776995]), 'targetState': array([21, 20], dtype=int32)}
episode index:391
model initialize at round 391
at step 0:
{'currentState': array([14.80418681, 19.21072131,  1.81453967]), 'targetState': array([16, 17], dtype=int32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5643680364534964
{'currentState': array([15.97701357, 16.97907951,  0.31983181]), 'targetState': array([16, 17], dtype=int32)}
episode index:392
model initialize at round 392
at step 0:
{'currentState': array([17.24176426, 23.84412724,  5.20552349]), 'targetState': array([ 2, 22], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.564395992467551
{'currentState': array([ 2.06952595, 21.96056679,  4.08740054]), 'targetState': array([ 2, 22], dtype=int32)}
episode index:393
model initialize at round 393
at step 0:
{'currentState': array([19.73600123, 21.88575936,  3.04499388]), 'targetState': array([5, 9], dtype=int32)}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.5641341294598112
{'currentState': array([5.00538649, 9.00550068, 5.70973769]), 'targetState': array([5, 9], dtype=int32)}
episode index:394
model initialize at round 394
at step 0:
{'currentState': array([23.98541376, 13.71271353,  4.15666008]), 'targetState': array([ 9, 13], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5641625360940318
{'currentState': array([ 9.08287174, 13.02064501,  2.67546561]), 'targetState': array([ 9, 13], dtype=int32)}
episode index:395
model initialize at round 395
at step 0:
{'currentState': array([21.14899483, 11.75393738,  4.75185442]), 'targetState': array([16, 23], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5643124423031155
{'currentState': array([15.98415582, 22.96771467,  2.18158343]), 'targetState': array([16, 23], dtype=int32)}
episode index:396
model initialize at round 396
at step 0:
{'currentState': array([12.86132082,  4.74797944,  3.70433903]), 'targetState': array([17, 14], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5645096694058399
{'currentState': array([17.05580109, 13.97880586,  2.7255154 ]), 'targetState': array([17, 14], dtype=int32)}
episode index:397
model initialize at round 397
at step 0:
{'currentState': array([23.77754416, 21.81762765,  3.32329893]), 'targetState': array([11, 12], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5645224622025015
{'currentState': array([11.0087119 , 12.02933121,  3.96848688]), 'targetState': array([11, 12], dtype=int32)}
episode index:398
model initialize at round 398
at step 0:
{'currentState': array([16.83792889, 17.23765359,  1.66431451]), 'targetState': array([26, 26], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5646703392267839
{'currentState': array([26.02166486, 26.09796102,  1.35610984]), 'targetState': array([26, 26], dtype=int32)}
episode index:399
model initialize at round 399
at step 0:
{'currentState': array([20.77753804, 18.18236489,  1.94991994]), 'targetState': array([17, 25], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5651454615967261
{'currentState': array([16.92381988, 25.0373449 ,  2.79955909]), 'targetState': array([17, 25], dtype=int32)}
episode index:400
model initialize at round 400
at step 0:
{'currentState': array([23.71698376, 15.05145951,  3.46673226]), 'targetState': array([10, 25], dtype=int32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.565073448538398
{'currentState': array([10.07623058, 24.93431979,  2.01734111]), 'targetState': array([10, 25], dtype=int32)}
episode index:401
model initialize at round 401
at step 0:
{'currentState': array([14.71595047,  4.9545892 ,  3.80512047]), 'targetState': array([12,  3], dtype=int32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5657436980730042
{'currentState': array([11.98042226,  3.01520384,  5.39634655]), 'targetState': array([12,  3], dtype=int32)}
episode index:402
model initialize at round 402
at step 0:
{'currentState': array([ 7.95459764, 20.28405088,  1.22429442]), 'targetState': array([ 4, 11], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5660166145766947
{'currentState': array([ 3.90932828, 11.09636312,  4.84410816]), 'targetState': array([ 4, 11], dtype=int32)}
episode index:403
model initialize at round 403
at step 0:
{'currentState': array([10.87724877, 14.26015075,  1.50666857]), 'targetState': array([ 5, 28], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5660833220812116
{'currentState': array([ 4.96650027, 27.97755259,  2.74854638]), 'targetState': array([ 5, 28], dtype=int32)}
episode index:404
model initialize at round 404
at step 0:
{'currentState': array([18.03152705, 27.71407637,  4.31720924]), 'targetState': array([ 4, 29], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5661794276245605
{'currentState': array([ 4.06299153, 29.09439476,  3.35638813]), 'targetState': array([ 4, 29], dtype=int32)}
episode index:405
model initialize at round 405
at step 0:
{'currentState': array([19.21531494,  7.19075049,  0.21997797]), 'targetState': array([23,  4], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5668611117766772
{'currentState': array([22.90742584,  3.9741965 ,  5.54921106]), 'targetState': array([23,  4], dtype=int32)}
episode index:406
model initialize at round 406
at step 0:
{'currentState': array([ 8.88371716, 20.26310564,  1.48194623]), 'targetState': array([ 4, 27], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5671794216200996
{'currentState': array([ 3.98207666, 27.00855956,  4.42227901]), 'targetState': array([ 4, 27], dtype=int32)}
episode index:407
model initialize at round 407
at step 0:
{'currentState': array([26.71787474,  8.9438608 ,  2.83301377]), 'targetState': array([16, 14], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.5657892759788739
{'currentState': array([16.11554843, 13.84270938,  3.92634994]), 'targetState': array([16, 14], dtype=int32)}
episode index:408
model initialize at round 408
at step 0:
{'currentState': array([ 6.71263966, 14.98694971,  2.68197584]), 'targetState': array([ 6, 24], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5661784961744215
{'currentState': array([ 5.97366586, 23.94228722,  2.86558684]), 'targetState': array([ 6, 24], dtype=int32)}
episode index:409
model initialize at round 409
at step 0:
{'currentState': array([ 6.26644408, 22.89158494,  0.11856287]), 'targetState': array([21, 14], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5661321014090955
{'currentState': array([21.0169414 , 14.04467954,  5.95208842]), 'targetState': array([21, 14], dtype=int32)}
episode index:410
model initialize at round 410
at step 0:
{'currentState': array([17.71240392, 19.99410348,  3.66709256]), 'targetState': array([ 4, 13], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5661545409433238
{'currentState': array([ 4.01223843, 12.91796401,  3.45540701]), 'targetState': array([ 4, 13], dtype=int32)}
episode index:411
model initialize at round 411
at step 0:
{'currentState': array([13.79832475,  8.20511793,  1.84273171]), 'targetState': array([ 5, 20], dtype=int32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5660560990660962
{'currentState': array([ 4.9221989 , 19.93533775,  2.13479561]), 'targetState': array([ 5, 20], dtype=int32)}
episode index:412
model initialize at round 412
at step 0:
{'currentState': array([15.72613089, 22.08798857,  2.32572937]), 'targetState': array([16, 29], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5665878715642587
{'currentState': array([16.08048458, 28.92928048,  2.3163526 ]), 'targetState': array([16, 29], dtype=int32)}
episode index:413
model initialize at round 413
at step 0:
{'currentState': array([ 8.27210759, 21.0932938 ,  6.10848189]), 'targetState': array([9, 8], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5667254018138407
{'currentState': array([9.03604185, 7.94926306, 6.20061615]), 'targetState': array([9, 8], dtype=int32)}
episode index:414
model initialize at round 414
at step 0:
{'currentState': array([18.06854142, 11.72062865,  4.4479785 ]), 'targetState': array([15, 13], dtype=int32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5674956655967377
{'currentState': array([15.06244034, 13.02779072,  2.8063192 ]), 'targetState': array([15, 13], dtype=int32)}
episode index:415
model initialize at round 415
at step 0:
{'currentState': array([ 4.79374528, 24.79948749,  4.41787505]), 'targetState': array([ 2, 16], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5678568071039529
{'currentState': array([ 1.97705241, 15.91705046,  4.42700055]), 'targetState': array([ 2, 16], dtype=int32)}
episode index:416
model initialize at round 416
at step 0:
{'currentState': array([16.74697241, 22.13683316,  3.15085292]), 'targetState': array([ 8, 23], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5682511633690619
{'currentState': array([ 8.07321582, 23.08896005,  3.69134502]), 'targetState': array([ 8, 23], dtype=int32)}
episode index:417
model initialize at round 417
at step 0:
{'currentState': array([11.72387599, 13.91936623,  2.92071247]), 'targetState': array([ 5, 22], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5685411105520763
{'currentState': array([ 4.96646671, 22.07596519,  2.44681174]), 'targetState': array([ 5, 22], dtype=int32)}
episode index:418
model initialize at round 418
at step 0:
{'currentState': array([17.71820024,  8.94224908,  2.83872986]), 'targetState': array([21, 20], dtype=int32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5687648359230356
{'currentState': array([21.00958695, 20.08701075,  1.66350496]), 'targetState': array([21, 20], dtype=int32)}
episode index:419
model initialize at round 419
at step 0:
{'currentState': array([ 7.06532315, 19.71985868,  4.4364748 ]), 'targetState': array([ 2, 28], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5690687606423845
{'currentState': array([ 2.00794959, 28.05584408,  3.10627627]), 'targetState': array([ 2, 28], dtype=int32)}
episode index:420
model initialize at round 420
at step 0:
{'currentState': array([20.94774627, 15.71712932,  4.02472162]), 'targetState': array([21, 17], dtype=int32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5699309853152221
{'currentState': array([21.01962408, 17.08029791,  1.92568751]), 'targetState': array([21, 17], dtype=int32)}
episode index:421
model initialize at round 421
at step 0:
{'currentState': array([25.72526974,  3.0852617 ,  2.33567047]), 'targetState': array([24,  4], dtype=int32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5707891236152026
{'currentState': array([23.93316561,  4.02719765,  2.916739  ]), 'targetState': array([24,  4], dtype=int32)}
episode index:422
model initialize at round 422
at step 0:
{'currentState': array([14.79922549, 26.20599968,  1.83834982]), 'targetState': array([19, 26], dtype=int32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5713348481209328
{'currentState': array([19.04420085, 26.01733869,  6.07326312]), 'targetState': array([19, 26], dtype=int32)}
episode index:423
model initialize at round 423
at step 0:
{'currentState': array([15.2930811 ,  6.05113986,  6.23022357]), 'targetState': array([24, 13], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5714727933640515
{'currentState': array([24.02453326, 13.03743643,  1.52327389]), 'targetState': array([24, 13], dtype=int32)}
episode index:424
model initialize at round 424
at step 0:
{'currentState': array([ 5.09772171, 22.27054896,  0.7191807 ]), 'targetState': array([13,  7], dtype=int32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5713899590860354
{'currentState': array([13.00932756,  6.96555445,  0.10382205]), 'targetState': array([13,  7], dtype=int32)}
episode index:425
model initialize at round 425
at step 0:
{'currentState': array([12.26983877, 28.900334  ,  5.42437382]), 'targetState': array([ 3, 19], dtype=int32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5714129085109934
{'currentState': array([ 3.00161165, 18.97292461,  5.49502786]), 'targetState': array([ 3, 19], dtype=int32)}
episode index:426
model initialize at round 426
at step 0:
{'currentState': array([13.28583041,  8.03236122,  5.8909235 ]), 'targetState': array([13,  7], dtype=int32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5723243443458622
{'currentState': array([12.99502395,  7.07540533,  4.08310789]), 'targetState': array([13,  7], dtype=int32)}
episode index:427
model initialize at round 427
at step 0:
{'currentState': array([20.21177828, 24.80533045,  6.04485559]), 'targetState': array([25, 27], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5728044775586035
{'currentState': array([25.07165319, 27.05018547,  2.25048355]), 'targetState': array([25, 27], dtype=int32)}
episode index:428
model initialize at round 428
at step 0:
{'currentState': array([ 8.75301209, 26.85254409,  3.17481565]), 'targetState': array([ 3, 25], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5733567465345071
{'currentState': array([ 2.9863309 , 24.92939289,  4.46653843]), 'targetState': array([ 3, 25], dtype=int32)}
episode index:429
model initialize at round 429
at step 0:
{'currentState': array([10.18821756, 15.76746082,  5.26233605]), 'targetState': array([25,  2], dtype=int32)}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.573151245659483
{'currentState': array([25.08041531,  1.98954816,  6.27112181]), 'targetState': array([25,  2], dtype=int32)}
episode index:430
model initialize at round 430
at step 0:
{'currentState': array([20.16408443, 28.76373198,  4.81440353]), 'targetState': array([19, 19], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5734535576069079
{'currentState': array([19.0334398, 18.908348 ,  5.2008943]), 'targetState': array([19, 19], dtype=int32)}
episode index:431
model initialize at round 431
at step 0:
{'currentState': array([23.23170194,  2.82952863,  5.14387608]), 'targetState': array([10,  3], dtype=int32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5734714114414246
{'currentState': array([9.9479047 , 2.97082712, 3.34515852]), 'targetState': array([10,  3], dtype=int32)}
episode index:432
model initialize at round 432
at step 0:
{'currentState': array([4.78318324, 1.8109583 , 3.35366201]), 'targetState': array([2, 8], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5739614962667497
{'currentState': array([2.04634946, 7.9805258 , 1.65756971]), 'targetState': array([2, 8], dtype=int32)}
episode index:433
model initialize at round 433
at step 0:
{'currentState': array([19.77909182, 10.815756  ,  3.33174181]), 'targetState': array([26, 18], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5742598515633692
{'currentState': array([26.05186795, 17.98343814,  0.37690717]), 'targetState': array([26, 18], dtype=int32)}
episode index:434
model initialize at round 434
at step 0:
{'currentState': array([19.84780794,  4.24409804,  1.62330723]), 'targetState': array([26, 13], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5744169820243379
{'currentState': array([26.07813481, 13.03803174,  2.19021628]), 'targetState': array([26, 13], dtype=int32)}
episode index:435
model initialize at round 435
at step 0:
{'currentState': array([ 8.2090488 , 11.19759776,  0.2522459 ]), 'targetState': array([23,  3], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.574354458309582
{'currentState': array([22.96783966,  2.94704655,  5.67500952]), 'targetState': array([23,  3], dtype=int32)}
episode index:436
model initialize at round 436
at step 0:
{'currentState': array([9.82013248, 3.77551403, 3.53189135]), 'targetState': array([15, 19], dtype=int32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5743048679637074
{'currentState': array([15.05685781, 19.03518119,  1.48683387]), 'targetState': array([15, 19], dtype=int32)}
episode index:437
model initialize at round 437
at step 0:
{'currentState': array([6.23848938, 4.16083871, 1.09834159]), 'targetState': array([ 5, 11], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5746655951365172
{'currentState': array([ 4.95564733, 11.04194676,  2.10255561]), 'targetState': array([ 5, 11], dtype=int32)}
episode index:438
model initialize at round 438
at step 0:
{'currentState': array([ 5.13103351, 26.25607908,  0.59283924]), 'targetState': array([11, 20], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5749270609468419
{'currentState': array([11.05964485, 19.98418152,  0.8929793 ]), 'targetState': array([11, 20], dtype=int32)}
episode index:439
model initialize at round 439
at step 0:
{'currentState': array([11.26622916, 21.89105822,  5.38977055]), 'targetState': array([25,  8], dtype=int32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5747678764646605
{'currentState': array([24.94494417,  7.92880121,  5.96797938]), 'targetState': array([25,  8], dtype=int32)}
episode index:440
model initialize at round 440
at step 0:
{'currentState': array([ 3.96579627, 20.71438421,  5.09820223]), 'targetState': array([ 8, 17], dtype=int32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5753379461865901
{'currentState': array([ 7.96537135, 16.98204905,  5.67310041]), 'targetState': array([ 8, 17], dtype=int32)}
episode index:441
model initialize at round 441
at step 0:
{'currentState': array([15.23600583, 23.83553869,  5.16957474]), 'targetState': array([23,  9], dtype=int32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5752135176265533
{'currentState': array([23.03790284,  8.94218929,  0.55096462]), 'targetState': array([23,  9], dtype=int32)}
episode index:442
model initialize at round 442
at step 0:
{'currentState': array([17.24821713, 24.1453772 ,  0.02482772]), 'targetState': array([25, 23], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5755190248718033
{'currentState': array([25.04996495, 23.03384233,  0.89820604]), 'targetState': array([25, 23], dtype=int32)}
episode index:443
model initialize at round 443
at step 0:
{'currentState': array([19.24332174, 18.15343013,  0.05759096]), 'targetState': array([22,  4], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5755718108663851
{'currentState': array([21.99175077,  3.97247248,  5.60657669]), 'targetState': array([22,  4], dtype=int32)}
episode index:444
model initialize at round 444
at step 0:
{'currentState': array([19.06653864, 23.27985511,  0.83236945]), 'targetState': array([14, 11], dtype=int32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5754595067689899
{'currentState': array([13.99185939, 10.91109051,  1.33034294]), 'targetState': array([14, 11], dtype=int32)}
episode index:445
model initialize at round 445
at step 0:
{'currentState': array([13.79876391,  8.78205774,  4.2291339 ]), 'targetState': array([9, 4], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5759308489977751
{'currentState': array([8.97136819, 3.95991416, 4.38738929]), 'targetState': array([9, 4], dtype=int32)}
episode index:446
model initialize at round 446
at step 0:
{'currentState': array([15.19619822, 23.78963714,  4.95796108]), 'targetState': array([11, 23], dtype=int32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5765664810014519
{'currentState': array([11.07112765, 23.03590301,  3.22993293]), 'targetState': array([11, 23], dtype=int32)}
episode index:447
model initialize at round 447
at step 0:
{'currentState': array([4.17805504, 5.22592627, 1.40834677]), 'targetState': array([10, 17], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5766713000056701
{'currentState': array([10.00632058, 16.91036605,  1.42988592]), 'targetState': array([10, 17], dtype=int32)}
episode index:448
model initialize at round 448
at step 0:
{'currentState': array([18.72320388, 18.0782955 ,  2.36093068]), 'targetState': array([13, 25], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5770016096625792
{'currentState': array([12.97723781, 25.01450208,  3.04355225]), 'targetState': array([13, 25], dtype=int32)}
episode index:449
model initialize at round 449
at step 0:
{'currentState': array([10.21412275, 12.8079122 ,  6.05697918]), 'targetState': array([14, 13], dtype=int32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5776694305811045
{'currentState': array([14.05878284, 12.99974194,  0.34600815]), 'targetState': array([14, 13], dtype=int32)}
episode index:450
model initialize at round 450
at step 0:
{'currentState': array([25.11046865,  9.26559923,  1.68163961]), 'targetState': array([12, 22], dtype=int32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5775423154859143
{'currentState': array([12.09700987, 22.01628107,  2.77994766]), 'targetState': array([12, 22], dtype=int32)}
episode index:451
model initialize at round 451
at step 0:
{'currentState': array([ 9.92367359, 20.27734555,  1.33435154]), 'targetState': array([11, 18], dtype=int32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5782454038460312
{'currentState': array([11.06760885, 17.92218318,  4.70270293]), 'targetState': array([11, 18], dtype=int32)}
episode index:452
model initialize at round 452
at step 0:
{'currentState': array([3.00701775, 3.71242909, 4.23178768]), 'targetState': array([2, 7], dtype=int32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5788866851019316
{'currentState': array([1.94415398, 7.02448802, 2.85097556]), 'targetState': array([2, 7], dtype=int32)}
episode index:453
model initialize at round 453
at step 0:
{'currentState': array([17.75328003, 21.85209621,  3.17663002]), 'targetState': array([ 6, 10], dtype=int32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5787462675519799
{'currentState': array([5.94408649, 9.97971851, 6.21167057]), 'targetState': array([ 6, 10], dtype=int32)}
episode index:454
model initialize at round 454
at step 0:
{'currentState': array([7.27673651, 2.9214941 , 0.22857779]), 'targetState': array([19,  3], dtype=int32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.578830979361629
{'currentState': array([19.01627265,  3.0234223 ,  1.47083858]), 'targetState': array([19,  3], dtype=int32)}
episode index:455
model initialize at round 455
at step 0:
{'currentState': array([8.06561749, 3.28007255, 0.83566028]), 'targetState': array([8, 7], dtype=int32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5794860013871495
{'currentState': array([8.07763374, 6.99542427, 2.16239095]), 'targetState': array([8, 7], dtype=int32)}
episode index:456
model initialize at round 456
at step 0:
{'currentState': array([ 6.72453819, 18.91713222,  2.92881274]), 'targetState': array([ 5, 19], dtype=int32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5802781111202213
{'currentState': array([ 4.94974672, 19.03332812,  2.66524087]), 'targetState': array([ 5, 19], dtype=int32)}
episode index:457
model initialize at round 457
at step 0:
{'currentState': array([10.9885778 ,  8.28742966,  1.10551453]), 'targetState': array([25,  8], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5803454454617336
{'currentState': array([2.49459512e+01, 7.98564966e+00, 2.87145376e-03]), 'targetState': array([25,  8], dtype=int32)}
episode index:458
model initialize at round 458
at step 0:
{'currentState': array([23.71293299,  9.01840667,  2.57256055]), 'targetState': array([21, 21], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.580495243245844
{'currentState': array([20.99859849, 21.03876343,  2.54558322]), 'targetState': array([21, 21], dtype=int32)}
episode index:459
model initialize at round 459
at step 0:
{'currentState': array([23.75032223,  3.85714592,  3.15628529]), 'targetState': array([27, 12], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5808093412734788
{'currentState': array([27.01047147, 12.04972309,  1.57675445]), 'targetState': array([27, 12], dtype=int32)}
episode index:460
model initialize at round 460
at step 0:
{'currentState': array([4.91076955, 8.726533  , 3.8919878 ]), 'targetState': array([18, 23], dtype=int32)}
done in step count: 72
reward sum = 0.48499137027416284
running average episode reward sum: 0.5806014931802047
{'currentState': array([18.00665407, 23.0268938 ,  0.78817803]), 'targetState': array([18, 23], dtype=int32)}
episode index:461
model initialize at round 461
at step 0:
{'currentState': array([21.80237388, 12.20902199,  2.83317792]), 'targetState': array([12, 19], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5807639557936115
{'currentState': array([11.93409613, 19.02715123,  2.78017922]), 'targetState': array([12, 19], dtype=int32)}
episode index:462
model initialize at round 462
at step 0:
{'currentState': array([13.112946  , 27.72231647,  5.03090462]), 'targetState': array([15, 24], dtype=int32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5813859468453941
{'currentState': array([14.97868488, 24.00178351,  4.99018311]), 'targetState': array([15, 24], dtype=int32)}
episode index:463
model initialize at round 463
at step 0:
{'currentState': array([14.27587944,  6.0814666 ,  0.79213732]), 'targetState': array([21, 14], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5816338504471273
{'currentState': array([20.97616599, 13.9666212 ,  1.71363858]), 'targetState': array([21, 14], dtype=int32)}
episode index:464
model initialize at round 464
at step 0:
{'currentState': array([ 2.28143039, 23.05952485,  0.7134363 ]), 'targetState': array([ 5, 26], dtype=int32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5822512955273894
{'currentState': array([ 5.04295639, 25.99545237,  0.68883855]), 'targetState': array([ 5, 26], dtype=int32)}
episode index:465
model initialize at round 465
at step 0:
{'currentState': array([ 6.71483255, 10.96224022,  3.77823949]), 'targetState': array([2, 5], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5825420235039364
{'currentState': array([2.04172685, 4.96124514, 5.968543  ]), 'targetState': array([2, 5], dtype=int32)}
episode index:466
model initialize at round 466
at step 0:
{'currentState': array([ 3.71258452, 13.01177366,  2.59565163]), 'targetState': array([10, 10], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5828161374306352
{'currentState': array([10.04026709, 10.03910236,  1.50783271]), 'targetState': array([10, 10], dtype=int32)}
episode index:467
model initialize at round 467
at step 0:
{'currentState': array([13.23863027, 18.1606296 ,  0.08746505]), 'targetState': array([9, 6], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5829165807933971
{'currentState': array([8.95470525, 6.03923406, 5.30405057]), 'targetState': array([9, 6], dtype=int32)}
episode index:468
model initialize at round 468
at step 0:
{'currentState': array([ 5.92227399, 29.27695657,  1.33940148]), 'targetState': array([ 7, 20], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.58315857788776
{'currentState': array([ 6.93311047, 19.97980435,  4.79607457]), 'targetState': array([ 7, 20], dtype=int32)}
episode index:469
model initialize at round 469
at step 0:
{'currentState': array([ 5.08258092, 26.27554794,  0.77461743]), 'targetState': array([ 5, 20], dtype=int32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5834916455377612
{'currentState': array([ 5.01081043, 19.91901501,  6.14760581]), 'targetState': array([ 5, 20], dtype=int32)}
episode index:470
model initialize at round 470
at step 0:
{'currentState': array([17.14791983, 14.75328965,  4.74749136]), 'targetState': array([2, 3], dtype=int32)}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.583174744648704
{'currentState': array([1.97656825, 2.96513844, 4.69475658]), 'targetState': array([2, 3], dtype=int32)}
episode index:471
model initialize at round 471
at step 0:
{'currentState': array([14.80626199, 20.7873692 ,  4.47844911]), 'targetState': array([ 4, 10], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5831829269310954
{'currentState': array([ 4.0169136 , 10.04059946,  3.85885832]), 'targetState': array([ 4, 10], dtype=int32)}
episode index:472
model initialize at round 472
at step 0:
{'currentState': array([17.05324959, 25.71731509,  4.39357805]), 'targetState': array([10, 12], dtype=int32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5830173940808966
{'currentState': array([ 9.96682831, 11.92780266,  0.8253018 ]), 'targetState': array([10, 12], dtype=int32)}
episode index:473
model initialize at round 473
at step 0:
{'currentState': array([16.74469458, 10.86746542,  3.11542034]), 'targetState': array([27, 25], dtype=int32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.5828633188745014
{'currentState': array([26.95357324, 25.05331669,  2.12884429]), 'targetState': array([27, 25], dtype=int32)}
episode index:474
model initialize at round 474
at step 0:
{'currentState': array([12.75489564, 26.84943397,  4.19745588]), 'targetState': array([10, 13], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.582909935186634
{'currentState': array([ 9.99941388, 12.98322813,  4.45020592]), 'targetState': array([10, 13], dtype=int32)}
episode index:475
model initialize at round 475
at step 0:
{'currentState': array([11.74248219, 16.87181711,  3.09844923]), 'targetState': array([18, 25], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5831049333250241
{'currentState': array([17.96644673, 25.05298102,  1.82655349]), 'targetState': array([18, 25], dtype=int32)}
episode index:476
model initialize at round 476
at step 0:
{'currentState': array([15.84481596, 21.24220691,  1.63561201]), 'targetState': array([17,  7], dtype=int32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5830067431612551
{'currentState': array([16.95811198,  6.94604493,  5.93573359]), 'targetState': array([17,  7], dtype=int32)}
episode index:477
model initialize at round 477
at step 0:
{'currentState': array([ 4.18169694, 12.22330173,  1.3880251 ]), 'targetState': array([10, 17], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5832885920931318
{'currentState': array([10.00519888, 17.05043119,  2.67373932]), 'targetState': array([10, 17], dtype=int32)}
episode index:478
model initialize at round 478
at step 0:
{'currentState': array([16.71295406, 25.98126752,  3.71175975]), 'targetState': array([ 9, 26], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5836624002815534
{'currentState': array([ 8.94802068, 26.0122552 ,  2.98957025]), 'targetState': array([ 9, 26], dtype=int32)}
episode index:479
model initialize at round 479
at step 0:
{'currentState': array([ 8.28110069, 16.93893713,  5.56428081]), 'targetState': array([ 5, 16], dtype=int32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.584274605745548
{'currentState': array([ 5.07324994, 16.04766281,  3.64387425]), 'targetState': array([ 5, 16], dtype=int32)}
episode index:480
model initialize at round 480
at step 0:
{'currentState': array([19.83044574, 14.23237389,  1.69615316]), 'targetState': array([26, 23], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5844789300475539
{'currentState': array([26.04174038, 23.00016966,  0.64745429]), 'targetState': array([26, 23], dtype=int32)}
episode index:481
model initialize at round 481
at step 0:
{'currentState': array([ 6.12771469, 22.74224967,  5.67742825]), 'targetState': array([19, 10], dtype=int32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5842826880204931
{'currentState': array([18.96656744,  9.97586663,  5.06740681]), 'targetState': array([19, 10], dtype=int32)}
episode index:482
model initialize at round 482
at step 0:
{'currentState': array([ 4.71256907, 20.01139019,  2.59698582]), 'targetState': array([16, 28], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5842522915700927
{'currentState': array([16.01510081, 28.02693691,  2.24229329]), 'targetState': array([16, 28], dtype=int32)}
episode index:483
model initialize at round 483
at step 0:
{'currentState': array([12.73167429, 19.89630917,  3.00550306]), 'targetState': array([2, 5], dtype=int32)}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.5839697934754516
{'currentState': array([2.04103253, 4.98313647, 6.28235442]), 'targetState': array([2, 5], dtype=int32)}
episode index:484
model initialize at round 484
at step 0:
{'currentState': array([ 7.78476358, 29.19083909,  1.91120315]), 'targetState': array([19, 25], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5840907044210378
{'currentState': array([18.91536079, 24.94004267,  0.04726455]), 'targetState': array([19, 25], dtype=int32)}
episode index:485
model initialize at round 485
at step 0:
{'currentState': array([14.91935278,  3.27612008,  1.34996486]), 'targetState': array([27, 12], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5840967663089317
{'currentState': array([27.02474175, 11.9675439 ,  0.35568634]), 'targetState': array([27, 12], dtype=int32)}
episode index:486
model initialize at round 486
at step 0:
{'currentState': array([11.00515891, 19.28761026,  2.05786109]), 'targetState': array([ 8, 19], dtype=int32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5847358658735103
{'currentState': array([ 7.92499736, 18.92759753,  4.40368216]), 'targetState': array([ 8, 19], dtype=int32)}
episode index:487
model initialize at round 487
at step 0:
{'currentState': array([18.74396831, 18.13112608,  2.16327405]), 'targetState': array([20, 18], dtype=int32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5854668992413945
{'currentState': array([19.93729385, 17.9841031 ,  5.4934022 ]), 'targetState': array([20, 18], dtype=int32)}
episode index:488
model initialize at round 488
at step 0:
{'currentState': array([16.03801953, 27.71486707,  4.33994675]), 'targetState': array([12, 28], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5859255106299014
{'currentState': array([11.9489801 , 27.97862491,  4.68893452]), 'targetState': array([12, 28], dtype=int32)}
episode index:489
model initialize at round 489
at step 0:
{'currentState': array([10.28753674, 10.00830037,  5.80704445]), 'targetState': array([6, 5], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5863331690588347
{'currentState': array([5.99340392, 5.05700628, 4.3083337 ]), 'targetState': array([6, 5], dtype=int32)}
episode index:490
model initialize at round 490
at step 0:
{'currentState': array([14.20076988,  2.2060042 ,  0.29326534]), 'targetState': array([26, 14], dtype=int32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5862760060639041
{'currentState': array([26.03225725, 13.9899461 ,  1.04901382]), 'targetState': array([26, 14], dtype=int32)}
episode index:491
model initialize at round 491
at step 0:
{'currentState': array([ 6.92762151, 15.27840192,  1.32014418]), 'targetState': array([ 8, 10], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5867301765154433
{'currentState': array([ 7.9562286 , 10.02495199,  4.42051264]), 'targetState': array([ 8, 10], dtype=int32)}
episode index:492
model initialize at round 492
at step 0:
{'currentState': array([ 7.20969549, 16.80308866,  6.03421783]), 'targetState': array([21,  7], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5866499056551499
{'currentState': array([21.00497364,  6.99272777,  6.13987832]), 'targetState': array([21,  7], dtype=int32)}
episode index:493
model initialize at round 493
at step 0:
{'currentState': array([10.7816785 , 21.81269811,  3.34566545]), 'targetState': array([12, 29], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5870055591140404
{'currentState': array([12.05192159, 28.99454637,  1.21429539]), 'targetState': array([12, 29], dtype=int32)}
episode index:494
model initialize at round 494
at step 0:
{'currentState': array([15.76800549, 15.829927  ,  3.26918364]), 'targetState': array([27, 26], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5869820221258847
{'currentState': array([27.05747571, 25.96050731,  5.97844647]), 'targetState': array([27, 26], dtype=int32)}
episode index:495
model initialize at round 495
at step 0:
{'currentState': array([18.87082758,  6.25702288,  1.53149915]), 'targetState': array([24,  9], dtype=int32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5874475965522382
{'currentState': array([23.92242405,  8.96588535,  0.52887172]), 'targetState': array([24,  9], dtype=int32)}
episode index:496
model initialize at round 496
at step 0:
{'currentState': array([16.79633307, 12.79685951,  3.42069674]), 'targetState': array([ 8, 22], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5875328602034475
{'currentState': array([ 8.08568957, 21.98083957,  2.67963038]), 'targetState': array([ 8, 22], dtype=int32)}
episode index:497
model initialize at round 497
at step 0:
{'currentState': array([25.9913826 , 10.28752742,  2.10575807]), 'targetState': array([11, 18], dtype=int32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.587462881522642
{'currentState': array([10.99636908, 17.99478562,  2.99019919]), 'targetState': array([11, 18], dtype=int32)}
episode index:498
model initialize at round 498
at step 0:
{'currentState': array([ 9.72006424, 22.93380145,  2.86880445]), 'targetState': array([ 4, 29], dtype=int32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5877829400653454
{'currentState': array([ 4.01208074, 29.04637662,  2.49431748]), 'targetState': array([ 4, 29], dtype=int32)}
episode index:499
model initialize at round 499
at step 0:
{'currentState': array([13.91183136, 10.27381119,  1.37731719]), 'targetState': array([25, 11], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5878670214476212
{'currentState': array([25.0296865 , 11.04561063,  1.81703132]), 'targetState': array([25, 11], dtype=int32)}
episode index:500
model initialize at round 500
at step 0:
{'currentState': array([4.80618971, 7.78743508, 3.46810913]), 'targetState': array([10, 23], dtype=int32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.587732238016888
{'currentState': array([ 9.96269875, 23.03345387,  3.11892671]), 'targetState': array([10, 23], dtype=int32)}
episode index:501
model initialize at round 501
at step 0:
{'currentState': array([27.04053617, 18.71521395,  4.34877825]), 'targetState': array([23, 26], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5880202283189547
{'currentState': array([22.95308123, 26.01899301,  3.04603383]), 'targetState': array([23, 26], dtype=int32)}
episode index:502
model initialize at round 502
at step 0:
{'currentState': array([15.02917171, 13.71382648,  4.30897522]), 'targetState': array([17,  6], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5883516379787652
{'currentState': array([16.93905666,  5.94963281,  5.21369217]), 'targetState': array([17,  6], dtype=int32)}
episode index:503
model initialize at round 503
at step 0:
{'currentState': array([15.73559286, 12.11329227,  2.23178101]), 'targetState': array([ 7, 19], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5884339236796075
{'currentState': array([ 7.01137789, 19.07943531,  4.03598019]), 'targetState': array([ 7, 19], dtype=int32)}
episode index:504
model initialize at round 504
at step 0:
{'currentState': array([ 8.72226199, 21.07488573,  3.3832283 ]), 'targetState': array([ 5, 14], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5887632016271797
{'currentState': array([ 4.94738548, 14.02790144,  3.44254407]), 'targetState': array([ 5, 14], dtype=int32)}
episode index:505
model initialize at round 505
at step 0:
{'currentState': array([25.76400081, 26.16447084,  3.03794169]), 'targetState': array([20, 16], dtype=int32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5889084957761063
{'currentState': array([20.04291426, 16.06201367,  4.076261  ]), 'targetState': array([20, 16], dtype=int32)}
episode index:506
model initialize at round 506
at step 0:
{'currentState': array([ 8.71921243, 27.93751306,  3.86556602]), 'targetState': array([14, 17], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5890272218758937
{'currentState': array([13.90621335, 16.97282595,  5.01164629]), 'targetState': array([14, 17], dtype=int32)}
episode index:507
model initialize at round 507
at step 0:
{'currentState': array([12.25056083, 16.85870051,  5.26470804]), 'targetState': array([17, 11], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5892805748497567
{'currentState': array([17.06908078, 10.97560626,  0.79327758]), 'targetState': array([17, 11], dtype=int32)}
episode index:508
model initialize at round 508
at step 0:
{'currentState': array([ 7.10848517, 10.73358446,  5.60408926]), 'targetState': array([13, 16], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5896056017895482
{'currentState': array([13.09896015, 15.95642615,  1.13856148]), 'targetState': array([13, 16], dtype=int32)}
episode index:509
model initialize at round 509
at step 0:
{'currentState': array([16.86585762,  2.25446433,  2.56093192]), 'targetState': array([4, 4], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5896969348959684
{'currentState': array([4.06889685, 4.08872156, 2.98315051]), 'targetState': array([4, 4], dtype=int32)}
episode index:510
model initialize at round 510
at step 0:
{'currentState': array([22.7294073 ,  5.09760055,  2.29042482]), 'targetState': array([25, 16], dtype=int32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5898520715372086
{'currentState': array([25.03504123, 16.06662417,  1.1527115 ]), 'targetState': array([25, 16], dtype=int32)}
episode index:511
model initialize at round 511
at step 0:
{'currentState': array([25.14446978, 26.75125363,  4.73356485]), 'targetState': array([14, 29], dtype=int32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5899935363212844
{'currentState': array([13.93644879, 29.03461332,  2.60436482]), 'targetState': array([14, 29], dtype=int32)}
episode index:512
model initialize at round 512
at step 0:
{'currentState': array([19.72008254, 11.06627584,  2.40410471]), 'targetState': array([10, 24], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5899426818459064
{'currentState': array([10.05864124, 24.06134574,  2.11273976]), 'targetState': array([10, 24], dtype=int32)}
episode index:513
model initialize at round 513
at step 0:
{'currentState': array([9.76631009, 8.1677358 , 3.02403927]), 'targetState': array([2, 8], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5902780904694496
{'currentState': array([2.03260073, 8.08759641, 4.00844708]), 'targetState': array([2, 8], dtype=int32)}
episode index:514
model initialize at round 514
at step 0:
{'currentState': array([13.80957429, 12.78439776,  4.49391842]), 'targetState': array([12,  8], dtype=int32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5907523344907711
{'currentState': array([12.01355842,  7.91282253,  5.22373366]), 'targetState': array([12,  8], dtype=int32)}
episode index:515
model initialize at round 515
at step 0:
{'currentState': array([24.72143002, 11.07172893,  3.39457726]), 'targetState': array([22, 10], dtype=int32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5913601440654185
{'currentState': array([21.96279173, 10.05042541,  2.84583065]), 'targetState': array([22, 10], dtype=int32)}
episode index:516
model initialize at round 516
at step 0:
{'currentState': array([14.18916102, 13.78328732,  4.92500877]), 'targetState': array([12,  4], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5915769478389855
{'currentState': array([11.95795374,  4.00007987,  4.82182753]), 'targetState': array([12,  4], dtype=int32)}
episode index:517
model initialize at round 517
at step 0:
{'currentState': array([23.71882917, 21.93926092,  2.84934545]), 'targetState': array([18, 25], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5919669813057898
{'currentState': array([17.90120539, 25.01535239,  3.24797797]), 'targetState': array([18, 25], dtype=int32)}
episode index:518
model initialize at round 518
at step 0:
{'currentState': array([10.21092189, 27.19559712,  0.24271846]), 'targetState': array([20, 24], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5921682264632924
{'currentState': array([20.05693763, 23.94072795,  5.70551709]), 'targetState': array([20, 24], dtype=int32)}
episode index:519
model initialize at round 519
at step 0:
{'currentState': array([17.28724094, 14.98454307,  5.72442544]), 'targetState': array([27,  8], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5922903245288902
{'currentState': array([26.93577088,  8.07171283,  5.60893058]), 'targetState': array([27,  8], dtype=int32)}
episode index:520
model initialize at round 520
at step 0:
{'currentState': array([3.24933401, 3.85654678, 6.26609135]), 'targetState': array([ 5, 18], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5923147309446457
{'currentState': array([ 5.00836223, 18.01659834,  1.61403495]), 'targetState': array([ 5, 18], dtype=int32)}
episode index:521
model initialize at round 521
at step 0:
{'currentState': array([21.90185906,  6.72960283,  3.85922337]), 'targetState': array([15, 22], dtype=int32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5921668811103147
{'currentState': array([14.92970565, 22.03291371,  3.79351843]), 'targetState': array([15, 22], dtype=int32)}
episode index:522
model initialize at round 522
at step 0:
{'currentState': array([20.72461383, 16.08311877,  2.34346128]), 'targetState': array([16, 29], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5921347355440941
{'currentState': array([15.93164212, 29.03041031,  4.26367648]), 'targetState': array([16, 29], dtype=int32)}
episode index:523
model initialize at round 523
at step 0:
{'currentState': array([23.95404532, 17.28396204,  2.23623919]), 'targetState': array([14, 17], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.592347164856032
{'currentState': array([14.07403967, 16.99118704,  3.11375932]), 'targetState': array([14, 17], dtype=int32)}
episode index:524
model initialize at round 524
at step 0:
{'currentState': array([14.15106047, 11.24479994,  1.52291459]), 'targetState': array([ 5, 22], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.592382917379226
{'currentState': array([ 5.02078818, 21.99793244,  2.32254422]), 'targetState': array([ 5, 22], dtype=int32)}
episode index:525
model initialize at round 525
at step 0:
{'currentState': array([15.07069942,  4.72116696,  4.45571041]), 'targetState': array([14,  4], dtype=int32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5929588780957269
{'currentState': array([13.91418737,  3.98128154,  2.99601103]), 'targetState': array([14,  4], dtype=int32)}
episode index:526
model initialize at round 526
at step 0:
{'currentState': array([22.90341124, 16.7290445 ,  4.8749578 ]), 'targetState': array([26, 11], dtype=int32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5932515027944666
{'currentState': array([25.96570855, 10.97418446,  4.90698672]), 'targetState': array([26, 11], dtype=int32)}
episode index:527
model initialize at round 527
at step 0:
{'currentState': array([ 5.22579762, 17.17821816,  0.16317177]), 'targetState': array([18, 28], dtype=int32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5931746694125877
{'currentState': array([17.9349294 , 28.07485604,  2.38864055]), 'targetState': array([18, 28], dtype=int32)}
episode index:528
model initialize at round 528
at step 0:
{'currentState': array([22.2213059 , 28.18376608,  0.18798769]), 'targetState': array([13, 18], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5931970350037501
{'currentState': array([12.94373705, 17.97957419,  3.38625996]), 'targetState': array([13, 18], dtype=int32)}
episode index:529
model initialize at round 529
at step 0:
{'currentState': array([20.72207751,  3.92580186,  2.89748216]), 'targetState': array([26,  6], dtype=int32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.593590305861364
{'currentState': array([26.08468295,  5.95998435,  6.05181814]), 'targetState': array([26,  6], dtype=int32)}
episode index:530
model initialize at round 530
at step 0:
{'currentState': array([8.26587369, 9.10980645, 0.89666479]), 'targetState': array([ 4, 16], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.593797193599854
{'currentState': array([ 3.99555284, 16.04963194,  1.95981862]), 'targetState': array([ 4, 16], dtype=int32)}
episode index:531
model initialize at round 531
at step 0:
{'currentState': array([13.92919057, 20.72119487,  3.958673  ]), 'targetState': array([11, 25], dtype=int32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5942339820025527
{'currentState': array([11.00630515, 24.97968722,  2.31550157]), 'targetState': array([11, 25], dtype=int32)}
episode index:532
model initialize at round 532
at step 0:
{'currentState': array([10.77500088,  5.8207748 ,  3.30923223]), 'targetState': array([ 2, 21], dtype=int32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5941252282374583
{'currentState': array([ 2.00224187, 20.98576564,  1.85958312]), 'targetState': array([ 2, 21], dtype=int32)}
episode index:533
model initialize at round 533
at step 0:
{'currentState': array([13.73591692, 26.88595438,  4.05425525]), 'targetState': array([10, 21], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5944692284830794
{'currentState': array([ 9.96411972, 20.9517988 ,  4.84014934]), 'targetState': array([10, 21], dtype=int32)}
episode index:534
model initialize at round 534
at step 0:
{'currentState': array([16.95215365,  3.28364943,  2.24290454]), 'targetState': array([9, 7], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5946467609267915
{'currentState': array([8.9681001 , 7.02361728, 4.1148378 ]), 'targetState': array([9, 7], dtype=int32)}
episode index:535
model initialize at round 535
at step 0:
{'currentState': array([10.76543702, 22.83348729,  3.25392199]), 'targetState': array([ 3, 18], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5948899205817002
{'currentState': array([ 2.96285944, 18.03519117,  3.17473551]), 'targetState': array([ 3, 18], dtype=int32)}
episode index:536
model initialize at round 536
at step 0:
{'currentState': array([19.23452839, 20.81293524,  5.5994483 ]), 'targetState': array([23, 17], dtype=int32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5953206071799384
{'currentState': array([23.07431658, 17.05592623,  0.37505086]), 'targetState': array([23, 17], dtype=int32)}
episode index:537
model initialize at round 537
at step 0:
{'currentState': array([22.75985221, 26.84164814,  4.22954297]), 'targetState': array([25, 25], dtype=int32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5958782607990439
{'currentState': array([25.06080959, 25.03595689,  0.40832968]), 'targetState': array([25, 25], dtype=int32)}
episode index:538
model initialize at round 538
at step 0:
{'currentState': array([25.75093993, 17.14392831,  2.11259222]), 'targetState': array([14, 23], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5959412392227994
{'currentState': array([14.06590216, 22.98450163,  2.79725692]), 'targetState': array([14, 23], dtype=int32)}
episode index:539
model initialize at round 539
at step 0:
{'currentState': array([17.06809454, 13.72006776,  4.45517755]), 'targetState': array([24,  7], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.596127298442849
{'currentState': array([23.94239526,  7.02541944,  5.78446753]), 'targetState': array([24,  7], dtype=int32)}
episode index:540
model initialize at round 540
at step 0:
{'currentState': array([24.7242252 , 17.91817988,  2.92501163]), 'targetState': array([19, 28], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5962013431519452
{'currentState': array([18.93738215, 28.01413176,  3.03157776]), 'targetState': array([19, 28], dtype=int32)}
episode index:541
model initialize at round 541
at step 0:
{'currentState': array([16.28026282,  2.93520012,  0.27778166]), 'targetState': array([25,  2], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5964524539019497
{'currentState': array([24.97179584,  2.04242393,  1.15292839]), 'targetState': array([25,  2], dtype=int32)}
episode index:542
model initialize at round 542
at step 0:
{'currentState': array([ 8.13153944, 14.74418043,  5.69232273]), 'targetState': array([16, 22], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5966110213809708
{'currentState': array([16.03893118, 21.97630787,  0.20378938]), 'targetState': array([16, 22], dtype=int32)}
episode index:543
model initialize at round 543
at step 0:
{'currentState': array([21.71235902,  9.98600749,  3.68843293]), 'targetState': array([19, 10], dtype=int32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5969878955871437
{'currentState': array([19.06465736,  9.90963535,  2.35291692]), 'targetState': array([19, 10], dtype=int32)}
episode index:544
model initialize at round 544
at step 0:
{'currentState': array([17.83473578, 17.23544428,  1.67781305]), 'targetState': array([22, 25], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5972361808606617
{'currentState': array([22.09039959, 25.01684017,  0.46429555]), 'targetState': array([22, 25], dtype=int32)}
episode index:545
model initialize at round 545
at step 0:
{'currentState': array([16.74008111, 12.12988619,  3.11901796]), 'targetState': array([10, 24], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.597261604045043
{'currentState': array([10.01953549, 24.08113815,  2.95346941]), 'targetState': array([10, 24], dtype=int32)}
episode index:546
model initialize at round 546
at step 0:
{'currentState': array([11.23523512, 19.16556181,  1.11828122]), 'targetState': array([15, 24], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5974687185299192
{'currentState': array([15.07989373, 23.98099592,  0.90060809]), 'targetState': array([15, 24], dtype=int32)}
episode index:547
model initialize at round 547
at step 0:
{'currentState': array([ 4.81434541, 13.21972402,  1.76734805]), 'targetState': array([16,  5], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5974074712159091
{'currentState': array([16.04855906,  4.98167215,  5.64496739]), 'targetState': array([16,  5], dtype=int32)}
episode index:548
model initialize at round 548
at step 0:
{'currentState': array([ 8.12457916, 13.74071964,  5.66529942]), 'targetState': array([18, 29], dtype=int32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5972670942604161
{'currentState': array([18.05333728, 29.01527095,  0.80182034]), 'targetState': array([18, 29], dtype=int32)}
episode index:549
model initialize at round 549
at step 0:
{'currentState': array([3.84847865, 9.24451494, 1.62056184]), 'targetState': array([17,  8], dtype=int32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5973034997998378
{'currentState': array([17.05951634,  7.94281808,  5.45303822]), 'targetState': array([17,  8], dtype=int32)}
episode index:550
model initialize at round 550
at step 0:
{'currentState': array([18.22188354,  8.18306821,  1.19483835]), 'targetState': array([16,  9], dtype=int32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5978773904489917
{'currentState': array([15.97910807,  9.03962705,  2.5828501 ]), 'targetState': array([16,  9], dtype=int32)}
episode index:551
model initialize at round 551
at step 0:
{'currentState': array([12.11545463, 17.73652988,  4.62039328]), 'targetState': array([ 7, 28], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5979013756828393
{'currentState': array([ 6.96462025, 28.05272643,  2.48978316]), 'targetState': array([ 7, 28], dtype=int32)}
episode index:552
model initialize at round 552
at step 0:
{'currentState': array([15.28180938, 18.05770397,  5.98015589]), 'targetState': array([24, 14], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5980058202486463
{'currentState': array([23.94649137, 13.93328982,  1.03408624]), 'targetState': array([24, 14], dtype=int32)}
episode index:553
model initialize at round 553
at step 0:
{'currentState': array([22.74206638, 21.13258027,  3.12050849]), 'targetState': array([24, 14], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5983024572416038
{'currentState': array([23.99832832, 13.90728416,  4.90049347]), 'targetState': array([24, 14], dtype=int32)}
episode index:554
model initialize at round 554
at step 0:
{'currentState': array([18.11266524, 10.73532507,  4.60983038]), 'targetState': array([ 4, 11], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5982507432690553
{'currentState': array([ 4.0339762 , 11.06912151,  4.72663864]), 'targetState': array([ 4, 11], dtype=int32)}
episode index:555
model initialize at round 555
at step 0:
{'currentState': array([ 4.00109991, 13.28765442,  1.06197262]), 'targetState': array([16,  3], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5981889706920468
{'currentState': array([16.04868851,  2.91690843,  5.280387  ]), 'targetState': array([16,  3], dtype=int32)}
episode index:556
model initialize at round 556
at step 0:
{'currentState': array([16.99164174, 22.71246493,  4.17832851]), 'targetState': array([ 7, 20], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5982803776178571
{'currentState': array([ 7.00565976, 19.93456075,  4.96026025]), 'targetState': array([ 7, 20], dtype=int32)}
episode index:557
model initialize at round 557
at step 0:
{'currentState': array([15.22683281, 25.8105849 ,  5.28436708]), 'targetState': array([19, 24], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5986593157730604
{'currentState': array([19.03329095, 24.01805418,  5.83628393]), 'targetState': array([19, 24], dtype=int32)}
episode index:558
model initialize at round 558
at step 0:
{'currentState': array([13.28119221, 11.06064003,  5.99058581]), 'targetState': array([19,  5], dtype=int32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5989249915844351
{'currentState': array([18.98031798,  4.96112029,  4.88026597]), 'targetState': array([19,  5], dtype=int32)}
episode index:559
model initialize at round 559
at step 0:
{'currentState': array([26.79374872,  3.79948395,  3.40789223]), 'targetState': array([12,  8], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5988325481037321
{'currentState': array([11.93777426,  7.98474504,  2.2574444 ]), 'targetState': array([12,  8], dtype=int32)}
episode index:560
model initialize at round 560
at step 0:
{'currentState': array([24.71682753,  2.05059274,  3.46979403]), 'targetState': array([11,  6], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5988435525939884
{'currentState': array([10.98073455,  6.036544  ,  2.23688353]), 'targetState': array([11,  6], dtype=int32)}
episode index:561
model initialize at round 561
at step 0:
{'currentState': array([19.71248501, 28.99097738,  2.66796374]), 'targetState': array([24, 29], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5992778935918353
{'currentState': array([23.92873535, 29.04704561,  6.22816915]), 'targetState': array([24, 29], dtype=int32)}
episode index:562
model initialize at round 562
at step 0:
{'currentState': array([19.22447493, 10.82011871,  5.10262632]), 'targetState': array([6, 4], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5992666832060619
{'currentState': array([6.07274122, 3.99087691, 3.85056224]), 'targetState': array([6, 4], dtype=int32)}
episode index:563
model initialize at round 563
at step 0:
{'currentState': array([16.14140329,  4.25050226,  1.56190473]), 'targetState': array([15, 17], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5993208621918724
{'currentState': array([14.95724628, 17.04561042,  2.11982632]), 'targetState': array([15, 17], dtype=int32)}
episode index:564
model initialize at round 564
at step 0:
{'currentState': array([23.10020655, 15.2696385 ,  0.70998085]), 'targetState': array([24,  5], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5993861093137698
{'currentState': array([24.04684502,  4.9389777 ,  4.83456884]), 'targetState': array([24,  5], dtype=int32)}
episode index:565
model initialize at round 565
at step 0:
{'currentState': array([ 7.28733195, 12.98633892,  5.73067648]), 'targetState': array([22, 13], dtype=int32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5993134591887418
{'currentState': array([22.0342391 , 13.0258715 ,  6.27643117]), 'targetState': array([22, 13], dtype=int32)}
episode index:566
model initialize at round 566
at step 0:
{'currentState': array([12.89358068, 16.26724746,  2.45475447]), 'targetState': array([ 3, 28], dtype=int32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5991834468930396
{'currentState': array([ 2.95359968, 28.09148885,  1.41389176]), 'targetState': array([ 3, 28], dtype=int32)}
episode index:567
model initialize at round 567
at step 0:
{'currentState': array([ 9.97040396, 20.28612995,  1.16886544]), 'targetState': array([21, 24], dtype=int32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5992945359671433
{'currentState': array([21.00459395, 23.97818361,  0.49807367]), 'targetState': array([21, 24], dtype=int32)}
episode index:568
model initialize at round 568
at step 0:
{'currentState': array([21.21528256,  8.19132037,  1.22414888]), 'targetState': array([26, 11], dtype=int32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5996932601988981
{'currentState': array([25.9358835 , 10.96656689,  0.65032461]), 'targetState': array([26, 11], dtype=int32)}
episode index:569
model initialize at round 569
at step 0:
{'currentState': array([4.22685394, 5.1768716 , 0.15722215]), 'targetState': array([17,  5], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5997572816477841
{'currentState': array([17.03682053,  5.00729889,  6.19511176]), 'targetState': array([17,  5], dtype=int32)}
episode index:570
model initialize at round 570
at step 0:
{'currentState': array([ 9.00105853, 18.28765457,  2.07211649]), 'targetState': array([ 5, 21], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6001831764143972
{'currentState': array([ 4.96594767, 21.06698034,  2.34361747]), 'targetState': array([ 5, 21], dtype=int32)}
episode index:571
model initialize at round 571
at step 0:
{'currentState': array([22.20064302, 18.20612776,  0.29388094]), 'targetState': array([21, 15], dtype=int32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.600563812360521
{'currentState': array([20.97718979, 14.96095107,  4.25013066]), 'targetState': array([21, 15], dtype=int32)}
episode index:572
model initialize at round 572
at step 0:
{'currentState': array([21.27887511, 16.07053332,  0.75272578]), 'targetState': array([16, 28], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.600614876616791
{'currentState': array([15.94859251, 27.96030281,  2.2151843 ]), 'targetState': array([16, 28], dtype=int32)}
episode index:573
model initialize at round 573
at step 0:
{'currentState': array([17.7575007 ,  9.84527327,  4.21452141]), 'targetState': array([20,  4], dtype=int32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6009650782072479
{'currentState': array([20.02778902,  3.9967629 ,  5.30573051]), 'targetState': array([20,  4], dtype=int32)}
episode index:574
model initialize at round 574
at step 0:
{'currentState': array([22.01430843, 16.71269956,  5.26715088]), 'targetState': array([23,  6], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6010375069444261
{'currentState': array([22.99929371,  5.96434259,  5.00277147]), 'targetState': array([23,  6], dtype=int32)}
episode index:575
model initialize at round 575
at step 0:
{'currentState': array([16.87093934, 11.26084603,  2.46005905]), 'targetState': array([13, 25], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6010443968058725
{'currentState': array([12.95858619, 24.97014144,  1.96802821]), 'targetState': array([13, 25], dtype=int32)}
episode index:576
model initialize at round 576
at step 0:
{'currentState': array([14.06702338, 23.27973942,  0.83063698]), 'targetState': array([24, 22], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.60122187219269
{'currentState': array([23.99224798, 21.9976921 ,  5.62895009]), 'targetState': array([24, 22], dtype=int32)}
episode index:577
model initialize at round 577
at step 0:
{'currentState': array([12.88709662, 22.26457343,  1.46913767]), 'targetState': array([20, 27], dtype=int32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6014614543746893
{'currentState': array([19.96995799, 27.0079205 ,  0.95342871]), 'targetState': array([20, 27], dtype=int32)}
episode index:578
model initialize at round 578
at step 0:
{'currentState': array([26.21882662, 24.8132885 ,  5.07181358]), 'targetState': array([20, 17], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6016375964137651
{'currentState': array([20.06742202, 16.98905841,  4.22771965]), 'targetState': array([20, 17], dtype=int32)}
episode index:579
model initialize at round 579
at step 0:
{'currentState': array([10.28636913, 27.02718453,  5.87282998]), 'targetState': array([ 2, 23], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6018253819842109
{'currentState': array([ 2.08025268, 23.03136085,  4.22385435]), 'targetState': array([ 2, 23], dtype=int32)}
episode index:580
model initialize at round 580
at step 0:
{'currentState': array([10.88984328,  6.73427125,  3.81440639]), 'targetState': array([10,  3], dtype=int32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6021972951608253
{'currentState': array([10.03509874,  3.04578715,  6.24314812]), 'targetState': array([10,  3], dtype=int32)}
episode index:581
model initialize at round 581
at step 0:
{'currentState': array([15.28218475, 25.05583943,  5.97354401]), 'targetState': array([23, 27], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6024593604392493
{'currentState': array([23.05946957, 27.03967039,  0.49420652]), 'targetState': array([23, 27], dtype=int32)}
episode index:582
model initialize at round 582
at step 0:
{'currentState': array([3.11459657, 7.26384446, 0.65604651]), 'targetState': array([5, 2], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6027336028987825
{'currentState': array([5.05763876, 1.92834294, 4.91718691]), 'targetState': array([5, 2], dtype=int32)}
episode index:583
model initialize at round 583
at step 0:
{'currentState': array([13.87314724,  3.25817562,  2.53249431]), 'targetState': array([10,  8], dtype=int32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6030741799307008
{'currentState': array([9.93504817, 8.08048418, 1.89022435]), 'targetState': array([10,  8], dtype=int32)}
episode index:584
model initialize at round 584
at step 0:
{'currentState': array([15.28699533, 10.01949237,  0.57281464]), 'targetState': array([26, 25], dtype=int32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.6029327548755207
{'currentState': array([26.07998812, 25.03962133,  0.28272511]), 'targetState': array([26, 25], dtype=int32)}
episode index:585
model initialize at round 585
at step 0:
{'currentState': array([ 9.25260867, 15.137605  ,  6.27697796]), 'targetState': array([12, 13], dtype=int32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.603416461559549
{'currentState': array([11.93487787, 12.98153021,  5.90100968]), 'targetState': array([12, 13], dtype=int32)}
episode index:586
model initialize at round 586
at step 0:
{'currentState': array([20.87303188, 27.72825291,  4.24199407]), 'targetState': array([18, 19], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.60362355504234
{'currentState': array([17.9970998 , 19.06724697,  5.20362637]), 'targetState': array([18, 19], dtype=int32)}
episode index:587
model initialize at round 587
at step 0:
{'currentState': array([17.00390503,  4.28763001,  2.06222057]), 'targetState': array([9, 3], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6037933239878455
{'currentState': array([8.94156403, 2.96588071, 3.99813685]), 'targetState': array([9, 3], dtype=int32)}
episode index:588
model initialize at round 588
at step 0:
{'currentState': array([6.92238326, 9.72301279, 3.93417835]), 'targetState': array([6, 7], dtype=int32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6043036614259116
{'currentState': array([5.94912732, 6.99177624, 3.73559429]), 'targetState': array([6, 7], dtype=int32)}
episode index:589
model initialize at round 589
at step 0:
{'currentState': array([14.28741473, 28.01179173,  5.81918919]), 'targetState': array([22, 27], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.604571524227473
{'currentState': array([21.93602923, 27.00674362,  6.23149759]), 'targetState': array([22, 27], dtype=int32)}
episode index:590
model initialize at round 590
at step 0:
{'currentState': array([25.24999694, 14.14229477,  1.02245396]), 'targetState': array([11, 26], dtype=int32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.6044290013821647
{'currentState': array([11.03720884, 26.00611018,  1.57286206]), 'targetState': array([11, 26], dtype=int32)}
episode index:591
model initialize at round 591
at step 0:
{'currentState': array([ 9.28382741, 20.953221  ,  0.34165352]), 'targetState': array([15, 23], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6047757900085822
{'currentState': array([14.99564334, 22.97791804,  0.44232023]), 'targetState': array([15, 23], dtype=int32)}
episode index:592
model initialize at round 592
at step 0:
{'currentState': array([22.13030252, 12.74354819,  4.67749357]), 'targetState': array([ 7, 25], dtype=int32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.6045820538922173
{'currentState': array([ 6.95336402, 25.01703631,  3.07846382]), 'targetState': array([ 7, 25], dtype=int32)}
episode index:593
model initialize at round 593
at step 0:
{'currentState': array([12.02478289, 11.71341305,  5.30365038]), 'targetState': array([22,  6], dtype=int32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.604690454068442
{'currentState': array([22.01601687,  6.03653716,  5.92275688]), 'targetState': array([22,  6], dtype=int32)}
episode index:594
model initialize at round 594
at step 0:
{'currentState': array([14.73298897, 21.10701114,  3.26541901]), 'targetState': array([ 3, 12], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6046707498538757
{'currentState': array([ 2.93037363, 11.95340639,  3.9459777 ]), 'targetState': array([ 3, 12], dtype=int32)}
episode index:595
model initialize at round 595
at step 0:
{'currentState': array([19.8405431 , 17.76058452,  3.61983013]), 'targetState': array([12, 29], dtype=int32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6046919233288564
{'currentState': array([12.09364211, 28.97939527,  2.88580912]), 'targetState': array([12, 29], dtype=int32)}
episode index:596
model initialize at round 596
at step 0:
{'currentState': array([21.19063084,  3.21542088,  1.35137391]), 'targetState': array([15, 16], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6046823154279138
{'currentState': array([14.93652644, 16.02152042,  2.67211846]), 'targetState': array([15, 16], dtype=int32)}
episode index:597
model initialize at round 597
at step 0:
{'currentState': array([20.17383513, 20.22918905,  1.42689058]), 'targetState': array([20, 26], dtype=int32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6050388783412404
{'currentState': array([19.97055022, 25.92151162,  2.10326178]), 'targetState': array([20, 26], dtype=int32)}
episode index:598
model initialize at round 598
at step 0:
{'currentState': array([20.71250949, 24.99022858,  2.67056823]), 'targetState': array([14, 25], dtype=int32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.605263688850501
{'currentState': array([13.90062192, 24.96346929,  2.73192406]), 'targetState': array([14, 25], dtype=int32)}
episode index:599
model initialize at round 599
at step 0:
{'currentState': array([1.72495570e+01, 2.81430650e+01, 1.55376196e-02]), 'targetState': array([ 9, 17], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6052531760465271
{'currentState': array([ 8.95322081, 16.93044196,  4.11626274]), 'targetState': array([ 9, 17], dtype=int32)}
episode index:600
model initialize at round 600
at step 0:
{'currentState': array([25.77918911, 15.81563942,  3.33226967]), 'targetState': array([26, 20], dtype=int32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6055799271505079
{'currentState': array([25.99588418, 20.08508779,  1.74637716]), 'targetState': array([26, 20], dtype=int32)}
episode index:601
model initialize at round 601
at step 0:
{'currentState': array([19.98189679, 25.71291369,  4.14441395]), 'targetState': array([18, 28], dtype=int32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6059602159118028
{'currentState': array([17.93455076, 28.06382134,  1.7671828 ]), 'targetState': array([18, 28], dtype=int32)}
episode index:602
model initialize at round 602
at step 0:
{'currentState': array([12.15820281,  9.74619031,  5.40701663]), 'targetState': array([13,  9], dtype=int32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.606548334973309
{'currentState': array([13.04329379,  8.96420992,  5.30457398]), 'targetState': array([13,  9], dtype=int32)}
episode index:603
model initialize at round 603
at step 0:
{'currentState': array([18.05183869, 21.70535744,  4.75619689]), 'targetState': array([25,  8], dtype=int32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6065063119255355
{'currentState': array([25.0415737 ,  7.94023963,  5.05861202]), 'targetState': array([25,  8], dtype=int32)}
episode index:604
model initialize at round 604
at step 0:
{'currentState': array([10.76092089, 15.15996079,  2.04692769]), 'targetState': array([25, 28], dtype=int32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.606329999062682
{'currentState': array([25.0805721 , 28.08975339,  0.66534056]), 'targetState': array([25, 28], dtype=int32)}
episode index:605
model initialize at round 605
at step 0:
{'currentState': array([14.78021701,  9.81441521,  3.33782673]), 'targetState': array([2, 6], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6062981620707262
{'currentState': array([2.02542926, 5.97268642, 4.77792618]), 'targetState': array([2, 6], dtype=int32)}
episode index:606
model initialize at round 606
at step 0:
{'currentState': array([23.0451671 ,  4.71591162,  4.3650589 ]), 'targetState': array([12,  2], dtype=int32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6064014134652879
{'currentState': array([12.02870079,  2.048048  ,  3.83852693]), 'targetState': array([12,  2], dtype=int32)}
episode index:607
model initialize at round 607
at step 0:
{'currentState': array([25.7730458 , 28.17674294,  2.98493755]), 'targetState': array([18, 20], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.606515439181727
{'currentState': array([18.04283313, 20.07205277,  4.10969418]), 'targetState': array([18, 20], dtype=int32)}
episode index:608
model initialize at round 608
at step 0:
{'currentState': array([19.27979674, 15.06678365,  6.01248777]), 'targetState': array([16, 12], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6069036620950311
{'currentState': array([15.98739405, 11.9912136 ,  3.88354858]), 'targetState': array([16, 12], dtype=int32)}
episode index:609
model initialize at round 609
at step 0:
{'currentState': array([4.18776211, 1.78207418, 5.9285717 ]), 'targetState': array([17, 16], dtype=int32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.6067447802657763
{'currentState': array([16.98884157, 16.03451524,  0.75144379]), 'targetState': array([17, 16], dtype=int32)}
episode index:610
model initialize at round 610
at step 0:
{'currentState': array([25.92662317, 14.29022447,  1.7038244 ]), 'targetState': array([23, 23], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6068576841426904
{'currentState': array([22.95064002, 23.03384927,  2.18951032]), 'targetState': array([23, 23], dtype=int32)}
episode index:611
model initialize at round 611
at step 0:
{'currentState': array([12.26997666, 22.09929187,  6.13061065]), 'targetState': array([23, 27], dtype=int32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6068747306407289
{'currentState': array([22.95926284, 27.00674815,  2.35441181]), 'targetState': array([23, 27], dtype=int32)}
episode index:612
model initialize at round 612
at step 0:
{'currentState': array([14.18270872, 14.22217965,  0.37757707]), 'targetState': array([26,  4], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6068233114226803
{'currentState': array([26.03679894,  4.0324336 ,  0.36094326]), 'targetState': array([26,  4], dtype=int32)}
episode index:613
model initialize at round 613
at step 0:
{'currentState': array([15.2223598 , 24.81751057,  6.10095263]), 'targetState': array([26, 28], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6069355357510803
{'currentState': array([25.97604853, 27.95937103,  0.4955815 ]), 'targetState': array([26, 28], dtype=int32)}
episode index:614
model initialize at round 614
at step 0:
{'currentState': array([22.02091514, 20.28689516,  2.00302339]), 'targetState': array([19, 24], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6072390784305804
{'currentState': array([18.92208927, 24.01968191,  1.81695579]), 'targetState': array([19, 24], dtype=int32)}
episode index:615
model initialize at round 615
at step 0:
{'currentState': array([ 6.07786311, 25.72308194,  4.48148918]), 'targetState': array([ 7, 18], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6074908700473279
{'currentState': array([ 6.97009222, 18.04503584,  4.26224571]), 'targetState': array([ 7, 18], dtype=int32)}
episode index:616
model initialize at round 616
at step 0:
{'currentState': array([25.07507347, 15.72231268,  4.47142935]), 'targetState': array([14,  7], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6074294605374898
{'currentState': array([13.9483925 ,  6.92313439,  3.31121983]), 'targetState': array([14,  7], dtype=int32)}
episode index:617
model initialize at round 617
at step 0:
{'currentState': array([19.10053987, 11.2695144 ,  0.70874441]), 'targetState': array([25,  3], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6075734472001308
{'currentState': array([24.93789876,  3.07420996,  5.55715673]), 'targetState': array([25,  3], dtype=int32)}
episode index:618
model initialize at round 618
at step 0:
{'currentState': array([25.71631128,  3.04761285,  2.47030759]), 'targetState': array([23, 16], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6075402700349246
{'currentState': array([23.01031522, 16.0188396 ,  2.00820531]), 'targetState': array([23, 16], dtype=int32)}
episode index:619
model initialize at round 619
at step 0:
{'currentState': array([20.84588697, 19.24288979,  2.64119637]), 'targetState': array([10,  5], dtype=int32)}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.6074166656364088
{'currentState': array([9.9720702 , 5.05671617, 4.00483626]), 'targetState': array([10,  5], dtype=int32)}
episode index:620
model initialize at round 620
at step 0:
{'currentState': array([12.28479956, 26.04044112,  5.91924084]), 'targetState': array([26, 24], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6073465988486728
{'currentState': array([26.026963  , 24.00282496,  0.31467627]), 'targetState': array([26, 24], dtype=int32)}
episode index:621
model initialize at round 621
at step 0:
{'currentState': array([15.79632503, 16.79686758,  3.42065716]), 'targetState': array([ 4, 27], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6072859149316768
{'currentState': array([ 4.06249356, 27.03736984,  3.37326662]), 'targetState': array([ 4, 27], dtype=int32)}
episode index:622
model initialize at round 622
at step 0:
{'currentState': array([17.76839741, 23.17060632,  2.00170088]), 'targetState': array([26, 26], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6074865850034629
{'currentState': array([25.98034011, 25.94009414,  0.30072577]), 'targetState': array([26, 26], dtype=int32)}
episode index:623
model initialize at round 623
at step 0:
{'currentState': array([16.71603212, 14.04591859,  2.47627687]), 'targetState': array([ 7, 26], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6074350916780998
{'currentState': array([ 7.0396278 , 25.98204934,  2.98731674]), 'targetState': array([ 7, 26], dtype=int32)}
episode index:624
model initialize at round 624
at step 0:
{'currentState': array([ 9.77141469, 14.82537174,  4.29896355]), 'targetState': array([24,  8], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6073745574553783
{'currentState': array([24.07249719,  8.05554379,  0.66231688]), 'targetState': array([24,  8], dtype=int32)}
episode index:625
model initialize at round 625
at step 0:
{'currentState': array([16.09832041, 12.72966803,  5.56621838]), 'targetState': array([24,  4], dtype=int32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.607462269090408
{'currentState': array([23.96890102,  3.97457386,  6.24835876]), 'targetState': array([24,  4], dtype=int32)}
episode index:626
model initialize at round 626
at step 0:
{'currentState': array([18.71271244,  8.01456471,  3.59593868]), 'targetState': array([7, 9], dtype=int32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6075497009435078
{'currentState': array([6.9658539 , 9.02315264, 2.37932159]), 'targetState': array([7, 9], dtype=int32)}
episode index:627
model initialize at round 627
at step 0:
{'currentState': array([13.98950807, 19.28746512,  1.10227823]), 'targetState': array([22,  8], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.60755538173744
{'currentState': array([21.93164596,  8.08453265,  5.16787309]), 'targetState': array([22,  8], dtype=int32)}
episode index:628
model initialize at round 628
at step 0:
{'currentState': array([14.79363593,  6.79960003,  3.40732956]), 'targetState': array([5, 3], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6076855784053757
{'currentState': array([5.02577393, 3.01115222, 3.74837857]), 'targetState': array([5, 3], dtype=int32)}
episode index:629
model initialize at round 629
at step 0:
{'currentState': array([18.2728373 ,  1.90886229,  0.18261641]), 'targetState': array([25,  4], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6079556352006039
{'currentState': array([25.08561913,  4.01724203,  0.12851181]), 'targetState': array([25,  4], dtype=int32)}
episode index:630
model initialize at round 630
at step 0:
{'currentState': array([21.74292668, 11.87092799,  3.10190487]), 'targetState': array([15, 15], dtype=int32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6082125092269186
{'currentState': array([15.03701097, 15.04827043,  3.52012137]), 'targetState': array([15, 15], dtype=int32)}
episode index:631
model initialize at round 631
at step 0:
{'currentState': array([17.97661335, 20.28670427,  1.14718676]), 'targetState': array([23, 28], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6083972684464296
{'currentState': array([23.00680023, 27.90048399,  0.92000484]), 'targetState': array([23, 28], dtype=int32)}
episode index:632
model initialize at round 632
at step 0:
{'currentState': array([18.71817002, 10.05760326,  2.43497944]), 'targetState': array([ 9, 24], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6083005218017918
{'currentState': array([ 9.07256388, 24.00467946,  2.9628011 ]), 'targetState': array([ 9, 24], dtype=int32)}
episode index:633
model initialize at round 633
at step 0:
{'currentState': array([4.79363895, 9.20040308, 1.86584067]), 'targetState': array([ 9, 23], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6082304976198527
{'currentState': array([ 8.97643944, 23.06388174,  2.11371497]), 'targetState': array([ 9, 23], dtype=int32)}
episode index:634
model initialize at round 634
at step 0:
{'currentState': array([25.12116705,  1.73910764,  4.64218044]), 'targetState': array([21,  2], dtype=int32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6086270769222487
{'currentState': array([20.97059938,  2.03679649,  2.44327137]), 'targetState': array([21,  2], dtype=int32)}
episode index:635
model initialize at round 635
at step 0:
{'currentState': array([14.72993459, 27.90094975,  3.99812317]), 'targetState': array([26, 18], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6085931299175557
{'currentState': array([25.9730744 , 17.9805936 ,  5.37324531]), 'targetState': array([26, 18], dtype=int32)}
episode index:636
model initialize at round 636
at step 0:
{'currentState': array([14.9305563 ,  6.27914843,  1.3096168 ]), 'targetState': array([25, 13], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6086567241066464
{'currentState': array([2.50870623e+01, 1.30508010e+01, 1.75013542e-02]), 'targetState': array([25, 13], dtype=int32)}
episode index:637
model initialize at round 637
at step 0:
{'currentState': array([19.77610832, 10.18060673,  2.96779776]), 'targetState': array([15, 13], dtype=int32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6089096808804685
{'currentState': array([15.06903378, 13.04287902,  4.58680778]), 'targetState': array([15, 13], dtype=int32)}
episode index:638
model initialize at round 638
at step 0:
{'currentState': array([ 8.2540117 , 13.13499753,  6.26668448]), 'targetState': array([ 7, 10], dtype=int32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6093300429182126
{'currentState': array([6.92545157, 9.95331477, 3.51911389]), 'targetState': array([ 7, 10], dtype=int32)}
episode index:639
model initialize at round 639
at step 0:
{'currentState': array([ 6.1912922 , 23.21483381,  0.33829963]), 'targetState': array([ 7, 16], dtype=int32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6095811571414734
{'currentState': array([ 7.00978017, 16.01769097,  4.61718154]), 'targetState': array([ 7, 16], dtype=int32)}
episode index:640
model initialize at round 640
at step 0:
{'currentState': array([25.02910273,  3.71381945,  5.31873417]), 'targetState': array([26,  2], dtype=int32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6100842526028861
{'currentState': array([26.02898544,  1.91736793,  4.56756695]), 'targetState': array([26,  2], dtype=int32)}
episode index:641
model initialize at round 641
at step 0:
{'currentState': array([18.86920968, 27.73170862,  4.43542144]), 'targetState': array([14, 17], dtype=int32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6101655575692119
{'currentState': array([14.06367773, 17.02627014,  4.3049221 ]), 'targetState': array([14, 17], dtype=int32)}
episode index:642
model initialize at round 642
at step 0:
{'currentState': array([10.74194543,  4.87290116,  3.094244  ]), 'targetState': array([14, 12], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6103106308778128
{'currentState': array([13.93443278, 12.03831037,  2.88838324]), 'targetState': array([14, 12], dtype=int32)}
episode index:643
model initialize at round 643
at step 0:
{'currentState': array([14.22905832, 15.17400736,  1.15465742]), 'targetState': array([ 9, 22], dtype=int32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6105115466270526
{'currentState': array([ 9.06969665, 21.96742259,  1.97688718]), 'targetState': array([ 9, 22], dtype=int32)}
episode index:644
model initialize at round 644
at step 0:
{'currentState': array([14.71369693, 14.02787162,  2.5395484 ]), 'targetState': array([ 2, 16], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.610551351184319
{'currentState': array([ 1.96142384, 15.97399742,  3.56909931]), 'targetState': array([ 2, 16], dtype=int32)}
episode index:645
model initialize at round 645
at step 0:
{'currentState': array([19.90402919,  5.27117499,  2.41594779]), 'targetState': array([9, 7], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.610591032507662
{'currentState': array([8.96511055, 7.07141933, 4.09053028]), 'targetState': array([9, 7], dtype=int32)}
episode index:646
model initialize at round 646
at step 0:
{'currentState': array([21.75042327, 19.16157947,  2.33267969]), 'targetState': array([12, 25], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.61061102379419
{'currentState': array([11.99882581, 25.04104931,  2.98834542]), 'targetState': array([12, 25], dtype=int32)}
episode index:647
model initialize at round 647
at step 0:
{'currentState': array([15.96774543, 23.28584247,  1.17816138]), 'targetState': array([16, 24], dtype=int32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.611078471670254
{'currentState': array([15.91749453, 24.00946347,  2.52153093]), 'targetState': array([16, 24], dtype=int32)}
episode index:648
model initialize at round 648
at step 0:
{'currentState': array([ 3.25005217, 28.84473175,  6.06839889]), 'targetState': array([10, 23], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6112652588782419
{'currentState': array([ 9.98614426, 22.97619473,  5.03119259]), 'targetState': array([10, 23], dtype=int32)}
episode index:649
model initialize at round 649
at step 0:
{'currentState': array([11.2675963 ,  3.10631929,  6.16235501]), 'targetState': array([18,  8], dtype=int32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6114743463174008
{'currentState': array([17.97001003,  7.91522346,  1.43971926]), 'targetState': array([18,  8], dtype=int32)}
episode index:650
model initialize at round 650
at step 0:
{'currentState': array([16.71486957, 16.96196168,  2.7692163 ]), 'targetState': array([27, 27], dtype=int32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6113926132793525
{'currentState': array([27.02017385, 27.04521564,  0.23137552]), 'targetState': array([27, 27], dtype=int32)}
episode index:651
model initialize at round 651
at step 0:
{'currentState': array([18.16805276, 19.23346209,  1.45188597]), 'targetState': array([21, 25], dtype=int32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6116008640171626
{'currentState': array([20.96980152, 25.05514536,  0.95245858]), 'targetState': array([21, 25], dtype=int32)}
episode index:652
model initialize at round 652
at step 0:
{'currentState': array([24.724998  , 13.08438113,  3.34887409]), 'targetState': array([26,  8], dtype=int32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6119294517044802
{'currentState': array([25.98638764,  7.95453021,  4.47578318]), 'targetState': array([26,  8], dtype=int32)}
episode index:653
model initialize at round 653
at step 0:
{'currentState': array([14.22153746,  5.81651314,  5.08645868]), 'targetState': array([5, 8], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6120802525845532
{'currentState': array([5.07456664, 7.9261532 , 3.71869504]), 'targetState': array([5, 8], dtype=int32)}
episode index:654
model initialize at round 654
at step 0:
{'currentState': array([14.27843301, 29.07225881,  6.0321033 ]), 'targetState': array([24, 24], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6121170544677278
{'currentState': array([24.07393636, 23.98307173,  5.50162766]), 'targetState': array([24, 24], dtype=int32)}
episode index:655
model initialize at round 655
at step 0:
{'currentState': array([21.28233491, 26.9449057 ,  0.31220775]), 'targetState': array([23, 27], dtype=int32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6126191323563456
{'currentState': array([23.02425486, 27.00718092,  0.57217562]), 'targetState': array([23, 27], dtype=int32)}
episode index:656
model initialize at round 656
at step 0:
{'currentState': array([17.81683403, 18.22180284,  1.75607538]), 'targetState': array([16, 27], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6128013001452315
{'currentState': array([16.02943243, 26.96272999,  1.29758771]), 'targetState': array([16, 27], dtype=int32)}
episode index:657
model initialize at round 657
at step 0:
{'currentState': array([10.27307951, 13.09040938,  0.82471531]), 'targetState': array([21, 23], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.612780258665476
{'currentState': array([20.9230803 , 22.94132193,  1.25497502]), 'targetState': array([21, 23], dtype=int32)}
episode index:658
model initialize at round 658
at step 0:
{'currentState': array([13.13114131, 25.74397611,  4.68076706]), 'targetState': array([ 4, 23], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.612928624323453
{'currentState': array([ 3.90114536, 22.99718089,  2.96593184]), 'targetState': array([ 4, 23], dtype=int32)}
episode index:659
model initialize at round 659
at step 0:
{'currentState': array([25.73811942,  5.11901612,  3.22003007]), 'targetState': array([21,  3], dtype=int32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6132643593797055
{'currentState': array([20.96075104,  3.03865659,  2.93464937]), 'targetState': array([21,  3], dtype=int32)}
episode index:660
model initialize at round 660
at step 0:
{'currentState': array([ 7.72466062, 17.08327364,  2.34289885]), 'targetState': array([ 3, 28], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6133284968399089
{'currentState': array([ 2.9853655 , 28.07050847,  1.87762475]), 'targetState': array([ 3, 28], dtype=int32)}
episode index:661
model initialize at round 661
at step 0:
{'currentState': array([14.72308267,  5.9221343 ,  3.92070222]), 'targetState': array([6, 3], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6134971552071565
{'currentState': array([6.01749611, 3.00808125, 4.21706265]), 'targetState': array([6, 3], dtype=int32)}
episode index:662
model initialize at round 662
at step 0:
{'currentState': array([ 9.73196672, 20.1044243 ,  2.26508856]), 'targetState': array([23, 25], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6134752228561142
{'currentState': array([23.05522479, 25.06147724,  0.33497147]), 'targetState': array([23, 25], dtype=int32)}
episode index:663
model initialize at round 663
at step 0:
{'currentState': array([16.73121632,  6.10247735,  2.27234221]), 'targetState': array([25,  5], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6136541809085213
{'currentState': array([25.07338014,  5.00557525,  5.7666688 ]), 'targetState': array([25,  5], dtype=int32)}
episode index:664
model initialize at round 664
at step 0:
{'currentState': array([19.9599936 , 20.28486095,  2.21532559]), 'targetState': array([15, 29], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6137173463817027
{'currentState': array([14.99962479, 29.050225  ,  3.53943116]), 'targetState': array([15, 29], dtype=int32)}
episode index:665
model initialize at round 665
at step 0:
{'currentState': array([20.71378159, 15.971272  ,  3.74662852]), 'targetState': array([6, 4], dtype=int32)}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.6134348730739053
{'currentState': array([6.07510753, 3.9389148 , 4.2770159 ]), 'targetState': array([6, 4], dtype=int32)}
episode index:666
model initialize at round 666
at step 0:
{'currentState': array([21.28512185, 11.03810258,  0.63784903]), 'targetState': array([15, 23], dtype=int32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6133864945747213
{'currentState': array([14.98201884, 23.03751004,  3.01935717]), 'targetState': array([15, 23], dtype=int32)}
episode index:667
model initialize at round 667
at step 0:
{'currentState': array([22.2402456 , 13.17311066,  0.90641704]), 'targetState': array([27, 16], dtype=int32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6137050306963693
{'currentState': array([26.91973011, 15.90009758,  0.4138386 ]), 'targetState': array([27, 16], dtype=int32)}
episode index:668
model initialize at round 668
at step 0:
{'currentState': array([ 1.91678601, 26.72464258,  4.92391181]), 'targetState': array([12, 19], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6137386337686674
{'currentState': array([11.98780764, 19.08804455,  0.64931696]), 'targetState': array([12, 19], dtype=int32)}
episode index:669
model initialize at round 669
at step 0:
{'currentState': array([ 4.02732415, 16.71364416,  4.30252123]), 'targetState': array([2, 2], dtype=int32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.613647506669255
{'currentState': array([2.00696433, 1.96263382, 0.05998494]), 'targetState': array([2, 2], dtype=int32)}
episode index:670
model initialize at round 670
at step 0:
{'currentState': array([ 2.95563138, 24.71578582,  5.06252909]), 'targetState': array([15, 18], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.613643735779335
{'currentState': array([15.01221389, 18.01598095,  0.50228658]), 'targetState': array([15, 18], dtype=int32)}
episode index:671
model initialize at round 671
at step 0:
{'currentState': array([ 7.78647668, 13.8072461 ,  3.37091398]), 'targetState': array([2, 8], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6137773726234127
{'currentState': array([1.9728739 , 7.96201389, 3.53021869]), 'targetState': array([2, 8], dtype=int32)}
episode index:672
model initialize at round 672
at step 0:
{'currentState': array([6.02661683, 4.71357755, 5.31005144]), 'targetState': array([21,  8], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6136783819395603
{'currentState': array([21.06707357,  8.03937439,  6.10079985]), 'targetState': array([21,  8], dtype=int32)}
episode index:673
model initialize at round 673
at step 0:
{'currentState': array([19.96482979, 15.28549839,  2.1983676 ]), 'targetState': array([11, 15], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6138543834050125
{'currentState': array([11.06793232, 14.99932793,  3.40067609]), 'targetState': array([11, 15], dtype=int32)}
episode index:674
model initialize at round 674
at step 0:
{'currentState': array([ 4.28266089, 24.05337693,  5.96482486]), 'targetState': array([ 5, 22], dtype=int32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6142320003374035
{'currentState': array([ 5.05591094, 21.9452486 ,  5.82459346]), 'targetState': array([ 5, 22], dtype=int32)}
episode index:675
model initialize at round 675
at step 0:
{'currentState': array([21.91376693, 23.27442691,  1.37025619]), 'targetState': array([22, 24], dtype=int32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6146747299929453
{'currentState': array([21.97498928, 24.06861131,  0.84621601]), 'targetState': array([22, 24], dtype=int32)}
episode index:676
model initialize at round 676
at step 0:
{'currentState': array([16.28482279, 24.92418697,  5.69456675]), 'targetState': array([27, 28], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.6137667909530738
{'currentState': array([27.41569948, 27.86591988,  4.49739164]), 'targetState': array([27, 28], dtype=int32)}
episode index:677
model initialize at round 677
at step 0:
{'currentState': array([16.11485138,  6.26373365,  1.66508055]), 'targetState': array([10, 10], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6140087593431123
{'currentState': array([10.0369261 , 10.019445  ,  2.29634085]), 'targetState': array([10, 10], dtype=int32)}
episode index:678
model initialize at round 678
at step 0:
{'currentState': array([16.2803594 , 17.93561924,  5.55246216]), 'targetState': array([ 9, 16], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6141509455992672
{'currentState': array([ 9.06812418, 16.09324689,  5.29714496]), 'targetState': array([ 9, 16], dtype=int32)}
episode index:679
model initialize at round 679
at step 0:
{'currentState': array([7.28706401, 3.01845342, 0.56919497]), 'targetState': array([22,  9], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6140854312711465
{'currentState': array([21.906281  ,  8.92533786,  1.45884436]), 'targetState': array([22,  9], dtype=int32)}
episode index:680
model initialize at round 680
at step 0:
{'currentState': array([18.21129107,  3.80480175,  5.03235626]), 'targetState': array([4, 4], dtype=int32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.6139176656303653
{'currentState': array([4.07774222, 3.91005254, 3.73064   ]), 'targetState': array([4, 4], dtype=int32)}
episode index:681
model initialize at round 681
at step 0:
{'currentState': array([21.72411795,  8.91854223,  2.92369795]), 'targetState': array([14, 14], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6140805141205815
{'currentState': array([14.08237927, 13.92569776,  3.32024309]), 'targetState': array([14, 14], dtype=int32)}
episode index:682
model initialize at round 682
at step 0:
{'currentState': array([ 9.10509697, 18.73222976,  5.59140396]), 'targetState': array([17, 19], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.614286427404744
{'currentState': array([17.0586311 , 19.01447103,  6.18110523]), 'targetState': array([17, 19], dtype=int32)}
episode index:683
model initialize at round 683
at step 0:
{'currentState': array([16.71775399, 17.05552893,  3.45233417]), 'targetState': array([8, 7], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6142465887417802
{'currentState': array([7.97908382, 7.07360097, 6.13287237]), 'targetState': array([8, 7], dtype=int32)}
episode index:684
model initialize at round 684
at step 0:
{'currentState': array([14.26308725, 15.88367556,  5.36187727]), 'targetState': array([25, 13], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6142786163291117
{'currentState': array([24.92937523, 13.03674093,  0.98276676]), 'targetState': array([25, 13], dtype=int32)}
episode index:685
model initialize at round 685
at step 0:
{'currentState': array([16.25271326, 29.13741282,  6.27621735]), 'targetState': array([24, 19], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6142051856791456
{'currentState': array([24.02951879, 19.07587933,  5.83863926]), 'targetState': array([24, 19], dtype=int32)}
episode index:686
model initialize at round 686
at step 0:
{'currentState': array([21.72847536, 24.09497706,  2.30010343]), 'targetState': array([17, 28], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.614489789292744
{'currentState': array([17.08213088, 27.99325681,  1.85523384]), 'targetState': array([17, 28], dtype=int32)}
episode index:687
model initialize at round 687
at step 0:
{'currentState': array([22.25758433, 10.12804916,  6.23952264]), 'targetState': array([16,  2], dtype=int32)}
done in step count: 199
reward sum = 0.0
running average episode reward sum: 0.613596635529237
{'currentState': array([15.77089968,  1.65555931,  2.17861232]), 'targetState': array([16,  2], dtype=int32)}
episode index:688
model initialize at round 688
at step 0:
{'currentState': array([ 7.13322794, 27.25494429,  0.58425093]), 'targetState': array([18, 27], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6136967196503998
{'currentState': array([18.04312087, 26.93298457,  0.42821565]), 'targetState': array([18, 27], dtype=int32)}
episode index:689
model initialize at round 689
at step 0:
{'currentState': array([14.73945942, 25.87807838,  3.07427835]), 'targetState': array([ 9, 29], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6139574697431437
{'currentState': array([ 8.93416952, 28.9959443 ,  2.48172131]), 'targetState': array([ 9, 29], dtype=int32)}
episode index:690
model initialize at round 690
at step 0:
{'currentState': array([16.16245151,  2.76260627,  5.8175087 ]), 'targetState': array([27, 16], dtype=int32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.6138070041519809
{'currentState': array([27.04662318, 16.07627558,  1.6770183 ]), 'targetState': array([27, 16], dtype=int32)}
episode index:691
model initialize at round 691
at step 0:
{'currentState': array([16.13227154,  8.7445582 ,  5.69518661]), 'targetState': array([23, 14], dtype=int32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.6136496036962512
{'currentState': array([22.91999301, 14.07883361,  0.90456966]), 'targetState': array([23, 14], dtype=int32)}
episode index:692
model initialize at round 692
at step 0:
{'currentState': array([10.78377355,  9.81028338,  3.3567791 ]), 'targetState': array([ 8, 12], dtype=int32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6140431610815613
{'currentState': array([ 8.05807975, 12.00119915,  1.78307466]), 'targetState': array([ 8, 12], dtype=int32)}
episode index:693
model initialize at round 693
at step 0:
{'currentState': array([17.70903112,  3.0588296 ,  3.19575322]), 'targetState': array([13,  4], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6143729882174437
{'currentState': array([12.93845405,  3.95512747,  2.56958801]), 'targetState': array([13,  4], dtype=int32)}
episode index:694
model initialize at round 694
at step 0:
{'currentState': array([15.83809404, 27.23776614,  2.67361975]), 'targetState': array([12, 21], dtype=int32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6145969740556994
{'currentState': array([11.93667328, 20.99900484,  3.56843502]), 'targetState': array([12, 21], dtype=int32)}
episode index:695
model initialize at round 695
at step 0:
{'currentState': array([12.85821196, 14.74971531,  3.69196081]), 'targetState': array([13, 23], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.61463722495804
{'currentState': array([12.93592697, 22.91782672,  4.86030317]), 'targetState': array([13, 23], dtype=int32)}
episode index:696
model initialize at round 696
at step 0:
{'currentState': array([ 4.02911428, 21.28617937,  0.96441078]), 'targetState': array([ 4, 28], dtype=int32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6148166555870648
{'currentState': array([ 4.09139802, 28.08878669,  2.54102838]), 'targetState': array([ 4, 28], dtype=int32)}
episode index:697
model initialize at round 697
at step 0:
{'currentState': array([16.19441896, 20.21200836,  1.33364928]), 'targetState': array([10, 29], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.614913701345551
{'currentState': array([ 9.97661248, 29.08846705,  2.56977859]), 'targetState': array([10, 29], dtype=int32)}
episode index:698
model initialize at round 698
at step 0:
{'currentState': array([13.03570035, 28.71456742,  5.34181738]), 'targetState': array([23, 17], dtype=int32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6148011899347666
{'currentState': array([22.96483693, 16.90430451,  6.13002441]), 'targetState': array([23, 17], dtype=int32)}
episode index:699
model initialize at round 699
at step 0:
{'currentState': array([20.05231828, 10.71714125,  4.39028454]), 'targetState': array([18,  7], dtype=int32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6151515573129188
{'currentState': array([18.02961829,  6.93300944,  3.88314866]), 'targetState': array([18,  7], dtype=int32)}
episode index:700
model initialize at round 700
at step 0:
{'currentState': array([27.14063571,  8.250934  ,  1.56496626]), 'targetState': array([23, 17], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6151724875181832
{'currentState': array([22.92960715, 16.90117759,  2.11677279]), 'targetState': array([23, 17], dtype=int32)}
episode index:701
model initialize at round 701
at step 0:
{'currentState': array([21.81166103, 20.21742748,  2.7896291 ]), 'targetState': array([13, 22], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6153393406266393
{'currentState': array([13.07107627, 22.02807487,  2.53738469]), 'targetState': array([13, 22], dtype=int32)}
episode index:702
model initialize at round 702
at step 0:
{'currentState': array([14.76268166, 21.16256161,  2.03600907]), 'targetState': array([16, 22], dtype=int32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6158032678083951
{'currentState': array([16.08855094, 22.07673146,  6.15738467]), 'targetState': array([16, 22], dtype=int32)}
episode index:703
model initialize at round 703
at step 0:
{'currentState': array([18.1453072 , 11.24825811,  0.53625059]), 'targetState': array([22, 17], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6160445673439048
{'currentState': array([22.08671033, 16.94752612,  0.42214569]), 'targetState': array([22, 17], dtype=int32)}
episode index:704
model initialize at round 704
at step 0:
{'currentState': array([23.1111984 , 22.26529454,  0.66889048]), 'targetState': array([27, 13], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6161585654300121
{'currentState': array([26.9233296 , 12.95252473,  4.76375413]), 'targetState': array([27, 13], dtype=int32)}
episode index:705
model initialize at round 705
at step 0:
{'currentState': array([15.83727835, 12.23721575,  2.67691892]), 'targetState': array([13, 26], dtype=int32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.6160228458226754
{'currentState': array([12.93567344, 25.96352301,  5.06484325]), 'targetState': array([13, 26], dtype=int32)}
episode index:706
model initialize at round 706
at step 0:
{'currentState': array([13.23810547, 27.8385935 ,  6.19246101]), 'targetState': array([23, 23], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6161169501355294
{'currentState': array([22.92636479, 22.96918482,  5.40679972]), 'targetState': array([23, 23], dtype=int32)}
episode index:707
model initialize at round 707
at step 0:
{'currentState': array([13.71340035, 23.97536433,  2.72234035]), 'targetState': array([ 6, 29], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6162707119799112
{'currentState': array([ 6.01409145, 29.02305007,  1.9520506 ]), 'targetState': array([ 6, 29], dtype=int32)}
episode index:708
model initialize at round 708
at step 0:
{'currentState': array([16.97324871,  8.71359008,  5.12425709]), 'targetState': array([26,  3], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6163078641521325
{'currentState': array([26.05384158,  3.05395712,  5.78154949]), 'targetState': array([26,  3], dtype=int32)}
episode index:709
model initialize at round 709
at step 0:
{'currentState': array([ 8.07983984, 24.27635462,  0.78455055]), 'targetState': array([ 9, 24], dtype=int32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6167136024772827
{'currentState': array([ 8.99111882, 23.90283989,  5.82802475]), 'targetState': array([ 9, 24], dtype=int32)}
episode index:710
model initialize at round 710
at step 0:
{'currentState': array([15.27548489, 17.91720901,  0.21305865]), 'targetState': array([16, 22], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6168355913556544
{'currentState': array([15.91194   , 21.94842709,  3.7038708 ]), 'targetState': array([16, 22], dtype=int32)}
episode index:711
model initialize at round 711
at step 0:
{'currentState': array([14.75041545, 24.84755155,  4.08364183]), 'targetState': array([ 2, 23], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6168538329846538
{'currentState': array([ 1.92582673, 22.99842078,  2.46915776]), 'targetState': array([ 2, 23], dtype=int32)}
episode index:712
model initialize at round 712
at step 0:
{'currentState': array([11.97201532, 11.71370796,  5.11994982]), 'targetState': array([27,  3], dtype=int32)}
done in step count: 95
reward sum = 0.38489607889348454
running average episode reward sum: 0.6165285065413282
{'currentState': array([26.90181835,  3.06814033,  2.94424456]), 'targetState': array([27,  3], dtype=int32)}
episode index:713
model initialize at round 713
at step 0:
{'currentState': array([ 5.02328243, 20.71328725,  4.28841591]), 'targetState': array([ 8, 13], dtype=int32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6167114807539196
{'currentState': array([ 7.90930054, 12.98994514,  4.69636064]), 'targetState': array([ 8, 13], dtype=int32)}
episode index:714
model initialize at round 714
at step 0:
{'currentState': array([13.87584888,  7.26237592,  2.46060419]), 'targetState': array([14, 19], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6167659531173045
{'currentState': array([14.04046105, 19.08054041,  2.04242915]), 'targetState': array([14, 19], dtype=int32)}
episode index:715
model initialize at round 715
at step 0:
{'currentState': array([17.77799705,  6.81707662,  3.32577848]), 'targetState': array([10,  7], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6168202733232498
{'currentState': array([10.05701301,  6.93902278,  0.24374928]), 'targetState': array([10,  7], dtype=int32)}
episode index:716
model initialize at round 716
at step 0:
{'currentState': array([ 6.21693699, 23.18890373,  0.21143317]), 'targetState': array([21, 10], dtype=int32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.616643244034102
{'currentState': array([20.93383186, 10.08309917,  5.56256719]), 'targetState': array([21, 10], dtype=int32)}
episode index:717
model initialize at round 717
at step 0:
{'currentState': array([19.93725779,  7.71926937,  3.98750639]), 'targetState': array([11,  4], dtype=int32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6167161249735665
{'currentState': array([11.08844064,  3.90040985,  3.60824517]), 'targetState': array([11,  4], dtype=int32)}
episode index:718
model initialize at round 718
at step 0:
{'currentState': array([17.10697679, 18.73297521,  5.59843397]), 'targetState': array([26, 25], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6168076944729224
{'currentState': array([25.90247448, 24.92125319,  0.80780932]), 'targetState': array([26, 25], dtype=int32)}
episode index:719
model initialize at round 719
at step 0:
{'currentState': array([9.27771721, 1.92503716, 0.24135798]), 'targetState': array([12,  9], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6169478650814298
{'currentState': array([11.98384786,  9.03072507,  1.23466732]), 'targetState': array([12,  9], dtype=int32)}
episode index:720
model initialize at round 720
at step 0:
{'currentState': array([23.00119619, 15.71234597,  4.21154737]), 'targetState': array([11,  4], dtype=int32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.6168066573870364
{'currentState': array([10.96341773,  3.94893532,  6.12061773]), 'targetState': array([11,  4], dtype=int32)}
episode index:721
model initialize at round 721
at step 0:
{'currentState': array([15.89536976,  3.26795296,  1.43806887]), 'targetState': array([25,  2], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6169464411477169
{'currentState': array([25.02219567,  1.9561832 ,  0.04594404]), 'targetState': array([25,  2], dtype=int32)}
episode index:722
model initialize at round 722
at step 0:
{'currentState': array([16.819353  , 15.77614081,  4.53841424]), 'targetState': array([21,  3], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6169555406688005
{'currentState': array([21.01090846,  2.92206171,  4.25965939]), 'targetState': array([21,  3], dtype=int32)}
episode index:723
model initialize at round 723
at step 0:
{'currentState': array([15.13488168,  4.25407323,  0.57775319]), 'targetState': array([25, 12], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.61698210136686
{'currentState': array([25.02419221, 11.98176071,  6.04862896]), 'targetState': array([25, 12], dtype=int32)}
episode index:724
model initialize at round 724
at step 0:
{'currentState': array([24.79477202, 16.79843674,  4.42298222]), 'targetState': array([20,  6], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6170354491174908
{'currentState': array([19.96863678,  5.96260961,  4.13804881]), 'targetState': array([20,  6], dtype=int32)}
episode index:725
model initialize at round 725
at step 0:
{'currentState': array([23.26155189, 16.88026327,  5.34886931]), 'targetState': array([8, 2], dtype=int32)}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.6168337289969236
{'currentState': array([8.0003832 , 2.01633945, 3.77778437]), 'targetState': array([8, 2], dtype=int32)}
episode index:726
model initialize at round 726
at step 0:
{'currentState': array([14.72415862, 15.08159538,  3.35898852]), 'targetState': array([ 6, 18], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6169824863655081
{'currentState': array([ 5.96695653, 17.94657846,  2.31321996]), 'targetState': array([ 6, 18], dtype=int32)}
episode index:727
model initialize at round 727
at step 0:
{'currentState': array([ 8.72900043, 16.90353493,  3.98856735]), 'targetState': array([ 8, 10], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6170820283977932
{'currentState': array([8.0144304 , 9.97860639, 4.9906496 ]), 'targetState': array([ 8, 10], dtype=int32)}
episode index:728
model initialize at round 728
at step 0:
{'currentState': array([11.11602248, 19.73677945,  4.62254953]), 'targetState': array([ 4, 24], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.617181297338083
{'currentState': array([ 3.91772879, 23.99638602,  3.38577912]), 'targetState': array([ 4, 24], dtype=int32)}
episode index:729
model initialize at round 729
at step 0:
{'currentState': array([ 3.00388261, 10.28763032,  1.05229855]), 'targetState': array([ 8, 11], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6174905602093786
{'currentState': array([7.91231565e+00, 1.10146900e+01, 9.97458398e-03]), 'targetState': array([ 8, 11], dtype=int32)}
episode index:730
model initialize at round 730
at step 0:
{'currentState': array([ 7.28708139, 17.01818102,  0.56824606]), 'targetState': array([ 6, 26], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6176376050462439
{'currentState': array([ 6.02921176, 26.00303506,  1.15578192]), 'targetState': array([ 6, 26], dtype=int32)}
episode index:731
model initialize at round 731
at step 0:
{'currentState': array([15.09255689, 17.7200275 ,  4.70746922]), 'targetState': array([19, 12], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6178671686196878
{'currentState': array([18.9350633 , 11.98659827,  5.64310843]), 'targetState': array([19, 12], dtype=int32)}
episode index:732
model initialize at round 732
at step 0:
{'currentState': array([14.98293081,  3.28714964,  2.13516998]), 'targetState': array([ 5, 12], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6178748878915453
{'currentState': array([ 5.00955833, 12.01634611,  1.90838364]), 'targetState': array([ 5, 12], dtype=int32)}
episode index:733
model initialize at round 733
at step 0:
{'currentState': array([14.15490011, 15.24238859,  1.50715244]), 'targetState': array([ 9, 28], dtype=int32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6178740912335763
{'currentState': array([ 9.02480605, 28.03763547,  1.45010927]), 'targetState': array([ 9, 28], dtype=int32)}
episode index:734
model initialize at round 734
at step 0:
{'currentState': array([ 9.76710402, 16.83116358,  4.27386379]), 'targetState': array([19,  2], dtype=int32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.6177485434734293
{'currentState': array([18.96519269,  1.9953394 ,  4.48744365]), 'targetState': array([19,  2], dtype=int32)}
episode index:735
model initialize at round 735
at step 0:
{'currentState': array([17.28060931, 24.93671739,  5.55637736]), 'targetState': array([26, 14], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6176753867437812
{'currentState': array([26.08517859, 14.00113557,  5.59840369]), 'targetState': array([26, 14], dtype=int32)}
episode index:736
model initialize at round 736
at step 0:
{'currentState': array([7.81288153, 9.78152128, 4.50915599]), 'targetState': array([23,  4], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.617602428539858
{'currentState': array([22.96207196,  3.99737614,  5.93105254]), 'targetState': array([23,  4], dtype=int32)}
episode index:737
model initialize at round 737
at step 0:
{'currentState': array([22.1978664 , 23.20879455,  0.30726457]), 'targetState': array([27, 24], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6179077683296196
{'currentState': array([27.05331585, 23.97874502,  6.07288078]), 'targetState': array([27, 24], dtype=int32)}
episode index:738
model initialize at round 738
at step 0:
{'currentState': array([6.26690832, 7.10726708, 0.88713234]), 'targetState': array([ 9, 12], dtype=int32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6181784032000764
{'currentState': array([ 9.04676994, 12.09414822,  0.92778276]), 'targetState': array([ 9, 12], dtype=int32)}
episode index:739
model initialize at round 739
at step 0:
{'currentState': array([15.15075051, 27.75500907,  4.75900507]), 'targetState': array([23, 19], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6182290529532847
{'currentState': array([22.97123342, 18.99228645,  5.92889223]), 'targetState': array([23, 19], dtype=int32)}
episode index:740
model initialize at round 740
at step 0:
{'currentState': array([24.75765132, 28.83011275,  3.46543384]), 'targetState': array([22, 25], dtype=int32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6185438015607632
{'currentState': array([22.07060002, 25.0927473 ,  4.48868155]), 'targetState': array([22, 25], dtype=int32)}
episode index:741
model initialize at round 741
at step 0:
{'currentState': array([23.71829651, 11.94178127,  2.84039021]), 'targetState': array([20, 18], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6187584613422166
{'currentState': array([19.98004645, 18.09390596,  2.22252663]), 'targetState': array([20, 18], dtype=int32)}
episode index:742
model initialize at round 742
at step 0:
{'currentState': array([26.74234534, 21.12790759,  3.18580484]), 'targetState': array([16, 29], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6187648771343417
{'currentState': array([16.04758526, 29.00851638,  1.99901655]), 'targetState': array([16, 29], dtype=int32)}
episode index:743
model initialize at round 743
at step 0:
{'currentState': array([13.7214764 , 22.92809118,  2.88925385]), 'targetState': array([ 8, 25], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6190215478212865
{'currentState': array([ 8.0400294 , 25.03531181,  2.68366281]), 'targetState': array([ 8, 25], dtype=int32)}
episode index:744
model initialize at round 744
at step 0:
{'currentState': array([21.20774652, 19.80103353,  6.02437162]), 'targetState': array([27, 18], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6192775294594073
{'currentState': array([26.97047897, 17.97926113,  5.61969192]), 'targetState': array([27, 18], dtype=int32)}
episode index:745
model initialize at round 745
at step 0:
{'currentState': array([13.85552447, 10.75125697,  3.68119001]), 'targetState': array([16, 26], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6192033038039019
{'currentState': array([16.03067606, 26.08935317,  1.6104    ]), 'targetState': array([16, 26], dtype=int32)}
episode index:746
model initialize at round 746
at step 0:
{'currentState': array([21.05352589, 14.71736728,  4.39455557]), 'targetState': array([ 9, 27], dtype=int32)}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.6190851006434617
{'currentState': array([ 9.08448255, 27.03912472,  2.02154483]), 'targetState': array([ 9, 27], dtype=int32)}
episode index:747
model initialize at round 747
at step 0:
{'currentState': array([3.74571079, 7.86552595, 3.12303209]), 'targetState': array([ 4, 23], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6189889396030169
{'currentState': array([ 4.00128596, 23.062782  ,  1.0333036 ]), 'targetState': array([ 4, 23], dtype=int32)}
episode index:748
model initialize at round 748
at step 0:
{'currentState': array([12.8016835 , 18.2247512 ,  2.22058988]), 'targetState': array([ 6, 25], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6191111883181961
{'currentState': array([ 6.03714976, 24.96249441,  2.2946029 ]), 'targetState': array([ 6, 25], dtype=int32)}
episode index:749
model initialize at round 749
at step 0:
{'currentState': array([10.03284239, 13.71422447,  4.32181072]), 'targetState': array([ 7, 16], dtype=int32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6194440344841305
{'currentState': array([ 6.94459392, 16.05094587,  2.81082961]), 'targetState': array([ 7, 16], dtype=int32)}
episode index:750
model initialize at round 750
at step 0:
{'currentState': array([20.26516697, 20.88849774,  5.3801338 ]), 'targetState': array([21, 12], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6195943132260351
{'currentState': array([20.98441101, 11.99462567,  4.6646301 ]), 'targetState': array([21, 12], dtype=int32)}
episode index:751
model initialize at round 751
at step 0:
{'currentState': array([ 6.89073415, 14.73390369,  4.82775664]), 'targetState': array([9, 6], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6197344542137103
{'currentState': array([8.96342741, 5.99020647, 4.27186811]), 'targetState': array([9, 6], dtype=int32)}
episode index:752
model initialize at round 752
at step 0:
{'currentState': array([10.71338041, 24.0244026 ,  3.56165814]), 'targetState': array([ 3, 19], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6198088162254587
{'currentState': array([ 2.93797809, 18.96840635,  3.22311198]), 'targetState': array([ 3, 19], dtype=int32)}
episode index:753
model initialize at round 753
at step 0:
{'currentState': array([26.7128351 , 27.0168106 ,  2.57811952]), 'targetState': array([27, 28], dtype=int32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6202607886309952
{'currentState': array([27.0308098 , 28.01515708,  0.83925048]), 'targetState': array([27, 28], dtype=int32)}
episode index:754
model initialize at round 754
at step 0:
{'currentState': array([24.86901664, 12.74389527,  3.73462772]), 'targetState': array([22, 17], dtype=int32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6205445674029411
{'currentState': array([22.06078452, 16.90823309,  2.40823268]), 'targetState': array([22, 17], dtype=int32)}
episode index:755
model initialize at round 755
at step 0:
{'currentState': array([ 8.78143415, 19.81298331,  3.34435987]), 'targetState': array([10, 24], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6207734955990267
{'currentState': array([ 9.93461186, 23.99869724,  3.27692574]), 'targetState': array([10, 24], dtype=int32)}
episode index:756
model initialize at round 756
at step 0:
{'currentState': array([23.94436286,  3.2822247 ,  2.27043819]), 'targetState': array([21,  3], dtype=int32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6211481436563713
{'currentState': array([21.03424467,  3.00188033,  3.4748475 ]), 'targetState': array([21,  3], dtype=int32)}
episode index:757
model initialize at round 757
at step 0:
{'currentState': array([ 5.26163267, 12.88043989,  5.3495445 ]), 'targetState': array([3, 2], dtype=int32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6212112354966263
{'currentState': array([2.97235738, 2.00174246, 4.42953208]), 'targetState': array([3, 2], dtype=int32)}
episode index:758
model initialize at round 758
at step 0:
{'currentState': array([11.9912597 , 23.71247629,  5.1869998 ]), 'targetState': array([23, 24], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6212830639729947
{'currentState': array([22.93547351, 24.01232859,  6.20782334]), 'targetState': array([23, 24], dtype=int32)}
episode index:759
model initialize at round 759
at step 0:
{'currentState': array([11.12439462, 14.25936895,  0.61859751]), 'targetState': array([6, 2], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.621269687888205
{'currentState': array([5.94281498, 1.94682092, 3.56270109]), 'targetState': array([6, 2], dtype=int32)}
episode index:760
model initialize at round 760
at step 0:
{'currentState': array([15.1970217 ,  9.20959181,  1.32130247]), 'targetState': array([ 7, 14], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6213870118558582
{'currentState': array([ 6.91541183, 14.03968145,  1.8378434 ]), 'targetState': array([ 7, 14], dtype=int32)}
episode index:761
model initialize at round 761
at step 0:
{'currentState': array([7.7797323 , 6.18500923, 1.93797445]), 'targetState': array([ 8, 13], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6216026170119623
{'currentState': array([ 8.03613183, 13.05540287,  1.27397568]), 'targetState': array([ 8, 13], dtype=int32)}
episode index:762
model initialize at round 762
at step 0:
{'currentState': array([ 3.2643859 , 13.11334182,  0.90999907]), 'targetState': array([ 9, 26], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6215729360020726
{'currentState': array([ 9.00105237, 25.96737233,  0.89929223]), 'targetState': array([ 9, 26], dtype=int32)}
episode index:763
model initialize at round 763
at step 0:
{'currentState': array([18.88240925, 22.73747631,  3.78626251]), 'targetState': array([13, 25], dtype=int32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6218086135590583
{'currentState': array([13.01782014, 25.02727807,  2.25199948]), 'targetState': array([13, 25], dtype=int32)}
episode index:764
model initialize at round 764
at step 0:
{'currentState': array([22.76446137,  2.83487026,  3.24803829]), 'targetState': array([22, 18], dtype=int32)}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.621703876908611
{'currentState': array([22.02501669, 18.08365154,  0.94816218]), 'targetState': array([22, 18], dtype=int32)}
episode index:765
model initialize at round 765
at step 0:
{'currentState': array([1.95789766, 3.28455872, 1.21268725]), 'targetState': array([13,  6], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6217482050335007
{'currentState': array([13.04137511,  5.9442748 ,  5.75385099]), 'targetState': array([13,  6], dtype=int32)}
episode index:766
model initialize at round 766
at step 0:
{'currentState': array([21.73328117,  4.10773737,  3.2626977 ]), 'targetState': array([16,  3], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6219315094784988
{'currentState': array([15.95592279,  2.98225933,  3.78077668]), 'targetState': array([16,  3], dtype=int32)}
episode index:767
model initialize at round 767
at step 0:
{'currentState': array([17.75712172, 23.84586883,  4.21206737]), 'targetState': array([23, 19], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6221344910539163
{'currentState': array([22.93777255, 19.03401048,  5.17792827]), 'targetState': array([23, 19], dtype=int32)}
episode index:768
model initialize at round 768
at step 0:
{'currentState': array([19.71667883, 16.95024669,  3.82042766]), 'targetState': array([12,  7], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.622096561216917
{'currentState': array([12.00383862,  6.99400807,  6.28112222]), 'targetState': array([12,  7], dtype=int32)}
episode index:769
model initialize at round 769
at step 0:
{'currentState': array([20.98974353, 18.71245593,  4.17319536]), 'targetState': array([27,  4], dtype=int32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6220136645641001
{'currentState': array([26.94343311,  4.00009209,  5.88062956]), 'targetState': array([27,  4], dtype=int32)}
episode index:770
model initialize at round 770
at step 0:
{'currentState': array([19.84683747,  6.75650973,  3.64589739]), 'targetState': array([ 8, 10], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6219837583927669
{'currentState': array([ 7.90489984, 10.03188873,  2.80052542]), 'targetState': array([ 8, 10], dtype=int32)}
episode index:771
model initialize at round 771
at step 0:
{'currentState': array([ 6.88339475, 16.7370371 ,  3.79001331]), 'targetState': array([ 4, 15], dtype=int32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6223262468815277
{'currentState': array([ 3.93916133, 14.92124725,  3.24338526]), 'targetState': array([ 4, 15], dtype=int32)}
episode index:772
model initialize at round 772
at step 0:
{'currentState': array([22.76528662,  8.16630065,  3.0301671 ]), 'targetState': array([17, 14], dtype=int32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6224877550929766
{'currentState': array([16.95835965, 14.02816842,  2.33916032]), 'targetState': array([17, 14], dtype=int32)}
episode index:773
model initialize at round 773
at step 0:
{'currentState': array([24.84727125, 22.75623741,  3.6476779 ]), 'targetState': array([11, 15], dtype=int32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6224047814281897
{'currentState': array([10.90841029, 15.08701838,  2.79384503]), 'targetState': array([11, 15], dtype=int32)}
episode index:774
model initialize at round 774
at step 0:
{'currentState': array([19.28720831, 19.0160519 ,  5.83401663]), 'targetState': array([11,  9], dtype=int32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6223981818920789
{'currentState': array([10.97347039,  9.06547775,  3.52601968]), 'targetState': array([11,  9], dtype=int32)}
episode index:775
model initialize at round 775
at step 0:
{'currentState': array([17.77229007, 14.82423179,  3.29396009]), 'targetState': array([23, 28], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6223526130777044
{'currentState': array([23.06715619, 28.04974747,  1.2424637 ]), 'targetState': array([23, 28], dtype=int32)}
episode index:776
model initialize at round 776
at step 0:
{'currentState': array([ 9.02734975, 13.71364661,  5.31261063]), 'targetState': array([12,  2], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6223870403818108
{'currentState': array([11.96817743,  1.96863462,  5.20540437]), 'targetState': array([12,  2], dtype=int32)}
episode index:777
model initialize at round 777
at step 0:
{'currentState': array([ 6.73386844, 15.89082003,  4.03590226]), 'targetState': array([16,  7], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6224047761731759
{'currentState': array([15.96585318,  6.97341056,  5.06691995]), 'targetState': array([16,  7], dtype=int32)}
episode index:778
model initialize at round 778
at step 0:
{'currentState': array([ 5.03291563, 28.7142329 ,  5.33206701]), 'targetState': array([ 6, 18], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6224307156159379
{'currentState': array([ 5.97794695, 17.96362821,  4.92191642]), 'targetState': array([ 6, 18], dtype=int32)}
episode index:779
model initialize at round 779
at step 0:
{'currentState': array([26.21532141, 11.80925681,  5.05324125]), 'targetState': array([26,  7], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6227134239207686
{'currentState': array([25.96590905,  6.96191797,  4.8194592 ]), 'targetState': array([26,  7], dtype=int32)}
episode index:780
model initialize at round 780
at step 0:
{'currentState': array([22.73492598,  9.88827696,  4.04547691]), 'targetState': array([14,  4], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.622790045138553
{'currentState': array([13.95626279,  3.9920757 ,  4.42649706]), 'targetState': array([14,  4], dtype=int32)}
episode index:781
model initialize at round 781
at step 0:
{'currentState': array([11.04678613,  5.28382624,  1.91242474]), 'targetState': array([ 6, 15], dtype=int32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6228491010380812
{'currentState': array([ 6.0142117 , 15.01559761,  1.86265177]), 'targetState': array([ 6, 15], dtype=int32)}
episode index:782
model initialize at round 782
at step 0:
{'currentState': array([14.15201576, 17.75579213,  4.7641778 ]), 'targetState': array([ 4, 14], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6228580084840137
{'currentState': array([ 4.01666707, 13.94082123,  6.16279777]), 'targetState': array([ 4, 14], dtype=int32)}
episode index:783
model initialize at round 783
at step 0:
{'currentState': array([22.76744243, 29.16930225,  3.01731992]), 'targetState': array([19, 24], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6230758098553908
{'currentState': array([18.9258805 , 23.95045141,  3.79565411]), 'targetState': array([19, 24], dtype=int32)}
episode index:784
model initialize at round 784
at step 0:
{'currentState': array([11.96449812, 14.70317344,  4.44635314]), 'targetState': array([12, 14], dtype=int32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6234457989479109
{'currentState': array([12.07110352, 13.98458409,  0.05391698]), 'targetState': array([12, 14], dtype=int32)}
episode index:785
model initialize at round 785
at step 0:
{'currentState': array([13.28163935, 18.0585282 ,  0.70989626]), 'targetState': array([ 7, 25], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6235566226480691
{'currentState': array([ 6.94089176, 25.04716006,  2.78622538]), 'targetState': array([ 7, 25], dtype=int32)}
episode index:786
model initialize at round 786
at step 0:
{'currentState': array([25.76738105,  7.1692179 ,  2.00768256]), 'targetState': array([26, 11], dtype=int32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.623879322013191
{'currentState': array([26.02835612, 10.93220697,  1.88568312]), 'targetState': array([26, 11], dtype=int32)}
episode index:787
model initialize at round 787
at step 0:
{'currentState': array([12.19861768, 12.79191999,  5.97952509]), 'targetState': array([25,  3], dtype=int32)}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6237960565519406
{'currentState': array([24.99364678,  2.94821014,  0.49870109]), 'targetState': array([25,  3], dtype=int32)}
episode index:788
model initialize at round 788
at step 0:
{'currentState': array([27.0435912 , 20.28433445,  1.92367119]), 'targetState': array([27, 23], dtype=int32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6240846011838075
{'currentState': array([26.99334628, 23.04314415,  2.1514341 ]), 'targetState': array([27, 23], dtype=int32)}
episode index:789
model initialize at round 789
at step 0:
{'currentState': array([9.88690134, 6.73550999, 3.80330944]), 'targetState': array([5, 3], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6243195926610701
{'currentState': array([5.09187473, 3.09276808, 4.33553926]), 'targetState': array([5, 3], dtype=int32)}
episode index:790
model initialize at round 790
at step 0:
{'currentState': array([16.92411553, 22.27746679,  1.33275843]), 'targetState': array([20, 23], dtype=int32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6245853248592862
{'currentState': array([20.05190861, 23.00554797,  2.35475474]), 'targetState': array([20, 23], dtype=int32)}
episode index:791
model initialize at round 791
at step 0:
{'currentState': array([19.19224023, 21.78601409,  4.93930721]), 'targetState': array([20, 18], dtype=int32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6248294178046624
{'currentState': array([20.04599817, 17.95617673,  6.01938418]), 'targetState': array([20, 18], dtype=int32)}
episode index:792
model initialize at round 792
at step 0:
{'currentState': array([23.95820873, 19.71539542,  4.06159115]), 'targetState': array([24, 23], dtype=int32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6250728951310086
{'currentState': array([24.01964295, 23.08788904,  1.41559369]), 'targetState': array([24, 23], dtype=int32)}
episode index:793
model initialize at round 793
at step 0:
{'currentState': array([ 1.7399577 , 13.12298079,  2.19483781]), 'targetState': array([14, 19], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6250476220478933
{'currentState': array([14.09986553, 18.98243603,  6.14244351]), 'targetState': array([14, 19], dtype=int32)}
episode index:794
model initialize at round 794
at step 0:
{'currentState': array([19.02637638,  2.71355531,  4.29921198]), 'targetState': array([10, 14], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6249851152905713
{'currentState': array([ 9.95496082, 14.01146765,  2.65041567]), 'targetState': array([10, 14], dtype=int32)}
episode index:795
model initialize at round 795
at step 0:
{'currentState': array([25.7220323 , 17.07402858,  2.37631321]), 'targetState': array([27, 19], dtype=int32)}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6253591851136082
{'currentState': array([26.94209521, 18.90321999,  1.18149201]), 'targetState': array([27, 19], dtype=int32)}
episode index:796
model initialize at round 796
at step 0:
{'currentState': array([10.99648972, 21.7123649 ,  4.19518566]), 'targetState': array([ 7, 10], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6253727689291041
{'currentState': array([ 6.99341948, 10.04935202,  4.62326007]), 'targetState': array([ 7, 10], dtype=int32)}
episode index:797
model initialize at round 797
at step 0:
{'currentState': array([26.04367367, 10.7156782 ,  4.35980415]), 'targetState': array([23, 11], dtype=int32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6257110715422991
{'currentState': array([22.99821002, 11.0878827 ,  3.46479742]), 'targetState': array([23, 11], dtype=int32)}
episode index:798
model initialize at round 798
at step 0:
{'currentState': array([20.76332668, 16.83650071,  4.25113273]), 'targetState': array([16,  2], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6256337174983818
{'currentState': array([15.92444578,  1.92218644,  3.70696415]), 'targetState': array([16,  2], dtype=int32)}
episode index:799
model initialize at round 799
at step 0:
{'currentState': array([18.03707738, 25.71474302,  4.33664322]), 'targetState': array([ 6, 18], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6255565568395743
{'currentState': array([ 5.95503403, 17.96327105,  3.3689684 ]), 'targetState': array([ 6, 18], dtype=int32)}
episode index:800
model initialize at round 800
at step 0:
{'currentState': array([11.06194932,  5.28090667,  0.84873724]), 'targetState': array([25,  9], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6254795888415878
{'currentState': array([25.02996099,  8.98492515,  0.64980915]), 'targetState': array([25,  9], dtype=int32)}
episode index:801
model initialize at round 801
at step 0:
{'currentState': array([12.01183282, 19.28741305,  2.03464949]), 'targetState': array([ 8, 25], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6256127855757684
{'currentState': array([ 7.9376728 , 25.03808476,  1.89083178]), 'targetState': array([ 8, 25], dtype=int32)}
episode index:802
model initialize at round 802
at step 0:
{'currentState': array([ 3.1123047 , 21.26482811,  0.66471672]), 'targetState': array([ 4, 16], dtype=int32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6258522552545
{'currentState': array([ 3.96302376, 16.04769398,  5.45726269]), 'targetState': array([ 4, 16], dtype=int32)}
episode index:803
model initialize at round 803
at step 0:
{'currentState': array([ 2.73869756, 11.87971988,  4.07798719]), 'targetState': array([8, 3], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6259313557900902
{'currentState': array([7.96492405, 2.93542205, 4.82398519]), 'targetState': array([8, 3], dtype=int32)}
episode index:804
model initialize at round 804
at step 0:
{'currentState': array([23.98943307, 13.71253763,  4.17064619]), 'targetState': array([18,  3], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6259520765929407
{'currentState': array([17.96359671,  3.04106004,  3.59940866]), 'targetState': array([18,  3], dtype=int32)}
episode index:805
model initialize at round 805
at step 0:
{'currentState': array([2.15129273, 6.75084419, 4.83968145]), 'targetState': array([4, 5], dtype=int32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6263088572019863
{'currentState': array([3.99651175, 4.91965643, 5.45345899]), 'targetState': array([4, 5], dtype=int32)}
episode index:806
model initialize at round 806
at step 0:
{'currentState': array([25.88441115, 13.73658874,  3.79387522]), 'targetState': array([21, 19], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6264966050361834
{'currentState': array([21.08701699, 18.9569695 ,  2.62275355]), 'targetState': array([21, 19], dtype=int32)}
episode index:807
model initialize at round 807
at step 0:
{'currentState': array([25.01784634, 16.28710239,  1.00371599]), 'targetState': array([25,  6], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6265659837366466
{'currentState': array([24.95808777,  6.04884676,  4.39086171]), 'targetState': array([25,  6], dtype=int32)}
episode index:808
model initialize at round 808
at step 0:
{'currentState': array([14.28754606, 10.99202907,  5.75047186]), 'targetState': array([9, 3], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6266697998596821
{'currentState': array([8.98713589, 3.05003554, 4.34123454]), 'targetState': array([9, 3], dtype=int32)}
episode index:809
model initialize at round 809
at step 0:
{'currentState': array([17.23543622,  4.16527571,  1.1170655 ]), 'targetState': array([18,  7], dtype=int32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.62701265452036
{'currentState': array([18.04363527,  6.96400595,  1.94993093]), 'targetState': array([18,  7], dtype=int32)}
episode index:810
model initialize at round 810
at step 0:
{'currentState': array([9.82836449, 7.76915909, 3.56804609]), 'targetState': array([ 9, 19], dtype=int32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6270006662175511
{'currentState': array([ 9.02020668, 19.0633991 ,  1.12636005]), 'targetState': array([ 9, 19], dtype=int32)}
episode index:811
model initialize at round 811
at step 0:
{'currentState': array([9.17674958, 4.22694903, 1.41411194]), 'targetState': array([18, 13], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.627019891508028
{'currentState': array([18.03816237, 13.04756216,  1.32458383]), 'targetState': array([18, 13], dtype=int32)}
episode index:812
model initialize at round 812
at step 0:
{'currentState': array([2.74669654, 9.1363218 , 2.14287281]), 'targetState': array([18,  2], dtype=int32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.6268696651824178
{'currentState': array([18.05091452,  1.96888953,  5.9505745 ]), 'targetState': array([18,  2], dtype=int32)}
episode index:813
model initialize at round 813
at step 0:
{'currentState': array([9.98899721, 9.28744602, 2.1140554 ]), 'targetState': array([4, 5], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6269812878696609
{'currentState': array([3.95296237, 4.98189623, 4.07813997]), 'targetState': array([4, 5], dtype=int32)}
episode index:814
model initialize at round 814
at step 0:
{'currentState': array([19.25766585, 27.12788504,  6.23888559]), 'targetState': array([18, 22], dtype=int32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6272256895088829
{'currentState': array([17.91286845, 22.02741609,  4.48188277]), 'targetState': array([18, 22], dtype=int32)}
episode index:815
model initialize at round 815
at step 0:
{'currentState': array([12.20200643,  5.79520821,  5.99594021]), 'targetState': array([21,  7], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6273544611757279
{'currentState': array([2.10328843e+01, 6.95147259e+00, 1.48748411e-05]), 'targetState': array([21,  7], dtype=int32)}
episode index:816
model initialize at round 816
at step 0:
{'currentState': array([19.22332642, 21.18130523,  1.18691863]), 'targetState': array([23, 29], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6274220255990262
{'currentState': array([22.99916759, 29.06248142,  1.63979213]), 'targetState': array([23, 29], dtype=int32)}
episode index:817
model initialize at round 817
at step 0:
{'currentState': array([15.16209311, 17.76236141,  4.80599976]), 'targetState': array([ 6, 28], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6273946222268239
{'currentState': array([ 5.9884986 , 28.06420015,  1.93053448]), 'targetState': array([ 6, 28], dtype=int32)}
episode index:818
model initialize at round 818
at step 0:
{'currentState': array([ 2.76811632, 15.82977593,  3.269835  ]), 'targetState': array([14, 27], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6273171015531066
{'currentState': array([14.07087828, 26.97056425,  0.50039832]), 'targetState': array([14, 27], dtype=int32)}
episode index:819
model initialize at round 819
at step 0:
{'currentState': array([17.73922519,  2.12141981,  2.20083213]), 'targetState': array([21,  2], dtype=int32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6276222282865771
{'currentState': array([21.03444604,  1.94801612,  6.02454489]), 'targetState': array([21,  2], dtype=int32)}
episode index:820
model initialize at round 820
at step 0:
{'currentState': array([23.25463538, 18.13381741,  6.26204425]), 'targetState': array([18, 10], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6276483919894782
{'currentState': array([17.99761978,  9.95333569,  5.86543174]), 'targetState': array([18, 10], dtype=int32)}
episode index:821
model initialize at round 821
at step 0:
{'currentState': array([22.73754567,  4.11786054,  3.223535  ]), 'targetState': array([17, 10], dtype=int32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6277937979533982
{'currentState': array([17.06442666, 10.00144393,  2.22811285]), 'targetState': array([17, 10], dtype=int32)}
episode index:822
model initialize at round 822
at step 0:
{'currentState': array([15.95938914, 24.28477541,  2.21744788]), 'targetState': array([ 3, 24], dtype=int32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6277810353081841
{'currentState': array([ 2.91096444, 23.95275254,  3.36798968]), 'targetState': array([ 3, 24], dtype=int32)}
episode index:823
model initialize at round 823
at step 0:
{'currentState': array([26.71575853, 12.95580652,  2.79083633]), 'targetState': array([17, 26], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6277035160789902
{'currentState': array([17.0454026 , 26.0433349 ,  1.94287036]), 'targetState': array([17, 26], dtype=int32)}
episode index:824
model initialize at round 824
at step 0:
{'currentState': array([25.76178815, 23.16124946,  3.05152774]), 'targetState': array([25,  8], dtype=int32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6276471074705527
{'currentState': array([24.95643977,  8.02607468,  4.7915608 ]), 'targetState': array([25,  8], dtype=int32)}
episode index:825
model initialize at round 825
at step 0:
{'currentState': array([16.28698004, 10.01971624,  5.84678001]), 'targetState': array([19,  2], dtype=int32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6277827651774749
{'currentState': array([18.95009175,  1.99703135,  4.36730639]), 'targetState': array([19,  2], dtype=int32)}
episode index:826
model initialize at round 826
at step 0:
{'currentState': array([27.15360076,  6.75678594,  4.77068138]), 'targetState': array([12, 17], dtype=int32)}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.6276403262186745
{'currentState': array([12.02666613, 17.01332223,  3.08609669]), 'targetState': array([12, 17], dtype=int32)}
episode index:827
model initialize at round 827
at step 0:
{'currentState': array([21.24925918, 22.85641679,  5.25556993]), 'targetState': array([11, 25], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6276506464600333
{'currentState': array([10.97974868, 25.02593673,  3.26543671]), 'targetState': array([11, 25], dtype=int32)}
episode index:828
model initialize at round 828
at step 0:
{'currentState': array([11.28720667,  1.98391884,  0.44906681]), 'targetState': array([23,  8], dtype=int32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6275945738034086
{'currentState': array([23.03943353,  7.9881106 ,  0.32450757]), 'targetState': array([23,  8], dtype=int32)}
episode index:829
model initialize at round 829
at step 0:
{'currentState': array([18.08225307, 15.27564598,  0.77580702]), 'targetState': array([24,  9], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6277207289791326
{'currentState': array([24.04413456,  8.92175627,  4.99641323]), 'targetState': array([24,  9], dtype=int32)}
episode index:830
model initialize at round 830
at step 0:
{'currentState': array([12.7569429 , 16.15384901,  2.07227921]), 'targetState': array([16, 14], dtype=int32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6279899672969613
{'currentState': array([16.03774853, 13.96406863,  5.88005727]), 'targetState': array([16, 14], dtype=int32)}
episode index:831
model initialize at round 831
at step 0:
{'currentState': array([11.72932041, 13.09735929,  2.29131627]), 'targetState': array([ 3, 14], dtype=int32)}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.628115343982487
{'currentState': array([ 3.06558199, 14.05543292,  3.77396691]), 'targetState': array([ 3, 14], dtype=int32)}
episode index:832
model initialize at round 832
at step 0:
{'currentState': array([ 2.26126754, 15.87964407,  0.07331532]), 'targetState': array([ 9, 21], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6281973342274655
{'currentState': array([ 9.01002348, 21.0327536 ,  1.33663483]), 'targetState': array([ 9, 21], dtype=int32)}
episode index:833
model initialize at round 833
at step 0:
{'currentState': array([10.86082526,  8.74825277,  3.7023716 ]), 'targetState': array([12, 21], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6281992842238393
{'currentState': array([12.03895776, 21.01846255,  1.95567131]), 'targetState': array([12, 21], dtype=int32)}
episode index:834
model initialize at round 834
at step 0:
{'currentState': array([16.0406444 ,  7.28490382,  0.92683095]), 'targetState': array([14, 14], dtype=int32)}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6283784723378216
{'currentState': array([13.98773528, 14.07017544,  1.94634875]), 'targetState': array([14, 14], dtype=int32)}
episode index:835
model initialize at round 835
at step 0:
{'currentState': array([16.77860977, 21.18366448,  2.95406401]), 'targetState': array([16, 19], dtype=int32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6286556013836392
{'currentState': array([16.01800252, 18.91307977,  4.84854153]), 'targetState': array([16, 19], dtype=int32)}
episode index:836
model initialize at round 836
at step 0:
{'currentState': array([14.75358084, 16.85159558,  4.18866038]), 'targetState': array([ 7, 13], dtype=int32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6287882713621393
{'currentState': array([ 6.98689309, 13.01643518,  3.56605844]), 'targetState': array([ 7, 13], dtype=int32)}
episode index:837
model initialize at round 837
at step 0:
{'currentState': array([15.7653901 ,  9.8335534 ,  4.26364017]), 'targetState': array([18,  5], dtype=int32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.629023808775592
{'currentState': array([18.00987175,  4.91311361,  5.3408886 ]), 'targetState': array([18,  5], dtype=int32)}
episode index:838
model initialize at round 838
at step 0:
{'currentState': array([ 5.72495442, 10.08423897,  2.33939099]), 'targetState': array([ 4, 12], dtype=int32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6292991777217968
{'currentState': array([ 4.02287951, 12.03630152,  2.49029273]), 'targetState': array([ 4, 12], dtype=int32)}
episode index:839
model initialize at round 839
at step 0:
{'currentState': array([ 4.8114241 , 18.78277799,  3.49246597]), 'targetState': array([ 2, 13], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6294947909431323
{'currentState': array([ 1.9599696 , 12.96763249,  4.35015391]), 'targetState': array([ 2, 13], dtype=int32)}
episode index:840
model initialize at round 840
at step 0:
{'currentState': array([25.71922791, 12.89816397,  3.65440369]), 'targetState': array([23, 12], dtype=int32)}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6298216485936267
{'currentState': array([22.96928716, 12.04615718,  3.48458143]), 'targetState': array([23, 12], dtype=int32)}
episode index:841
model initialize at round 841
at step 0:
{'currentState': array([11.28477866, 26.04058804,  5.91975671]), 'targetState': array([ 6, 23], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6300161766637573
{'currentState': array([ 5.97089477, 23.02346916,  4.18664046]), 'targetState': array([ 6, 23], dtype=int32)}
episode index:842
model initialize at round 842
at step 0:
{'currentState': array([21.74065223, 17.87556122,  4.09396172]), 'targetState': array([13,  3], dtype=int32)}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.6298499537649916
{'currentState': array([12.94495247,  2.94497468,  3.33978552]), 'targetState': array([13,  3], dtype=int32)}
episode index:843
model initialize at round 843
at step 0:
{'currentState': array([ 8.20118686, 18.20559699,  1.30123919]), 'targetState': array([ 6, 26], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6299979032121938
{'currentState': array([ 6.05404755, 25.96947945,  2.27143842]), 'targetState': array([ 6, 26], dtype=int32)}
episode index:844
model initialize at round 844
at step 0:
{'currentState': array([ 2.94305176, 13.28196307,  1.26508594]), 'targetState': array([17,  3], dtype=int32)}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.6298619732881839
{'currentState': array([16.95979996,  2.98792317,  5.2400568 ]), 'targetState': array([17,  3], dtype=int32)}
episode index:845
model initialize at round 845
at step 0:
{'currentState': array([22.99416399, 20.71240269,  4.18709946]), 'targetState': array([18,  8], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6298325927844598
{'currentState': array([18.06288173,  8.09764476,  4.44006784]), 'targetState': array([18,  8], dtype=int32)}
episode index:846
model initialize at round 846
at step 0:
{'currentState': array([10.28765744, 22.02843917,  0.57363051]), 'targetState': array([18, 19], dtype=int32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6299623068111466
{'currentState': array([18.0367087 , 18.95527269,  5.72575865]), 'targetState': array([18, 19], dtype=int32)}
episode index:847
model initialize at round 847
at step 0:
{'currentState': array([23.73697427, 24.11646346,  2.21975613]), 'targetState': array([13, 27], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.630016277026063
{'currentState': array([13.01366884, 27.04640791,  3.16186681]), 'targetState': array([13, 27], dtype=int32)}
episode index:848
model initialize at round 848
at step 0:
{'currentState': array([ 8.75203977, 18.14581494,  2.1050005 ]), 'targetState': array([ 3, 29], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6300311125090531
{'currentState': array([ 3.01815925, 29.00468231,  1.35720894]), 'targetState': array([ 3, 29], dtype=int32)}
episode index:849
model initialize at round 849
at step 0:
{'currentState': array([ 7.19988618, 23.20686177,  0.29754615]), 'targetState': array([20, 23], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6300383529485294
{'currentState': array([20.07422927, 23.04223618,  0.05847162]), 'targetState': array([20, 23], dtype=int32)}
episode index:850
model initialize at round 850
at step 0:
{'currentState': array([16.87482068, 23.74100885,  4.76716256]), 'targetState': array([16, 17], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6302212434160483
{'currentState': array([16.02324622, 16.94080417,  4.4635639 ]), 'targetState': array([16, 17], dtype=int32)}
episode index:851
model initialize at round 851
at step 0:
{'currentState': array([17.82419772, 14.77231637,  3.54987192]), 'targetState': array([5, 5], dtype=int32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.630130236648145
{'currentState': array([5.03208606, 4.99133298, 3.59330587]), 'targetState': array([5, 5], dtype=int32)}
episode index:852
model initialize at round 852
at step 0:
{'currentState': array([11.05913583, 18.28151239,  0.85874212]), 'targetState': array([12,  3], dtype=int32)}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6300592764674054
{'currentState': array([12.07400294,  2.90766705,  5.58293808]), 'targetState': array([12,  3], dtype=int32)}
episode index:853
model initialize at round 853
at step 0:
{'currentState': array([15.12853879, 19.74265965,  4.67062807]), 'targetState': array([20,  5], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.629995219644817
{'currentState': array([19.99245936,  5.04724461,  5.02594607]), 'targetState': array([20,  5], dtype=int32)}
episode index:854
model initialize at round 854
at step 0:
{'currentState': array([10.71692586, 11.05114005,  3.46786094]), 'targetState': array([10,  8], dtype=int32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6303055623753595
{'currentState': array([9.99351679, 7.92630776, 4.67377838]), 'targetState': array([10,  8], dtype=int32)}
episode index:855
model initialize at round 855
at step 0:
{'currentState': array([17.17647491, 18.77283733,  4.86786366]), 'targetState': array([21,  4], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6302279918474122
{'currentState': array([21.03186286,  4.05474853,  4.4245922 ]), 'targetState': array([21,  4], dtype=int32)}
episode index:856
model initialize at round 856
at step 0:
{'currentState': array([ 5.76347327, 21.83628872,  3.24202871]), 'targetState': array([ 2, 18], dtype=int32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6304566273573166
{'currentState': array([ 2.01825349, 17.96481636,  4.19919556]), 'targetState': array([ 2, 18], dtype=int32)}
episode index:857
model initialize at round 857
at step 0:
{'currentState': array([7.0131966 , 2.71264634, 5.26328135]), 'targetState': array([21,  3], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6304129325077178
{'currentState': array([20.99733163,  3.01178509,  5.80257308]), 'targetState': array([21,  3], dtype=int32)}
episode index:858
model initialize at round 858
at step 0:
{'currentState': array([3.2774815 , 9.07583069, 0.77176827]), 'targetState': array([17, 23], dtype=int32)}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.6301948126580302
{'currentState': array([16.97860615, 23.06852217,  1.21804118]), 'targetState': array([17, 23], dtype=int32)}
episode index:859
model initialize at round 859
at step 0:
{'currentState': array([ 7.94743169, 18.71718761,  5.03360939]), 'targetState': array([21, 20], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.630187057521092
{'currentState': array([21.09602223, 20.02922168,  6.26505302]), 'targetState': array([21, 20], dtype=int32)}
episode index:860
model initialize at round 860
at step 0:
{'currentState': array([12.76073513,  8.1596828 ,  3.05808997]), 'targetState': array([ 5, 18], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6301438280075967
{'currentState': array([ 4.91601259, 18.02835971,  2.20822253]), 'targetState': array([ 5, 18], dtype=int32)}
episode index:861
model initialize at round 861
at step 0:
{'currentState': array([23.83651372, 24.7633177 ,  3.60290384]), 'targetState': array([18, 28], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6303334689074066
{'currentState': array([18.05255748, 27.94090143,  2.88340923]), 'targetState': array([18, 28], dtype=int32)}
episode index:862
model initialize at round 862
at step 0:
{'currentState': array([10.71249497, 16.99066542,  3.67904878]), 'targetState': array([4, 4], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6302971103182511
{'currentState': array([4.01663298, 4.0172271 , 4.61154929]), 'targetState': array([4, 4], dtype=int32)}
episode index:863
model initialize at round 863
at step 0:
{'currentState': array([17.00339537, 12.28763648,  1.05399251]), 'targetState': array([24,  7], dtype=int32)}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6304066973849636
{'currentState': array([24.03109432,  6.9948998 ,  5.40257961]), 'targetState': array([24,  7], dtype=int32)}
episode index:864
model initialize at round 864
at step 0:
{'currentState': array([27.10223947, 11.73112574,  4.57075453]), 'targetState': array([15, 15], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6304133780655171
{'currentState': array([15.06398703, 14.95783181,  2.95823939]), 'targetState': array([15, 15], dtype=int32)}
episode index:865
model initialize at round 865
at step 0:
{'currentState': array([24.71456282, 13.03566358,  3.52229309]), 'targetState': array([14,  4], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6303840393692954
{'currentState': array([14.00572341,  3.98304547,  3.61546074]), 'targetState': array([14,  4], dtype=int32)}
episode index:866
model initialize at round 866
at step 0:
{'currentState': array([17.80418571, 22.21072029,  2.82454491]), 'targetState': array([ 4, 18], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6303477901963969
{'currentState': array([ 3.94270753, 17.91135822,  4.10335764]), 'targetState': array([ 4, 18], dtype=int32)}
episode index:867
model initialize at round 867
at step 0:
{'currentState': array([20.71794792, 17.94349426,  2.83431315]), 'targetState': array([ 7, 28], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6302519478602152
{'currentState': array([ 6.92432745, 28.00927433,  2.71820702]), 'targetState': array([ 7, 28], dtype=int32)}
episode index:868
model initialize at round 868
at step 0:
{'currentState': array([ 6.24308459, 19.15380557,  0.05913472]), 'targetState': array([20, 26], dtype=int32)}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6301954627811104
{'currentState': array([19.95801943, 25.99653164,  0.43929914]), 'targetState': array([20, 26], dtype=int32)}
episode index:869
model initialize at round 869
at step 0:
{'currentState': array([6.15304618, 1.75643658, 5.77840281]), 'targetState': array([16, 11], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6301735337888711
{'currentState': array([16.0714813 , 11.08180004,  0.68372561]), 'targetState': array([16, 11], dtype=int32)}
episode index:870
model initialize at round 870
at step 0:
{'currentState': array([16.28547215, 26.03538254,  5.90150035]), 'targetState': array([14, 23], dtype=int32)}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6304474399645085
{'currentState': array([14.01152633, 23.01404792,  4.29429727]), 'targetState': array([14, 23], dtype=int32)}
episode index:871
model initialize at round 871
at step 0:
{'currentState': array([12.12390599, 23.74039726,  4.65270472]), 'targetState': array([11, 19], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.630691127755127
{'currentState': array([10.99792787, 18.95227757,  4.72798596]), 'targetState': array([11, 19], dtype=int32)}
episode index:872
model initialize at round 872
at step 0:
{'currentState': array([24.054429  , 19.28246019,  1.88543344]), 'targetState': array([22, 27], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6308331989572444
{'currentState': array([21.97752536, 27.04428341,  2.08983079]), 'targetState': array([22, 27], dtype=int32)}
episode index:873
model initialize at round 873
at step 0:
{'currentState': array([21.02586701,  4.71350886,  4.29743385]), 'targetState': array([6, 7], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6307697224709969
{'currentState': array([5.93604224, 6.99872847, 2.99946909]), 'targetState': array([6, 7], dtype=int32)}
episode index:874
model initialize at round 874
at step 0:
{'currentState': array([20.17675734, 14.22694298,  0.40407777]), 'targetState': array([25, 11], dtype=int32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6309835935739984
{'currentState': array([2.49514435e+01, 1.10195956e+01, 1.27802016e-02]), 'targetState': array([25, 11], dtype=int32)}
episode index:875
model initialize at round 875
at step 0:
{'currentState': array([7.27905164, 7.06983164, 0.75021046]), 'targetState': array([ 6, 15], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6310663151509682
{'currentState': array([ 5.93003248, 15.00161881,  2.60686904]), 'targetState': array([ 6, 15], dtype=int32)}
episode index:876
model initialize at round 876
at step 0:
{'currentState': array([20.84189681,  6.24031158,  2.65771151]), 'targetState': array([15,  6], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6311651340990267
{'currentState': array([15.0271529 ,  5.96900106,  4.62481746]), 'targetState': array([15,  6], dtype=int32)}
episode index:877
model initialize at round 877
at step 0:
{'currentState': array([25.76706962, 16.16878896,  2.0095253 ]), 'targetState': array([16, 25], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6311636061914004
{'currentState': array([16.05496357, 24.96706483,  2.97394411]), 'targetState': array([16, 25], dtype=int32)}
episode index:878
model initialize at round 878
at step 0:
{'currentState': array([ 6.20757943, 28.8008592 ,  5.01353216]), 'targetState': array([ 4, 29], dtype=int32)}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6314848276263177
{'currentState': array([ 3.97207051, 28.99088288,  3.10160518]), 'targetState': array([ 4, 29], dtype=int32)}
episode index:879
model initialize at round 879
at step 0:
{'currentState': array([ 9.83640093, 21.76339565,  4.61242723]), 'targetState': array([20, 11], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6314410567385623
{'currentState': array([20.00193733, 11.01148806,  5.35682417]), 'targetState': array([20, 11], dtype=int32)}
episode index:880
model initialize at round 880
at step 0:
{'currentState': array([ 8.71279163, 16.98394924,  2.69242001]), 'targetState': array([15, 24], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6315308548889979
{'currentState': array([14.93133557, 23.94590542,  1.20950061]), 'targetState': array([15, 24], dtype=int32)}
episode index:881
model initialize at round 881
at step 0:
{'currentState': array([10.71882472, 20.9392815 ,  3.85927224]), 'targetState': array([3, 6], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6314351925165509
{'currentState': array([2.94009014, 5.93586581, 4.24015385]), 'targetState': array([3, 6], dtype=int32)}
episode index:882
model initialize at round 882
at step 0:
{'currentState': array([20.7509216 , 26.85610342,  4.17046571]), 'targetState': array([16, 25], dtype=int32)}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6316651795708358
{'currentState': array([15.96955517, 24.98156745,  3.51602551]), 'targetState': array([16, 25], dtype=int32)}
episode index:883
model initialize at round 883
at step 0:
{'currentState': array([19.75726063, 19.15434983,  3.08021736]), 'targetState': array([14, 18], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6318130048364198
{'currentState': array([14.00754066, 17.91429235,  4.61202719]), 'targetState': array([14, 18], dtype=int32)}
episode index:884
model initialize at round 884
at step 0:
{'currentState': array([12.25801576, 16.12717761,  0.9629566 ]), 'targetState': array([20, 20], dtype=int32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6319349114675518
{'currentState': array([19.99228383, 19.99451665,  0.55663305]), 'targetState': array([20, 20], dtype=int32)}
episode index:885
model initialize at round 885
at step 0:
{'currentState': array([17.85764067, 21.75003981,  3.68967676]), 'targetState': array([16, 28], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6321084365571
{'currentState': array([15.95735269, 28.05525232,  1.74525091]), 'targetState': array([16, 28], dtype=int32)}
episode index:886
model initialize at round 886
at step 0:
{'currentState': array([25.08713306,  4.72585751,  4.51513004]), 'targetState': array([10,  7], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6320126622682991
{'currentState': array([9.9693615 , 6.94845462, 3.52942264]), 'targetState': array([10,  7], dtype=int32)}
episode index:887
model initialize at round 887
at step 0:
{'currentState': array([ 5.01751385, 10.71287714,  5.27831125]), 'targetState': array([6, 4], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6321857089783656
{'currentState': array([6.06035989, 3.90432286, 5.04738522]), 'targetState': array([6, 4], dtype=int32)}
episode index:888
model initialize at round 888
at step 0:
{'currentState': array([20.28434487, 16.9564768 ,  5.62629933]), 'targetState': array([5, 5], dtype=int32)}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.6319933988078815
{'currentState': array([5.00081603, 4.9427869 , 4.18095654]), 'targetState': array([5, 5], dtype=int32)}
episode index:889
model initialize at round 889
at step 0:
{'currentState': array([18.91310562, 20.72578176,  3.90051842]), 'targetState': array([16, 11], dtype=int32)}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6320736845339395
{'currentState': array([16.00170055, 11.07536068,  4.79784797]), 'targetState': array([16, 11], dtype=int32)}
episode index:890
model initialize at round 890
at step 0:
{'currentState': array([3.28553746, 8.9651484 , 0.38354462]), 'targetState': array([17,  5], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6320365154227524
{'currentState': array([17.08261132,  4.95386989,  5.78548585]), 'targetState': array([17,  5], dtype=int32)}
episode index:891
model initialize at round 891
at step 0:
{'currentState': array([15.19137858, 23.21475687,  1.3478975 ]), 'targetState': array([ 9, 26], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6322087593973987
{'currentState': array([ 9.00506576, 25.99511163,  2.94004654]), 'targetState': array([ 9, 26], dtype=int32)}
episode index:892
model initialize at round 892
at step 0:
{'currentState': array([ 7.06670076, 23.70760901,  4.98128859]), 'targetState': array([ 7, 19], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6323895046653117
{'currentState': array([ 7.06814958, 19.03881483,  0.72656907]), 'targetState': array([ 7, 19], dtype=int32)}
episode index:893
model initialize at round 893
at step 0:
{'currentState': array([16.75126622, 13.14449146,  2.11032963]), 'targetState': array([14, 26], dtype=int32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6323726149967177
{'currentState': array([13.9927417 , 26.05109999,  1.70704994]), 'targetState': array([14, 26], dtype=int32)}
episode index:894
model initialize at round 894
at step 0:
{'currentState': array([13.76628536, 27.83229865,  4.26899862]), 'targetState': array([ 4, 13], dtype=int32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.6322474394745429
{'currentState': array([ 3.97894736, 12.94752436,  4.0742728 ]), 'targetState': array([ 4, 13], dtype=int32)}
episode index:895
model initialize at round 895
at step 0:
{'currentState': array([ 7.06399744, 23.28044715,  0.8414402 ]), 'targetState': array([15, 27], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6323112805977511
{'currentState': array([14.98946667, 27.05883467,  1.22940296]), 'targetState': array([15, 27], dtype=int32)}
episode index:896
model initialize at round 896
at step 0:
{'currentState': array([18.05863074, 17.28161802,  0.86053598]), 'targetState': array([26, 20], dtype=int32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6324393305573207
{'currentState': array([25.96412901, 19.99287121,  0.70005295]), 'targetState': array([26, 20], dtype=int32)}
episode index:897
model initialize at round 897
at step 0:
{'currentState': array([23.26152209, 21.11980179,  0.93456477]), 'targetState': array([22, 24], dtype=int32)}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6327320910514203
{'currentState': array([21.95794065, 24.04059163,  1.93644791]), 'targetState': array([22, 24], dtype=int32)}
episode index:898
model initialize at round 898
at step 0:
{'currentState': array([ 8.84615404, 22.75694097,  4.65308809]), 'targetState': array([2, 9], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6326369014533549
{'currentState': array([2.04401069, 8.9760773 , 4.91264847]), 'targetState': array([2, 9], dtype=int32)}
episode index:899
model initialize at round 899
at step 0:
{'currentState': array([11.73710433, 16.11675675,  3.22864079]), 'targetState': array([4, 3], dtype=int32)}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.632535843869481
{'currentState': array([4.08724424, 2.98353341, 6.13867297]), 'targetState': array([4, 3], dtype=int32)}
episode index:900
model initialize at round 900
at step 0:
{'currentState': array([ 3.7291904 , 16.0969971 ,  2.29265404]), 'targetState': array([13, 22], dtype=int32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6325762832864624
{'currentState': array([13.08796185, 22.06191734,  0.66307451]), 'targetState': array([13, 22], dtype=int32)}
episode index:901
model initialize at round 901
at step 0:
{'currentState': array([10.24057714, 12.1576988 ,  1.08523336]), 'targetState': array([16, 24], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6325732315657492
{'currentState': array([15.98794887, 23.95889381,  1.43127898]), 'targetState': array([16, 24], dtype=int32)}
episode index:902
model initialize at round 902
at step 0:
{'currentState': array([25.27413992, 12.08714114,  0.81277054]), 'targetState': array([27, 15], dtype=int32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6328543075792048
{'currentState': array([27.02958185, 15.08688392,  1.39075939]), 'targetState': array([27, 15], dtype=int32)}
episode index:903
model initialize at round 903
at step 0:
{'currentState': array([12.77319152, 21.82307011,  3.29907179]), 'targetState': array([ 5, 28], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6329246161084862
{'currentState': array([ 5.0182345 , 27.97811934,  2.4037989 ]), 'targetState': array([ 5, 28], dtype=int32)}
episode index:904
model initialize at round 904
at step 0:
{'currentState': array([22.10874218, 28.73368926,  4.59505415]), 'targetState': array([19, 16], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6329211896058283
{'currentState': array([18.97955475, 15.95128475,  4.54492578]), 'targetState': array([19, 16], dtype=int32)}
episode index:905
model initialize at round 905
at step 0:
{'currentState': array([10.27956039, 22.93223378,  5.54036963]), 'targetState': array([ 6, 18], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6330985550517862
{'currentState': array([ 6.05523293, 18.09611743,  4.44330356]), 'targetState': array([ 6, 18], dtype=int32)}
episode index:906
model initialize at round 906
at step 0:
{'currentState': array([14.87503563,  8.25909492,  1.51519299]), 'targetState': array([22, 21], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.633054307963969
{'currentState': array([22.02659838, 21.09201861,  1.24839428]), 'targetState': array([22, 21], dtype=int32)}
episode index:907
model initialize at round 907
at step 0:
{'currentState': array([ 5.78837943, 22.19484098,  2.90245318]), 'targetState': array([ 9, 10], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6330438135663118
{'currentState': array([ 9.00322663, 10.02285861,  4.86328802]), 'targetState': array([ 9, 10], dtype=int32)}
episode index:908
model initialize at round 908
at step 0:
{'currentState': array([ 9.75845554, 12.84378684,  4.22066307]), 'targetState': array([5, 6], dtype=int32)}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6331693672305201
{'currentState': array([5.00834926, 6.04530644, 4.49201687]), 'targetState': array([5, 6], dtype=int32)}
episode index:909
model initialize at round 909
at step 0:
{'currentState': array([16.89416555, 14.73252039,  4.84061837]), 'targetState': array([14,  3], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6331726816468204
{'currentState': array([14.03579532,  3.07999553,  4.57759611]), 'targetState': array([14,  3], dtype=int32)}
episode index:910
model initialize at round 910
at step 0:
{'currentState': array([25.86229562,  4.7474455 ,  3.70820284]), 'targetState': array([13,  2], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6331092152015186
{'currentState': array([13.04238321,  1.94772552,  5.49899556]), 'targetState': array([13,  2], dtype=int32)}
episode index:911
model initialize at round 911
at step 0:
{'currentState': array([12.12676931, 17.25821661,  1.61942154]), 'targetState': array([13, 29], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6331339410846026
{'currentState': array([12.99691085, 29.06660543,  1.75776446]), 'targetState': array([13, 29], dtype=int32)}
episode index:912
model initialize at round 912
at step 0:
{'currentState': array([ 8.12961006, 20.74012345,  4.7303912 ]), 'targetState': array([16, 12], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6331586128036494
{'currentState': array([16.04124014, 11.98063172,  5.42086467]), 'targetState': array([16, 12], dtype=int32)}
episode index:913
model initialize at round 913
at step 0:
{'currentState': array([14.8332384 ,  8.2343861 ,  2.69418716]), 'targetState': array([ 4, 20], dtype=int32)}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.6330467385477976
{'currentState': array([ 4.00175738, 19.99813538,  1.899791  ]), 'targetState': array([ 4, 20], dtype=int32)}
episode index:914
model initialize at round 914
at step 0:
{'currentState': array([12.28198516, 24.94316124,  0.30609864]), 'targetState': array([22, 28], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6330714516429083
{'currentState': array([21.95766627, 28.02709459,  2.40850414]), 'targetState': array([22, 28], dtype=int32)}
episode index:915
model initialize at round 915
at step 0:
{'currentState': array([ 7.94496815, 11.71765664,  5.02489138]), 'targetState': array([7, 3], dtype=int32)}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6331254725417812
{'currentState': array([7.02249596, 3.02082606, 0.89113522]), 'targetState': array([7, 3], dtype=int32)}
episode index:916
model initialize at round 916
at step 0:
{'currentState': array([16.79300762, 20.19975092,  2.87899613]), 'targetState': array([4, 6], dtype=int32)}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.6329801198235232
{'currentState': array([4.00508896, 6.00304945, 4.39704728]), 'targetState': array([4, 6], dtype=int32)}
episode index:917
model initialize at round 917
at step 0:
{'currentState': array([7.75980842, 1.84171456, 3.21926641]), 'targetState': array([13, 12], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6330048247263017
{'currentState': array([12.99049946, 12.00197024,  1.84708093]), 'targetState': array([13, 12], dtype=int32)}
episode index:918
model initialize at round 918
at step 0:
{'currentState': array([20.27370952,  4.08848376,  0.81767195]), 'targetState': array([10, 10], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6330223413787959
{'currentState': array([9.95710308, 9.99648459, 3.62975726]), 'targetState': array([10, 10], dtype=int32)}
episode index:919
model initialize at round 919
at step 0:
{'currentState': array([13.87584124, 10.74051806,  3.76109934]), 'targetState': array([ 5, 25], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6329290090972871
{'currentState': array([ 5.0841029 , 24.92080269,  2.70405618]), 'targetState': array([ 5, 25], dtype=int32)}
episode index:920
model initialize at round 920
at step 0:
{'currentState': array([ 3.71475436, 14.96283554,  2.76615214]), 'targetState': array([ 5, 26], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6329754803676052
{'currentState': array([ 5.06025029, 25.9087285 ,  2.32632446]), 'targetState': array([ 5, 26], dtype=int32)}
episode index:921
model initialize at round 921
at step 0:
{'currentState': array([18.17594328, 21.22757469,  1.41765985]), 'targetState': array([ 5, 16], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6329320866214381
{'currentState': array([ 4.9045781 , 15.96073215,  3.60191404]), 'targetState': array([ 5, 16], dtype=int32)}
episode index:922
model initialize at round 922
at step 0:
{'currentState': array([15.77665711, 15.81871507,  4.3284204 ]), 'targetState': array([13,  6], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6330008635785649
{'currentState': array([12.97522483,  5.95950727,  4.81566001]), 'targetState': array([13,  6], dtype=int32)}
episode index:923
model initialize at round 923
at step 0:
{'currentState': array([ 6.21363854, 14.80737382,  6.05446196]), 'targetState': array([19, 26], dtype=int32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6328961745759986
{'currentState': array([19.01353381, 26.0579588 ,  1.77259931]), 'targetState': array([19, 26], dtype=int32)}
episode index:924
model initialize at round 924
at step 0:
{'currentState': array([20.81483362, 24.77986439,  3.50805712]), 'targetState': array([21, 29], dtype=int32)}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6331324573830459
{'currentState': array([20.99641586, 29.06399815,  1.4453966 ]), 'targetState': array([21, 29], dtype=int32)}
episode index:925
model initialize at round 925
at step 0:
{'currentState': array([ 3.05751989, 17.71815299,  5.41870642]), 'targetState': array([18, 11], dtype=int32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.6330163278259644
{'currentState': array([17.98668281, 11.05827626,  1.85155752]), 'targetState': array([18, 11], dtype=int32)}
episode index:926
model initialize at round 926
at step 0:
{'currentState': array([21.81472746, 27.7784341 ,  3.53565305]), 'targetState': array([17, 17], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6330407538159841
{'currentState': array([16.97376935, 17.02594131,  5.45559847]), 'targetState': array([17, 17], dtype=int32)}
episode index:927
model initialize at round 927
at step 0:
{'currentState': array([ 4.10950929, 12.26599622,  0.67524898]), 'targetState': array([13,  9], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6331242801882431
{'currentState': array([13.01081396,  8.97805565,  0.86088336]), 'targetState': array([13,  9], dtype=int32)}
episode index:928
model initialize at round 928
at step 0:
{'currentState': array([ 2.12304517, 15.26001195,  1.63379198]), 'targetState': array([ 3, 17], dtype=int32)}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6334460682051631
{'currentState': array([ 3.02742971, 17.05315472,  1.61873756]), 'targetState': array([ 3, 17], dtype=int32)}
episode index:929
model initialize at round 929
at step 0:
{'currentState': array([ 2.92167868, 15.72321119,  4.9416337 ]), 'targetState': array([4, 4], dtype=int32)}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6334699533152374
{'currentState': array([4.05491699, 3.93455169, 5.41987815]), 'targetState': array([4, 4], dtype=int32)}
episode index:930
model initialize at round 930
at step 0:
{'currentState': array([14.90686595, 25.27216231,  1.39550591]), 'targetState': array([25, 18], dtype=int32)}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6334660367501331
{'currentState': array([25.06524616, 18.01171306,  6.22093654]), 'targetState': array([25, 18], dtype=int32)}
episode index:931
model initialize at round 931
at step 0:
{'currentState': array([27.19583164,  5.78929584,  5.9662199 ]), 'targetState': array([26, 14], dtype=int32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6335800220898736
{'currentState': array([26.07092956, 13.93981657,  2.50115093]), 'targetState': array([26, 14], dtype=int32)}
episode index:932
model initialize at round 932
at step 0:
{'currentState': array([ 8.18019013, 23.7757729 ,  4.88432455]), 'targetState': array([12, 14], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6336399031871718
{'currentState': array([11.99061873, 14.0017749 ,  5.54954001]), 'targetState': array([12, 14], dtype=int32)}
episode index:933
model initialize at round 933
at step 0:
{'currentState': array([17.10374841,  5.26829562,  1.70681268]), 'targetState': array([ 6, 18], dtype=int32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.6335532260715137
{'currentState': array([ 5.99897817, 18.00007844,  2.8686066 ]), 'targetState': array([ 6, 18], dtype=int32)}
episode index:934
model initialize at round 934
at step 0:
{'currentState': array([12.95522213, 23.28414999,  2.23209631]), 'targetState': array([ 8, 20], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6337416481486791
{'currentState': array([ 8.0217223 , 20.04794633,  4.409796  ]), 'targetState': array([ 8, 20], dtype=int32)}
episode index:935
model initialize at round 935
at step 0:
{'currentState': array([17.94572566,  9.71751005,  5.02757359]), 'targetState': array([22, 10], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6339296676145686
{'currentState': array([21.98076342, 10.0033568 ,  2.30177847]), 'targetState': array([22, 10], dtype=int32)}
episode index:936
model initialize at round 936
at step 0:
{'currentState': array([18.92843517,  1.72138781,  4.96596265]), 'targetState': array([23,  4], dtype=int32)}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6341527343443117
{'currentState': array([23.0459359 ,  4.01341047,  0.87077921]), 'targetState': array([23,  4], dtype=int32)}
episode index:937
model initialize at round 937
at step 0:
{'currentState': array([ 2.76592658, 16.16720021,  2.01632929]), 'targetState': array([18, 20], dtype=int32)}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6340778435725719
{'currentState': array([17.92510395, 19.99033572,  0.18166262]), 'targetState': array([18, 20], dtype=int32)}
episode index:938
model initialize at round 938
at step 0:
{'currentState': array([14.73036816, 12.10022448,  2.2807107 ]), 'targetState': array([27, 26], dtype=int32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.6339567175651999
{'currentState': array([27.03249668, 25.95055181,  1.47786136]), 'targetState': array([27, 26], dtype=int32)}
episode index:939
model initialize at round 939
at step 0:
{'currentState': array([12.27995805, 18.06610419,  0.73687476]), 'targetState': array([ 8, 22], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6341437081510042
{'currentState': array([ 7.96883173, 22.07333036,  2.26465022]), 'targetState': array([ 8, 22], dtype=int32)}
episode index:940
model initialize at round 940
at step 0:
{'currentState': array([ 7.96924933, 10.28600817,  1.17290163]), 'targetState': array([14, 21], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6341596049843914
{'currentState': array([13.99439202, 21.06811871,  1.61766487]), 'targetState': array([14, 21], dtype=int32)}
episode index:941
model initialize at round 941
at step 0:
{'currentState': array([ 6.24429543, 13.15187505,  1.06121217]), 'targetState': array([14, 15], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6342875876619065
{'currentState': array([14.00480554, 15.00644268,  0.81363707]), 'targetState': array([14, 15], dtype=int32)}
episode index:942
model initialize at round 942
at step 0:
{'currentState': array([26.98278093, 17.71285931,  4.14749336]), 'targetState': array([17, 16], dtype=int32)}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6343534685000695
{'currentState': array([16.98808347, 16.02305583,  3.45551477]), 'targetState': array([17, 16], dtype=int32)}
episode index:943
model initialize at round 943
at step 0:
{'currentState': array([15.85354378, 10.75241232,  4.68311745]), 'targetState': array([4, 2], dtype=int32)}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6342610989808859
{'currentState': array([3.97579684, 2.0028749 , 5.29846996]), 'targetState': array([4, 2], dtype=int32)}
episode index:944
model initialize at round 944
at step 0:
{'currentState': array([18.20628875, 26.2004775 ,  0.26611269]), 'targetState': array([21, 15], dtype=int32)}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6342907507713653
{'currentState': array([20.99617985, 15.02396767,  5.19335133]), 'targetState': array([21, 15], dtype=int32)}
episode index:945
model initialize at round 945
at step 0:
{'currentState': array([20.91631031,  6.72478679,  3.91218376]), 'targetState': array([ 6, 17], dtype=int32)}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.6341758519730083
{'currentState': array([ 5.95294315, 17.05698038,  3.00147181]), 'targetState': array([ 6, 17], dtype=int32)}
episode index:946
model initialize at round 946
at step 0:
{'currentState': array([17.11916047,  2.26181493,  1.64868248]), 'targetState': array([26, 12], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6341646054502187
{'currentState': array([26.05985174, 12.05211792,  1.19090945]), 'targetState': array([26, 12], dtype=int32)}
episode index:947
model initialize at round 947
at step 0:
{'currentState': array([15.77701337,  3.18172297,  2.96280205]), 'targetState': array([ 3, 10], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6341274655778727
{'currentState': array([ 2.93909603, 10.0720208 ,  2.9581707 ]), 'targetState': array([ 3, 10], dtype=int32)}
episode index:948
model initialize at round 948
at step 0:
{'currentState': array([19.99254306,  2.71244015,  5.19146299]), 'targetState': array([24,  9], dtype=int32)}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6342079985195949
{'currentState': array([23.97069892,  9.02951411,  3.14172247]), 'targetState': array([24,  9], dtype=int32)}
episode index:949
model initialize at round 949
at step 0:
{'currentState': array([11.8105283 , 13.7835589 ,  3.48833466]), 'targetState': array([ 7, 11], dtype=int32)}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.634410062335717
{'currentState': array([ 7.00033125, 11.02757153,  4.05146018]), 'targetState': array([ 7, 11], dtype=int32)}
episode index:950
model initialize at round 950
at step 0:
{'currentState': array([ 6.72536688, 14.08557409,  3.34453321]), 'targetState': array([10, 10], dtype=int32)}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6345526838745913
{'currentState': array([10.04692491,  9.91629903,  0.92647321]), 'targetState': array([10, 10], dtype=int32)}
episode index:951
model initialize at round 951
at step 0:
{'currentState': array([15.75023311,  8.8573018 ,  4.1656611 ]), 'targetState': array([14,  5], dtype=int32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6347895595791782
{'currentState': array([13.93568776,  4.94656558,  4.25428714]), 'targetState': array([14,  5], dtype=int32)}
episode index:952
model initialize at round 952
at step 0:
{'currentState': array([22.83125403, 18.23296152,  1.69267917]), 'targetState': array([24, 25], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6349478896749053
{'currentState': array([24.05217342, 24.99870072,  1.566729  ]), 'targetState': array([24, 25], dtype=int32)}
episode index:953
model initialize at round 953
at step 0:
{'currentState': array([ 5.14450708, 23.7512753 ,  5.74371481]), 'targetState': array([17, 19], dtype=int32)}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6349491869457533
{'currentState': array([16.92888469, 18.99074344,  5.89106018]), 'targetState': array([17, 19], dtype=int32)}
episode index:954
model initialize at round 954
at step 0:
{'currentState': array([14.0508896 , 11.71688073,  5.39523602]), 'targetState': array([24,  2], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.634917832893598
{'currentState': array([24.02800452,  1.94384203,  5.72024935]), 'targetState': array([24,  2], dtype=int32)}
episode index:955
model initialize at round 955
at step 0:
{'currentState': array([6.7662167 , 8.16760562, 2.01459622]), 'targetState': array([ 3, 19], dtype=int32)}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6349534541547656
{'currentState': array([ 3.01753488, 19.03798873,  1.76949009]), 'targetState': array([ 3, 19], dtype=int32)}
episode index:956
model initialize at round 956
at step 0:
{'currentState': array([19.84853222, 14.75545187,  3.65284252]), 'targetState': array([7, 9], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6349221611693766
{'currentState': array([6.93976917, 9.01804083, 3.66449965]), 'targetState': array([7, 9], dtype=int32)}
episode index:957
model initialize at round 957
at step 0:
{'currentState': array([ 9.83422746, 23.76491334,  3.59321165]), 'targetState': array([ 6, 24], dtype=int32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6351131682428921
{'currentState': array([ 6.01334169, 23.96699152,  5.50381866]), 'targetState': array([ 6, 24], dtype=int32)}
episode index:958
model initialize at round 958
at step 0:
{'currentState': array([ 4.07937255, 12.27648919,  0.78624105]), 'targetState': array([10, 14], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6352952482220144
{'currentState': array([ 9.90090155, 13.90102691,  0.74494749]), 'targetState': array([10, 14], dtype=int32)}
episode index:959
model initialize at round 959
at step 0:
{'currentState': array([ 2.25009821, 24.14211671,  1.02174186]), 'targetState': array([11, 27], dtype=int32)}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6353373667645543
{'currentState': array([11.08339225, 26.91983118,  1.40911743]), 'targetState': array([11, 27], dtype=int32)}
episode index:960
model initialize at round 960
at step 0:
{'currentState': array([20.02439366, 20.28662035,  0.98089302]), 'targetState': array([24, 18], dtype=int32)}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.635502066990235
{'currentState': array([24.03043088, 18.04529092,  1.42752356]), 'targetState': array([24, 18], dtype=int32)}
episode index:961
model initialize at round 961
at step 0:
{'currentState': array([15.85900727, 26.74926643,  4.70513558]), 'targetState': array([19, 22], dtype=int32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6356916770428411
{'currentState': array([19.02194683, 21.90597244,  5.07129962]), 'targetState': array([19, 22], dtype=int32)}
episode index:962
model initialize at round 962
at step 0:
{'currentState': array([23.81448861, 26.78015506,  3.5064888 ]), 'targetState': array([10, 25], dtype=int32)}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6356661584161433
{'currentState': array([10.0890056 , 25.07031728,  3.46896221]), 'targetState': array([10, 25], dtype=int32)}
episode index:963
model initialize at round 963
at step 0:
{'currentState': array([20.11615287, 21.73683696,  4.62304497]), 'targetState': array([ 7, 26], dtype=int32)}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6356343533422029
{'currentState': array([ 7.05888763, 25.98824815,  2.04639295]), 'targetState': array([ 7, 26], dtype=int32)}
episode index:964
model initialize at round 964
at step 0:
{'currentState': array([12.28601899, 19.03064981,  0.61175268]), 'targetState': array([12, 20], dtype=int32)}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6359711011729363
{'currentState': array([12.03652662, 20.07444771,  2.6058079 ]), 'targetState': array([12, 20], dtype=int32)}
episode index:965
model initialize at round 965
at step 0:
{'currentState': array([7.87137429, 7.74270309, 3.74381208]), 'targetState': array([7, 9], dtype=int32)}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6362873631276237
{'currentState': array([7.02056779, 9.03914219, 1.90839839]), 'targetState': array([7, 9], dtype=int32)}
episode index:966
model initialize at round 966
at step 0:
{'currentState': array([12.27751528, 25.07570697,  6.04450775]), 'targetState': array([14, 21], dtype=int32)}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6364583488839954
{'currentState': array([13.95113669, 20.92305059,  0.24864723]), 'targetState': array([14, 21], dtype=int32)}
episode index:967
model initialize at round 967
at step 0:
{'currentState': array([19.29204326,  2.94109393,  5.87816583]), 'targetState': array([23,  2], dtype=int32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6367073805721307
{'currentState': array([22.99965064,  1.97459865,  6.03314185]), 'targetState': array([23,  2], dtype=int32)}
episode index:968
model initialize at round 968
at step 0:
{'currentState': array([ 6.98550069, 22.28729087,  1.11622262]), 'targetState': array([13, 23], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6368859362869388
{'currentState': array([12.97048744, 22.94962766,  0.69041098]), 'targetState': array([13, 23], dtype=int32)}
episode index:969
model initialize at round 969
at step 0:
{'currentState': array([ 1.75655284, 19.84676895,  3.19836569]), 'targetState': array([ 2, 22], dtype=int32)}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6371160109450361
{'currentState': array([ 2.00101353, 22.04088873,  3.41616251]), 'targetState': array([ 2, 22], dtype=int32)}
episode index:970
model initialize at round 970
at step 0:
{'currentState': array([15.05617531,  8.28211808,  1.87924719]), 'targetState': array([17, 16], dtype=int32)}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6372371265745507
{'currentState': array([16.92016392, 16.069073  ,  2.51436257]), 'targetState': array([17, 16], dtype=int32)}
episode index:971
model initialize at round 971
at step 0:
{'currentState': array([ 9.28547033, 15.96460276,  0.38163346]), 'targetState': array([24, 20], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6371854801294509
{'currentState': array([23.94070716, 19.97937184,  0.29081623]), 'targetState': array([24, 20], dtype=int32)}
episode index:972
model initialize at round 972
at step 0:
{'currentState': array([14.75283515, 13.85284087,  3.1736145 ]), 'targetState': array([21, 28], dtype=int32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6370817624984927
{'currentState': array([20.98517619, 27.98596963,  3.38628117]), 'targetState': array([21, 28], dtype=int32)}
episode index:973
model initialize at round 973
at step 0:
{'currentState': array([9.21739946, 8.18837131, 0.20898151]), 'targetState': array([10,  5], dtype=int32)}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.63733772051617
{'currentState': array([9.99186717, 4.93188716, 4.89439322]), 'targetState': array([10,  5], dtype=int32)}
episode index:974
model initialize at round 974
at step 0:
{'currentState': array([11.02293968,  9.28674038,  0.98596478]), 'targetState': array([22, 24], dtype=int32)}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.6372177233901536
{'currentState': array([21.98185818, 24.09267976,  1.75980027]), 'targetState': array([22, 24], dtype=int32)}
episode index:975
model initialize at round 975
at step 0:
{'currentState': array([25.75073567, 28.83555291,  3.89252731]), 'targetState': array([18, 25], dtype=int32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6373227261053156
{'currentState': array([18.00174217, 25.01815215,  4.14156149]), 'targetState': array([18, 25], dtype=int32)}
episode index:976
model initialize at round 976
at step 0:
{'currentState': array([19.97459094,  9.28653212,  2.16424286]), 'targetState': array([10, 20], dtype=int32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.6372360943254355
{'currentState': array([ 9.99534389, 19.97035086,  4.02228829]), 'targetState': array([10, 20], dtype=int32)}
episode index:977
model initialize at round 977
at step 0:
{'currentState': array([11.87220353, 13.74229021,  4.75703239]), 'targetState': array([27, 14], dtype=int32)}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6371847657851616
{'currentState': array([27.00391663, 13.97467871,  0.28718668]), 'targetState': array([27, 14], dtype=int32)}
episode index:978
model initialize at round 978
at step 0:
{'currentState': array([19.90492653, 15.7285091 ,  4.88054466]), 'targetState': array([22,  3], dtype=int32)}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.637139598962502
{'currentState': array([22.00809535,  2.99749593,  5.35654444]), 'targetState': array([22,  3], dtype=int32)}
episode index:979
model initialize at round 979
at step 0:
{'currentState': array([25.73774981, 13.88180053,  3.06003904]), 'targetState': array([19, 14], dtype=int32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6372442528139569
{'currentState': array([18.99795957, 13.98278064,  5.18639692]), 'targetState': array([19, 14], dtype=int32)}
episode index:980
model initialize at round 980
at step 0:
{'currentState': array([26.27603595,  9.08093473,  0.79020995]), 'targetState': array([24, 12], dtype=int32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6374891832626675
{'currentState': array([24.01144238, 12.00614382,  2.57479818]), 'targetState': array([24, 12], dtype=int32)}
episode index:981
model initialize at round 981
at step 0:
{'currentState': array([18.23560491,  8.1772228 ,  0.31889123]), 'targetState': array([27, 17], dtype=int32)}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6375010095815123
{'currentState': array([26.99154878, 16.97619953,  1.22149213]), 'targetState': array([27, 17], dtype=int32)}
episode index:982
model initialize at round 982
at step 0:
{'currentState': array([12.72294805,  7.92261466,  2.90896797]), 'targetState': array([20, 18], dtype=int32)}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6375062085565919
{'currentState': array([19.98249166, 17.9769157 ,  1.5262883 ]), 'targetState': array([20, 18], dtype=int32)}
episode index:983
model initialize at round 983
at step 0:
{'currentState': array([7.28750517, 5.99066981, 0.47255915]), 'targetState': array([ 4, 13], dtype=int32)}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6376330749242651
{'currentState': array([ 4.04869465, 12.94832898,  2.10682314]), 'targetState': array([ 4, 13], dtype=int32)}
episode index:984
model initialize at round 984
at step 0:
{'currentState': array([24.08509895,  6.72521928,  5.51771879]), 'targetState': array([27,  4], dtype=int32)}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.637816094074187
{'currentState': array([27.0093538 ,  3.99977644,  1.12386446]), 'targetState': array([27,  4], dtype=int32)}
episode index:985
model initialize at round 985
at step 0:
{'currentState': array([14.9512026 , 19.28348737,  2.24625838]), 'targetState': array([3, 4], dtype=int32)}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.637681276421766
{'currentState': array([2.99107064, 4.07625683, 4.3766441 ]), 'targetState': array([3, 4], dtype=int32)}
episode index:986
model initialize at round 986
at step 0:
{'currentState': array([ 2.12424267, 22.73815062,  4.6975323 ]), 'targetState': array([12, 12], dtype=int32)}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6376420410925304
{'currentState': array([12.0732686 , 11.98663299,  6.06410703]), 'targetState': array([12, 12], dtype=int32)}
episode index:987
model initialize at round 987
at step 0:
{'currentState': array([12.82668768, 26.77041534,  3.56076241]), 'targetState': array([ 7, 25], dtype=int32)}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6378162170309197
{'currentState': array([ 6.94567889, 25.01554688,  3.32006462]), 'targetState': array([ 7, 25], dtype=int32)}
episode index:988
model initialize at round 988
at step 0:
{'currentState': array([27.07132925,  4.27867259,  1.82521546]), 'targetState': array([16, 12], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6378017672613143
{'currentState': array([16.02312112, 11.99259226,  3.21236865]), 'targetState': array([16, 12], dtype=int32)}
episode index:989
model initialize at round 989
at step 0:
{'currentState': array([11.8763549 , 26.7402729 ,  4.77307796]), 'targetState': array([25, 29], dtype=int32)}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6377873466831627
{'currentState': array([24.97929329, 28.97450121,  0.20755529]), 'targetState': array([25, 29], dtype=int32)}
episode index:990
model initialize at round 990
at step 0:
{'currentState': array([19.97942137,  9.71308051,  4.13578892]), 'targetState': array([11,  7], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.637868015891957
{'currentState': array([10.97887887,  7.00251836,  3.49047262]), 'targetState': array([11,  7], dtype=int32)}
episode index:991
model initialize at round 991
at step 0:
{'currentState': array([13.19028385, 10.78427255,  5.94020176]), 'targetState': array([25, 18], dtype=int32)}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6378472720664028
{'currentState': array([24.98564929, 17.95086333,  0.9847486 ]), 'targetState': array([25, 18], dtype=int32)}
episode index:992
model initialize at round 992
at step 0:
{'currentState': array([12.1406999 , 21.74910199,  4.71847486]), 'targetState': array([13, 13], dtype=int32)}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6378992376392152
{'currentState': array([13.02233908, 12.99297064,  0.57279125]), 'targetState': array([13, 13], dtype=int32)}
episode index:993
model initialize at round 993
at step 0:
{'currentState': array([22.84735018, 18.75618797,  3.64800167]), 'targetState': array([20, 25], dtype=int32)}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6380479085679557
{'currentState': array([20.08225169, 24.91578647,  2.10245825]), 'targetState': array([20, 25], dtype=int32)}
episode index:994
model initialize at round 994
at step 0:
{'currentState': array([19.29804088, 10.03419745,  0.10565983]), 'targetState': array([23, 11], dtype=int32)}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6382885850648713
{'currentState': array([23.02639267, 10.96818247,  0.59610534]), 'targetState': array([23, 11], dtype=int32)}
episode index:995
model initialize at round 995
at step 0:
{'currentState': array([17.76352337,  9.16378363,  3.04085064]), 'targetState': array([ 8, 21], dtype=int32)}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6382253984834576
{'currentState': array([ 7.90027059, 21.05356924,  3.13813469]), 'targetState': array([ 8, 21], dtype=int32)}
episode index:996
model initialize at round 996
at step 0:
{'currentState': array([13.1613693 , 15.76186932,  4.80295706]), 'targetState': array([14,  9], dtype=int32)}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.63832717879931
{'currentState': array([14.02052252,  8.99384273,  0.61063011]), 'targetState': array([14,  9], dtype=int32)}
episode index:997
model initialize at round 997
at step 0:
{'currentState': array([25.85464368, 12.24822936,  2.60553992]), 'targetState': array([16, 27], dtype=int32)}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6382249153187568
{'currentState': array([15.9600919 , 26.99952192,  3.23718718]), 'targetState': array([16, 27], dtype=int32)}
episode index:998
model initialize at round 998
at step 0:
{'currentState': array([19.13440284,  3.74567314,  5.70354843]), 'targetState': array([27,  9], dtype=int32)}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6383045005212388
{'currentState': array([26.9789754 ,  8.98933646,  0.18789923]), 'targetState': array([27,  9], dtype=int32)}
episode index:999
model initialize at round 999
at step 0:
{'currentState': array([18.81218272, 17.78212172,  4.50595307]), 'targetState': array([21,  3], dtype=int32)}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.6382188794978799
{'currentState': array([20.93745276,  2.98972214,  6.07304886]), 'targetState': array([21,  3], dtype=int32)}

Process finished with exit code 0
