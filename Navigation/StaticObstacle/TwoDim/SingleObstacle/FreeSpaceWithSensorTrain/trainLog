/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/StaticObstacle/TwoDim/SingleObstacle/DDPGHER_CNN.py
episode index:0
target Thresh 3.1999999999999993
target distance 3.0
model initialize at round 0
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.        ,  7.81529838,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 1.0169142977639565}
done in step count: 1
reward sum = 0.9860108873606311
running average episode reward sum: 0.9860108873606311
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.81284195,  7.81469363,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.26337537061518107}
episode index:1
target Thresh 3.2575424383808063
target distance 2.0
model initialize at round 1
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([27.8792648 , 20.20996697,  0.        ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 4.759676060183482}
done in step count: 99
reward sum = -0.24238859504857524
running average episode reward sum: 0.37181114615602795
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([27.87007127, 29.87007127,  0.        ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 6.534144618475146}
episode index:2
target Thresh 3.314969906893044
target distance 3.0
model initialize at round 2
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([26., 17.,  0.]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 7.071067811865448}
done in step count: 99
reward sum = -0.25900947175211503
running average episode reward sum: 0.16153760685331361
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.19406733, 29.87723177,  0.        ]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 17.895388876878847}
episode index:3
target Thresh 3.3722826352466626
target distance 2.0
model initialize at round 3
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([ 5.71977664, 29.8837946 ,  0.        ]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 3.7826640604309514}
done in step count: 99
reward sum = -0.2956300125409991
running average episode reward sum: 0.047245702004735435
{'scaleFactor': 20, 'currentTarget': array([ 9., 28.]), 'previousTarget': array([ 9., 28.]), 'currentState': array([27.85124769, 29.87205105,  0.        ]), 'targetState': array([ 9, 28], dtype=int32), 'currentDistance': 18.943973041304044}
episode index:4
target Thresh 3.4294808526926523
target distance 3.0
model initialize at round 4
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 1.58496344, 24.        ,  0.        ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 6.054954558047729}
done in step count: 99
reward sum = -0.2289444455913456
running average episode reward sum: -0.007992327514480774
{'scaleFactor': 20, 'currentTarget': array([ 8.21697755, 19.60292746]), 'previousTarget': array([ 8.1452436 , 19.50597929]), 'currentState': array([27.87470439, 23.28718795,  0.        ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 20.0}
episode index:5
target Thresh 3.4865647880239585
target distance 3.0
model initialize at round 5
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([27.64894349, 12.60272906,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 5.127810961446443}
done in step count: 99
reward sum = -0.21718330441621256
running average episode reward sum: -0.04285749033143607
{'scaleFactor': 20, 'currentTarget': array([16.42034397, 15.37755275]), 'previousTarget': array([16.18289039, 15.84130313]), 'currentState': array([ 1.11683351, 28.25398734,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 20.0}
episode index:6
target Thresh 3.543534669576399
target distance 2.0
model initialize at round 6
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([24.        , 25.75339913,  0.        ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 0.7533991336820804}
done in step count: 0
reward sum = 0.9958344900456747
running average episode reward sum: 0.10552707829386547
{'scaleFactor': 20, 'currentTarget': array([24., 25.]), 'previousTarget': array([24., 25.]), 'currentState': array([24.        , 25.75339913,  0.        ]), 'targetState': array([24, 25], dtype=int32), 'currentDistance': 0.7533991336820804}
episode index:7
target Thresh 3.600390725229577
target distance 2.0
model initialize at round 7
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([11.7228837,  2.8489002,  0.       ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.430203733321191}
done in step count: 99
reward sum = -0.2769242281778312
running average episode reward sum: 0.05772066498490339
{'scaleFactor': 20, 'currentTarget': array([ 9.51483105, 11.71260964]), 'previousTarget': array([ 9.51761503, 11.70052942]), 'currentState': array([ 1.12992873, 29.87007127,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 20.0}
episode index:8
target Thresh 3.6571331824077866
target distance 3.0
model initialize at round 8
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([20.        ,  9.75287545,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 5.298355625442008}
done in step count: 99
reward sum = -0.25277228648201044
running average episode reward sum: 0.0232214481552463
{'scaleFactor': 20, 'currentTarget': array([11.98840942, 12.88666871]), 'previousTarget': array([11.8537448, 12.9722994]), 'currentState': array([ 1.49531867, 29.91297326,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:9
target Thresh 3.713762268080938
target distance 2.0
model initialize at round 9
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([27.86775028,  8.97380116,  0.        ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 4.900519086032625}
done in step count: 99
reward sum = -0.24586355587819672
running average episode reward sum: -0.0036870522480980023
{'scaleFactor': 20, 'currentTarget': array([14.97904359, 15.44065846]), 'previousTarget': array([14.98209632, 15.43083485]), 'currentState': array([ 1.12989995, 29.86983794,  0.        ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 20.0}
episode index:10
target Thresh 3.770278208765447
target distance 1.0
model initialize at round 10
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 1.12992873, 29.87007127,  0.        ]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 2.999055375773238}
done in step count: 35
reward sum = 0.5711158384832152
running average episode reward sum: 0.0485677560002032
{'scaleFactor': 20, 'currentTarget': array([ 4., 29.]), 'previousTarget': array([ 4., 29.]), 'currentState': array([ 3.7045743 , 29.87695723,  0.        ]), 'targetState': array([ 4, 29], dtype=int32), 'currentDistance': 0.9253811782920511}
episode index:11
target Thresh 3.82668123052515
target distance 3.0
model initialize at round 11
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 7.27098167, 24.        ,  0.        ]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 1.2375248386650668}
done in step count: 99
reward sum = -0.2734637423338977
running average episode reward sum: 0.021731797805694796
{'scaleFactor': 20, 'currentTarget': array([ 8., 25.]), 'previousTarget': array([ 8., 25.]), 'currentState': array([ 1.12992873, 29.87007127,  0.        ]), 'targetState': array([ 8, 25], dtype=int32), 'currentDistance': 8.421132557116184}
episode index:12
target Thresh 3.882971558972212
target distance 3.0
model initialize at round 12
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([18.       , 13.7375654,  0.       ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 2.0047776717328}
done in step count: 99
reward sum = -0.23692481575665011
running average episode reward sum: 0.001835135223975956
{'scaleFactor': 20, 'currentTarget': array([14.41465213, 14.91291025]), 'previousTarget': array([14.41078937, 14.91565306]), 'currentState': array([ 1.13858287, 29.87105134,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 20.0}
episode index:13
target Thresh 3.9391494192680163
target distance 3.0
model initialize at round 13
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 11.]), 'previousTarget': array([22., 11.]), 'currentState': array([21., 10.,  0.]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 1.4142135623730938}
done in step count: 99
reward sum = -0.24006233309370476
running average episode reward sum: -0.015443255370144095
{'scaleFactor': 20, 'currentTarget': array([15.96801445, 16.4505071 ]), 'previousTarget': array([15.96514492, 16.45637961]), 'currentState': array([ 1.12873004, 29.85929669,  0.        ]), 'targetState': array([22, 11], dtype=int32), 'currentDistance': 20.0}
episode index:14
target Thresh 3.9952150361240903
target distance 3.0
model initialize at round 14
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([ 9.        , 27.21812272,  0.        ]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 5.946117543877146}
done in step count: 99
reward sum = -0.2894775164301455
running average episode reward sum: -0.03371220610747752
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([ 1.12992873, 29.87007127,  0.        ]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 14.145545991334135}
episode index:15
target Thresh 4.051168633802966
target distance 1.0
model initialize at round 15
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'previousTarget': array([26.,  5.]), 'currentState': array([25.01947057,  7.64382517,  0.        ]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 2.819796004259212}
done in step count: 99
reward sum = -0.2231763849570707
running average episode reward sum: -0.0455537172855771
{'scaleFactor': 20, 'currentTarget': array([15.54530435, 15.06489808]), 'previousTarget': array([15.76230537, 15.52304549]), 'currentState': array([ 1.13711909, 28.9358821 ,  0.        ]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 20.0}
episode index:16
target Thresh 4.107010436119108
target distance 2.0
model initialize at round 16
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 1.19191546, 19.3130208 ,  0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 2.89089591494014}
done in step count: 45
reward sum = 0.5130296443237164
running average episode reward sum: -0.012695872485030417
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 3.79160631, 20.52514584,  0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.5649832586175509}
episode index:17
target Thresh 4.1627406664398094
target distance 4.0
model initialize at round 17
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([ 8.03888059, 26.53099859,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 3.531212642380416}
done in step count: 99
reward sum = -0.28407272196036637
running average episode reward sum: -0.027772364122549083
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'previousTarget': array([ 8., 23.]), 'currentState': array([ 1.134104  , 29.86627963,  0.        ]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 9.71011451201708}
episode index:18
target Thresh 4.218359547686056
target distance 1.0
model initialize at round 18
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([16.41229475, 28.        ,  0.        ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 2.553587275841642}
done in step count: 99
reward sum = -0.2770820732052441
running average episode reward sum: -0.0408939277584804
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([ 3.10420903, 29.86762747,  0.        ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 15.389708604066863}
episode index:19
target Thresh 4.2738673023334535
target distance 2.0
model initialize at round 19
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([23.40008283, 29.        ,  0.        ]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 0.5999171733856912}
done in step count: 0
reward sum = 0.9955298262631231
running average episode reward sum: 0.010927259942599776
{'scaleFactor': 20, 'currentTarget': array([24., 29.]), 'previousTarget': array([24., 29.]), 'currentState': array([23.40008283, 29.        ,  0.        ]), 'targetState': array([24, 29], dtype=int32), 'currentDistance': 0.5999171733856912}
episode index:20
target Thresh 4.329264152413092
target distance 4.0
model initialize at round 20
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 4.61481833, 23.        ,  0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 6.456709036115679}
done in step count: 99
reward sum = -0.2681808856052216
running average episode reward sum: -0.0023636041311060024
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 1.12910305, 29.86287478,  0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 14.13934151871024}
episode index:21
target Thresh 4.384550319512446
target distance 4.0
model initialize at round 21
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'previousTarget': array([15., 13.]), 'currentState': array([10.        , 18.78614402,  0.        ]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 7.647186580634946}
done in step count: 99
reward sum = -0.2861389928745624
running average episode reward sum: -0.015262485437626748
{'scaleFactor': 20, 'currentTarget': array([13.83527839, 14.41575099]), 'previousTarget': array([13.83848798, 14.4111613 ]), 'currentState': array([ 1.12887704, 29.86072962,  0.        ]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:22
target Thresh 4.439726024776256
target distance 3.0
model initialize at round 22
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([12.        , 11.87223971,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 5.2667560965916795}
done in step count: 99
reward sum = -0.21340069388601915
running average episode reward sum: -0.023877190152774244
{'scaleFactor': 20, 'currentTarget': array([11.84636304, 10.39178732]), 'previousTarget': array([11.48631423, 11.34458677]), 'currentState': array([ 1.1257703, 27.2757706,  0.       ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 20.0}
episode index:23
target Thresh 4.494791488907417
target distance 3.0
model initialize at round 23
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([14.46173286, 18.35623491,  0.        ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 3.7179251673867144}
done in step count: 99
reward sum = -0.22815771702896115
running average episode reward sum: -0.03238887877261536
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'previousTarget': array([11., 17.]), 'currentState': array([ 1.17852255, 29.87470648,  0.        ]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 16.193192592704943}
episode index:24
target Thresh 4.549746932167864
target distance 3.0
model initialize at round 24
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 26.]), 'previousTarget': array([24., 26.]), 'currentState': array([25.88762331, 26.52949631,  0.        ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 1.9604816003634264}
done in step count: 99
reward sum = -0.2865928618234991
running average episode reward sum: -0.042557038094650716
{'scaleFactor': 20, 'currentTarget': array([21.17059716, 26.4912776 ]), 'previousTarget': array([23.07565046, 26.17557723]), 'currentState': array([ 1.46543115, 29.91274399,  0.        ]), 'targetState': array([24, 26], dtype=int32), 'currentDistance': 20.0}
episode index:25
target Thresh 4.604592574379435
target distance 4.0
model initialize at round 25
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([20.00147188,  9.62710226,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 5.627102448320194}
done in step count: 99
reward sum = -0.22453407757651916
running average episode reward sum: -0.0495561549977995
{'scaleFactor': 20, 'currentTarget': array([12.91601317, 13.71184695]), 'previousTarget': array([12.91601317, 13.71184695]), 'currentState': array([ 1.12992873, 29.87007127,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 20.0}
episode index:26
target Thresh 4.65932863492478
target distance 4.0
model initialize at round 26
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([18., 11.,  0.]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 6.082762530298205}
done in step count: 99
reward sum = -0.28244319907129267
running average episode reward sum: -0.058181601074595544
{'scaleFactor': 20, 'currentTarget': array([19.0974374 , 16.44805389]), 'previousTarget': array([19.42736685, 16.25432978]), 'currentState': array([ 4.28537772, 29.88691129,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 20.0}
episode index:27
target Thresh 4.713955332748206
target distance 3.0
model initialize at round 27
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 2.0741353, 27.0809387,  0.       ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 4.5125398033452395}
done in step count: 99
reward sum = -0.2640444880121225
running average episode reward sum: -0.06553384703665008
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'previousTarget': array([ 4., 23.]), 'currentState': array([ 5.60438961, 29.89902749,  0.        ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 7.08312404797185}
episode index:28
target Thresh 4.768472886356587
target distance 2.0
model initialize at round 28
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 9.87869298, 27.        ,  0.        ]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 4.001839001258011}
done in step count: 88
reward sum = 0.178702964162857
running average episode reward sum: -0.05711188802977052
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'previousTarget': array([10., 23.]), 'currentState': array([ 9.83649839, 22.343167  ,  0.        ]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.6768769236946771}
episode index:29
target Thresh 4.822881513820203
target distance 2.0
model initialize at round 29
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([22.95837331, 23.58106446,  0.        ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 2.1227047506690986}
done in step count: 8
reward sum = 0.9107865383182591
running average episode reward sum: -0.024848607151502868
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'previousTarget': array([25., 23.]), 'currentState': array([25.47159757, 22.67874316,  0.        ]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 0.5706226630502901}
episode index:30
target Thresh 4.877181432773636
target distance 3.0
model initialize at round 30
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 7.42684424, 19.69852626,  0.        ]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 4.019193397621187}
done in step count: 99
reward sum = -0.22787224580635027
running average episode reward sum: -0.0313977567855302
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'previousTarget': array([ 9., 16.]), 'currentState': array([ 7.25342917, 27.50877172,  0.        ]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 11.640547077622273}
episode index:31
target Thresh 4.931372860416641
target distance 3.0
model initialize at round 31
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([15.44125056, 25.18364823,  0.        ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 3.841776008798138}
done in step count: 68
reward sum = 0.3688239232153839
running average episode reward sum: -0.018890829285501634
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([15.76850472, 29.86696626,  0.        ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 1.1585465027355437}
episode index:32
target Thresh 4.985456013514991
target distance 4.0
model initialize at round 32
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 5.0288111, 21.       ,  0.       ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 4.446385194982375}
done in step count: 50
reward sum = 0.47535811600037414
running average episode reward sum: -0.003913588519262975
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.53961255, 23.05205329,  0.        ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.5421173792555054}
episode index:33
target Thresh 5.039431108401377
target distance 4.0
model initialize at round 33
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([ 8.23760462, 17.23264599,  0.        ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 5.762789943618365}
done in step count: 99
reward sum = -0.21510693020440627
running average episode reward sum: -0.010125157392355425
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'previousTarget': array([ 5., 22.]), 'currentState': array([18.10710631, 25.73546656,  0.        ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 13.629011200168978}
episode index:34
target Thresh 5.093298360976249
target distance 4.0
model initialize at round 34
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([14.        , 17.99552047,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 5.0044795274733325}
done in step count: 99
reward sum = -0.1830233977054881
running average episode reward sum: -0.015065107115587789
{'scaleFactor': 20, 'currentTarget': array([14., 23.]), 'previousTarget': array([14., 23.]), 'currentState': array([16.46957745, 25.12904331,  0.        ]), 'targetState': array([14, 23], dtype=int32), 'currentDistance': 3.2606192897711925}
episode index:35
target Thresh 5.147057986708688
target distance 4.0
model initialize at round 35
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'previousTarget': array([26., 13.]), 'currentState': array([25.3695243 , 17.65566444,  0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 4.698160387109897}
done in step count: 99
reward sum = -0.2249878806587789
running average episode reward sum: -0.020896295269565317
{'scaleFactor': 20, 'currentTarget': array([19.50772389,  9.84016431]), 'previousTarget': array([19.19149489,  9.74810408]), 'currentState': array([1.52458179, 1.08764466, 0.        ]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 20.0}
episode index:36
target Thresh 5.200710200637275
target distance 4.0
model initialize at round 36
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 16.]), 'previousTarget': array([18., 16.]), 'currentState': array([21.0401617, 18.       ,  0.       ]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 3.6390360171012426}
done in step count: 99
reward sum = -0.17668156038669258
running average episode reward sum: -0.025106707840298486
{'scaleFactor': 20, 'currentTarget': array([16.15188682, 14.36969719]), 'previousTarget': array([16.151188  , 14.36726371]), 'currentState': array([1.15357857, 1.13902261, 0.        ]), 'targetState': array([18, 16], dtype=int32), 'currentDistance': 20.0}
episode index:37
target Thresh 5.254255217370929
target distance 5.0
model initialize at round 37
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'previousTarget': array([21., 22.]), 'currentState': array([15.10295284, 18.26147115,  0.        ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 6.9822462844384745}
done in step count: 99
reward sum = -0.15928900645267677
running average episode reward sum: -0.02863782096167686
{'scaleFactor': 20, 'currentTarget': array([19.85345412, 19.41545759]), 'previousTarget': array([19.56674294, 19.08791655]), 'currentState': array([11.74332506,  1.13361396,  0.        ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 20.0}
episode index:38
target Thresh 5.307693251089791
target distance 3.0
model initialize at round 38
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.,  2.,  0.]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.0000000000000195}
done in step count: 1
reward sum = 0.9795736006916054
running average episode reward sum: -0.002786246047490138
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.48442369,  1.08889328,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.031882633550112}
episode index:39
target Thresh 5.361024515546067
target distance 4.0
model initialize at round 39
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 22.]), 'previousTarget': array([ 7., 22.]), 'currentState': array([10., 24.,  0.]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 3.605551275463983}
done in step count: 99
reward sum = -0.24810917876763716
running average episode reward sum: -0.008919319365493814
{'scaleFactor': 20, 'currentTarget': array([ 6.54604594, 20.38387812]), 'previousTarget': array([ 6.54633104, 20.38421778]), 'currentState': array([1.13754084, 1.12905866, 0.        ]), 'targetState': array([ 7, 22], dtype=int32), 'currentDistance': 20.0}
episode index:40
target Thresh 5.414249224064889
target distance 4.0
model initialize at round 40
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.59277761,  7.        ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.0410365197761284}
done in step count: 1
reward sum = 0.9821152170867472
running average episode reward sum: 0.015252254694316942
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.46523976,  5.        ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5347602367400999}
episode index:41
target Thresh 5.467367589545155
target distance 2.0
model initialize at round 41
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([13.87508154,  4.        ,  0.        ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.8750815391540687}
done in step count: 0
reward sum = 0.9943630513633726
running average episode reward sum: 0.03856441651977065
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'previousTarget': array([13.,  4.]), 'currentState': array([13.87508154,  4.        ,  0.        ]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.8750815391540687}
episode index:42
target Thresh 5.520379824460409
target distance 4.0
model initialize at round 42
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([18.71930051, 14.        ,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 2.6374218924027453}
done in step count: 19
reward sum = 0.7927309512605302
running average episode reward sum: 0.05610317314164878
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'previousTarget': array([17., 12.]), 'currentState': array([17.75298763, 12.98156011,  0.        ]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 1.237113828040936}
episode index:43
target Thresh 5.573286140859654
target distance 5.0
model initialize at round 43
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([12.27821314, 15.        ,  0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 5.644425035964222}
done in step count: 99
reward sum = -0.2517586102554314
running average episode reward sum: 0.04910631442807878
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([1.12933579, 1.13499281, 0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 16.916357509978603}
episode index:44
target Thresh 5.62608675036822
target distance 2.0
model initialize at round 44
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([ 5.74485302, 10.        ,  0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 2.25514698028565}
done in step count: 99
reward sum = -0.19548773620111126
running average episode reward sum: 0.04367089108076345
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'previousTarget': array([ 8., 10.]), 'currentState': array([1.15629958, 1.1272606 , 0.        ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 11.205433494678566}
episode index:45
target Thresh 5.678781864188629
target distance 3.0
model initialize at round 45
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([13.,  5.,  0.]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 2.23606797749978}
done in step count: 99
reward sum = -0.20763833388147543
running average episode reward sum: 0.03820764705984521
{'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([9.93828789, 5.87210003, 0.        ]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 2.3500670580460303}
episode index:46
target Thresh 5.731371693101398
target distance 1.0
model initialize at round 46
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([27.86446458, 25.01842009,  0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 1.3079748229673638}
done in step count: 0
reward sum = 0.9948244591504226
running average episode reward sum: 0.05856119625326175
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([27.86446458, 25.01842009,  0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 1.3079748229673638}
episode index:47
target Thresh 5.7838564474659115
target distance 3.0
model initialize at round 47
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([24.        ,  4.74363327,  0.        ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 5.295305183006013}
done in step count: 86
reward sum = 0.20444711260044224
running average episode reward sum: 0.061600486177161344
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'previousTarget': array([19.,  3.]), 'currentState': array([18.9699199 ,  3.80004946,  0.        ]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 0.8006147314020803}
episode index:48
target Thresh 5.836236337221262
target distance 2.0
model initialize at round 48
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 4., 18.,  0.]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 4.472135954999555}
done in step count: 50
reward sum = 0.49664481135503186
running average episode reward sum: 0.07047894179303625
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 2.36897011, 22.47916817,  0.        ]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.6047653126035755}
episode index:49
target Thresh 5.888511571887076
target distance 5.0
model initialize at round 49
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([ 7.37273407, 10.48491281,  0.        ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 5.791637975309463}
done in step count: 43
reward sum = 0.5685687470076382
running average episode reward sum: 0.0804407378973283
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([10.45615289, 15.96811342,  0.        ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 1.1104113120187948}
episode index:50
target Thresh 5.940682360564367
target distance 5.0
model initialize at round 50
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([ 6.41105878, 22.        ,  0.        ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 7.866415896408509}
done in step count: 27
reward sum = 0.7159107807308092
running average episode reward sum: 0.09290093481563183
{'scaleFactor': 20, 'currentTarget': array([10., 29.]), 'previousTarget': array([10., 29.]), 'currentState': array([10.51363349, 29.91234869,  0.        ]), 'targetState': array([10, 29], dtype=int32), 'currentDistance': 1.0469954559278125}
episode index:51
target Thresh 5.992748911936349
target distance 3.0
model initialize at round 51
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([8., 9., 0.]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 1.4142135623730956}
done in step count: 99
reward sum = -0.14582844510305762
running average episode reward sum: 0.0883099852018109
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'previousTarget': array([9., 8.]), 'currentState': array([12.17019595, 14.70239009,  0.        ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 7.414322306543373}
episode index:52
target Thresh 6.044711434269299
target distance 2.0
model initialize at round 52
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([10., 24.,  0.]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 0.9999999999999822}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.10539847605093468
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([10., 24.,  0.]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 0.9999999999999822}
episode index:53
target Thresh 6.096570135413387
target distance 4.0
model initialize at round 53
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([6.        , 8.73530304, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 3.021399063525053}
done in step count: 8
reward sum = 0.9036083902713349
running average episode reward sum: 0.12018014112909023
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.72565666, 11.48389383,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5562530997311409}
episode index:54
target Thresh 6.148325222803475
target distance 6.0
model initialize at round 54
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([18.41798145, 22.17614627,  0.        ]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 6.837260358111572}
done in step count: 56
reward sum = 0.4832019496707155
running average episode reward sum: 0.12678053764802888
{'scaleFactor': 20, 'currentTarget': array([22., 28.]), 'previousTarget': array([22., 28.]), 'currentState': array([22.67696626, 28.00863737,  0.        ]), 'targetState': array([22, 28], dtype=int32), 'currentDistance': 0.6770213563334689}
episode index:55
target Thresh 6.199976903459987
target distance 2.0
model initialize at round 55
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([ 8.        , 27.11633778,  0.        ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 3.0022548989330318}
done in step count: 99
reward sum = -0.13316707809868816
running average episode reward sum: 0.12213861593826607
{'scaleFactor': 20, 'currentTarget': array([11., 27.]), 'previousTarget': array([11., 27.]), 'currentState': array([11.2699707, 29.886598 ,  0.       ]), 'targetState': array([11, 27], dtype=int32), 'currentDistance': 2.8991950953373897}
episode index:56
target Thresh 6.251525383989712
target distance 4.0
model initialize at round 56
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([22., 15.,  0.]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 2.2360679774998}
done in step count: 99
reward sum = -0.17881553120362217
running average episode reward sum: 0.11685871861998733
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([27.91033502, 29.54863901,  0.        ]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 17.4451590744482}
episode index:57
target Thresh 6.302970870586641
target distance 4.0
model initialize at round 57
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([21.        , 26.84470564,  0.        ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 2.1710660101417334}
done in step count: 99
reward sum = -0.17741652561545507
running average episode reward sum: 0.11178500751247969
{'scaleFactor': 20, 'currentTarget': array([23., 26.]), 'previousTarget': array([23., 26.]), 'currentState': array([16.19130533, 26.52423577,  0.        ]), 'targetState': array([23, 26], dtype=int32), 'currentDistance': 6.828846624432545}
episode index:58
target Thresh 6.3543135690327865
target distance 5.0
model initialize at round 58
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 4.50002813, 21.        ,  0.        ]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 3.4999718666076625}
done in step count: 91
reward sum = 0.21097755133811807
running average episode reward sum: 0.11346623706884645
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'previousTarget': array([ 8., 21.]), 'currentState': array([ 8.95128105, 21.0073411 ,  0.        ]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.9513093738471836}
episode index:59
target Thresh 6.405553684699019
target distance 5.0
model initialize at round 59
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.03720331, 15.09033775,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 5.987577584691353}
done in step count: 99
reward sum = -0.12982847829908664
running average episode reward sum: 0.10941132514604757
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([20.4213388 , 29.90676792,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 9.94377954030413}
episode index:60
target Thresh 6.4566914225458625
target distance 3.0
model initialize at round 60
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 7.48971748, 23.        ,  0.        ]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 3.3495161114266616}
done in step count: 83
reward sum = 0.30253287603119244
running average episode reward sum: 0.11257725220973847
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'previousTarget': array([ 6., 20.]), 'currentState': array([ 5.05469065, 20.79762033,  0.        ]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 1.2368540553997012}
episode index:61
target Thresh 6.5077269871243395
target distance 4.0
model initialize at round 61
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([22.19880056, 14.07444596,  0.        ]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 5.464347321667946}
done in step count: 99
reward sum = -0.15634931745657532
running average episode reward sum: 0.10823972689253986
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'previousTarget': array([26., 18.]), 'currentState': array([27.91183276, 28.51355267,  0.        ]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 10.685967163244937}
episode index:62
target Thresh 6.558660582576774
target distance 4.0
model initialize at round 62
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 3.22041655, 16.45510197,  0.        ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 3.7686324014919412}
done in step count: 6
reward sum = 0.9353289187492217
running average episode reward sum: 0.12136812676328085
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'previousTarget': array([ 6., 19.]), 'currentState': array([ 5.79195833, 18.47852516,  0.        ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 0.5614421987462793}
episode index:63
target Thresh 6.609492412637618
target distance 5.0
model initialize at round 63
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([20.        , 18.28014821,  0.        ]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 4.445151546052469}
done in step count: 99
reward sum = -0.16194539745641323
running average episode reward sum: 0.11694135294734813
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'previousTarget': array([23., 15.]), 'currentState': array([11.54797539, 14.24021863,  0.        ]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 11.477200676077615}
episode index:64
target Thresh 6.660222680634259
target distance 5.0
model initialize at round 64
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([18.90822697,  5.10717738,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.809065605479212}
done in step count: 18
reward sum = 0.8027406325047558
running average episode reward sum: 0.12749211109438516
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.27111012,  1.82549283,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7494886347411798}
episode index:65
target Thresh 6.7108515894878344
target distance 6.0
model initialize at round 65
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([24.53582633, 11.66651583,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 5.360332384539406}
done in step count: 99
reward sum = -0.1460370146013643
running average episode reward sum: 0.1233477304020253
{'scaleFactor': 20, 'currentTarget': array([24., 17.]), 'previousTarget': array([24., 17.]), 'currentState': array([13.49604758, 15.88092256,  0.        ]), 'targetState': array([24, 17], dtype=int32), 'currentDistance': 10.563396739327825}
episode index:66
target Thresh 6.761379341714051
target distance 4.0
model initialize at round 66
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([19.04074037, 24.30648804,  0.        ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 4.790535834549814}
done in step count: 78
reward sum = 0.3135216078913743
running average episode reward sum: 0.1261861464839559
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([20.35634212, 29.86695684,  0.        ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 0.9373333801593864}
episode index:67
target Thresh 6.811806139423979
target distance 4.0
model initialize at round 67
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([8., 9., 0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.605551275463984}
done in step count: 99
reward sum = -0.13214575036870052
running average episode reward sum: 0.1223871480008286
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 7.65340926, 29.89498467,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 19.080383285072607}
episode index:68
target Thresh 6.862132184324885
target distance 3.0
model initialize at round 68
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 2.39043999, 28.        ,  0.        ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 4.716783174130961}
done in step count: 10
reward sum = 0.8765831364702987
running average episode reward sum: 0.13331752464531366
{'scaleFactor': 20, 'currentTarget': array([ 7., 29.]), 'previousTarget': array([ 7., 29.]), 'currentState': array([ 7.81707653, 29.86923261,  0.        ]), 'targetState': array([ 7, 29], dtype=int32), 'currentDistance': 1.192970826064423}
episode index:69
target Thresh 6.912357677721008
target distance 4.0
model initialize at round 69
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([11.92798102, 27.79801714,  0.        ]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 6.293036397283088}
done in step count: 99
reward sum = -0.12589205679226687
running average episode reward sum: 0.1296145306247768
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'previousTarget': array([16., 23.]), 'currentState': array([ 5.38269657, 24.45583399,  0.        ]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 10.716649887981692}
episode index:70
target Thresh 6.962482820514392
target distance 2.0
model initialize at round 70
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([14.50957   , 29.03903925,  0.        ]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 2.5256806325034207}
done in step count: 7
reward sum = 0.9161050841202986
running average episode reward sum: 0.14069186236415035
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([15.24834137, 26.97550016,  0.        ]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 0.752057806293701}
episode index:71
target Thresh 7.012507813205676
target distance 4.0
model initialize at round 71
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 4.2639308 , 17.61407804,  0.        ]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 2.4968823453708677}
done in step count: 61
reward sum = 0.41775057840254154
running average episode reward sum: 0.1445399000869058
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'previousTarget': array([ 5., 20.]), 'currentState': array([ 4.44631496, 20.735529  ,  0.        ]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.9206356691460116}
episode index:72
target Thresh 7.062432855894897
target distance 6.0
model initialize at round 72
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([17.46463609, 15.62519372,  0.        ]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 4.73915795691255}
done in step count: 99
reward sum = 0.22465572605821596
running average episode reward sum: 0.14563737715500594
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'previousTarget': array([22., 17.]), 'currentState': array([22.9965616 , 16.40194223,  0.        ]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 1.1622427140272118}
episode index:73
target Thresh 7.112258148282287
target distance 1.0
model initialize at round 73
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.83805025, 10.53360248,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9935088413004757}
done in step count: 0
reward sum = 0.9957022260343478
running average episode reward sum: 0.15712473997769977
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'previousTarget': array([11., 10.]), 'currentState': array([11.83805025, 10.53360248,  0.        ]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.9935088413004757}
episode index:74
target Thresh 7.161983889669088
target distance 4.0
model initialize at round 74
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([20.90197921,  7.8287617 ,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 2.2557786225168206}
done in step count: 90
reward sum = 0.2808507732926263
running average episode reward sum: 0.15877442042189877
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([23.62861405,  6.8454335 ,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.6473379520386336}
episode index:75
target Thresh 7.211610278958336
target distance 3.0
model initialize at round 75
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([22.71859121, 26.        ,  0.        ]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 1.0388411357245837}
done in step count: 1
reward sum = 0.9830204509427916
running average episode reward sum: 0.16961976292875264
{'scaleFactor': 20, 'currentTarget': array([23., 27.]), 'previousTarget': array([23., 27.]), 'currentState': array([23.63405478, 28.        ,  0.        ]), 'targetState': array([23, 27], dtype=int32), 'currentDistance': 1.1840715620472033}
episode index:76
target Thresh 7.261137514655644
target distance 4.0
model initialize at round 76
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([10.58931291, 18.14771318,  0.        ]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 4.730648850731307}
done in step count: 67
reward sum = 0.39607555794958904
running average episode reward sum: 0.17256074727967258
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'previousTarget': array([ 6., 17.]), 'currentState': array([ 6.03551513, 17.44731942,  0.        ]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 0.4487270774526879}
episode index:77
target Thresh 7.310565794870023
target distance 2.0
model initialize at round 77
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([11.73342657, 24.        ,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 2.7334265708923393}
done in step count: 22
reward sum = 0.7533791100705064
running average episode reward sum: 0.18000713654622172
{'scaleFactor': 20, 'currentTarget': array([ 9., 24.]), 'previousTarget': array([ 9., 24.]), 'currentState': array([ 8.89269322, 24.75140984,  0.        ]), 'targetState': array([ 9, 24], dtype=int32), 'currentDistance': 0.759033259602303}
episode index:78
target Thresh 7.359895317314667
target distance 7.0
model initialize at round 78
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([ 9.04769647, 20.        ,  0.        ]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 7.037422130384351}
done in step count: 99
reward sum = -0.1754521495450872
running average episode reward sum: 0.17550765191215453
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([ 9.60895683, 25.46338196,  0.        ]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 4.415425566344172}
episode index:79
target Thresh 7.409126279307728
target distance 6.0
model initialize at round 79
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([22.        ,  8.02003098,  0.        ]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 5.671035976142866}
done in step count: 11
reward sum = 0.857886435173037
running average episode reward sum: 0.18403738670291558
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([26.20452307,  3.63912961,  0.        ]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.4147976943201906}
episode index:80
target Thresh 7.458258877773112
target distance 7.0
model initialize at round 80
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([13.09056234, 22.        ,  0.        ]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 9.025364205151835}
done in step count: 42
reward sum = 0.5543312169451458
running average episode reward sum: 0.18860891547133818
{'scaleFactor': 20, 'currentTarget': array([ 5., 26.]), 'previousTarget': array([ 5., 26.]), 'currentState': array([ 5.82059763, 26.4467617 ,  0.        ]), 'targetState': array([ 5, 26], dtype=int32), 'currentDistance': 0.934332105668112}
episode index:81
target Thresh 7.507293309241291
target distance 4.0
model initialize at round 81
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([25.        ,  3.67725813,  0.        ]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 3.065147594584669}
done in step count: 99
reward sum = -0.13303878921165505
running average episode reward sum: 0.18468638248739924
{'scaleFactor': 20, 'currentTarget': array([23.,  6.]), 'previousTarget': array([23.,  6.]), 'currentState': array([22.16918656, 11.99118088,  0.        ]), 'targetState': array([23,  6], dtype=int32), 'currentDistance': 6.048512156706263}
episode index:82
target Thresh 7.556229769850045
target distance 3.0
model initialize at round 82
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.23185349, 7.98052657, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.032739281646016}
done in step count: 19
reward sum = 0.7789874365461413
running average episode reward sum: 0.19184663615075756
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.2973493 , 9.17954559, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8726752201227693}
episode index:83
target Thresh 7.60506845534529
target distance 7.0
model initialize at round 83
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([12.53657806, 20.76146996,  0.        ]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 5.541713921293419}
done in step count: 58
reward sum = 0.4591716489090607
running average episode reward sum: 0.19502907677883258
{'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'previousTarget': array([ 7., 21.]), 'currentState': array([ 7.74716947, 20.43435583,  0.        ]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 0.9371315516851382}
episode index:84
target Thresh 7.653809561081829
target distance 6.0
model initialize at round 84
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([15.74414086, 13.92710579,  0.        ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 5.8434798057870685}
done in step count: 99
reward sum = -0.15086046478745715
running average episode reward sum: 0.1909597880545233
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'previousTarget': array([10., 15.]), 'currentState': array([12.24048182,  9.06907997,  0.        ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 6.339997724048851}
episode index:85
target Thresh 7.702453282024148
target distance 6.0
model initialize at round 85
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([15.5451169 , 21.45682347,  0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 5.762664908147851}
done in step count: 59
reward sum = 0.4560029117171201
running average episode reward sum: 0.1940416848412977
{'scaleFactor': 20, 'currentTarget': array([12., 26.]), 'previousTarget': array([12., 26.]), 'currentState': array([11.05705794, 25.13529901,  0.        ]), 'targetState': array([12, 26], dtype=int32), 'currentDistance': 1.2793934220618788}
episode index:86
target Thresh 7.750999812747203
target distance 2.0
model initialize at round 86
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([12.33220363, 27.28179532,  0.        ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 2.9597528173055334}
done in step count: 8
reward sum = 0.9070223568913087
running average episode reward sum: 0.20223686497980356
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([14.89718788, 25.78191413,  0.        ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.24110532858870476}
episode index:87
target Thresh 7.799449347437175
target distance 6.0
model initialize at round 87
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([17.        , 17.51705742,  0.        ]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 6.033556805479312}
done in step count: 99
reward sum = -0.13728170032670392
running average episode reward sum: 0.1983786994649569
{'scaleFactor': 20, 'currentTarget': array([21., 13.]), 'previousTarget': array([21., 13.]), 'currentState': array([ 5.83005847, 18.12238463,  0.        ]), 'targetState': array([21, 13], dtype=int32), 'currentDistance': 16.011431868483957}
episode index:88
target Thresh 7.8478020798922685
target distance 7.0
model initialize at round 88
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([7.14059377, 2.82839167, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.20691242867679}
done in step count: 19
reward sum = 0.7722355881557835
running average episode reward sum: 0.20482652967496617
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.0991806 , 2.60694521, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.0862127183256536}
episode index:89
target Thresh 7.896058203523477
target distance 1.0
model initialize at round 89
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.        , 8.61062366, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.711831364513903}
done in step count: 4
reward sum = 0.9541200486804463
running average episode reward sum: 0.21315201321947153
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.21585774, 10.37550673,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8694161187703001}
episode index:90
target Thresh 7.944217911355366
target distance 5.0
model initialize at round 90
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([14.70720744, 19.80624504,  0.        ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 3.755662923233502}
done in step count: 67
reward sum = 0.3991919795863697
running average episode reward sum: 0.2151964084542726
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'previousTarget': array([18., 18.]), 'currentState': array([17.98645232, 18.96285865,  0.        ]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.9629539526460108}
episode index:91
target Thresh 7.992281396026822
target distance 4.0
model initialize at round 91
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([21.        , 28.75035775,  0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 6.6003384562393}
done in step count: 20
reward sum = 0.7883687272457165
running average episode reward sum: 0.22142654235417963
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([27.24037403, 26.6792574 ,  0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 0.72053472754416}
episode index:92
target Thresh 8.040248849791851
target distance 3.0
model initialize at round 92
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([27.52748859, 12.97188401,  0.        ]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 4.068959351571333}
done in step count: 10
reward sum = 0.8783570118781909
running average episode reward sum: 0.22849031084368512
{'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'previousTarget': array([24., 15.]), 'currentState': array([24.63163856, 14.64418149,  0.        ]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 0.7249648855374997}
episode index:93
target Thresh 8.08812046452034
target distance 7.0
model initialize at round 93
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([25.74004912, 25.83700776,  0.        ]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 6.026836770413319}
done in step count: 45
reward sum = 0.5570661179099601
running average episode reward sum: 0.23198579815290082
{'scaleFactor': 20, 'currentTarget': array([20., 24.]), 'previousTarget': array([20., 24.]), 'currentState': array([20.91818468, 24.25779815,  0.        ]), 'targetState': array([20, 24], dtype=int32), 'currentDistance': 0.9536891506664434}
episode index:94
target Thresh 8.135896431698797
target distance 3.0
model initialize at round 94
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([25.01618552, 10.        ,  0.        ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 2.2216029984691263}
done in step count: 9
reward sum = 0.8962958145778531
running average episode reward sum: 0.23897853516790032
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'previousTarget': array([27.,  9.]), 'currentState': array([27.41558677,  9.44535893,  0.        ]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.6091444339226728}
episode index:95
target Thresh 8.183576942431166
target distance 6.0
model initialize at round 95
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([21., 16.,  0.]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 5.000000000000027}
done in step count: 3
reward sum = 0.9562166258618814
running average episode reward sum: 0.24644976527929596
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'previousTarget': array([24., 20.]), 'currentState': array([23.34357418, 20.26358449,  0.        ]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.7073695220789512}
episode index:96
target Thresh 8.231162187439548
target distance 6.0
model initialize at round 96
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 3.93644851, 22.        ,  0.        ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 4.108154795067655}
done in step count: 25
reward sum = 0.7164116973836614
running average episode reward sum: 0.25129473365150595
{'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 3.90291896, 25.31853726,  0.        ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 1.1312179790236538}
episode index:97
target Thresh 8.278652357064985
target distance 7.0
model initialize at round 97
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([18.        , 19.92427492,  0.        ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 5.084710820721397}
done in step count: 5
reward sum = 0.9327467002640812
running average episode reward sum: 0.2582483251475527
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([22.45700839, 18.64583141,  0.        ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.6482864179248486}
episode index:98
target Thresh 8.326047641268229
target distance 8.0
model initialize at round 98
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([11.65693486,  8.87599313,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 6.977072785914446}
done in step count: 96
reward sum = 0.2377302818472223
running average episode reward sum: 0.2580410721849231
{'scaleFactor': 20, 'currentTarget': array([15., 15.]), 'previousTarget': array([15., 15.]), 'currentState': array([14.96618006, 15.72930674,  0.        ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.7300904850663472}
episode index:99
target Thresh 8.373348229630466
target distance 5.0
model initialize at round 99
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([25.       ,  9.3881501,  0.       ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 4.525434904251308}
done in step count: 99
reward sum = -0.14410781027192476
running average episode reward sum: 0.25401958336035463
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'previousTarget': array([22.,  6.]), 'currentState': array([24.79508888,  1.15305837,  0.        ]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 5.595119752985205}
episode index:100
target Thresh 8.420554311354124
target distance 5.0
model initialize at round 100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([11.71080446, 16.83543336,  0.        ]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 3.24341255131678}
done in step count: 2
reward sum = 0.9715321380767823
running average episode reward sum: 0.2611236680605173
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([11.88673224, 20.72274148,  0.        ]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 1.1439621109056617}
episode index:101
target Thresh 8.467666075263587
target distance 8.0
model initialize at round 101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([15.10474253, 27.        ,  0.        ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 10.21909688304185}
done in step count: 45
reward sum = 0.5496936016211993
running average episode reward sum: 0.2639527850562102
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'previousTarget': array([13., 17.]), 'currentState': array([12.40644644, 17.19113867,  0.        ]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.6235702245735107}
episode index:102
target Thresh 8.514683709805968
target distance 2.0
model initialize at round 102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([14.09328079, 26.98329866,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 2.018857499597676}
done in step count: 32
reward sum = 0.6796940181549466
running average episode reward sum: 0.2679891077076543
{'scaleFactor': 20, 'currentTarget': array([14., 29.]), 'previousTarget': array([14., 29.]), 'currentState': array([13.47489807, 28.21217835,  0.        ]), 'targetState': array([14, 29], dtype=int32), 'currentDistance': 0.9467813843716494}
episode index:103
target Thresh 8.561607403051884
target distance 6.0
model initialize at round 103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([18.74293971, 13.35228288,  0.        ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 6.741351419375417}
done in step count: 20
reward sum = 0.7768582224684164
running average episode reward sum: 0.27288207996496927
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([20.58790234,  7.32245339,  0.        ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 0.523259656344474}
episode index:104
target Thresh 8.608437342696156
target distance 5.0
model initialize at round 104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([23.6142925 , 20.01890928,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 4.685084012100438}
done in step count: 18
reward sum = 0.804251949068431
running average episode reward sum: 0.27794274538500224
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([20.92046511, 23.83406705,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 1.2421448628127603}
episode index:105
target Thresh 8.655173716058613
target distance 6.0
model initialize at round 105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([4.        , 4.81525803, 0.        ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 5.112981658501597}
done in step count: 82
reward sum = 0.3312795381902973
running average episode reward sum: 0.27844592267561824
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'previousTarget': array([8., 8.]), 'currentState': array([8.56884768, 7.82036011, 0.        ]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 0.5965384941861285}
episode index:106
target Thresh 8.701816710084806
target distance 2.0
model initialize at round 106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([7.91509312, 5.        , 0.        ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.9150931239128335}
done in step count: 0
reward sum = 0.9969945930936222
running average episode reward sum: 0.28516133081036593
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([7.91509312, 5.        , 0.        ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.9150931239128335}
episode index:107
target Thresh 8.748366511346774
target distance 3.0
model initialize at round 107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([25.48015463, 17.        ,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 1.7862972146697649}
done in step count: 1
reward sum = 0.9821383330916904
running average episode reward sum: 0.29161482157223007
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'previousTarget': array([24., 16.]), 'currentState': array([24.41823399, 15.24308181,  0.        ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.8647802149652325}
episode index:108
target Thresh 8.794823306043785
target distance 7.0
model initialize at round 108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([ 9.07975101, 25.70398116,  0.        ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 6.775919733445887}
done in step count: 6
reward sum = 0.9174271483332801
running average episode reward sum: 0.2973562190654507
{'scaleFactor': 20, 'currentTarget': array([15., 29.]), 'previousTarget': array([15., 29.]), 'currentState': array([15.09118851, 29.86853826,  0.        ]), 'targetState': array([15, 29], dtype=int32), 'currentDistance': 0.8733121149668945}
episode index:109
target Thresh 8.841187280003084
target distance 6.0
model initialize at round 109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([22.01175147, 22.        ,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 5.007059709531871}
done in step count: 4
reward sum = 0.9485810520439261
running average episode reward sum: 0.3032764448198005
{'scaleFactor': 20, 'currentTarget': array([19., 26.]), 'previousTarget': array([19., 26.]), 'currentState': array([19.43559039, 25.95584661,  0.        ]), 'targetState': array([19, 26], dtype=int32), 'currentDistance': 0.4378224603293951}
episode index:110
target Thresh 8.887458618680618
target distance 1.0
model initialize at round 110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([11.65867972, 18.69392467,  0.        ]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 1.7979849463402295}
done in step count: 27
reward sum = 0.702977766290002
running average episode reward sum: 0.30687735762583834
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'previousTarget': array([10., 18.]), 'currentState': array([ 9.82112008, 18.87385887,  0.        ]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.8919794547232329}
episode index:111
target Thresh 8.933637507161816
target distance 6.0
model initialize at round 111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([14.24822628,  9.        ,  0.        ]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 6.045153359375101}
done in step count: 12
reward sum = 0.8598928247201846
running average episode reward sum: 0.311814995724895
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'previousTarget': array([9., 6.]), 'currentState': array([9.31876555, 6.93872806, 0.        ]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 0.9913737189286984}
episode index:112
target Thresh 8.979724130162285
target distance 5.0
model initialize at round 112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.92650628,  8.46263695,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 5.463131313484923}
done in step count: 4
reward sum = 0.9520551853253604
running average episode reward sum: 0.31748083811073985
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([26.57752315,  3.54006994,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.685683765871594}
episode index:113
target Thresh 9.025718672028582
target distance 6.0
model initialize at round 113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 3.08999407, 15.37194562,  0.        ]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 6.717082100789985}
done in step count: 17
reward sum = 0.806933536764653
running average episode reward sum: 0.3217742828357742
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'previousTarget': array([ 2., 22.]), 'currentState': array([ 1.53329482, 21.42743119,  0.        ]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.7386804251340642}
episode index:114
target Thresh 9.07162131673893
target distance 5.0
model initialize at round 114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 6.15866792, 15.        ,  0.        ]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 4.330800403310993}
done in step count: 2
reward sum = 0.9716992090826458
running average episode reward sum: 0.32742580393357307
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([ 9.09005821, 16.22041738,  0.        ]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 1.1982249899762962}
episode index:115
target Thresh 9.117432247903977
target distance 4.0
model initialize at round 115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([17.67210639, 12.06256181,  0.        ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 5.614129354931009}
done in step count: 6
reward sum = 0.9275087049301687
running average episode reward sum: 0.33259893239044025
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([15.95987472, 17.15900415,  0.        ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.9729551913398525}
episode index:116
target Thresh 9.16315164876751
target distance 6.0
model initialize at round 116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([10.70762289, 19.50030708,  0.        ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 5.500925091619918}
done in step count: 99
reward sum = -0.13756778377390266
running average episode reward sum: 0.3285804134488647
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([ 9.48424851, 20.20056413,  0.        ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 6.877317788775377}
episode index:117
target Thresh 9.208779702207185
target distance 8.0
model initialize at round 117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([23.10588324, 12.92779124,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 7.928498292318182}
done in step count: 32
reward sum = 0.671704080412103
running average episode reward sum: 0.33148824113499387
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.5787688 ,  5.17902358,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.4576954969737624}
episode index:118
target Thresh 9.254316590735282
target distance 3.0
model initialize at round 118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([17.91588187, 24.60342366,  0.        ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 2.5087793561892258}
done in step count: 10
reward sum = 0.8903644894797708
running average episode reward sum: 0.33618468019671466
{'scaleFactor': 20, 'currentTarget': array([20., 26.]), 'previousTarget': array([20., 26.]), 'currentState': array([19.98009682, 26.83902031,  0.        ]), 'targetState': array([20, 26], dtype=int32), 'currentDistance': 0.8392563496098149}
episode index:119
target Thresh 9.299762496499412
target distance 6.0
model initialize at round 119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([13.18902242,  7.04770517,  0.        ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 6.017845172389958}
done in step count: 3
reward sum = 0.9596430741546091
running average episode reward sum: 0.3413801668130304
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'previousTarget': array([8., 4.]), 'currentState': array([7.7137419 , 4.35262591, 0.        ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.4541901920620121}
episode index:120
target Thresh 9.34511760128326
target distance 8.0
model initialize at round 120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([1.13259695, 4.99505755, 0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 10.649188088368295}
done in step count: 74
reward sum = 0.3449836423258229
running average episode reward sum: 0.34140994760239235
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'previousTarget': array([11.,  9.]), 'currentState': array([10.69507341,  8.23893525,  0.        ]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.8198779049563556}
episode index:121
target Thresh 9.390382086507309
target distance 8.0
model initialize at round 121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([16.40501904, 26.54235554,  0.        ]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 6.427940455090505}
done in step count: 52
reward sum = 0.4835200017007889
running average episode reward sum: 0.3425747841113956
{'scaleFactor': 20, 'currentTarget': array([10., 26.]), 'previousTarget': array([10., 26.]), 'currentState': array([10.5503899 , 25.57372845,  0.        ]), 'targetState': array([10, 26], dtype=int32), 'currentDistance': 0.6961583712938798}
episode index:122
target Thresh 9.43555613322956
target distance 7.0
model initialize at round 122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([12.        , 18.89914131,  0.        ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 5.11975486246422}
done in step count: 56
reward sum = 0.4559359738307639
running average episode reward sum: 0.3434964198001709
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'previousTarget': array([ 7., 20.]), 'currentState': array([ 7.06654524, 19.74641104,  0.        ]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.2621748060106708}
episode index:123
target Thresh 9.480639922146256
target distance 7.0
model initialize at round 123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([17.        , 12.15823519,  0.        ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 5.070361721956767}
done in step count: 12
reward sum = 0.8627032975709575
running average episode reward sum: 0.34768357204025796
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'previousTarget': array([22., 13.]), 'currentState': array([21.55497372, 13.85698366,  0.        ]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.965644543758263}
episode index:124
target Thresh 9.525633633592612
target distance 1.0
model initialize at round 124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 7.08674769, 21.93908858,  0.        ]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 1.5187342027843926}
done in step count: 11
reward sum = 0.8847836520216106
running average episode reward sum: 0.35198037268010873
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'previousTarget': array([ 6., 23.]), 'currentState': array([ 5.95347916, 23.01548684,  0.        ]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 0.04903091921112864}
episode index:125
target Thresh 9.57053744754354
target distance 6.0
model initialize at round 125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([15.91200703, 19.        ,  0.        ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 5.053286114910414}
done in step count: 26
reward sum = 0.7240420641528825
running average episode reward sum: 0.354933243247353
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([19.92316401, 23.61357588,  0.        ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 1.1084706332430976}
episode index:126
target Thresh 9.615351543614352
target distance 1.0
model initialize at round 126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([16.36074674,  7.31824028,  0.        ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 2.6583706744038946}
done in step count: 29
reward sum = 0.6932868943728565
running average episode reward sum: 0.35759744522471915
{'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([19.68420692,  6.44581967,  0.        ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 0.8804856334480743}
episode index:127
target Thresh 9.660076101061488
target distance 5.0
model initialize at round 127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([22.02831829, 27.        ,  0.        ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 4.222664112816186}
done in step count: 79
reward sum = 0.3184431364183709
running average episode reward sum: 0.35729155218716957
{'scaleFactor': 20, 'currentTarget': array([25., 24.]), 'previousTarget': array([25., 24.]), 'currentState': array([24.23739725, 23.23458303,  0.        ]), 'targetState': array([25, 24], dtype=int32), 'currentDistance': 1.0804749381925158}
episode index:128
target Thresh 9.704711298783245
target distance 6.0
model initialize at round 128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([17., 12.,  0.]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 5.000000000000028}
done in step count: 2
reward sum = 0.9632625110799686
running average episode reward sum: 0.3619890014809122
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'previousTarget': array([21., 15.]), 'currentState': array([21.        , 15.63583159,  0.        ]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 0.6358315944671791}
episode index:129
target Thresh 9.74925731532047
target distance 9.0
model initialize at round 129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([15.,  7.,  0.]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 9.899494936611694}
done in step count: 4
reward sum = 0.9366898272559905
running average episode reward sum: 0.3664097770637974
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.82151973, 13.62886417,  0.        ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.41182157648449125}
episode index:130
target Thresh 9.793714328857291
target distance 4.0
model initialize at round 130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 9.49575281, 20.13177836,  0.        ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 3.802035964493396}
done in step count: 59
reward sum = 0.4629550370338751
running average episode reward sum: 0.3671467637811262
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 6.57001411, 23.17082717,  0.        ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.4626767639225559}
episode index:131
target Thresh 9.838082517221814
target distance 9.0
model initialize at round 131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([ 2.4220438 , 15.87052727,  0.        ]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 9.75889938572145}
done in step count: 20
reward sum = 0.7776142172698497
running average episode reward sum: 0.37025636570149534
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'previousTarget': array([12., 14.]), 'currentState': array([11.9762339 , 14.12244792,  0.        ]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.12473299332000017}
episode index:132
target Thresh 9.882362057886862
target distance 7.0
model initialize at round 132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([27.87339067, 21.17082795,  0.        ]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 8.909887007977435}
done in step count: 99
reward sum = -0.16688452874470944
running average episode reward sum: 0.36621771235979456
{'scaleFactor': 20, 'currentTarget': array([20., 17.]), 'previousTarget': array([20., 17.]), 'currentState': array([19.87822843, 29.86964736,  0.        ]), 'targetState': array([20, 17], dtype=int32), 'currentDistance': 12.870223448294679}
episode index:133
target Thresh 9.926553127970646
target distance 9.0
model initialize at round 133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([10.07772911, 28.        ,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 11.11087123923781}
done in step count: 18
reward sum = 0.7901793881413237
running average episode reward sum: 0.3693816054626418
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'previousTarget': array([20., 23.]), 'currentState': array([19.98237544, 22.12181142,  0.        ]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 0.8783654178485377}
episode index:134
target Thresh 9.970655904237518
target distance 1.0
model initialize at round 134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.        , 9.31082976, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.214477509701413}
done in step count: 0
reward sum = 0.9966437831410718
running average episode reward sum: 0.3740279919639635
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.        , 9.31082976, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.214477509701413}
episode index:135
target Thresh 10.01467056309863
target distance 8.0
model initialize at round 135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 5.6579476, 22.       ,  0.       ]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 6.56236888964265}
done in step count: 3
reward sum = 0.9539429396945744
running average episode reward sum: 0.3782920724619827
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 3.23765832, 28.        ,  0.        ]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 0.2376583218574546}
episode index:136
target Thresh 10.058597280612677
target distance 9.0
model initialize at round 136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([15.07326961,  6.        ,  0.        ]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 10.575114880932759}
done in step count: 45
reward sum = 0.546955470303948
running average episode reward sum: 0.37952319215425984
{'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([22.14889534, 12.07054599,  0.        ]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 1.2602634247557976}
episode index:137
target Thresh 10.102436232486596
target distance 8.0
model initialize at round 137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([12.        , 25.15563196,  0.        ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 6.110277017097968}
done in step count: 3
reward sum = 0.9564258690687697
running average episode reward sum: 0.3837036463347997
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 6.       , 24.8073498,  0.       ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.8073498010638147}
episode index:138
target Thresh 10.146187594076249
target distance 3.0
model initialize at round 138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([14.        , 26.75601161,  0.        ]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 1.7560116052628345}
done in step count: 99
reward sum = -0.14671818681660118
running average episode reward sum: 0.3798876619236386
{'scaleFactor': 20, 'currentTarget': array([14., 25.]), 'previousTarget': array([14., 25.]), 'currentState': array([19.80430969, 24.59547091,  0.        ]), 'targetState': array([14, 25], dtype=int32), 'currentDistance': 5.818389361340228}
episode index:139
target Thresh 10.189851540387135
target distance 9.0
model initialize at round 139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([12.72005463, 13.98561859,  0.        ]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 8.287685968511724}
done in step count: 12
reward sum = 0.8659483408981974
running average episode reward sum: 0.38335952391631406
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 4.27195764, 17.05624574,  0.        ]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 0.7302117964658457}
episode index:140
target Thresh 10.233428246075107
target distance 2.0
model initialize at round 140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.0453124, 7.       , 0.       ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.04531240463256925}
done in step count: 0
reward sum = 0.9941343969135705
running average episode reward sum: 0.38769126060423786
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.0453124, 7.       , 0.       ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.04531240463256925}
episode index:141
target Thresh 10.276917885447038
target distance 4.0
model initialize at round 141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([16.39086819, 23.31476736,  0.        ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 2.628050096406091}
done in step count: 57
reward sum = 0.4619903336004945
running average episode reward sum: 0.38821449351266224
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([19.63069402, 22.6068134 ,  0.        ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 0.7432164224839684}
episode index:142
target Thresh 10.320320632461549
target distance 6.0
model initialize at round 142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([17.00362706, 15.40664566,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 5.031482079379813}
done in step count: 12
reward sum = 0.8576834991868407
running average episode reward sum: 0.3914974935523418
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.51419443, 15.24490607,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.913544064129041}
episode index:143
target Thresh 10.363636660729682
target distance 9.0
model initialize at round 143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([18.05404425,  6.        ,  0.        ]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 8.193791114851471}
done in step count: 16
reward sum = 0.8109261594870791
running average episode reward sum: 0.39441019262133303
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'previousTarget': array([26.,  4.]), 'currentState': array([25.11085293,  3.39324482,  0.        ]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 1.0764452386922163}
episode index:144
target Thresh 10.406866143515611
target distance 6.0
model initialize at round 144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([23.        ,  6.40005434,  0.        ]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 5.946467704123257}
done in step count: 24
reward sum = 0.7324080788239542
running average episode reward sum: 0.39674121252617867
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([18.23812481,  1.1217938 ,  0.        ]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 1.1626263071391436}
episode index:145
target Thresh 10.45000925373732
target distance 8.0
model initialize at round 145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([21.57051408, 18.        ,  0.        ]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 10.12257449884235}
done in step count: 73
reward sum = 0.3666732220133851
running average episode reward sum: 0.39653526738568007
{'scaleFactor': 20, 'currentTarget': array([20., 28.]), 'previousTarget': array([20., 28.]), 'currentState': array([20.30212321, 27.36601699,  0.        ]), 'targetState': array([20, 28], dtype=int32), 'currentDistance': 0.702291173556417}
episode index:146
target Thresh 10.493066163967313
target distance 10.0
model initialize at round 146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([11.        ,  4.45757246,  0.        ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 10.439016633436726}
done in step count: 8
reward sum = 0.9006570404169779
running average episode reward sum: 0.3999646672022195
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'previousTarget': array([ 5., 13.]), 'currentState': array([ 4.00740838, 13.26724416,  0.        ]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 1.0279385021647531}
episode index:147
target Thresh 10.536037046433286
target distance 5.0
model initialize at round 147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([ 9.04231119, 15.        ,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 5.79471123709735}
done in step count: 99
reward sum = -0.11645120429528587
running average episode reward sum: 0.3964753707731823
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([18.05267835, 20.24974913,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 4.635253278978247}
episode index:148
target Thresh 10.578922073018827
target distance 10.0
model initialize at round 148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([21.42838717, 28.6095314 ,  0.        ]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 13.523139052628817}
done in step count: 13
reward sum = 0.848320199734728
running average episode reward sum: 0.3995078864037967
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'previousTarget': array([11., 20.]), 'currentState': array([11.78180374, 20.87695724,  0.        ]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 1.1748493894435215}
episode index:149
target Thresh 10.621721415264098
target distance 10.0
model initialize at round 149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([18.        , 16.93477058,  0.        ]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 8.521319051727607}
done in step count: 7
reward sum = 0.9060113382074749
running average episode reward sum: 0.4028845760824879
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'previousTarget': array([10., 14.]), 'currentState': array([ 9.16134411, 13.23221555,  0.        ]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 1.137029751794174}
episode index:150
target Thresh 10.664435244366526
target distance 10.0
model initialize at round 150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([ 4.37695539, 27.        ,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 10.844491116069156}
done in step count: 72
reward sum = 0.4006364689969102
running average episode reward sum: 0.40286968795609335
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'previousTarget': array([14., 22.]), 'currentState': array([13.4428214 , 22.33794724,  0.        ]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.6516566047122995}
episode index:151
target Thresh 10.707063731181481
target distance 10.0
model initialize at round 151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([25.73357511, 14.        ,  0.        ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 8.03356287307537}
done in step count: 7
reward sum = 0.910942074153464
running average episode reward sum: 0.4062122694442339
{'scaleFactor': 20, 'currentTarget': array([25.,  6.]), 'previousTarget': array([25.,  6.]), 'currentState': array([25.87377096,  6.05893836,  0.        ]), 'targetState': array([25,  6], dtype=int32), 'currentDistance': 0.8757564884388938}
episode index:152
target Thresh 10.749607046222977
target distance 6.0
model initialize at round 152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([3.86926246, 5.97767013, 0.        ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 4.244859425141066}
done in step count: 20
reward sum = 0.7783367756360243
running average episode reward sum: 0.40864445575921293
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'previousTarget': array([8., 5.]), 'currentState': array([7.97346592, 5.69254178, 0.        ]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.6930499057246449}
episode index:153
target Thresh 10.792065359664317
target distance 7.0
model initialize at round 153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([23.95070702, 14.99274313,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 5.410381044034405}
done in step count: 14
reward sum = 0.8311202364037185
running average episode reward sum: 0.41138780498417726
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.10936208, 20.24090414,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.9226433303243374}
episode index:154
target Thresh 10.83443884133882
target distance 8.0
model initialize at round 154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([13.        , 27.86812305,  0.        ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 7.138793733172958}
done in step count: 3
reward sum = 0.9556768041104317
running average episode reward sum: 0.414899346914024
{'scaleFactor': 20, 'currentTarget': array([19., 24.]), 'previousTarget': array([19., 24.]), 'currentState': array([18.06574607, 23.84512675,  0.        ]), 'targetState': array([19, 24], dtype=int32), 'currentDistance': 0.9470037654795231}
episode index:155
target Thresh 10.876727660740471
target distance 9.0
model initialize at round 155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([12.009022  , 13.37005317,  0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 8.412630370741896}
done in step count: 9
reward sum = 0.8937853972726248
running average episode reward sum: 0.41796912928811764
{'scaleFactor': 20, 'currentTarget': array([20., 16.]), 'previousTarget': array([20., 16.]), 'currentState': array([20.45426035, 16.53725636,  0.        ]), 'targetState': array([20, 16], dtype=int32), 'currentDistance': 0.703560133421834}
episode index:156
target Thresh 10.918931987024596
target distance 7.0
model initialize at round 156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 7.08721757, 27.        ,  0.        ]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 5.184571592515213}
done in step count: 4
reward sum = 0.9459011835023265
running average episode reward sum: 0.42133175383725274
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 2.99954987, 26.29683661,  0.        ]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 1.0426945429216399}
episode index:157
target Thresh 10.96105198900856
target distance 5.0
model initialize at round 157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([10.8368963 ,  9.47076523,  0.        ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 4.528076706402544}
done in step count: 6
reward sum = 0.9294806125132685
running average episode reward sum: 0.42454788585418957
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.40680516, 13.79187542,  0.        ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.8902567720992731}
episode index:158
target Thresh 11.003087835172437
target distance 8.0
model initialize at round 158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([18.77894151, 19.        ,  0.        ]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 9.388486868796162}
done in step count: 36
reward sum = 0.6189957046983829
running average episode reward sum: 0.42577082811107125
{'scaleFactor': 20, 'currentTarget': array([26., 25.]), 'previousTarget': array([26., 25.]), 'currentState': array([25.78530171, 24.72748267,  0.        ]), 'targetState': array([26, 25], dtype=int32), 'currentDistance': 0.3469309018712176}
episode index:159
target Thresh 11.04503969365965
target distance 7.0
model initialize at round 159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([13., 23.,  0.]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 5.099019513592806}
done in step count: 24
reward sum = 0.735913113279234
running average episode reward sum: 0.4277092173933723
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'previousTarget': array([ 8., 22.]), 'currentState': array([ 8.76941708, 21.62618899,  0.        ]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 0.8554164526474847}
episode index:160
target Thresh 11.086907732277702
target distance 7.0
model initialize at round 160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([20.        , 23.85011733,  0.        ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 6.57448649899182}
done in step count: 31
reward sum = 0.6814003075247541
running average episode reward sum: 0.42928493845008897
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'previousTarget': array([17., 18.]), 'currentState': array([17.22647709, 17.23358517,  0.        ]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 0.7991768016064124}
episode index:161
target Thresh 11.128692118498797
target distance 8.0
model initialize at round 161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([11.33104289, 18.26164412,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.382624909997318}
done in step count: 7
reward sum = 0.9192030244958261
running average episode reward sum: 0.4323091241664207
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.08124518, 10.69922727,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.3115525845535582}
episode index:162
target Thresh 11.17039301946054
target distance 5.0
model initialize at round 162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([24.        , 21.83341599,  0.        ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 5.134240192390656}
done in step count: 2
reward sum = 0.9676647130210145
running average episode reward sum: 0.43559351428209303
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.46910489, 25.15334749,  0.        ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.9993348242429212}
episode index:163
target Thresh 11.212010601966583
target distance 7.0
model initialize at round 163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([18.        , 13.42713678,  0.        ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 7.379282730465411}
done in step count: 3
reward sum = 0.948681680011181
running average episode reward sum: 0.4387221006584899
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([12.        ,  7.74882579,  0.        ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 1.031061823812505}
episode index:164
target Thresh 11.25354503248732
target distance 8.0
model initialize at round 164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([13.        , 19.98830247,  0.        ]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 7.626526225135164}
done in step count: 71
reward sum = 0.36730305133921926
running average episode reward sum: 0.4382892579353428
{'scaleFactor': 20, 'currentTarget': array([10., 27.]), 'previousTarget': array([10., 27.]), 'currentState': array([10.16056679, 27.58655881,  0.        ]), 'targetState': array([10, 27], dtype=int32), 'currentDistance': 0.6081389120599925}
episode index:165
target Thresh 11.294996477160527
target distance 11.0
model initialize at round 165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([14.        ,  8.94566667,  0.        ]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 10.786609854827153}
done in step count: 44
reward sum = 0.5496334811542961
running average episode reward sum: 0.4389600062679871
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'previousTarget': array([5., 3.]), 'currentState': array([4.44280322, 3.91917567, 0.        ]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 1.0748730913085018}
episode index:166
target Thresh 11.336365101792033
target distance 7.0
model initialize at round 166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([13.28425074, 28.09604645,  0.        ]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 5.396723418802427}
done in step count: 27
reward sum = 0.6848787472506339
running average episode reward sum: 0.44043257357926047
{'scaleFactor': 20, 'currentTarget': array([ 8., 27.]), 'previousTarget': array([ 8., 27.]), 'currentState': array([ 7.67194265, 26.59143376,  0.        ]), 'targetState': array([ 8, 27], dtype=int32), 'currentDistance': 0.5239732757107722}
episode index:167
target Thresh 11.377651071856395
target distance 2.0
model initialize at round 167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([12.12739575, 21.2223407 ,  0.        ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.25625195628901265}
done in step count: 0
reward sum = 0.9962494272969782
running average episode reward sum: 0.4437410072323421
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([12.12739575, 21.2223407 ,  0.        ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.25625195628901265}
episode index:168
target Thresh 11.41885455249755
target distance 10.0
model initialize at round 168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([18.39094424, 24.        ,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 8.776404928038986}
done in step count: 44
reward sum = 0.5676376202626413
running average episode reward sum: 0.44447412328577585
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'previousTarget': array([22., 16.]), 'currentState': array([22.41918259, 16.84505683,  0.        ]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.943310709019108}
episode index:169
target Thresh 11.45997570852947
target distance 1.0
model initialize at round 169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([26.04511222,  8.38466895,  0.        ]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 0.38730519081774845}
done in step count: 0
reward sum = 0.9985604925800082
running average episode reward sum: 0.44773345486985955
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([26.04511222,  8.38466895,  0.        ]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 0.38730519081774845}
episode index:170
target Thresh 11.501014704436841
target distance 7.0
model initialize at round 170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([ 8., 20.,  0.]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 5.830951894845325}
done in step count: 3
reward sum = 0.9535018267570298
running average episode reward sum: 0.45069116464697756
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'previousTarget': array([ 3., 23.]), 'currentState': array([ 2.90710938, 23.37257341,  0.        ]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 0.3839786580216438}
episode index:171
target Thresh 11.541971704375694
target distance 9.0
model initialize at round 171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([ 6.        , 20.53171897,  0.        ]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 8.528000388906634}
done in step count: 20
reward sum = 0.7722210182306033
running average episode reward sum: 0.4525605242608359
{'scaleFactor': 20, 'currentTarget': array([10., 13.]), 'previousTarget': array([10., 13.]), 'currentState': array([ 9.90959817, 12.69922921,  0.        ]), 'targetState': array([10, 13], dtype=int32), 'currentDistance': 0.3140629846148326}
episode index:172
target Thresh 11.582846872174088
target distance 7.0
model initialize at round 172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([ 8., 26.,  0.]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 5.099019513592807}
done in step count: 29
reward sum = 0.6890763459326901
running average episode reward sum: 0.45392766773870785
{'scaleFactor': 20, 'currentTarget': array([13., 27.]), 'previousTarget': array([13., 27.]), 'currentState': array([12.23690422, 26.74612683,  0.        ]), 'targetState': array([13, 27], dtype=int32), 'currentDistance': 0.8042181031597396}
episode index:173
target Thresh 11.623640371332751
target distance 9.0
model initialize at round 173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([12., 13.,  0.]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 7.280109889280541}
done in step count: 4
reward sum = 0.941918980870515
running average episode reward sum: 0.45673221551532744
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.8533591 ,  6.39314723,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.4196049298886617}
episode index:174
target Thresh 11.664352365025728
target distance 5.0
model initialize at round 174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([13.        , 11.86592782,  0.        ]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 4.134072184562708}
done in step count: 8
reward sum = 0.903992742710359
running average episode reward sum: 0.4592879899564419
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'previousTarget': array([13., 16.]), 'currentState': array([12.85034034, 16.5537945 ,  0.        ]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 0.5736604977236787}
episode index:175
target Thresh 11.704983016101053
target distance 10.0
model initialize at round 175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([16.94449914, 13.        ,  0.        ]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 11.445614702395876}
done in step count: 16
reward sum = 0.8134096354209017
running average episode reward sum: 0.4613000447602172
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([25.4278729 ,  5.37644058,  0.        ]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 0.8462598698797267}
episode index:176
target Thresh 11.74553248708138
target distance 5.0
model initialize at round 176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([22.        ,  5.96485555,  0.        ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 3.6251638480717583}
done in step count: 54
reward sum = 0.5043802913841833
running average episode reward sum: 0.4615434359840814
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([19.88940999,  7.87494413,  0.        ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 0.8981587265288363}
episode index:177
target Thresh 11.786000940164651
target distance 4.0
model initialize at round 177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([13., 27.,  0.]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 2.236067977499796}
done in step count: 78
reward sum = 0.3268945386207852
running average episode reward sum: 0.46078698150451236
{'scaleFactor': 20, 'currentTarget': array([11., 26.]), 'previousTarget': array([11., 26.]), 'currentState': array([10.99343799, 25.30613854,  0.        ]), 'targetState': array([11, 26], dtype=int32), 'currentDistance': 0.6938924903311047}
episode index:178
target Thresh 11.82638853722473
target distance 10.0
model initialize at round 178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([11.        , 23.69038415,  0.        ]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 8.810160904958549}
done in step count: 57
reward sum = 0.47282221142870995
running average episode reward sum: 0.46085421742587657
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'previousTarget': array([ 3., 20.]), 'currentState': array([ 2.42308314, 20.21059504,  0.        ]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.6141525332980324}
episode index:179
target Thresh 11.866695439812059
target distance 11.0
model initialize at round 179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([9., 8., 0.]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 9.219544457292908}
done in step count: 18
reward sum = 0.785682291928075
running average episode reward sum: 0.46265881783977764
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'previousTarget': array([ 7., 17.]), 'currentState': array([ 7.87670892, 16.17001855,  0.        ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 1.207264569147002}
episode index:180
target Thresh 11.906921809154305
target distance 6.0
model initialize at round 180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([18.09156859, 15.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.5138297667692395}
done in step count: 3
reward sum = 0.9568027271172648
running average episode reward sum: 0.4653888946866146
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.10138083, 11.34357345,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.3582189106501473}
episode index:181
target Thresh 11.947067806156994
target distance 7.0
model initialize at round 181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([19.        ,  4.96129441,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 5.091570184895917}
done in step count: 11
reward sum = 0.8775364154317045
running average episode reward sum: 0.4676534415038953
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.0898538,  3.0852693,  0.       ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9191332695734507}
episode index:182
target Thresh 11.987133591404174
target distance 10.0
model initialize at round 182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([11.        ,  9.63297248,  0.        ]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 9.244697614845794}
done in step count: 99
reward sum = -0.14133472434950783
running average episode reward sum: 0.464325637318904
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'previousTarget': array([19.,  5.]), 'currentState': array([24.62408824,  1.10026195,  0.        ]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 6.84385310718131}
episode index:183
target Thresh 12.027119325159031
target distance 5.0
model initialize at round 183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([13., 14.,  0.]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 7.615773105863898}
done in step count: 88
reward sum = 0.26958703617301194
running average episode reward sum: 0.4632672753561546
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'previousTarget': array([16., 21.]), 'currentState': array([15.56105478, 20.1943338 ,  0.        ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.9174807534981247}
episode index:184
target Thresh 12.067025167364566
target distance 5.0
model initialize at round 184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 1.13816641, 22.97963786,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 4.160850749111943}
done in step count: 20
reward sum = 0.7706279913317216
running average episode reward sum: 0.4649286846316982
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 3.69196552, 26.48260541,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.5725322920236021}
episode index:185
target Thresh 12.106851277644186
target distance 4.0
model initialize at round 185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 7.30303216, 22.        ,  0.        ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 2.3870259339786917}
done in step count: 1
reward sum = 0.9824995396580734
running average episode reward sum: 0.46771132363721635
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 6.71465909, 24.        ,  0.        ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 0.7146590948104441}
episode index:186
target Thresh 12.146597815302396
target distance 9.0
model initialize at round 186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([22.14674878,  5.        ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 7.750872089103396}
done in step count: 5
reward sum = 0.9303855638560332
running average episode reward sum: 0.47018551743517795
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.59516537,  7.5950726 ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7198527775791517}
episode index:187
target Thresh 12.18626493932539
target distance 9.0
model initialize at round 187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([21.55906117, 12.        ,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 7.146768830239622}
done in step count: 4
reward sum = 0.941413264547827
running average episode reward sum: 0.47269204800492615
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'previousTarget': array([23.,  5.]), 'currentState': array([22.84752661,  4.57552671,  0.        ]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.4510273882383364}
episode index:188
target Thresh 12.225852808381731
target distance 9.0
model initialize at round 188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([14.        , 25.35891759,  0.        ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 8.896834726723563}
done in step count: 7
reward sum = 0.9059719741353979
running average episode reward sum: 0.47498453438656885
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'previousTarget': array([ 9., 18.]), 'currentState': array([ 9.69720316, 17.4036479 ,  0.        ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 0.9174573957420405}
episode index:189
target Thresh 12.265361580822933
target distance 7.0
model initialize at round 189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([19.89255166, 21.        ,  0.        ]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 5.976802244688014}
done in step count: 10
reward sum = 0.8838821112935523
running average episode reward sum: 0.47713662689660563
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'previousTarget': array([14., 20.]), 'currentState': array([14.79725575, 20.3822698 ,  0.        ]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.8841645422698061}
episode index:190
target Thresh 12.30479141468415
target distance 7.0
model initialize at round 190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([ 7.        , 12.85751796,  0.        ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 5.9422652772011215}
done in step count: 3
reward sum = 0.9574067817255063
running average episode reward sum: 0.4796511303250292
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'previousTarget': array([6., 7.]), 'currentState': array([6.18860233, 6.98840964, 0.        ]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 0.1889581295479669}
episode index:191
target Thresh 12.344142467684765
target distance 6.0
model initialize at round 191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 2.93476897, 21.13725126,  0.        ]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 6.8630587483931444}
done in step count: 5
reward sum = 0.9342959130439609
running average episode reward sum: 0.4820190719016903
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 3.19597344, 27.6088648 ,  0.        ]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 0.43748409538400107}
episode index:192
target Thresh 12.383414897229041
target distance 9.0
model initialize at round 192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([23.52265918, 27.        ,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 7.816130702637691}
done in step count: 48
reward sum = 0.549467908367193
running average episode reward sum: 0.48236854773829907
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([27.83144486, 20.92955351,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 1.2471448520496202}
episode index:193
target Thresh 12.422608860406754
target distance 11.0
model initialize at round 193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([10.        ,  5.54216719,  0.        ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 9.329573523905175}
done in step count: 46
reward sum = 0.5481387401587992
running average episode reward sum: 0.48270756934871406
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'previousTarget': array([19.,  8.]), 'currentState': array([18.5036034 ,  7.84677242,  0.        ]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 0.5195077263571177}
episode index:194
target Thresh 12.461724513993804
target distance 11.0
model initialize at round 194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([15.        , 18.82749689,  0.        ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 12.156883117978769}
done in step count: 55
reward sum = 0.48488077134626595
running average episode reward sum: 0.4827187139743426
{'scaleFactor': 20, 'currentTarget': array([ 6., 27.]), 'previousTarget': array([ 6., 27.]), 'currentState': array([ 5.25680759, 26.26992627,  0.        ]), 'targetState': array([ 6, 27], dtype=int32), 'currentDistance': 1.041797775789739}
episode index:195
target Thresh 12.500762014452857
target distance 7.0
model initialize at round 195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([11.56845522, 25.        ,  0.        ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 6.745493214368302}
done in step count: 3
reward sum = 0.9536063238880081
running average episode reward sum: 0.48512120178002455
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.20486498, 20.28899032,  0.        ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 1.0666651155585185}
episode index:196
target Thresh 12.53972151793397
target distance 6.0
model initialize at round 196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([14.08427095, 28.        ,  0.        ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 5.181680338947135}
done in step count: 5
reward sum = 0.9399646043769551
running average episode reward sum: 0.48743005153939983
{'scaleFactor': 20, 'currentTarget': array([ 9., 29.]), 'previousTarget': array([ 9., 29.]), 'currentState': array([ 9.98533389, 29.81180157,  0.        ]), 'targetState': array([ 9, 29], dtype=int32), 'currentDistance': 1.2766771974179607}
episode index:197
target Thresh 12.578603180275206
target distance 11.0
model initialize at round 197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([12.4468025, 18.       ,  0.       ]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 13.158103298327086}
done in step count: 19
reward sum = 0.7884507764189663
running average episode reward sum: 0.48895035823071076
{'scaleFactor': 20, 'currentTarget': array([ 2., 26.]), 'previousTarget': array([ 2., 26.]), 'currentState': array([ 1.76302153, 25.03690249,  0.        ]), 'targetState': array([ 2, 26], dtype=int32), 'currentDistance': 0.9918243871071017}
episode index:198
target Thresh 12.61740715700327
target distance 4.0
model initialize at round 198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([20.94929588,  7.3293848 ,  0.        ]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 4.772861987975719}
done in step count: 49
reward sum = 0.5235525168439122
running average episode reward sum: 0.48912423842474695
{'scaleFactor': 20, 'currentTarget': array([24., 11.]), 'previousTarget': array([24., 11.]), 'currentState': array([24.40606534, 10.35221526,  0.        ]), 'targetState': array([24, 11], dtype=int32), 'currentDistance': 0.7645352400561136}
episode index:199
target Thresh 12.65613360333412
target distance 6.0
model initialize at round 199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([7.        , 3.59782183, 0.        ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 5.94803939339923}
done in step count: 4
reward sum = 0.9401616110018682
running average episode reward sum: 0.49137942528763257
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'previousTarget': array([11.,  8.]), 'currentState': array([11.93826473,  8.83133978,  0.        ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 1.2535814784296404}
episode index:200
target Thresh 12.694782674173588
target distance 5.0
model initialize at round 200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 7.36478949, 29.26704347,  0.        ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 6.476862457775385}
done in step count: 13
reward sum = 0.8563884628611467
running average episode reward sum: 0.49319539064869483
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.25991208, 23.09210326,  0.        ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.275748617525813}
episode index:201
target Thresh 12.733354524118013
target distance 6.0
model initialize at round 201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 3.71035802, 18.54541403,  0.        ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 4.3241767713684585}
done in step count: 7
reward sum = 0.9168445450064308
running average episode reward sum: 0.49529266369006975
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 8.07372928, 18.70714337,  0.        ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.7109766147983448}
episode index:202
target Thresh 12.771849307454847
target distance 12.0
model initialize at round 202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([21.22907698,  7.76099503,  0.        ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 10.312509539098246}
done in step count: 7
reward sum = 0.9103956407211307
running average episode reward sum: 0.49733750594145426
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'previousTarget': array([20., 18.]), 'currentState': array([20.75136992, 18.89525533,  0.        ]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 1.1687766521642573}
episode index:203
target Thresh 12.810267178163272
target distance 8.0
model initialize at round 203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([11., 18.,  0.]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 6.000000000000018}
done in step count: 9
reward sum = 0.8796384695316372
running average episode reward sum: 0.4992115302727787
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'previousTarget': array([11., 12.]), 'currentState': array([11.48957014, 11.93464768,  0.        ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 0.49391279454767095}
episode index:204
target Thresh 12.848608289914825
target distance 10.0
model initialize at round 204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([11.39032328, 27.        ,  0.        ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 9.2330449717866}
done in step count: 5
reward sum = 0.9280868260897104
running average episode reward sum: 0.5013036048865198
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([15.92385596, 19.47901386,  0.        ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 0.48502803327203786}
episode index:205
target Thresh 12.886872796074002
target distance 9.0
model initialize at round 205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 5.57943344, 10.        ,  0.        ]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 7.023940711077879}
done in step count: 82
reward sum = 0.3167819089593169
running average episode reward sum: 0.5004078684985236
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 4.486446 , 16.3248226,  0.       ]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 0.8482937147844873}
episode index:206
target Thresh 12.925060849698877
target distance 2.0
model initialize at round 206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([24.73349386,  7.06939685,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 2.308953507684331}
done in step count: 10
reward sum = 0.8917938996530664
running average episode reward sum: 0.502298622272217
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([26.89199382,  8.13988078,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 1.2391360114629992}
episode index:207
target Thresh 12.963172603541718
target distance 8.0
model initialize at round 207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([8.02972257, 3.66086626, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 8.053812971238404}
done in step count: 8
reward sum = 0.8981471228335078
running average episode reward sum: 0.504201740063377
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.24211708, 9.48772526, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5445150192576454}
episode index:208
target Thresh 13.001208210049594
target distance 11.0
model initialize at round 208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([11.       , 23.2184149,  0.       ]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 9.17464143489912}
done in step count: 28
reward sum = 0.679460643839581
running average episode reward sum: 0.5050402994115886
{'scaleFactor': 20, 'currentTarget': array([20., 25.]), 'previousTarget': array([20., 25.]), 'currentState': array([19.00095201, 24.38025381,  0.        ]), 'targetState': array([20, 25], dtype=int32), 'currentDistance': 1.1756624684150507}
episode index:209
target Thresh 13.039167821364977
target distance 6.0
model initialize at round 209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([16.05814373, 23.        ,  0.        ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 5.781171450890654}
done in step count: 3
reward sum = 0.9540816533883419
running average episode reward sum: 0.5071785915733826
{'scaleFactor': 20, 'currentTarget': array([21., 26.]), 'previousTarget': array([21., 26.]), 'currentState': array([20.96317518, 26.14337951,  0.        ]), 'targetState': array([21, 26], dtype=int32), 'currentDistance': 0.1480329398362323}
episode index:210
target Thresh 13.077051589326366
target distance 7.0
model initialize at round 210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([13.92258799, 25.57065964,  0.        ]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 6.813145040404504}
done in step count: 18
reward sum = 0.8051929215862458
running average episode reward sum: 0.508590981763017
{'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([ 9.19211322, 19.24536905,  0.        ]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 1.105508445701466}
episode index:211
target Thresh 13.114859665468877
target distance 3.0
model initialize at round 211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([25.        , 19.30976987,  0.        ]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 3.8233229553416717}
done in step count: 9
reward sum = 0.9023595285141853
running average episode reward sum: 0.510448380568447
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'previousTarget': array([24., 23.]), 'currentState': array([24.41793278, 22.8400056 ,  0.        ]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 0.44751091136347254}
episode index:212
target Thresh 13.152592201024873
target distance 6.0
model initialize at round 212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 8.48608303, 20.18942153,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 6.138093657838191}
done in step count: 15
reward sum = 0.832732031393844
running average episode reward sum: 0.5119614493516648
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.68283767, 16.45099384,  0.        ]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 0.8183292228050975}
episode index:213
target Thresh 13.190249346924542
target distance 6.0
model initialize at round 213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([18.        ,  7.30767334,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 4.749320957960173}
done in step count: 10
reward sum = 0.8850781431989786
running average episode reward sum: 0.5137049853042224
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'previousTarget': array([20.,  3.]), 'currentState': array([20.02198479,  3.93003553,  0.        ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.9302953405759832}
episode index:214
target Thresh 13.22783125379652
target distance 9.0
model initialize at round 214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([26.32096988, 13.        ,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 7.007354826439732}
done in step count: 8
reward sum = 0.9043159130871515
running average episode reward sum: 0.5155217803171662
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'previousTarget': array([26., 20.]), 'currentState': array([25.74631366, 19.35709104,  0.        ]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.6911502631240456}
episode index:215
target Thresh 13.265338071968483
target distance 12.0
model initialize at round 215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([15.03444147, 20.76217294,  0.        ]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 11.03520229482262}
done in step count: 5
reward sum = 0.9310060568928836
running average episode reward sum: 0.5174453186346464
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'previousTarget': array([26., 22.]), 'currentState': array([25.03444147, 21.41086054,  0.        ]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 1.1311006046540515}
episode index:216
target Thresh 13.302769951467752
target distance 6.0
model initialize at round 216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([26.72644329, 17.52141333,  0.        ]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 5.746407396452129}
done in step count: 5
reward sum = 0.9365337344493212
running average episode reward sum: 0.5193766016568339
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'previousTarget': array([21., 18.]), 'currentState': array([20.84305046, 17.31878561,  0.        ]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 0.6990609448829466}
episode index:217
target Thresh 13.340127042021901
target distance 5.0
model initialize at round 217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([5.97977328, 5.13634014, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 4.326162075097971}
done in step count: 3
reward sum = 0.9596254573778082
running average episode reward sum: 0.5213960918206916
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.60681903, 2.31540996, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6838953035520073}
episode index:218
target Thresh 13.37740949305934
target distance 9.0
model initialize at round 218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([24.030007  , 25.42492902,  0.        ]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 8.18302273486943}
done in step count: 75
reward sum = 0.36128038674048996
running average episode reward sum: 0.5206649698796861
{'scaleFactor': 20, 'currentTarget': array([16., 27.]), 'previousTarget': array([16., 27.]), 'currentState': array([16.74650049, 27.71928329,  0.        ]), 'targetState': array([16, 27], dtype=int32), 'currentDistance': 1.0366443181547527}
episode index:219
target Thresh 13.414617453709916
target distance 12.0
model initialize at round 219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([23.035326, 22.      ,  0.      ]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 13.035326004028427}
done in step count: 12
reward sum = 0.8527414511282814
running average episode reward sum: 0.5221744084308161
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'previousTarget': array([10., 22.]), 'currentState': array([ 9.92368263, 22.99852157,  0.        ]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 1.0014338018567877}
episode index:220
target Thresh 13.451751072805529
target distance 4.0
model initialize at round 220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 4., 18.,  0.]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 2.828427124746216}
done in step count: 24
reward sum = 0.7506079607926018
running average episode reward sum: 0.5232080444143535
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'previousTarget': array([ 6., 16.]), 'currentState': array([ 5.05704212, 15.90469392,  0.        ]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 0.9477620004295365}
episode index:221
target Thresh 13.4888104988807
target distance 6.0
model initialize at round 221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 7.36239338, 13.        ,  0.        ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 5.362393379211406}
done in step count: 10
reward sum = 0.8864689985364022
running average episode reward sum: 0.5248443550185069
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'previousTarget': array([ 2., 13.]), 'currentState': array([ 2.80126381, 12.22043836,  0.        ]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 1.1179177253621637}
episode index:222
target Thresh 13.525795880173188
target distance 5.0
model initialize at round 222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([24.,  8.,  0.]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 3.1622776601683906}
done in step count: 5
reward sum = 0.9339085294788961
running average episode reward sum: 0.5266787235138449
{'scaleFactor': 20, 'currentTarget': array([25.,  5.]), 'previousTarget': array([25.,  5.]), 'currentState': array([24.57076097,  4.03989685,  0.        ]), 'targetState': array([25,  5], dtype=int32), 'currentDistance': 1.0516863674788375}
episode index:223
target Thresh 13.562707364624561
target distance 7.0
model initialize at round 223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([22.,  8.,  0.]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 7.071067811865503}
done in step count: 10
reward sum = 0.872466606735048
running average episode reward sum: 0.5282224194210824
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'previousTarget': array([17.,  3.]), 'currentState': array([17.88117919,  2.74341742,  0.        ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 0.9177752411159259}
episode index:224
target Thresh 13.59954509988081
target distance 3.0
model initialize at round 224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([23., 27.,  0.]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 1.0000000000000178}
done in step count: 1
reward sum = 0.9806764998809147
running average episode reward sum: 0.5302333264453484
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([21.      , 27.689996,  0.      ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 1.2149462892162304}
episode index:225
target Thresh 13.636309233292927
target distance 4.0
model initialize at round 225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([5., 7., 0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 2.828427124746218}
done in step count: 28
reward sum = 0.7108012753441822
running average episode reward sum: 0.5310322996705644
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.29412906, 8.19722968, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8549573601564109}
episode index:226
target Thresh 13.672999911917493
target distance 5.0
model initialize at round 226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.32723963, 6.51235652, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.551938000250117}
done in step count: 2
reward sum = 0.9710973923901128
running average episode reward sum: 0.5329709124138223
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.32249576, 9.760427  , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.718614792796537}
episode index:227
target Thresh 13.709617282517268
target distance 13.0
model initialize at round 227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([23.67889762, 14.        ,  0.        ]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 14.011540382319875}
done in step count: 15
reward sum = 0.8191248421535117
running average episode reward sum: 0.5342259735091719
{'scaleFactor': 20, 'currentTarget': array([15., 25.]), 'previousTarget': array([15., 25.]), 'currentState': array([15.15403235, 24.46291608,  0.        ]), 'targetState': array([15, 25], dtype=int32), 'currentDistance': 0.5587352735788843}
episode index:228
target Thresh 13.74616149156179
target distance 9.0
model initialize at round 228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([17.        ,  7.85035288,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 8.137540856598758}
done in step count: 14
reward sum = 0.8463252945257975
running average episode reward sum: 0.5355888526402488
{'scaleFactor': 20, 'currentTarget': array([24., 12.]), 'previousTarget': array([24., 12.]), 'currentState': array([24.14132443, 12.06584764,  0.        ]), 'targetState': array([24, 12], dtype=int32), 'currentDistance': 0.15591185261018195}
episode index:229
target Thresh 13.782632685227934
target distance 7.0
model initialize at round 229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([20., 22.,  0.]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 5.000000000000018}
done in step count: 3
reward sum = 0.9565470734998734
running average episode reward sum: 0.5374191057744211
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'previousTarget': array([25., 22.]), 'currentState': array([25.45346725, 22.02087936,  0.        ]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 0.45394767792946766}
episode index:230
target Thresh 13.819031009400533
target distance 8.0
model initialize at round 230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([23., 17.,  0.]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 6.082762530298243}
done in step count: 5
reward sum = 0.9259967992915549
running average episode reward sum: 0.5391012602918113
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'previousTarget': array([22., 23.]), 'currentState': array([21.03508082, 22.97220433,  0.        ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.9653194407629585}
episode index:231
target Thresh 13.855356609672928
target distance 7.0
model initialize at round 231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([16.37885952, 14.69645143,  0.        ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 8.48334419542024}
done in step count: 3
reward sum = 0.9526635020191505
running average episode reward sum: 0.5408838561613256
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'previousTarget': array([23., 20.]), 'currentState': array([22.32748616, 19.88433909,  0.        ]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.6823872167651589}
episode index:232
target Thresh 13.891609631347563
target distance 8.0
model initialize at round 232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 4., 19.,  0.]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 6.324555320336781}
done in step count: 10
reward sum = 0.8746910311049089
running average episode reward sum: 0.542316504980826
{'scaleFactor': 20, 'currentTarget': array([ 6., 13.]), 'previousTarget': array([ 6., 13.]), 'currentState': array([ 6.36824787, 12.82863545,  0.        ]), 'targetState': array([ 6, 13], dtype=int32), 'currentDistance': 0.40616782103301724}
episode index:233
target Thresh 13.927790219436588
target distance 12.0
model initialize at round 233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([20.7942965,  6.       ,  0.       ]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 14.131313562736896}
done in step count: 26
reward sum = 0.7049082451008656
running average episode reward sum: 0.5430113414770654
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'previousTarget': array([ 8., 12.]), 'currentState': array([ 7.62980383, 11.52303906,  0.        ]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.6037689441838343}
episode index:234
target Thresh 13.96389851866239
target distance 11.0
model initialize at round 234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([23., 10.,  0.]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.055385138137432}
done in step count: 15
reward sum = 0.8209419084392281
running average episode reward sum: 0.5441940247407342
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.89853942, 11.08533542,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.13257594864238426}
episode index:235
target Thresh 13.999934673458217
target distance 6.0
model initialize at round 235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([18.36561716, 24.        ,  0.        ]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 5.227559549249993}
done in step count: 4
reward sum = 0.951100185005976
running average episode reward sum: 0.5459182033859259
{'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([15.25809503, 28.06662172,  0.        ]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 0.26655486515039234}
episode index:236
target Thresh 14.035898827968737
target distance 6.0
model initialize at round 236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([21.,  6.,  0.]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 5.000000000000028}
done in step count: 2
reward sum = 0.9622794006099732
running average episode reward sum: 0.5476750016864493
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'previousTarget': array([25.,  3.]), 'currentState': array([25.,  2.,  0.]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.9999999999999427}
episode index:237
target Thresh 14.07179112605062
target distance 6.0
model initialize at round 237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([17.,  6.,  0.]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 5.656854249492408}
done in step count: 2
reward sum = 0.9622794006099732
running average episode reward sum: 0.549417036976044
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([21.,  2.,  0.]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 8.070336330495142e-14}
episode index:238
target Thresh 14.107611711273101
target distance 10.0
model initialize at round 238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.        , 13.03085375,  0.        ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 8.969146251678414}
done in step count: 13
reward sum = 0.8497170479566416
running average episode reward sum: 0.5506735223776364
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'previousTarget': array([12., 22.]), 'currentState': array([12.26048723, 21.89161541,  0.        ]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.28213616671111424}
episode index:239
target Thresh 14.143360726918573
target distance 2.0
model initialize at round 239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([12.59053433,  2.498712  ,  0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 1.6132564716930486}
done in step count: 2
reward sum = 0.9774842565069729
running average episode reward sum: 0.5524519004365087
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([12.7019189 ,  3.42004699,  0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 0.9105139436767972}
episode index:240
target Thresh 14.179038315983142
target distance 6.0
model initialize at round 240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([21.        ,  7.35242617,  0.        ]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 7.758236771843195}
done in step count: 19
reward sum = 0.7949951873018611
running average episode reward sum: 0.5534583041164479
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([16.95658594, 13.00894132,  0.        ]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 0.9920091140728758}
episode index:241
target Thresh 14.214644621177214
target distance 13.0
model initialize at round 241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([ 4.76324821, 15.        ,  0.        ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 11.225108399993694}
done in step count: 7
reward sum = 0.9049153580212299
running average episode reward sum: 0.5549106059920875
{'scaleFactor': 20, 'currentTarget': array([7., 4.]), 'previousTarget': array([7., 4.]), 'currentState': array([7.81853414, 3.4429822 , 0.        ]), 'targetState': array([7, 4], dtype=int32), 'currentDistance': 0.990084321950697}
episode index:242
target Thresh 14.250179784926058
target distance 7.0
model initialize at round 242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([18.62729645, 13.3956989 ,  0.        ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 8.676592549177272}
done in step count: 38
reward sum = 0.624346066156437
running average episode reward sum: 0.5551963486265088
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([13.06922865, 19.93566921,  0.        ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.09450426418488668}
episode index:243
target Thresh 14.285643949370375
target distance 8.0
model initialize at round 243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([ 3.53120744, 25.18757206,  0.        ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 6.471511448897806}
done in step count: 23
reward sum = 0.7599270808235725
running average episode reward sum: 0.5560354090043655
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([10.98677002, 25.87804472,  0.        ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 1.320862449327541}
episode index:244
target Thresh 14.321037256366868
target distance 9.0
model initialize at round 244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([ 5.        , 16.08665156,  0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 10.371462608479284}
done in step count: 18
reward sum = 0.7862831664596632
running average episode reward sum: 0.5569751957694892
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([10.49628568,  6.23816513,  0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.9092259602869611}
episode index:245
target Thresh 14.356359847488815
target distance 6.0
model initialize at round 245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([18.32034528, 18.01410949,  0.        ]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 4.026871753140971}
done in step count: 23
reward sum = 0.7440427816956228
running average episode reward sum: 0.5577356331106523
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'previousTarget': array([18., 14.]), 'currentState': array([18.42956284, 13.20319736,  0.        ]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 0.9052174754617971}
episode index:246
target Thresh 14.391611864026629
target distance 14.0
model initialize at round 246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 7., 16.,  0.]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 12.649110640673545}
done in step count: 99
reward sum = -0.15733900026273193
running average episode reward sum: 0.5548405941091407
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([11.41326473,  3.24391713,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 8.44717021671384}
episode index:247
target Thresh 14.42679344698842
target distance 9.0
model initialize at round 247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([27., 18.,  0.]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 7.28010988928053}
done in step count: 11
reward sum = 0.8690913549581178
running average episode reward sum: 0.5561077342738543
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'previousTarget': array([25., 11.]), 'currentState': array([25.15185224, 10.35429198,  0.        ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.6633234121087658}
episode index:248
target Thresh 14.461904737100571
target distance 11.0
model initialize at round 248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 2.84816301, 10.58086324,  0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 9.489302703964674}
done in step count: 36
reward sum = 0.6224753084099144
running average episode reward sum: 0.5563742707161677
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.25866401, 19.65263711,  0.        ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.4330912715747498}
episode index:249
target Thresh 14.496945874808283
target distance 9.0
model initialize at round 249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.33140349, 21.        ,  0.        ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 7.031857598671534}
done in step count: 17
reward sum = 0.8116906777601949
running average episode reward sum: 0.5573955363443439
{'scaleFactor': 20, 'currentTarget': array([19., 14.]), 'previousTarget': array([19., 14.]), 'currentState': array([18.08583456, 14.99748451,  0.        ]), 'targetState': array([19, 14], dtype=int32), 'currentDistance': 1.353023940911087}
episode index:250
target Thresh 14.531917000276156
target distance 13.0
model initialize at round 250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([10.33710456, 25.02802053,  0.        ]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 11.662929101179595}
done in step count: 77
reward sum = 0.3496440731185082
running average episode reward sum: 0.556567841271731
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([22.37660965, 24.09825735,  0.        ]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.9772280362556662}
episode index:251
target Thresh 14.566818253388739
target distance 10.0
model initialize at round 251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([12.94103742, 19.48140502,  0.        ]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 10.919795122202089}
done in step count: 11
reward sum = 0.8703214781277918
running average episode reward sum: 0.5578128953862391
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.07267487, 24.00196838,  0.        ]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.07270152223744202}
episode index:252
target Thresh 14.601649773751095
target distance 9.0
model initialize at round 252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([15.94535136, 10.93692446,  0.        ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 8.06326073476883}
done in step count: 16
reward sum = 0.8258264594739969
running average episode reward sum: 0.5588722375367836
{'scaleFactor': 20, 'currentTarget': array([16., 19.]), 'previousTarget': array([16., 19.]), 'currentState': array([16.44700593, 18.63200796,  0.        ]), 'targetState': array([16, 19], dtype=int32), 'currentDistance': 0.5789926107923035}
episode index:253
target Thresh 14.636411700689347
target distance 14.0
model initialize at round 253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([12.4778595,  5.       ,  0.       ]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 12.506217402651306}
done in step count: 78
reward sum = 0.31817616318004227
running average episode reward sum: 0.5579246151967965
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([16.40932826, 16.08422711,  0.        ]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 1.003090029562226}
episode index:254
target Thresh 14.671104173251251
target distance 2.0
model initialize at round 254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([27., 23.,  0.]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.9999999999999822}
done in step count: 0
reward sum = 0.9940000002053713
running average episode reward sum: 0.5596347147458497
{'scaleFactor': 20, 'currentTarget': array([27., 24.]), 'previousTarget': array([27., 24.]), 'currentState': array([27., 23.,  0.]), 'targetState': array([27, 24], dtype=int32), 'currentDistance': 0.9999999999999822}
episode index:255
target Thresh 14.705727330206741
target distance 6.0
model initialize at round 255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([21.48324752,  3.91334224,  0.        ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 6.264776605704982}
done in step count: 12
reward sum = 0.8574221597242812
running average episode reward sum: 0.5607979469527967
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'previousTarget': array([20., 10.]), 'currentState': array([19.57408363, 10.95796325,  0.        ]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 1.0483789142979063}
episode index:256
target Thresh 14.740281310048495
target distance 4.0
model initialize at round 256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 5.02619884, 29.86775028,  0.        ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 2.2041942457664434}
done in step count: 14
reward sum = 0.8472421736490221
running average episode reward sum: 0.5619125159282684
{'scaleFactor': 20, 'currentTarget': array([ 3., 29.]), 'previousTarget': array([ 3., 29.]), 'currentState': array([ 3.92186424, 28.90757833,  0.        ]), 'targetState': array([ 3, 29], dtype=int32), 'currentDistance': 0.9264855344925221}
episode index:257
target Thresh 14.774766250992474
target distance 9.0
model initialize at round 257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([18.73482114, 10.59223199,  0.        ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 7.412512848795619}
done in step count: 7
reward sum = 0.9195893543789604
running average episode reward sum: 0.5632988602633486
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'previousTarget': array([19., 18.]), 'currentState': array([19.27756795, 18.33899802,  0.        ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 0.43813653787198126}
episode index:258
target Thresh 14.809182290978491
target distance 14.0
model initialize at round 258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 6.        , 21.02899671,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.676622647739075}
done in step count: 6
reward sum = 0.9092620818538933
running average episode reward sum: 0.564634625597675
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.1543436 , 9.09991746, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8515387508005287}
episode index:259
target Thresh 14.84352956767075
target distance 2.0
model initialize at round 259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([16.02054834, 26.30234593,  0.        ]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 1.2025168382868145}
done in step count: 0
reward sum = 0.9988538198586147
running average episode reward sum: 0.5663046994217555
{'scaleFactor': 20, 'currentTarget': array([17., 27.]), 'previousTarget': array([17., 27.]), 'currentState': array([16.02054834, 26.30234593,  0.        ]), 'targetState': array([17, 27], dtype=int32), 'currentDistance': 1.2025168382868145}
episode index:260
target Thresh 14.877808218458402
target distance 7.0
model initialize at round 260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([27.85491939, 13.01503897,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 5.087386665542347}
done in step count: 3
reward sum = 0.9549024013541861
running average episode reward sum: 0.5677935795057879
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([27.88646273,  7.270889  ,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 1.1477887522995167}
episode index:261
target Thresh 14.912018380456104
target distance 6.0
model initialize at round 261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([21.        , 10.23231173,  0.        ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 5.170188727860354}
done in step count: 45
reward sum = 0.5558461241658846
running average episode reward sum: 0.5677479785312082
{'scaleFactor': 20, 'currentTarget': array([19., 15.]), 'previousTarget': array([19., 15.]), 'currentState': array([19.85910544, 14.03142381,  0.        ]), 'targetState': array([19, 15], dtype=int32), 'currentDistance': 1.2946821984408632}
episode index:262
target Thresh 14.94616019050454
target distance 1.0
model initialize at round 262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([19.52527899, 26.25522316,  0.        ]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.8832058595953236}
done in step count: 0
reward sum = 0.9975477953387437
running average episode reward sum: 0.5693821983669782
{'scaleFactor': 20, 'currentTarget': array([20., 27.]), 'previousTarget': array([20., 27.]), 'currentState': array([19.52527899, 26.25522316,  0.        ]), 'targetState': array([20, 27], dtype=int32), 'currentDistance': 0.8832058595953236}
episode index:263
target Thresh 14.980233785170999
target distance 7.0
model initialize at round 263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([19.19731605, 21.20278573,  0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 9.719449129662232}
done in step count: 25
reward sum = 0.7443259131696421
running average episode reward sum: 0.5700448639533519
{'scaleFactor': 20, 'currentTarget': array([25., 29.]), 'previousTarget': array([25., 29.]), 'currentState': array([25.00441039, 29.54078245,  0.        ]), 'targetState': array([25, 29], dtype=int32), 'currentDistance': 0.5408004349840748}
episode index:264
target Thresh 15.014239300749907
target distance 9.0
model initialize at round 264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([21.28712404, 12.3329767 ,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 8.431796306394897}
done in step count: 36
reward sum = 0.6283482150488009
running average episode reward sum: 0.570264876598995
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([20.84021711,  3.20358748,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 1.1576863564878679}
episode index:265
target Thresh 15.048176873263369
target distance 15.0
model initialize at round 265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([18.72340977, 11.        ,  0.        ]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 13.759795629587146}
done in step count: 72
reward sum = 0.35746003213876576
running average episode reward sum: 0.5694648583867385
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'previousTarget': array([ 5., 12.]), 'currentState': array([ 5.82228984, 12.2457812 ,  0.        ]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 0.8582359694165074}
episode index:266
target Thresh 15.082046638461726
target distance 3.0
model initialize at round 266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 3.07019755, 11.        ,  0.        ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 1.464692049972986}
done in step count: 1
reward sum = 0.9843842552722517
running average episode reward sum: 0.5710188636185195
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'previousTarget': array([ 2., 12.]), 'currentState': array([ 1.48374912, 12.00054634,  0.        ]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.516251167443942}
episode index:267
target Thresh 15.115848731824073
target distance 11.0
model initialize at round 267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([17.93546236,  9.        ,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 9.1195308380692}
done in step count: 17
reward sum = 0.8120024763117406
running average episode reward sum: 0.5719180562031957
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'previousTarget': array([27.,  8.]), 'currentState': array([26.19542444,  8.01248407,  0.        ]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.8046724102572624}
episode index:268
target Thresh 15.14958328855884
target distance 14.0
model initialize at round 268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([22.,  8.,  0.]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 13.000000000000025}
done in step count: 15
reward sum = 0.8156009159899988
running average episode reward sum: 0.5728239404403213
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([16.84001428, 19.14223528,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 0.8725570167091247}
episode index:269
target Thresh 15.183250443604294
target distance 15.0
model initialize at round 269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([ 7.17443693, 17.22793698,  0.        ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 14.733265884454026}
done in step count: 11
reward sum = 0.8589324097368729
running average episode reward sum: 0.5738836014377159
{'scaleFactor': 20, 'currentTarget': array([11.,  3.]), 'previousTarget': array([11.,  3.]), 'currentState': array([10.05033398,  2.18644649,  0.        ]), 'targetState': array([11,  3], dtype=int32), 'currentDistance': 1.2504938515514858}
episode index:270
target Thresh 15.216850331629097
target distance 7.0
model initialize at round 270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 3.33852065, 21.        ,  0.        ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 5.01144652050014}
done in step count: 9
reward sum = 0.8848267739512651
running average episode reward sum: 0.5750309932182087
{'scaleFactor': 20, 'currentTarget': array([ 3., 26.]), 'previousTarget': array([ 3., 26.]), 'currentState': array([ 2.93145143, 25.58139624,  0.        ]), 'targetState': array([ 3, 26], dtype=int32), 'currentDistance': 0.4241792210394365}
episode index:271
target Thresh 15.250383087032851
target distance 8.0
model initialize at round 271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([23.84564602, 17.88656187,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 10.439295758203555}
done in step count: 24
reward sum = 0.7236017423733234
running average episode reward sum: 0.5755772092077496
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.41223103, 10.11219358,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0647406237480386}
episode index:272
target Thresh 15.283848843946618
target distance 3.0
model initialize at round 272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.33072114, 22.90781713,  0.        ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 2.0218062162909676}
done in step count: 2
reward sum = 0.9759097792804949
running average episode reward sum: 0.5770436288783457
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([16.4185276 , 21.00576723,  0.        ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.5815009967298095}
episode index:273
target Thresh 15.317247736233476
target distance 15.0
model initialize at round 273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([13.90739059,  4.95767915,  0.        ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 17.121568751513514}
done in step count: 7
reward sum = 0.8916708965584667
running average episode reward sum: 0.5781919035779082
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'previousTarget': array([25., 18.]), 'currentState': array([25.49097967, 18.95767915,  0.        ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 1.0762018383724292}
episode index:274
target Thresh 15.350579897489034
target distance 13.0
model initialize at round 274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([15., 17.,  0.]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 11.045361017187282}
done in step count: 26
reward sum = 0.7021444697529178
running average episode reward sum: 0.5786426401821809
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'previousTarget': array([ 4., 18.]), 'currentState': array([ 4.77247192, 18.56447208,  0.        ]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 0.9567348625483199}
episode index:275
target Thresh 15.383845461041982
target distance 13.0
model initialize at round 275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([16.79643106, 13.55026281,  0.        ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 13.935311560833155}
done in step count: 25
reward sum = 0.7381673820558655
running average episode reward sum: 0.5792206283773755
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([8.09092876, 1.13451717, 0.        ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 1.2551777001743165}
episode index:276
target Thresh 15.417044559954622
target distance 14.0
model initialize at round 276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([13., 16.,  0.]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 12.649110640673543}
done in step count: 12
reward sum = 0.8508985880990723
running average episode reward sum: 0.5802014152355767
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([9.00710934, 3.69739934, 0.        ]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.3026841581665044}
episode index:277
target Thresh 15.450177327023386
target distance 15.0
model initialize at round 277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([17.        , 13.37004471,  0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 13.015254268342279}
done in step count: 11
reward sum = 0.8727413942926324
running average episode reward sum: 0.5812537173185157
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.92002696, 14.3863593 ,  0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.997859266309537}
episode index:278
target Thresh 15.483243894779399
target distance 6.0
model initialize at round 278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([17.54939163, 25.56782055,  0.        ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 4.600740780366555}
done in step count: 2
reward sum = 0.9710554434938595
running average episode reward sum: 0.5826508561220115
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'previousTarget': array([17., 21.]), 'currentState': array([17.58337496, 21.56782055,  0.        ]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 0.8140924532344849}
episode index:279
target Thresh 15.516244395488965
target distance 14.0
model initialize at round 279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([13.75671101,  3.53266644,  0.        ]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 13.689671484641499}
done in step count: 11
reward sum = 0.870916363904128
running average episode reward sum: 0.583680375792662
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'previousTarget': array([27.,  7.]), 'currentState': array([26.04097164,  6.69097751,  0.        ]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 1.0075863727040955}
episode index:280
target Thresh 15.54917896115413
target distance 2.0
model initialize at round 280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 5.87560517, 18.40909791,  0.        ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.4275922995601624}
done in step count: 0
reward sum = 0.9975267596863302
running average episode reward sum: 0.5851531387246679
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'previousTarget': array([ 6., 18.]), 'currentState': array([ 5.87560517, 18.40909791,  0.        ]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 0.4275922995601624}
episode index:281
target Thresh 15.582047723513213
target distance 10.0
model initialize at round 281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([11.85105705, 10.        ,  0.        ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 9.911350494908616}
done in step count: 11
reward sum = 0.8622534116584185
running average episode reward sum: 0.5861357638059933
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([6.44911486, 1.40280639, 0.        ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.7472244430081088}
episode index:282
target Thresh 15.614850814041294
target distance 6.0
model initialize at round 282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([ 5.82603467, 17.76643109,  0.        ]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 4.734006408765733}
done in step count: 11
reward sum = 0.8664021438620348
running average episode reward sum: 0.5871261043715624
{'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'previousTarget': array([10., 20.]), 'currentState': array([ 9.13784558, 19.26186371,  0.        ]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 1.1349693546461197}
episode index:283
target Thresh 15.647588363950785
target distance 11.0
model initialize at round 283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([14.        , 26.78299713,  0.        ]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 12.575414053193702}
done in step count: 5
reward sum = 0.9227815433488358
running average episode reward sum: 0.588307989720074
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'previousTarget': array([ 5., 18.]), 'currentState': array([ 4.00233471, 18.82189846,  0.        ]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 1.2926148313420047}
episode index:284
target Thresh 15.680260504191931
target distance 11.0
model initialize at round 284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([11.        , 15.94836593,  0.        ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 9.106705188824048}
done in step count: 41
reward sum = 0.5577840168446311
running average episode reward sum: 0.5882008880608619
{'scaleFactor': 20, 'currentTarget': array([10., 25.]), 'previousTarget': array([10., 25.]), 'currentState': array([10.89833428, 24.606031  ,  0.        ]), 'targetState': array([10, 25], dtype=int32), 'currentDistance': 0.980926118768952}
episode index:285
target Thresh 15.71286736545333
target distance 2.0
model initialize at round 285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([18.44952023,  7.        ,  0.        ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 0.5504797697067083}
done in step count: 0
reward sum = 0.9951970094500423
running average episode reward sum: 0.5896239514223626
{'scaleFactor': 20, 'currentTarget': array([19.,  7.]), 'previousTarget': array([19.,  7.]), 'currentState': array([18.44952023,  7.        ,  0.        ]), 'targetState': array([19,  7], dtype=int32), 'currentDistance': 0.5504797697067083}
episode index:286
target Thresh 15.745409078162478
target distance 5.0
model initialize at round 286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 5.00448138, 25.        ,  0.        ]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 3.6080390219056313}
done in step count: 2
reward sum = 0.967756857740756
running average episode reward sum: 0.5909414876813117
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 2.31219596, 29.        ,  0.        ]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 1.2137027650829593}
episode index:287
target Thresh 15.777885772486265
target distance 15.0
model initialize at round 287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6.37889464, 2.35363499]), 'currentState': array([19.        , 15.49664485,  0.        ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 18.739248179415682}
done in step count: 7
reward sum = 0.8904565503658046
running average episode reward sum: 0.591981470537855
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([5.61327136, 1.58592165, 0.        ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.5665862055504478}
episode index:288
target Thresh 15.810297578331515
target distance 6.0
model initialize at round 288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([17.92470336, 26.        ,  0.        ]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 5.895645510715185}
done in step count: 37
reward sum = 0.6294651730100915
running average episode reward sum: 0.5921111719304925
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'previousTarget': array([23., 23.]), 'currentState': array([23.7998502 , 22.71607333,  0.        ]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.8487488980740795}
episode index:289
target Thresh 15.842644625345486
target distance 14.0
model initialize at round 289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([18.54224443, 23.91545081,  0.        ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 12.542529410627017}
done in step count: 9
reward sum = 0.8820635407209926
running average episode reward sum: 0.5931110076849424
{'scaleFactor': 20, 'currentTarget': array([ 6., 24.]), 'previousTarget': array([ 6., 24.]), 'currentState': array([ 5.30316854, 24.9340924 ,  0.        ]), 'targetState': array([ 6, 24], dtype=int32), 'currentDistance': 1.1653766372381587}
episode index:290
target Thresh 15.874927042916422
target distance 9.0
model initialize at round 290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([15., 14.,  0.]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 7.615773105863934}
done in step count: 4
reward sum = 0.9409899191839275
running average episode reward sum: 0.5943064678619149
{'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'previousTarget': array([12.,  7.]), 'currentState': array([12.29807937,  7.96177518,  0.        ]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 1.0069075506835388}
episode index:291
target Thresh 15.907144960174026
target distance 8.0
model initialize at round 291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([21.        ,  3.01938009,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 8.586562425749811}
done in step count: 23
reward sum = 0.7477119156412888
running average episode reward sum: 0.594831828984447
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.30013362,  9.56789359,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.526114182439291}
episode index:292
target Thresh 15.93929850599001
target distance 9.0
model initialize at round 292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([12.66191816, 21.62598085,  0.        ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 7.364733333375332}
done in step count: 6
reward sum = 0.9277212497975725
running average episode reward sum: 0.5959679703524099
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'previousTarget': array([20., 21.]), 'currentState': array([20.04801184, 21.81964156,  0.        ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 0.8210465419932567}
episode index:293
target Thresh 15.971387808978612
target distance 5.0
model initialize at round 293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([5.        , 3.69792676, 0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 3.0801139202849526}
done in step count: 5
reward sum = 0.932844813936038
running average episode reward sum: 0.5971138099564358
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([8.14507887, 2.94673717, 0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.15454710246949108}
episode index:294
target Thresh 16.003412997497072
target distance 12.0
model initialize at round 294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([10.43014324, 16.90155685,  0.        ]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 13.355990000346804}
done in step count: 48
reward sum = 0.5196060184523079
running average episode reward sum: 0.5968510716801507
{'scaleFactor': 20, 'currentTarget': array([ 3., 28.]), 'previousTarget': array([ 3., 28.]), 'currentState': array([ 3.98562229, 28.96593538,  0.        ]), 'targetState': array([ 3, 28], dtype=int32), 'currentDistance': 1.3800298792373087}
episode index:295
target Thresh 16.035374199646196
target distance 12.0
model initialize at round 295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([24.49077749, 23.89693701,  0.        ]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 12.683588952484909}
done in step count: 18
reward sum = 0.789772046971084
running average episode reward sum: 0.5975028317318093
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'previousTarget': array([18., 13.]), 'currentState': array([17.43744354, 12.77778703,  0.        ]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.6048540116426859}
episode index:296
target Thresh 16.067271543270834
target distance 8.0
model initialize at round 296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([ 6.        , 27.13734102,  0.        ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 6.770739153992506}
done in step count: 9
reward sum = 0.8841933166280522
running average episode reward sum: 0.5984681195597428
{'scaleFactor': 20, 'currentTarget': array([12., 24.]), 'previousTarget': array([12., 24.]), 'currentState': array([11.68273747, 23.52864045,  0.        ]), 'targetState': array([12, 24], dtype=int32), 'currentDistance': 0.5681860077682495}
episode index:297
target Thresh 16.099105155960398
target distance 16.0
model initialize at round 297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([10.1265043 , 17.80879843,  0.        ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 16.191097341409797}
done in step count: 36
reward sum = 0.6091612882942549
running average episode reward sum: 0.5985040026763015
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([26.77783583, 20.92419325,  0.        ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.7815211027086542}
episode index:298
target Thresh 16.13087516504939
target distance 5.0
model initialize at round 298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20., 17.,  0.]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 3.1622776601684017}
done in step count: 2
reward sum = 0.9701831642851104
running average episode reward sum: 0.5997470767953945
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([20.73009855, 14.26739025,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.3799267581109538}
episode index:299
target Thresh 16.162581697617874
target distance 5.0
model initialize at round 299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([17.75774741,  2.96911573,  0.        ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 3.3839898274431017}
done in step count: 7
reward sum = 0.9231573688525828
running average episode reward sum: 0.6008251111022518
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([20.73766255,  1.119939  ,  0.        ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.9183290783471348}
episode index:300
target Thresh 16.19422488049204
target distance 9.0
model initialize at round 300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([17.05839622, 22.01957846,  0.        ]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 8.584516854706425}
done in step count: 4
reward sum = 0.9397002352249941
running average episode reward sum: 0.6019509420794037
{'scaleFactor': 20, 'currentTarget': array([22., 15.]), 'previousTarget': array([22., 15.]), 'currentState': array([22.75244445, 14.66067481,  0.        ]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 0.8254176086911743}
episode index:301
target Thresh 16.22580484024465
target distance 16.0
model initialize at round 301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([22.67882258,  9.40925592]), 'currentState': array([11., 22.,  0.]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 19.104973174542828}
done in step count: 12
reward sum = 0.8351508626636263
running average episode reward sum: 0.602723127246901
{'scaleFactor': 20, 'currentTarget': array([24.,  8.]), 'previousTarget': array([24.,  8.]), 'currentState': array([24.21502101,  8.25206488,  0.        ]), 'targetState': array([24,  8], dtype=int32), 'currentDistance': 0.3313166795573311}
episode index:302
target Thresh 16.257321703195586
target distance 4.0
model initialize at round 302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([15.88700485, 18.        ,  0.        ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 2.0031894327309585}
done in step count: 16
reward sum = 0.8069053581085203
running average episode reward sum: 0.6033969959956192
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'previousTarget': array([16., 20.]), 'currentState': array([15.67514193, 20.58553374,  0.        ]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.6696137156048828}
episode index:303
target Thresh 16.288775595412353
target distance 8.0
model initialize at round 303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([11.       ,  4.7153275,  0.       ]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 6.042490664793124}
done in step count: 3
reward sum = 0.956023506442538
running average episode reward sum: 0.6045569516220893
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'previousTarget': array([5., 4.]), 'currentState': array([5.        , 3.78666055, 0.        ]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.2133394479751458}
episode index:304
target Thresh 16.320166642710547
target distance 5.0
model initialize at round 304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 4.04140127, 27.        ,  0.        ]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 4.2134672732327445}
done in step count: 4
reward sum = 0.9531787324352179
running average episode reward sum: 0.6056999738542636
{'scaleFactor': 20, 'currentTarget': array([ 7., 24.]), 'previousTarget': array([ 7., 24.]), 'currentState': array([ 6.2713691 , 24.95845354,  0.        ]), 'targetState': array([ 7, 24], dtype=int32), 'currentDistance': 1.2039668474373533}
episode index:305
target Thresh 16.351494970654407
target distance 14.0
model initialize at round 305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 8.39250588, 14.        ,  0.        ]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 12.236179322230901}
done in step count: 6
reward sum = 0.9138766233992226
running average episode reward sum: 0.6067070870880706
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 6.84024942, 26.        ,  0.        ]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 0.8402494192123271}
episode index:306
target Thresh 16.382760704557285
target distance 14.0
model initialize at round 306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([13.71413743, 10.        ,  0.        ]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 12.447586873248326}
done in step count: 33
reward sum = 0.6420760849828926
running average episode reward sum: 0.6068222955502688
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'previousTarget': array([26., 12.]), 'currentState': array([25.23224409, 11.24512964,  0.        ]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 1.0766979131068113}
episode index:307
target Thresh 16.413963969482158
target distance 3.0
model initialize at round 307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([25.        , 14.11722767,  0.        ]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 2.1318610727059832}
done in step count: 1
reward sum = 0.9830699622974755
running average episode reward sum: 0.6080438788838636
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'previousTarget': array([26., 16.]), 'currentState': array([27.       , 15.2482003,  0.       ]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 1.2510806500366551}
episode index:308
target Thresh 16.445104890242128
target distance 15.0
model initialize at round 308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([9.        , 9.03824043, 0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 13.041393453008583}
done in step count: 29
reward sum = 0.6982805876019299
running average episode reward sum: 0.6083359070674172
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([22.44097686,  7.20184918,  0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.9118691380373737}
episode index:309
target Thresh 16.47618359140092
target distance 7.0
model initialize at round 309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 2.41750917, 13.95679271,  0.        ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 5.076734710078427}
done in step count: 7
reward sum = 0.921754369766965
running average episode reward sum: 0.609346934366448
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'previousTarget': array([ 3., 19.]), 'currentState': array([ 2.27455848, 18.31793339,  0.        ]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.9957310139928822}
episode index:310
target Thresh 16.507200197273377
target distance 10.0
model initialize at round 310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([16.19634092, 15.36623955,  0.        ]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 8.204519233889776}
done in step count: 88
reward sum = 0.28416836013731006
running average episode reward sum: 0.6083013440956148
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'previousTarget': array([ 8., 15.]), 'currentState': array([ 8.0997708 , 14.53353478,  0.        ]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 0.477015738732363}
episode index:311
target Thresh 16.538154831925965
target distance 2.0
model initialize at round 311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 5.89223719, 20.323125  ,  0.        ]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 0.6853995791929904}
done in step count: 0
reward sum = 0.9969709586210299
running average episode reward sum: 0.6095470800396066
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'previousTarget': array([ 6., 21.]), 'currentState': array([ 5.89223719, 20.323125  ,  0.        ]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 0.6853995791929904}
episode index:312
target Thresh 16.569047619177265
target distance 12.0
model initialize at round 312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([17., 14.,  0.]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 10.00000000000002}
done in step count: 69
reward sum = 0.3662030479521152
running average episode reward sum: 0.608769623068081
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'previousTarget': array([17.,  4.]), 'currentState': array([16.36104096,  3.15105865,  0.        ]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 1.062530033010341}
episode index:313
target Thresh 16.599878682598472
target distance 11.0
model initialize at round 313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([17.        ,  5.73760343,  0.        ]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 13.012178274311548}
done in step count: 18
reward sum = 0.7813582404083084
running average episode reward sum: 0.6093192683462346
{'scaleFactor': 20, 'currentTarget': array([25., 16.]), 'previousTarget': array([25., 16.]), 'currentState': array([25.01733834, 15.98800813,  0.        ]), 'targetState': array([25, 16], dtype=int32), 'currentDistance': 0.021081340973364435}
episode index:314
target Thresh 16.630648145513867
target distance 8.0
model initialize at round 314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([22.92402089, 25.        ,  0.        ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 6.000481049524315}
done in step count: 3
reward sum = 0.9563740294478068
running average episode reward sum: 0.6104210294925888
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([23.23789573, 19.12413037,  0.        ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.2683332355810175}
episode index:315
target Thresh 16.661356131001362
target distance 9.0
model initialize at round 315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([25.68450332, 17.79166687,  0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 8.313077912104186}
done in step count: 6
reward sum = 0.9255409472954121
running average episode reward sum: 0.6114182444223446
{'scaleFactor': 20, 'currentTarget': array([27., 26.]), 'previousTarget': array([27., 26.]), 'currentState': array([26.32701135, 26.92916048,  0.        ]), 'targetState': array([27, 26], dtype=int32), 'currentDistance': 1.1472806618142761}
episode index:316
target Thresh 16.692002761892923
target distance 15.0
model initialize at round 316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 13.038404810405316}
done in step count: 65
reward sum = 0.42171449660678595
running average episode reward sum: 0.6108198098866489
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'previousTarget': array([13., 19.]), 'currentState': array([13.33618085, 18.17768345,  0.        ]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 0.8883817140869341}
episode index:317
target Thresh 16.722588160775125
target distance 4.0
model initialize at round 317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 8.        , 12.61855686,  0.        ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 2.09346903141094}
done in step count: 3
reward sum = 0.9575146403960205
running average episode reward sum: 0.6119100452027161
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'previousTarget': array([10., 12.]), 'currentState': array([ 9.39112556, 11.61646345,  0.        ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.7196029232357052}
episode index:318
target Thresh 16.753112449989597
target distance 5.0
model initialize at round 318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([16.25011611, 18.09419751,  0.        ]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 4.835480856934855}
done in step count: 6
reward sum = 0.928865944325242
running average episode reward sum: 0.6129036373629748
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([20.55325377, 18.12411755,  0.        ]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.9832356048699735}
episode index:319
target Thresh 16.783575751633542
target distance 14.0
model initialize at round 319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([13.66191411, 12.2779634 ,  0.        ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 13.787123061152549}
done in step count: 36
reward sum = 0.6267243799567979
running average episode reward sum: 0.6129468271835805
{'scaleFactor': 20, 'currentTarget': array([15., 26.]), 'previousTarget': array([15., 26.]), 'currentState': array([14.96857467, 26.09318161,  0.        ]), 'targetState': array([15, 26], dtype=int32), 'currentDistance': 0.0983380070508406}
episode index:320
target Thresh 16.813978187560203
target distance 4.0
model initialize at round 320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([12.36355162,  6.        ,  0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 2.032773912165971}
done in step count: 1
reward sum = 0.9853432771685936
running average episode reward sum: 0.6141069407349357
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'previousTarget': array([12.,  4.]), 'currentState': array([12.80032507,  4.72275782,  0.        ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 1.0783779820053596}
episode index:321
target Thresh 16.844319879379363
target distance 4.0
model initialize at round 321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 7.        , 22.07203817,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 2.001296954094169}
done in step count: 40
reward sum = 0.5946832286949337
running average episode reward sum: 0.6140466186478549
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 8.89656567, 22.49713142,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.5077778191476701}
episode index:322
target Thresh 16.87460094845784
target distance 5.0
model initialize at round 322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([19., 23.,  0.]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 3.1622776601684017}
done in step count: 2
reward sum = 0.9660957592345084
running average episode reward sum: 0.6151365540676279
{'scaleFactor': 20, 'currentTarget': array([18., 26.]), 'previousTarget': array([18., 26.]), 'currentState': array([17.0768463 , 26.67400694,  0.        ]), 'targetState': array([18, 26], dtype=int32), 'currentDistance': 1.143021480412037}
episode index:323
target Thresh 16.90482151591994
target distance 9.0
model initialize at round 323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([15.58455169, 10.        ,  0.        ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 7.141673047164427}
done in step count: 4
reward sum = 0.9420548323888019
running average episode reward sum: 0.6161455610994834
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([16.38348895, 16.09407711,  0.        ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 1.0958020592392705}
episode index:324
target Thresh 16.934981702647978
target distance 4.0
model initialize at round 324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([18., 19.,  0.]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 2.0000000000000178}
done in step count: 5
reward sum = 0.9349022896651019
running average episode reward sum: 0.6171263510335314
{'scaleFactor': 20, 'currentTarget': array([18., 17.]), 'previousTarget': array([18., 17.]), 'currentState': array([18.64148368, 17.92614317,  0.        ]), 'targetState': array([18, 17], dtype=int32), 'currentDistance': 1.1266066222008255}
episode index:325
target Thresh 16.965081629282736
target distance 7.0
model initialize at round 325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([26.15383172,  4.        ,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 5.0023658600892755}
done in step count: 7
reward sum = 0.9083850300471555
running average episode reward sum: 0.618019782564248
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([26.29078742,  8.10168975,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.9442026401155635}
episode index:326
target Thresh 16.995121416223967
target distance 11.0
model initialize at round 326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([17.,  3.,  0.]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 9.05538513813744}
done in step count: 8
reward sum = 0.8928908908454878
running average episode reward sum: 0.6188603669932426
{'scaleFactor': 20, 'currentTarget': array([26.,  2.]), 'previousTarget': array([26.,  2.]), 'currentState': array([26.729453 ,  2.5076508,  0.       ]), 'targetState': array([26,  2], dtype=int32), 'currentDistance': 0.8887131200829262}
episode index:327
target Thresh 17.02510118363086
target distance 7.0
model initialize at round 327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([26.94526219, 15.40270996,  0.        ]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 7.085496607399697}
done in step count: 17
reward sum = 0.8126771555977838
running average episode reward sum: 0.6194512718365492
{'scaleFactor': 20, 'currentTarget': array([20., 14.]), 'previousTarget': array([20., 14.]), 'currentState': array([20.14002853, 14.99869177,  0.        ]), 'targetState': array([20, 14], dtype=int32), 'currentDistance': 1.0084608296537263}
episode index:328
target Thresh 17.055021051422514
target distance 9.0
model initialize at round 328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([ 9., 14.,  0.]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 7.615773105863932}
done in step count: 23
reward sum = 0.7513753887910353
running average episode reward sum: 0.6198522569944657
{'scaleFactor': 20, 'currentTarget': array([16., 17.]), 'previousTarget': array([16., 17.]), 'currentState': array([16.13084149, 17.52691324,  0.        ]), 'targetState': array([16, 17], dtype=int32), 'currentDistance': 0.5429153336549986}
episode index:329
target Thresh 17.084881139278455
target distance 13.0
model initialize at round 329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([16.        , 18.94335383,  0.        ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 11.000145853050123}
done in step count: 7
reward sum = 0.906581265014555
running average episode reward sum: 0.6207211327763448
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'previousTarget': array([ 5., 19.]), 'currentState': array([ 4.74010056, 18.94036692,  0.        ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 0.26665299801620257}
episode index:330
target Thresh 17.114681566639064
target distance 17.0
model initialize at round 330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([18.33935727, 16.53366395]), 'currentState': array([4.        , 4.86540008, 0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 19.293742905866267}
done in step count: 8
reward sum = 0.8786140678563189
running average episode reward sum: 0.6215002655107253
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([20.        , 16.50383234,  0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 1.1163253766671848}
episode index:331
target Thresh 17.144422452706088
target distance 12.0
model initialize at round 331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([ 7., 13.,  0.]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 10.049875621120906}
done in step count: 99
reward sum = -0.13048994882711432
running average episode reward sum: 0.6192352347446475
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'previousTarget': array([6., 3.]), 'currentState': array([6.43799032, 1.09113939, 0.        ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 1.9584647904878638}
episode index:332
target Thresh 17.174103916443116
target distance 10.0
model initialize at round 332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([25.        ,  7.58836389,  0.        ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 11.79938298391489}
done in step count: 22
reward sum = 0.7599549940181848
running average episode reward sum: 0.6196578166043278
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'previousTarget': array([22., 19.]), 'currentState': array([21.84037301, 18.46753967,  0.        ]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.5558729901508596}
episode index:333
target Thresh 17.203726076576046
target distance 8.0
model initialize at round 333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([16.        , 26.66467822,  0.        ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 6.009362753140978}
done in step count: 4
reward sum = 0.9386344069656
running average episode reward sum: 0.6206128363359484
{'scaleFactor': 20, 'currentTarget': array([22., 27.]), 'previousTarget': array([22., 27.]), 'currentState': array([22.97561705, 26.7526027 ,  0.        ]), 'targetState': array([22, 27], dtype=int32), 'currentDistance': 1.0064959295815692}
episode index:334
target Thresh 17.23328905159355
target distance 3.0
model initialize at round 334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 4.37518573, 14.        ,  0.        ]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 1.3751857280731512}
done in step count: 9
reward sum = 0.902272217075338
running average episode reward sum: 0.6214536106068123
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.83419436, 14.60540777,  0.        ]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 1.0307273195312139}
episode index:335
target Thresh 17.26279295974758
target distance 9.0
model initialize at round 335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([15.       , 18.4404819,  0.       ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 8.289624811450059}
done in step count: 8
reward sum = 0.893660961047435
running average episode reward sum: 0.6222637515307426
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.19387501, 13.63260853,  0.        ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 0.8858972750194433}
episode index:336
target Thresh 17.29223791905379
target distance 4.0
model initialize at round 336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([14.74438298, 25.03356218,  0.        ]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 2.049564640569573}
done in step count: 84
reward sum = 0.3290251794102935
running average episode reward sum: 0.6213936073998214
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([14.19823781, 23.31443418,  0.        ]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.8612151116680238}
episode index:337
target Thresh 17.32162404729207
target distance 5.0
model initialize at round 337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([11., 20.,  0.]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 3.0000000000000195}
done in step count: 2
reward sum = 0.9686514633413786
running average episode reward sum: 0.6224209975061574
{'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'previousTarget': array([ 8., 20.]), 'currentState': array([ 7.47876167, 19.74623406,  0.        ]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 0.5797297182338501}
episode index:338
target Thresh 17.350951462006968
target distance 5.0
model initialize at round 338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([21.        , 11.26654184,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 4.435120695496859}
done in step count: 13
reward sum = 0.853556973265103
running average episode reward sum: 0.6231028145437945
{'scaleFactor': 20, 'currentTarget': array([18.,  8.]), 'previousTarget': array([18.,  8.]), 'currentState': array([17.94975549,  8.1272893 ,  0.        ]), 'targetState': array([18,  8], dtype=int32), 'currentDistance': 0.1368469052417263}
episode index:339
target Thresh 17.380220280508176
target distance 13.0
model initialize at round 339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([15.78978193, 16.02747893,  0.        ]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 11.998542252525914}
done in step count: 9
reward sum = 0.887399491203845
running average episode reward sum: 0.6238801577104417
{'scaleFactor': 20, 'currentTarget': array([15., 28.]), 'previousTarget': array([15., 28.]), 'currentState': array([14.85517269, 28.87542319,  0.        ]), 'targetState': array([15, 28], dtype=int32), 'currentDistance': 0.8873222162467815}
episode index:340
target Thresh 17.40943061987102
target distance 10.0
model initialize at round 340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([22.13776314, 24.        ,  0.        ]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 11.303726886968285}
done in step count: 38
reward sum = 0.6336254650382817
running average episode reward sum: 0.6239087363243063
{'scaleFactor': 20, 'currentTarget': array([12., 29.]), 'previousTarget': array([12., 29.]), 'currentState': array([12.7976133 , 29.86129619,  0.        ]), 'targetState': array([12, 29], dtype=int32), 'currentDistance': 1.17389015938627}
episode index:341
target Thresh 17.438582596936886
target distance 10.0
model initialize at round 341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([17.        , 11.21518028,  0.        ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 9.042551895043827}
done in step count: 4
reward sum = 0.9398074742854285
running average episode reward sum: 0.6248324168446605
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'previousTarget': array([25.,  7.]), 'currentState': array([24.93424046,  7.02353549,  0.        ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.0698443722871945}
episode index:342
target Thresh 17.467676328313726
target distance 13.0
model initialize at round 342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([16.60791337, 21.67704237,  0.        ]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 15.305245632602276}
done in step count: 23
reward sum = 0.7475200735003371
running average episode reward sum: 0.6251901068057557
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'previousTarget': array([ 4., 13.]), 'currentState': array([ 3.43163037, 12.10789785,  0.        ]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 1.0577761023456718}
episode index:343
target Thresh 17.496711930376506
target distance 14.0
model initialize at round 343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([12.52948093, 16.52643144,  0.        ]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 17.69456581481}
done in step count: 45
reward sum = 0.5562049393581224
running average episode reward sum: 0.6249895685282917
{'scaleFactor': 20, 'currentTarget': array([26., 28.]), 'previousTarget': array([26., 28.]), 'currentState': array([26.25471408, 27.47262501,  0.        ]), 'targetState': array([26, 28], dtype=int32), 'currentDistance': 0.5856651266276827}
episode index:344
target Thresh 17.52568951926766
target distance 13.0
model initialize at round 344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 6., 16.,  0.]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.180339887498972}
done in step count: 16
reward sum = 0.8128949219265849
running average episode reward sum: 0.6255342217265476
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.49774516, 5.53813046, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.7330311282090045}
episode index:345
target Thresh 17.5546092108976
target distance 3.0
model initialize at round 345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([21.00060225,  5.83497202,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 2.8349720879327482}
done in step count: 5
reward sum = 0.9457606730929459
running average episode reward sum: 0.626459731701595
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([21.23216296,  3.78832775,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 0.8218030706590158}
episode index:346
target Thresh 17.58347112094512
target distance 12.0
model initialize at round 346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([11., 16.,  0.]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 10.77032961426903}
done in step count: 75
reward sum = 0.3243078490153994
running average episode reward sum: 0.6255889769964474
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'previousTarget': array([21., 20.]), 'currentState': array([21.40335468, 20.20860086,  0.        ]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.4541027621214936}
episode index:347
target Thresh 17.612275364857904
target distance 15.0
model initialize at round 347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([10., 11.,  0.]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 14.317821063276376}
done in step count: 8
reward sum = 0.8857152695278748
running average episode reward sum: 0.6263364663428022
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([16.98965651, 24.89203125,  0.        ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 1.3323437090311923}
episode index:348
target Thresh 17.641022057852965
target distance 13.0
model initialize at round 348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([4.51566339, 6.90404689, 0.        ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 14.650131412386918}
done in step count: 27
reward sum = 0.7025709077088461
running average episode reward sum: 0.6265549031375472
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([16.92010486, 15.84571788,  0.        ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.9329501214666869}
episode index:349
target Thresh 17.66971131491711
target distance 12.0
model initialize at round 349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([19.45429611, 12.838727  ,  0.        ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 10.171423400453781}
done in step count: 12
reward sum = 0.8487607763906186
running average episode reward sum: 0.6271897770611274
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'previousTarget': array([19., 23.]), 'currentState': array([19.27161655, 23.83091378,  0.        ]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 0.874181481516847}
episode index:350
target Thresh 17.698343250807405
target distance 8.0
model initialize at round 350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 8.        , 21.88269579,  0.        ]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 8.568629455604638}
done in step count: 4
reward sum = 0.939719545202427
running average episode reward sum: 0.6280801752609602
{'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 1.34445644, 27.12676799,  0.        ]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 1.0919118594557584}
episode index:351
target Thresh 17.72691798005164
target distance 7.0
model initialize at round 351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([13.15641606, 28.39590126,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 5.191681986900586}
done in step count: 6
reward sum = 0.9243052978360471
running average episode reward sum: 0.6289217239046394
{'scaleFactor': 20, 'currentTarget': array([ 8., 29.]), 'previousTarget': array([ 8., 29.]), 'currentState': array([ 7.20126784, 28.24242261,  0.        ]), 'targetState': array([ 8, 29], dtype=int32), 'currentDistance': 1.1008617383207582}
episode index:352
target Thresh 17.75543561694876
target distance 11.0
model initialize at round 352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([25., 14.,  0.]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 10.295630140987026}
done in step count: 22
reward sum = 0.7532523357019073
running average episode reward sum: 0.6292739352695043
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.7636878 ,  8.87740043,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7734660416207051}
episode index:353
target Thresh 17.783896275569354
target distance 16.0
model initialize at round 353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6., 25.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 14.035668847618219}
done in step count: 54
reward sum = 0.49849226731236984
running average episode reward sum: 0.6289044955295124
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.41956612, 10.99262027,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.41963102019955406}
episode index:354
target Thresh 17.8123000697561
target distance 6.0
model initialize at round 354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 7., 10.,  0.]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 5.0000000000000275}
done in step count: 2
reward sum = 0.9655941017287284
running average episode reward sum: 0.6298529169554258
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.        , 12.46729022,  0.        ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.532709777355258}
episode index:355
target Thresh 17.840647113124206
target distance 15.0
model initialize at round 355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([18.        , 26.69572455,  0.        ]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 13.065264423559718}
done in step count: 62
reward sum = 0.41774408503964616
running average episode reward sum: 0.6292571056298196
{'scaleFactor': 20, 'currentTarget': array([ 5., 28.]), 'previousTarget': array([ 5., 28.]), 'currentState': array([ 4.22197168, 27.09049457,  0.        ]), 'targetState': array([ 5, 28], dtype=int32), 'currentDistance': 1.1968827009422458}
episode index:356
target Thresh 17.868937519061884
target distance 10.0
model initialize at round 356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([11.        , 17.17910504,  0.        ]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 9.871675528923625}
done in step count: 96
reward sum = 0.24290429506899278
running average episode reward sum: 0.6281748848719462
{'scaleFactor': 20, 'currentTarget': array([12., 27.]), 'previousTarget': array([12., 27.]), 'currentState': array([11.42939029, 27.94108788,  0.        ]), 'targetState': array([12, 27], dtype=int32), 'currentDistance': 1.1005643264794869}
episode index:357
target Thresh 17.897171400730798
target distance 13.0
model initialize at round 357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([17.46317255, 16.15127671,  0.        ]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 12.163129092942839}
done in step count: 31
reward sum = 0.6617761229172296
running average episode reward sum: 0.6282687430787766
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'previousTarget': array([18.,  4.]), 'currentState': array([18.53929579,  3.40883321,  0.        ]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 0.800198806908342}
episode index:358
target Thresh 17.92534887106651
target distance 14.0
model initialize at round 358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([25.90825677,  7.15733814,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 12.888982383059673}
done in step count: 16
reward sum = 0.8043648755550818
running average episode reward sum: 0.6287592615536409
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'previousTarget': array([27., 20.]), 'currentState': array([27.87665669, 19.81803389,  0.        ]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.8953427382139841}
episode index:359
target Thresh 17.953470042778942
target distance 13.0
model initialize at round 359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([14.86162806,  8.        ,  0.        ]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 12.02644699613026}
done in step count: 45
reward sum = 0.5303179137704763
running average episode reward sum: 0.6284858133653544
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'previousTarget': array([10., 19.]), 'currentState': array([ 9.54012892, 18.7469716 ,  0.        ]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 0.5248854980656861}
episode index:360
target Thresh 17.981535028352816
target distance 4.0
model initialize at round 360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([5.       , 9.9765265, 0.       ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 4.451152996203981}
done in step count: 64
reward sum = 0.42212008045173544
running average episode reward sum: 0.6279141631356768
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.82046665, 5.14260417, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.186715270219588}
episode index:361
target Thresh 18.00954394004811
target distance 8.0
model initialize at round 361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 3.50778794, 13.89512074,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 6.912666831800551}
done in step count: 6
reward sum = 0.9224433138443842
running average episode reward sum: 0.6287277795740986
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.60102478, 6.96836758, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6018566211432808}
episode index:362
target Thresh 18.037496889900517
target distance 4.0
model initialize at round 362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 5.        , 22.96590638,  0.        ]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 2.2515216213931915}
done in step count: 6
reward sum = 0.9294049414588657
running average episode reward sum: 0.629556091314828
{'scaleFactor': 20, 'currentTarget': array([ 3., 24.]), 'previousTarget': array([ 3., 24.]), 'currentState': array([ 3.29104877, 24.68824807,  0.        ]), 'targetState': array([ 3, 24], dtype=int32), 'currentDistance': 0.7472581795801996}
episode index:363
target Thresh 18.06539398972186
target distance 16.0
model initialize at round 363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([20.22065604, 14.        ,  0.        ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 15.319809448383495}
done in step count: 17
reward sum = 0.7946127372183762
running average episode reward sum: 0.630009543638739
{'scaleFactor': 20, 'currentTarget': array([14., 28.]), 'previousTarget': array([14., 28.]), 'currentState': array([14.43938492, 28.86949443,  0.        ]), 'targetState': array([14, 28], dtype=int32), 'currentDistance': 0.9742071958322421}
episode index:364
target Thresh 18.09323535110058
target distance 9.0
model initialize at round 364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([12.50965869, 23.26463878,  0.        ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 8.05961903918974}
done in step count: 14
reward sum = 0.8441109241234995
running average episode reward sum: 0.6305961227633547
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'previousTarget': array([16., 16.]), 'currentState': array([15.14732701, 15.26452738,  0.        ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 1.1260422767435085}
episode index:365
target Thresh 18.12102108540217
target distance 7.0
model initialize at round 365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([5.82371283, 5.08489418, 0.        ]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 6.829668015251969}
done in step count: 7
reward sum = 0.9195014647367119
running average episode reward sum: 0.6313854816212056
{'scaleFactor': 20, 'currentTarget': array([12.,  8.]), 'previousTarget': array([12.,  8.]), 'currentState': array([11.83490288,  7.16781181,  0.        ]), 'targetState': array([12,  8], dtype=int32), 'currentDistance': 0.8484068836014559}
episode index:366
target Thresh 18.148751303769593
target distance 7.0
model initialize at round 366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([10., 27.,  0.]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 6.403124237432878}
done in step count: 6
reward sum = 0.9177069101977797
running average episode reward sum: 0.6321656490015232
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 5.20303404, 23.82405323,  0.        ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.8486969689816183}
episode index:367
target Thresh 18.17642611712376
target distance 14.0
model initialize at round 367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([14.96330476, 25.        ,  0.        ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 15.648707050330168}
done in step count: 8
reward sum = 0.8879898872798131
running average episode reward sum: 0.632860823562062
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'previousTarget': array([27., 15.]), 'currentState': array([26.39102066, 14.51949637,  0.        ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.7757187530199761}
episode index:368
target Thresh 18.20404563616397
target distance 10.0
model initialize at round 368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([21., 10.,  0.]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 8.00000000000002}
done in step count: 8
reward sum = 0.8928149536447144
running average episode reward sum: 0.6335653062994133
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([20.39600317,  1.10620527,  0.        ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 1.0787405572978734}
episode index:369
target Thresh 18.231609971368325
target distance 6.0
model initialize at round 369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 6.        , 22.13185358,  0.        ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 7.948046001378961}
done in step count: 31
reward sum = 0.6802386168353282
running average episode reward sum: 0.6336914503819429
{'scaleFactor': 20, 'currentTarget': array([ 2., 29.]), 'previousTarget': array([ 2., 29.]), 'currentState': array([ 1.30929721, 29.06468591,  0.        ]), 'targetState': array([ 2, 29], dtype=int32), 'currentDistance': 0.6937251703452268}
episode index:370
target Thresh 18.25911923299421
target distance 8.0
model initialize at round 370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([15.18657303, 13.18868196,  0.        ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 7.855436206845224}
done in step count: 35
reward sum = 0.6488100125557918
running average episode reward sum: 0.6337322012233818
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'previousTarget': array([23., 14.]), 'currentState': array([22.00812379, 14.22180917,  0.        ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 1.016374799852578}
episode index:371
target Thresh 18.2865735310787
target distance 8.0
model initialize at round 371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([16.4405359 , 17.57535803,  0.        ]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 6.745985633904034}
done in step count: 23
reward sum = 0.7513578784638327
running average episode reward sum: 0.6340483992804797
{'scaleFactor': 20, 'currentTarget': array([23., 16.]), 'previousTarget': array([23., 16.]), 'currentState': array([22.814868  , 15.76233649,  0.        ]), 'targetState': array([23, 16], dtype=int32), 'currentDistance': 0.30126035093285564}
episode index:372
target Thresh 18.31397297543904
target distance 18.0
model initialize at round 372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([20.06563667,  7.57099981]), 'currentState': array([ 3.52702582, 16.        ,  0.        ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 19.654638809604638}
done in step count: 34
reward sum = 0.6188825646487737
running average episode reward sum: 0.6340077402064
{'scaleFactor': 20, 'currentTarget': array([21.,  7.]), 'previousTarget': array([21.,  7.]), 'currentState': array([21.98132052,  6.47588834,  0.        ]), 'targetState': array([21,  7], dtype=int32), 'currentDistance': 1.1125120235293158}
episode index:373
target Thresh 18.341317675673025
target distance 7.0
model initialize at round 373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([21., 16.,  0.]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 5.83095189484533}
done in step count: 34
reward sum = 0.6498299390817143
running average episode reward sum: 0.6340500455509864
{'scaleFactor': 20, 'currentTarget': array([18., 11.]), 'previousTarget': array([18., 11.]), 'currentState': array([18.50408554, 11.81434371,  0.        ]), 'targetState': array([18, 11], dtype=int32), 'currentDistance': 0.9577358277122159}
episode index:374
target Thresh 18.368607741159508
target distance 13.0
model initialize at round 374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([ 7.332775, 13.      ,  0.      ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 11.020217293908154}
done in step count: 52
reward sum = 0.5292416567675968
running average episode reward sum: 0.6337705565142308
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'previousTarget': array([8., 2.]), 'currentState': array([8.12729364, 2.40290501, 0.        ]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 0.4225353424359712}
episode index:375
target Thresh 18.395843281058777
target distance 9.0
model initialize at round 375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([6.89980757, 6.39531624, 0.        ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 9.531639969001214}
done in step count: 27
reward sum = 0.7183076545170579
running average episode reward sum: 0.6339953892216851
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'previousTarget': array([11., 15.]), 'currentState': array([10.9859947 , 15.07306403,  0.        ]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 0.07439422617577737}
episode index:376
target Thresh 18.423024404313033
target distance 6.0
model initialize at round 376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([13., 13.,  0.]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 5.656854249492408}
done in step count: 2
reward sum = 0.9641370591728645
running average episode reward sum: 0.6348710965690356
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'previousTarget': array([17., 17.]), 'currentState': array([16.22396982, 17.        ,  0.        ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 0.7760301828384435}
episode index:377
target Thresh 18.4501512196468
target distance 9.0
model initialize at round 377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.91894424, 14.        ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 7.000469272576779}
done in step count: 13
reward sum = 0.8427079394887673
running average episode reward sum: 0.6354209294868127
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.80167771,  6.68346676,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8619051236267562}
episode index:378
target Thresh 18.47722383556738
target distance 16.0
model initialize at round 378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([ 9.        , 26.36116743,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 16.94824858142301}
done in step count: 29
reward sum = 0.6607088971690982
running average episode reward sum: 0.6354876523566868
{'scaleFactor': 20, 'currentTarget': array([18., 12.]), 'previousTarget': array([18., 12.]), 'currentState': array([18.65607619, 12.6378532 ,  0.        ]), 'targetState': array([18, 12], dtype=int32), 'currentDistance': 0.9150369824222709}
episode index:379
target Thresh 18.504242360365275
target distance 16.0
model initialize at round 379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([ 7., 19.,  0.]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 14.00000000000002}
done in step count: 7
reward sum = 0.9027300837313053
running average episode reward sum: 0.6361909219129358
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'previousTarget': array([21., 19.]), 'currentState': array([20.38548338, 19.64594197,  0.        ]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.8915557768950306}
episode index:380
target Thresh 18.53120690211461
target distance 10.0
model initialize at round 380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([ 3.09148932, 12.57900953,  0.        ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 12.258655842737507}
done in step count: 13
reward sum = 0.8440051623003407
running average episode reward sum: 0.6367363661134277
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'previousTarget': array([12., 21.]), 'currentState': array([12.62680291, 21.85798528,  0.        ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 1.0625538289802365}
episode index:381
target Thresh 18.5581175686736
target distance 6.0
model initialize at round 381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([24.32122719, 13.36124134,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 4.673189935067096}
done in step count: 3
reward sum = 0.9614078368560923
running average episode reward sum: 0.637586291429508
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([25.16457117,  9.76391405,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 1.1320362230829184}
episode index:382
target Thresh 18.584974467684944
target distance 2.0
model initialize at round 382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([9.3547554, 5.4669137, 0.       ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.5863955988213494}
done in step count: 0
reward sum = 0.9963560005464547
running average episode reward sum: 0.6385230269624503
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'previousTarget': array([9., 5.]), 'currentState': array([9.3547554, 5.4669137, 0.       ]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.5863955988213494}
episode index:383
target Thresh 18.611777706576273
target distance 3.0
model initialize at round 383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 7., 26.,  0.]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 1.0000000000000195}
done in step count: 2
reward sum = 0.969180828968046
running average episode reward sum: 0.6393841149885066
{'scaleFactor': 20, 'currentTarget': array([ 6., 26.]), 'previousTarget': array([ 6., 26.]), 'currentState': array([ 6.97207177, 26.99267089,  0.        ]), 'targetState': array([ 6, 26], dtype=int32), 'currentDistance': 1.3893592131092305}
episode index:384
target Thresh 18.638527392560576
target distance 18.0
model initialize at round 384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([11., 24.,  0.]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 16.031219541881416}
done in step count: 9
reward sum = 0.8689132371805266
running average episode reward sum: 0.6399802945266677
{'scaleFactor': 20, 'currentTarget': array([27., 25.]), 'previousTarget': array([27., 25.]), 'currentState': array([27.91207554, 25.52551491,  0.        ]), 'targetState': array([27, 25], dtype=int32), 'currentDistance': 1.0526384521391936}
episode index:385
target Thresh 18.665223632636632
target distance 9.0
model initialize at round 385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([17.61585999, 16.38358212,  0.        ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 7.409824206056725}
done in step count: 9
reward sum = 0.8826611427635755
running average episode reward sum: 0.6406090013873851
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'previousTarget': array([25., 17.]), 'currentState': array([24.38368475, 17.02821186,  0.        ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.6169606074481386}
episode index:386
target Thresh 18.69186653358944
target distance 11.0
model initialize at round 386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([13., 24.,  0.]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 9.055385138137437}
done in step count: 11
reward sum = 0.8696521485027908
running average episode reward sum: 0.6412008441447892
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'previousTarget': array([14., 15.]), 'currentState': array([14.91714692, 14.36351597,  0.        ]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 1.1163648111168676}
episode index:387
target Thresh 18.718456201990637
target distance 15.0
model initialize at round 387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([20.56206572, 16.        ,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 13.007374309717957}
done in step count: 8
reward sum = 0.8893759273042243
running average episode reward sum: 0.6418404706477774
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'previousTarget': array([21.,  3.]), 'currentState': array([20.4639008 ,  2.80718106,  0.        ]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 0.5697205375414511}
episode index:388
target Thresh 18.74499274419893
target distance 10.0
model initialize at round 388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([17.        , 16.28027809,  0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 11.624266057080103}
done in step count: 43
reward sum = 0.536357133233798
running average episode reward sum: 0.6415693052559677
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'previousTarget': array([10.,  7.]), 'currentState': array([10.98601073,  6.37155335,  0.        ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 1.169257180508045}
episode index:389
target Thresh 18.771476266360533
target distance 5.0
model initialize at round 389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 8.        , 18.38319945,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 3.752498660122384}
done in step count: 18
reward sum = 0.8079691055674331
running average episode reward sum: 0.6419959714106124
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'previousTarget': array([ 9., 22.]), 'currentState': array([ 8.78318185, 21.49813652,  0.        ]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 0.5466965008234008}
episode index:390
target Thresh 18.797906874409563
target distance 10.0
model initialize at round 390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([ 7.53526425, 12.        ,  0.        ]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 9.673738819550222}
done in step count: 8
reward sum = 0.9027618409014988
running average episode reward sum: 0.642662891792942
{'scaleFactor': 20, 'currentTarget': array([17., 14.]), 'previousTarget': array([17., 14.]), 'currentState': array([17.12853414, 13.08015734,  0.        ]), 'targetState': array([17, 14], dtype=int32), 'currentDistance': 0.9287795995956888}
episode index:391
target Thresh 18.824284674068487
target distance 1.0
model initialize at round 391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([7.2468605 , 4.16370183, 0.        ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.8719717505843038}
done in step count: 0
reward sum = 0.999934196200745
running average episode reward sum: 0.6435742981817374
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'previousTarget': array([7., 5.]), 'currentState': array([7.2468605 , 4.16370183, 0.        ]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.8719717505843038}
episode index:392
target Thresh 18.850609770848536
target distance 17.0
model initialize at round 392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4.20859686, 7.13497444]), 'currentState': array([19.29551017, 16.        ,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 17.746904841339962}
done in step count: 8
reward sum = 0.8869238163044795
running average episode reward sum: 0.6441935081515153
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.33225429, 7.33157921, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.46940141141171776}
episode index:393
target Thresh 18.87688227005014
target distance 12.0
model initialize at round 393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([4., 5., 0.]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 12.206555615733729}
done in step count: 5
reward sum = 0.919417229220127
running average episode reward sum: 0.6448920455146336
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'previousTarget': array([14., 12.]), 'currentState': array([13.74044013, 12.90567434,  0.        ]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.9421344559678074}
episode index:394
target Thresh 18.90310227676332
target distance 16.0
model initialize at round 394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([16.       , 25.4616164,  0.       ]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 14.007608278999358}
done in step count: 20
reward sum = 0.7530223277607563
running average episode reward sum: 0.6451657930646238
{'scaleFactor': 20, 'currentTarget': array([ 2., 25.]), 'previousTarget': array([ 2., 25.]), 'currentState': array([ 1.11735787, 25.2454894 ,  0.        ]), 'targetState': array([ 2, 25], dtype=int32), 'currentDistance': 0.9161452847993343}
episode index:395
target Thresh 18.929269895868153
target distance 14.0
model initialize at round 395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([ 2.78946388, 14.26257527,  0.        ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 14.580251515939288}
done in step count: 58
reward sum = 0.4844546524953697
running average episode reward sum: 0.6447599568510651
{'scaleFactor': 20, 'currentTarget': array([17., 11.]), 'previousTarget': array([17., 11.]), 'currentState': array([16.83966156, 11.05623838,  0.        ]), 'targetState': array([17, 11], dtype=int32), 'currentDistance': 0.1699151861956363}
episode index:396
target Thresh 18.955385232035137
target distance 15.0
model initialize at round 396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([9., 8., 0.]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 13.928388277184142}
done in step count: 9
reward sum = 0.8813758817119552
running average episode reward sum: 0.6453559667373646
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'previousTarget': array([22.,  3.]), 'currentState': array([22.01944909,  2.13524209,  0.        ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.8649765936848688}
episode index:397
target Thresh 18.981448389725657
target distance 12.0
model initialize at round 397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([ 3.94747722, 17.17620862,  0.        ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 10.216625138830237}
done in step count: 19
reward sum = 0.7950718948338104
running average episode reward sum: 0.6457321374109738
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'previousTarget': array([14., 19.]), 'currentState': array([14.07782525, 18.08420932,  0.        ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 0.9190915805693416}
episode index:398
target Thresh 19.00745947319238
target distance 15.0
model initialize at round 398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([ 5.84782231, 17.76082325,  0.        ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 14.328225498267727}
done in step count: 9
reward sum = 0.8886874292524595
running average episode reward sum: 0.6463410479168422
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'previousTarget': array([20., 20.]), 'currentState': array([20.28647089, 20.47746605,  0.        ]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.5568118144403208}
episode index:399
target Thresh 19.03341858647967
target distance 11.0
model initialize at round 399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([ 7., 20.,  0.]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 9.21954445729291}
done in step count: 41
reward sum = 0.5930714089709156
running average episode reward sum: 0.6462078738194773
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'previousTarget': array([16., 22.]), 'currentState': array([16.88997209, 21.12829354,  0.        ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 1.2457618021108785}
episode index:400
target Thresh 19.05932583342402
target distance 16.0
model initialize at round 400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([19.        , 20.00750017,  0.        ]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 14.319394445385738}
done in step count: 10
reward sum = 0.8784796972110289
running average episode reward sum: 0.6467871052992569
{'scaleFactor': 20, 'currentTarget': array([ 5., 17.]), 'previousTarget': array([ 5., 17.]), 'currentState': array([ 4.75243282, 16.7819404 ,  0.        ]), 'targetState': array([ 5, 17], dtype=int32), 'currentDistance': 0.32990831442745566}
episode index:401
target Thresh 19.085181317654445
target distance 15.0
model initialize at round 401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([ 6.19798493, 17.        ,  0.        ]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 13.298544598222565}
done in step count: 99
reward sum = -0.14982285303347312
running average episode reward sum: 0.6448054884874839
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'previousTarget': array([9., 4.]), 'currentState': array([3.91565622, 4.80985275, 0.        ]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 5.148437934627797}
episode index:402
target Thresh 19.110985142592924
target distance 9.0
model initialize at round 402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([11.71850586,  5.20312524,  0.        ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 9.818663084934302}
done in step count: 6
reward sum = 0.9220699024726852
running average episode reward sum: 0.6454934895147425
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'previousTarget': array([21.,  2.]), 'currentState': array([20.63576932,  1.10171006,  0.        ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.9693238898083123}
episode index:403
target Thresh 19.13673741145479
target distance 8.0
model initialize at round 403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([20., 18.,  0.]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 7.211102550928003}
done in step count: 18
reward sum = 0.7915650620016218
running average episode reward sum: 0.6458550528129773
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'previousTarget': array([26., 14.]), 'currentState': array([25.22337699, 14.51866901,  0.        ]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.9338955197248444}
episode index:404
target Thresh 19.162438227249147
target distance 11.0
model initialize at round 404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([ 2.42744493, 22.78745925,  0.        ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 10.933839659512259}
done in step count: 9
reward sum = 0.8915448250286
running average episode reward sum: 0.6464616942258553
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'previousTarget': array([13., 20.]), 'currentState': array([13.22227085, 20.81756625,  0.        ]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.847241934400495}
episode index:405
target Thresh 19.1880876927793
target distance 18.0
model initialize at round 405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.46601525,  9.43782106]), 'previousTarget': array([24.21358457, 10.29018892]), 'currentState': array([10.        , 22.11860859,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 8
reward sum = 0.8793795929005921
running average episode reward sum: 0.6470353836314581
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'previousTarget': array([26.,  9.]), 'currentState': array([26.        ,  9.75960636,  0.        ]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.7596063613893822}
episode index:406
target Thresh 19.213685910643136
target distance 19.0
model initialize at round 406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([24.76686234, 13.91410718]), 'currentState': array([8.        , 7.76065719, 0.        ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 18.10882101862984}
done in step count: 11
reward sum = 0.861657818239212
running average episode reward sum: 0.6475627114806172
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'previousTarget': array([25., 14.]), 'currentState': array([24.90987432, 14.22747511,  0.        ]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 0.24467848774561135}
episode index:407
target Thresh 19.239232983233567
target distance 3.0
model initialize at round 407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([10.65402412,  8.19654858,  0.        ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 2.3541950120652673}
done in step count: 2
reward sum = 0.9759190896592203
running average episode reward sum: 0.6483675065251726
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'previousTarget': array([13.,  8.]), 'currentState': array([12.50338376,  8.1199466 ,  0.        ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.5108961545480544}
episode index:408
target Thresh 19.26472901273892
target distance 12.0
model initialize at round 408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([24.        , 18.77505398,  0.        ]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 11.381982303338248}
done in step count: 15
reward sum = 0.8085831487858449
running average episode reward sum: 0.6487592318118734
{'scaleFactor': 20, 'currentTarget': array([19., 29.]), 'previousTarget': array([19., 29.]), 'currentState': array([18.22026339, 29.2461599 ,  0.        ]), 'targetState': array([19, 29], dtype=int32), 'currentDistance': 0.8176697845814073}
episode index:409
target Thresh 19.290174101143343
target distance 6.0
model initialize at round 409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([24.20672441, 23.75022101,  0.        ]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 4.254803921380597}
done in step count: 4
reward sum = 0.9527411319521834
running average episode reward sum: 0.6495006510805084
{'scaleFactor': 20, 'currentTarget': array([24., 28.]), 'previousTarget': array([24., 28.]), 'currentState': array([23.11968544, 27.13209927,  0.        ]), 'targetState': array([24, 28], dtype=int32), 'currentDistance': 1.2362060499962702}
episode index:410
target Thresh 19.31556835022722
target distance 13.0
model initialize at round 410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([13.78438878, 26.6922909 ,  0.        ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 12.126410105822174}
done in step count: 16
reward sum = 0.8181826668651739
running average episode reward sum: 0.6499110696103981
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'previousTarget': array([17., 15.]), 'currentState': array([16.48695874, 14.82243764,  0.        ]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 0.5428993668549396}
episode index:411
target Thresh 19.34091186156759
target distance 14.0
model initialize at round 411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([20.        ,  3.61277747,  0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 12.015635490045055}
done in step count: 11
reward sum = 0.8528249800837512
running average episode reward sum: 0.6504035791018383
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'previousTarget': array([8., 3.]), 'currentState': array([7.29869848, 2.63818313, 0.        ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.7891357685395544}
episode index:412
target Thresh 19.366204736538528
target distance 14.0
model initialize at round 412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([22., 12.,  0.]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 12.369316876853008}
done in step count: 7
reward sum = 0.9009816394171225
running average episode reward sum: 0.651010305640132
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'previousTarget': array([10.,  9.]), 'currentState': array([10.00793621,  8.03821623,  0.        ]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.9618165092144282}
episode index:413
target Thresh 19.391447076311565
target distance 18.0
model initialize at round 413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([ 4.4585427 , 26.22841096,  0.        ]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 17.58441689484627}
done in step count: 12
reward sum = 0.8523584041967117
running average episode reward sum: 0.6514966537042783
{'scaleFactor': 20, 'currentTarget': array([22., 25.]), 'previousTarget': array([22., 25.]), 'currentState': array([22.71225494, 24.60175727,  0.        ]), 'targetState': array([22, 25], dtype=int32), 'currentDistance': 0.8160296386683615}
episode index:414
target Thresh 19.416638981856096
target distance 17.0
model initialize at round 414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.69759729,  9.88269136]), 'previousTarget': array([14.28585494, 10.56139529]), 'currentState': array([26.09853816, 25.57398403,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7996959336135937
running average episode reward sum: 0.6518537604028549
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'previousTarget': array([13.,  9.]), 'currentState': array([12.7166255 ,  8.27649967,  0.        ]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.7770159773983515}
episode index:415
target Thresh 19.441780553939772
target distance 17.0
model initialize at round 415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([22.        ,  4.03036141,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 16.73407770296515}
done in step count: 48
reward sum = 0.5199469071229343
running average episode reward sum: 0.651536676620932
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'previousTarget': array([17., 20.]), 'currentState': array([17.98682107, 20.82273154,  0.        ]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 1.2847968742600888}
episode index:416
target Thresh 19.466871893128925
target distance 12.0
model initialize at round 416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([24.03412914, 23.89267325,  0.        ]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 14.819480087027332}
done in step count: 89
reward sum = 0.29533908256158337
running average episode reward sum: 0.650682485747888
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'previousTarget': array([13., 14.]), 'currentState': array([13.25050236, 13.09320923,  0.        ]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.9407555113906385}
episode index:417
target Thresh 19.49191309978894
target distance 3.0
model initialize at round 417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 6., 14.,  0.]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 1.0000000000000187}
done in step count: 16
reward sum = 0.8150399955322307
running average episode reward sum: 0.6510756855320611
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'previousTarget': array([ 5., 14.]), 'currentState': array([ 4.66237706, 14.56618625,  0.        ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.6592087065761416}
episode index:418
target Thresh 19.516904274084677
target distance 6.0
model initialize at round 418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([23.33551383,  3.4840796 ,  0.        ]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 4.5824853986698955}
done in step count: 6
reward sum = 0.9330049031189999
running average episode reward sum: 0.6517485476265407
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'previousTarget': array([19.,  2.]), 'currentState': array([19.83475876,  2.55852348,  0.        ]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 1.0043757555562485}
episode index:419
target Thresh 19.541845515980864
target distance 18.0
model initialize at round 419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([25.09400392,  9.35899411]), 'currentState': array([16., 24.,  0.]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 18.86796226411323}
done in step count: 9
reward sum = 0.8659084213252193
running average episode reward sum: 0.652258452087728
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([25.76357126,  7.34369916,  0.        ]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 0.6975882289080723}
episode index:420
target Thresh 19.566736925242502
target distance 15.0
model initialize at round 420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([ 9.        , 17.25812745,  0.        ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 15.665897218606968}
done in step count: 11
reward sum = 0.8679649010532852
running average episode reward sum: 0.6527708189498791
{'scaleFactor': 20, 'currentTarget': array([22., 26.]), 'previousTarget': array([22., 26.]), 'currentState': array([22.34464896, 25.41762733,  0.        ]), 'targetState': array([22, 26], dtype=int32), 'currentDistance': 0.6767132517888871}
episode index:421
target Thresh 19.591578601435266
target distance 16.0
model initialize at round 421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([16.19305527, 22.        ,  0.        ]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 15.048017077241418}
done in step count: 15
reward sum = 0.8258053444700789
running average episode reward sum: 0.6531808533705431
{'scaleFactor': 20, 'currentTarget': array([ 2., 27.]), 'previousTarget': array([ 2., 27.]), 'currentState': array([ 2.408885  , 26.87290877,  0.        ]), 'targetState': array([ 2, 27], dtype=int32), 'currentDistance': 0.42818118315601783}
episode index:422
target Thresh 19.616370643925894
target distance 10.0
model initialize at round 422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 7.73847997, 14.        ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.159066544839485}
done in step count: 29
reward sum = 0.689915937348951
running average episode reward sum: 0.6532676975407048
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.81101683,  5.58263698,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.4581555718220266}
episode index:423
target Thresh 19.641113151882582
target distance 18.0
model initialize at round 423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([22.        ,  7.77938008,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 16.3209188665723}
done in step count: 14
reward sum = 0.8258280893506358
running average episode reward sum: 0.6536746795968603
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.40146323, 10.61537482,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.5559759486516715}
episode index:424
target Thresh 19.6658062242754
target distance 12.0
model initialize at round 424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([13.        , 14.77980793,  0.        ]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 12.440412965620427}
done in step count: 18
reward sum = 0.7953408976097908
running average episode reward sum: 0.6540080118745377
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'previousTarget': array([9., 3.]), 'currentState': array([8.99859941, 3.58616829, 0.        ]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 0.5861699624661519}
episode index:425
target Thresh 19.69044995987667
target distance 9.0
model initialize at round 425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([10.58005217,  9.27798057,  0.        ]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 7.743774574155507}
done in step count: 36
reward sum = 0.631573420904813
running average episode reward sum: 0.6539553485154539
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'previousTarget': array([10., 17.]), 'currentState': array([10.39290464, 17.69073343,  0.        ]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.794661393730857}
episode index:426
target Thresh 19.715044457261367
target distance 8.0
model initialize at round 426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([23., 18.,  0.]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 6.324555320336781}
done in step count: 3
reward sum = 0.9522731099110038
running average episode reward sum: 0.6546539849590032
{'scaleFactor': 20, 'currentTarget': array([21., 12.]), 'previousTarget': array([21., 12.]), 'currentState': array([20.20310149, 12.        ,  0.        ]), 'targetState': array([21, 12], dtype=int32), 'currentDistance': 0.7968985140323568}
episode index:427
target Thresh 19.739589814807516
target distance 13.0
model initialize at round 427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([22., 26.,  0.]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 11.401754250991406}
done in step count: 6
reward sum = 0.9139143925877269
running average episode reward sum: 0.6552597335749583
{'scaleFactor': 20, 'currentTarget': array([25., 15.]), 'previousTarget': array([25., 15.]), 'currentState': array([25.44954553, 14.        ,  0.        ]), 'targetState': array([25, 15], dtype=int32), 'currentDistance': 1.0963991908784099}
episode index:428
target Thresh 19.764086130696572
target distance 11.0
model initialize at round 428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([11., 14.,  0.]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 9.486832980505163}
done in step count: 9
reward sum = 0.8925698061882472
running average episode reward sum: 0.6558129039073901
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.8411727 , 10.39859779,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.0340484123680846}
episode index:429
target Thresh 19.788533502913836
target distance 5.0
model initialize at round 429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([16.86711276, 20.56324601,  0.        ]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 4.475669059502895}
done in step count: 2
reward sum = 0.9681240300219393
running average episode reward sum: 0.6565392088518426
{'scaleFactor': 20, 'currentTarget': array([14., 24.]), 'previousTarget': array([14., 24.]), 'currentState': array([13.48696554, 24.15223181,  0.        ]), 'targetState': array([14, 24], dtype=int32), 'currentDistance': 0.5351437983223816}
episode index:430
target Thresh 19.812932029248834
target distance 6.0
model initialize at round 430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([18.19751275, 14.24925154,  0.        ]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 4.962309265612101}
done in step count: 2
reward sum = 0.9707498858953723
running average episode reward sum: 0.657268235944751
{'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'previousTarget': array([23., 13.]), 'currentState': array([22.19751275, 12.85577524,  0.        ]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 0.8153444519343822}
episode index:431
target Thresh 19.837281807295696
target distance 17.0
model initialize at round 431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([21.79140314, 20.86502556]), 'currentState': array([ 6.62043226, 11.66611397,  0.        ]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 17.99034553066622}
done in step count: 13
reward sum = 0.8327734514542979
running average episode reward sum: 0.6576744980176897
{'scaleFactor': 20, 'currentTarget': array([22., 21.]), 'previousTarget': array([22., 21.]), 'currentState': array([21.8534774 , 20.26630533,  0.        ]), 'targetState': array([22, 21], dtype=int32), 'currentDistance': 0.7481822931998304}
episode index:432
target Thresh 19.861582934453573
target distance 6.0
model initialize at round 432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([ 9.24810171, 15.77556276,  0.        ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 5.278266332585374}
done in step count: 22
reward sum = 0.7557120970228842
running average episode reward sum: 0.6579009127959927
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'previousTarget': array([10., 21.]), 'currentState': array([10.96569127, 20.79007918,  0.        ]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.9882440883302613}
episode index:433
target Thresh 19.885835507927002
target distance 18.0
model initialize at round 433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([10., 15.,  0.]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 17.08800749063509}
done in step count: 59
reward sum = 0.45399535709465866
running average episode reward sum: 0.6574310843266348
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'previousTarget': array([26., 21.]), 'currentState': array([25.98976894, 20.16242792,  0.        ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.8376345685427043}
episode index:434
target Thresh 19.91003962472631
target distance 12.0
model initialize at round 434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([23.        , 13.14501202,  0.        ]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 10.194178186962876}
done in step count: 8
reward sum = 0.8918747612600386
running average episode reward sum: 0.6579700353080908
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'previousTarget': array([24.,  3.]), 'currentState': array([23.57008014,  3.51942968,  0.        ]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 0.6742686993209632}
episode index:435
target Thresh 19.934195381667998
target distance 6.0
model initialize at round 435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.32008016, 6.16012657, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 4.1724218820781385}
done in step count: 6
reward sum = 0.9258431839239883
running average episode reward sum: 0.6585844232636319
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.12521207, 2.77633703, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.1695952777200609}
episode index:436
target Thresh 19.958302875375125
target distance 11.0
model initialize at round 436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([24.,  9.,  0.]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 9.055385138137437}
done in step count: 70
reward sum = 0.3298296712941027
running average episode reward sum: 0.6578321240600402
{'scaleFactor': 20, 'currentTarget': array([23., 18.]), 'previousTarget': array([23., 18.]), 'currentState': array([23.57946603, 17.52458841,  0.        ]), 'targetState': array([23, 18], dtype=int32), 'currentDistance': 0.7495312268978679}
episode index:437
target Thresh 19.982362202277695
target distance 18.0
model initialize at round 437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([22.87613249, 12.35942745,  0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 18.947291521453984}
done in step count: 15
reward sum = 0.8255511765323239
running average episode reward sum: 0.6582150442711642
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'previousTarget': array([ 4., 14.]), 'currentState': array([ 3.85645807, 14.40934983,  0.        ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.43378746955258574}
episode index:438
target Thresh 20.006373458613055
target distance 4.0
model initialize at round 438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([18.        ,  3.38221765,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.036195061540663}
done in step count: 22
reward sum = 0.7607230559252788
running average episode reward sum: 0.6584485477145675
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.82267022,  2.79993564,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.2673417269790884}
episode index:439
target Thresh 20.030336740426254
target distance 13.0
model initialize at round 439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 6.98808789, 28.22236848,  0.        ]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 12.264185963372855}
done in step count: 7
reward sum = 0.910692586750246
running average episode reward sum: 0.6590218296214668
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'previousTarget': array([ 8., 16.]), 'currentState': array([ 8.84915501, 16.7194674 ,  0.        ]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 1.1129679109417239}
episode index:440
target Thresh 20.05425214357046
target distance 11.0
model initialize at round 440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([11.        , 22.96283996,  0.        ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 10.83731061361045}
done in step count: 54
reward sum = 0.47827256679553753
running average episode reward sum: 0.6586119673474851
{'scaleFactor': 20, 'currentTarget': array([20., 29.]), 'previousTarget': array([20., 29.]), 'currentState': array([19.76930072, 29.63872344,  0.        ]), 'targetState': array([20, 29], dtype=int32), 'currentDistance': 0.6791095534971112}
episode index:441
target Thresh 20.078119763707306
target distance 12.0
model initialize at round 441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([ 8., 13.,  0.]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 12.20655561573373}
done in step count: 5
reward sum = 0.9229946518882075
running average episode reward sum: 0.6592101182174867
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'previousTarget': array([15., 23.]), 'currentState': array([15.48786616, 22.41786599,  0.        ]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.7595349873392884}
episode index:442
target Thresh 20.101939696307316
target distance 7.0
model initialize at round 442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([2.32649177, 5.        , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 5.27263025305271}
done in step count: 31
reward sum = 0.6658083957161627
running average episode reward sum: 0.6592250127490864
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.64345397, 10.68184721,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.7694418005395364}
episode index:443
target Thresh 20.125712036650242
target distance 10.0
model initialize at round 443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([18.        , 23.24268377,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 9.717366064952357}
done in step count: 6
reward sum = 0.9233730783393921
running average episode reward sum: 0.6598199408247403
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'previousTarget': array([21., 14.]), 'currentState': array([21.49181831, 14.47671568,  0.        ]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.684940210684271}
episode index:444
target Thresh 20.149436879825487
target distance 16.0
model initialize at round 444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([6.        , 7.87405753, 0.        ]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 14.027258341105794}
done in step count: 14
reward sum = 0.8411715891243232
running average episode reward sum: 0.660227472618672
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'previousTarget': array([20.,  7.]), 'currentState': array([19.51659662,  7.7653763 ,  0.        ]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.9052511874309868}
episode index:445
target Thresh 20.17311432073245
target distance 11.0
model initialize at round 445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([13.        , 21.03071415,  0.        ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 9.496590350474834}
done in step count: 99
reward sum = -0.12062250395069214
running average episode reward sum: 0.6584766879178438
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'previousTarget': array([22., 18.]), 'currentState': array([24.04344484, 23.19302666,  0.        ]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 5.58060863261944}
episode index:446
target Thresh 20.196744454080928
target distance 14.0
model initialize at round 446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([21.56654328, 29.90755765,  0.        ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 18.05106716528028}
done in step count: 9
reward sum = 0.878592014993537
running average episode reward sum: 0.6589691159426216
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 8.53954378, 18.24933443,  0.        ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.5943695356843528}
episode index:447
target Thresh 20.22032737439148
target distance 10.0
model initialize at round 447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([18.,  7.,  0.]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 8.000000000000018}
done in step count: 4
reward sum = 0.9410504991112045
running average episode reward sum: 0.6595987618871944
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'previousTarget': array([26.,  7.]), 'currentState': array([26.        ,  6.77709536,  0.        ]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 0.2229046374558905}
episode index:448
target Thresh 20.243863175995827
target distance 9.0
model initialize at round 448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([23.03746772, 17.63665915,  0.        ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 9.487784710670725}
done in step count: 13
reward sum = 0.8360173277345029
running average episode reward sum: 0.6599916762877452
{'scaleFactor': 20, 'currentTarget': array([16., 24.]), 'previousTarget': array([16., 24.]), 'currentState': array([15.95786226, 23.26500535,  0.        ]), 'targetState': array([16, 24], dtype=int32), 'currentDistance': 0.7362015516608971}
episode index:449
target Thresh 20.2673519530372
target distance 2.0
model initialize at round 449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 4.        , 27.04304934,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 1.0430493354798926}
done in step count: 6
reward sum = 0.92921781662411
running average episode reward sum: 0.6605899565996037
{'scaleFactor': 20, 'currentTarget': array([ 4., 26.]), 'previousTarget': array([ 4., 26.]), 'currentState': array([ 3.69425207, 26.43197238,  0.        ]), 'targetState': array([ 4, 26], dtype=int32), 'currentDistance': 0.5292276781566254}
episode index:450
target Thresh 20.290793799470745
target distance 7.0
model initialize at round 450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([12.,  8.,  0.]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 5.099019513592807}
done in step count: 16
reward sum = 0.815210109555341
running average episode reward sum: 0.6609327950762239
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'previousTarget': array([11., 13.]), 'currentState': array([11.57670468, 12.01599073,  0.        ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 1.1405536043397877}
episode index:451
target Thresh 20.314188809063875
target distance 14.0
model initialize at round 451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([7.46745956, 3.09231138, 0.        ]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 12.580052227639747}
done in step count: 17
reward sum = 0.8025533401006146
running average episode reward sum: 0.6612461148661009
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'previousTarget': array([20.,  2.]), 'currentState': array([20.76169287,  1.15894146,  0.        ]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 1.1347050249837793}
episode index:452
target Thresh 20.33753707539666
target distance 14.0
model initialize at round 452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([12.,  8.,  0.]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 13.416407864998764}
done in step count: 28
reward sum = 0.6903658344244505
running average episode reward sum: 0.6613103968077307
{'scaleFactor': 20, 'currentTarget': array([24.,  2.]), 'previousTarget': array([24.,  2.]), 'currentState': array([23.83771666,  2.31925786,  0.        ]), 'targetState': array([24,  2], dtype=int32), 'currentDistance': 0.3581360953903206}
episode index:453
target Thresh 20.360838691862195
target distance 19.0
model initialize at round 453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.31907892,  3.59352733]), 'previousTarget': array([11.13601924,  4.89888325]), 'currentState': array([23.07214129, 19.        ,  0.        ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 10
reward sum = 0.8592658754261311
running average episode reward sum: 0.6617464220910311
{'scaleFactor': 20, 'currentTarget': array([9., 2.]), 'previousTarget': array([9., 2.]), 'currentState': array([9.17907137, 1.9705857 , 0.        ]), 'targetState': array([9, 2], dtype=int32), 'currentDistance': 0.18147108645984664}
episode index:454
target Thresh 20.38409375166698
target distance 16.0
model initialize at round 454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([20.44573557, 14.43855453,  0.        ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 14.765409469678277}
done in step count: 64
reward sum = 0.4239400528973578
running average episode reward sum: 0.6612237707301659
{'scaleFactor': 20, 'currentTarget': array([18., 29.]), 'previousTarget': array([18., 29.]), 'currentState': array([18.15030707, 28.34707818,  0.        ]), 'targetState': array([18, 29], dtype=int32), 'currentDistance': 0.6699993409413846}
episode index:455
target Thresh 20.407302347831283
target distance 10.0
model initialize at round 455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([21.39892352, 13.11350143,  0.        ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 9.22927047764136}
done in step count: 19
reward sum = 0.7879075803188924
running average episode reward sum: 0.6615015861020711
{'scaleFactor': 20, 'currentTarget': array([17.,  5.]), 'previousTarget': array([17.,  5.]), 'currentState': array([16.92016623,  4.39234337,  0.        ]), 'targetState': array([17,  5], dtype=int32), 'currentDistance': 0.6128784682225845}
episode index:456
target Thresh 20.43046457318952
target distance 9.0
model initialize at round 456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([25., 11.,  0.]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 7.071067811865495}
done in step count: 4
reward sum = 0.9378755559035735
running average episode reward sum: 0.6621063431475886
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'previousTarget': array([18., 10.]), 'currentState': array([17.        , 10.78362596,  0.        ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 1.27046040699199}
episode index:457
target Thresh 20.453580520390627
target distance 5.0
model initialize at round 457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([11.        ,  6.21175057,  0.        ]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 3.0074637658013}
done in step count: 20
reward sum = 0.7757095831224394
running average episode reward sum: 0.6623543851562673
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'previousTarget': array([8., 6.]), 'currentState': array([8.21764963, 6.64177018, 0.        ]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 0.6776727277314588}
episode index:458
target Thresh 20.476650281898415
target distance 8.0
model initialize at round 458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([13.        , 18.30886522,  0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 6.141101543081392}
done in step count: 3
reward sum = 0.9583459561973039
running average episode reward sum: 0.6629992469668142
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'previousTarget': array([19., 17.]), 'currentState': array([18.47974575, 16.63572168,  0.        ]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 0.635108794641034}
episode index:459
target Thresh 20.499673949991973
target distance 10.0
model initialize at round 459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([20.07390976, 11.26655626,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 10.784557407149363}
done in step count: 59
reward sum = 0.44749844364356534
running average episode reward sum: 0.6625307669595898
{'scaleFactor': 20, 'currentTarget': array([27.,  3.]), 'previousTarget': array([27.,  3.]), 'currentState': array([27.8746293 ,  2.80108436,  0.        ]), 'targetState': array([27,  3], dtype=int32), 'currentDistance': 0.8969636831222494}
episode index:460
target Thresh 20.52265161676599
target distance 12.0
model initialize at round 460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([ 5.        , 13.45721626,  0.        ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 10.609020530117075}
done in step count: 38
reward sum = 0.5909208252806578
running average episode reward sum: 0.6623754308605031
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'previousTarget': array([15., 17.]), 'currentState': array([14.37914473, 17.4308955 ,  0.        ]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.755732885810069}
episode index:461
target Thresh 20.545583374131176
target distance 17.0
model initialize at round 461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.39889098, 12.37339256]), 'previousTarget': array([ 9.43600015, 14.29270602]), 'currentState': array([22.        , 26.04115081,  0.        ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7308151962700111
running average episode reward sum: 0.6625235688808699
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'previousTarget': array([ 7., 12.]), 'currentState': array([ 6.96368829, 11.51691932,  0.        ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 0.48444347535362026}
episode index:462
target Thresh 20.568469313814585
target distance 12.0
model initialize at round 462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([11., 17.,  0.]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 10.770329614269032}
done in step count: 55
reward sum = 0.47733544020810653
running average episode reward sum: 0.6621235945208855
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'previousTarget': array([7., 7.]), 'currentState': array([6.84417768, 6.21594179, 0.        ]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.7993921887239708}
episode index:463
target Thresh 20.591309527360004
target distance 5.0
model initialize at round 463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([26.8030158, 20.       ,  0.       ]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 3.1056133649197815}
done in step count: 2
reward sum = 0.9727530098193155
running average episode reward sum: 0.6627930544676495
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'previousTarget': array([26., 23.]), 'currentState': array([26.62568632, 22.97860134,  0.        ]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.6260521311643892}
episode index:464
target Thresh 20.614104106128323
target distance 15.0
model initialize at round 464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([18.62320578, 18.91698861,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 14.118488685564689}
done in step count: 20
reward sum = 0.7824699017685048
running average episode reward sum: 0.6630504240317372
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'previousTarget': array([21.,  5.]), 'currentState': array([21.7043826 ,  5.93084511,  0.        ]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 1.1673163524657142}
episode index:465
target Thresh 20.63685314129789
target distance 3.0
model initialize at round 465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.        , 11.32618421,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0518536689653384}
done in step count: 1
reward sum = 0.9839230990718894
running average episode reward sum: 0.6637389920039264
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.        , 11.51724344,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.125851136390739}
episode index:466
target Thresh 20.659556723864867
target distance 8.0
model initialize at round 466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([24.48640871, 20.        ,  0.        ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 6.4947846981673765}
done in step count: 96
reward sum = 0.2724734698461394
running average episode reward sum: 0.662901164333353
{'scaleFactor': 20, 'currentTarget': array([22., 14.]), 'previousTarget': array([22., 14.]), 'currentState': array([21.13881667, 14.81693552,  0.        ]), 'targetState': array([22, 14], dtype=int32), 'currentDistance': 1.187021641001778}
episode index:467
target Thresh 20.68221494464362
target distance 13.0
model initialize at round 467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([ 3.22063565, 16.27047539,  0.        ]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 12.054319300957786}
done in step count: 21
reward sum = 0.7724495242284406
running average episode reward sum: 0.6631352420254365
{'scaleFactor': 20, 'currentTarget': array([ 6., 28.]), 'previousTarget': array([ 6., 28.]), 'currentState': array([ 5.67246669, 28.97955139,  0.        ]), 'targetState': array([ 6, 28], dtype=int32), 'currentDistance': 1.0328596207447367}
episode index:468
target Thresh 20.704827894267055
target distance 11.0
model initialize at round 468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([15.1958282 , 17.25489354,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 12.106120433687853}
done in step count: 6
reward sum = 0.9161914505890307
running average episode reward sum: 0.6636748075021179
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([23.37213784,  8.66631866,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.7631953389372965}
episode index:469
target Thresh 20.72739566318701
target distance 20.0
model initialize at round 469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.80368799,  6.63557441]), 'previousTarget': array([15.9529684 ,  8.76121364]), 'currentState': array([ 5., 22.,  0.]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7803338113915157
running average episode reward sum: 0.6639230181486911
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'previousTarget': array([20.,  4.]), 'currentState': array([19.57951581,  3.62765957,  0.        ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.5616443254309176}
episode index:470
target Thresh 20.749918341674594
target distance 14.0
model initialize at round 470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([20.50963712,  6.        ,  0.        ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 17.334676830234837}
done in step count: 87
reward sum = 0.2961068898062372
running average episode reward sum: 0.6631420921861807
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'previousTarget': array([ 8., 18.]), 'currentState': array([ 8.08899478, 18.19136996,  0.        ]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 0.21105101547713015}
episode index:471
target Thresh 20.77239601982054
target distance 13.0
model initialize at round 471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([11.42529964, 17.06508076,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 15.338824565919714}
done in step count: 60
reward sum = 0.4319077036497915
running average episode reward sum: 0.6626521888206375
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'previousTarget': array([23.,  7.]), 'currentState': array([22.58160847,  7.70178657,  0.        ]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.8170409180293232}
episode index:472
target Thresh 20.794828787535593
target distance 9.0
model initialize at round 472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([21.        , 23.73249316,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 9.047733299238784}
done in step count: 60
reward sum = 0.45748233804127253
running average episode reward sum: 0.6622184259225838
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'previousTarget': array([14., 18.]), 'currentState': array([14.85806549, 17.92990279,  0.        ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.8609239204236867}
episode index:473
target Thresh 20.817216734550858
target distance 14.0
model initialize at round 473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([21., 18.,  0.]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 13.000000000000025}
done in step count: 28
reward sum = 0.6858250444437486
running average episode reward sum: 0.6622682289152445
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'previousTarget': array([26.,  6.]), 'currentState': array([25.889048  ,  5.34809544,  0.        ]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 0.6612789874943238}
episode index:474
target Thresh 20.83955995041815
target distance 11.0
model initialize at round 474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.29226089, 12.        ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 9.027784592831386}
done in step count: 7
reward sum = 0.9113686345780507
running average episode reward sum: 0.662792650821903
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.86166006,  3.58042777,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5966861247943895}
episode index:475
target Thresh 20.861858524510364
target distance 14.0
model initialize at round 475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([25.0866822, 22.       ,  0.       ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 17.031966610295857}
done in step count: 17
reward sum = 0.7860949005459215
running average episode reward sum: 0.6630516891616594
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'previousTarget': array([13., 10.]), 'currentState': array([12.62042272,  9.59115124,  0.        ]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.5578854913057506}
episode index:476
target Thresh 20.884112546021825
target distance 8.0
model initialize at round 476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([20.08080494, 14.        ,  0.        ]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 8.428337327760396}
done in step count: 7
reward sum = 0.9167155219531398
running average episode reward sum: 0.6635834791675116
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'previousTarget': array([26.,  8.]), 'currentState': array([26.00611734,  8.19366148,  0.        ]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 0.19375807383050914}
episode index:477
target Thresh 20.906322103968648
target distance 17.0
model initialize at round 477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([10.57870319, 21.        ,  0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 15.005915200281008}
done in step count: 12
reward sum = 0.859110654492832
running average episode reward sum: 0.6639925318355562
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'previousTarget': array([11.,  6.]), 'currentState': array([11.60248676,  6.984236  ,  0.        ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 1.1539977482008403}
episode index:478
target Thresh 20.9284872871891
target distance 17.0
model initialize at round 478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([23.43261719, 13.        ,  0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 15.464982161513154}
done in step count: 35
reward sum = 0.644600844618663
running average episode reward sum: 0.663952048146168
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'previousTarget': array([ 8., 14.]), 'currentState': array([ 7.58215958, 13.53460103,  0.        ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.6254492915663649}
episode index:479
target Thresh 20.950608184343935
target distance 12.0
model initialize at round 479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([22., 19.,  0.]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 10.198039027185592}
done in step count: 24
reward sum = 0.7307850215465359
running average episode reward sum: 0.6640912835074189
{'scaleFactor': 20, 'currentTarget': array([24.,  9.]), 'previousTarget': array([24.,  9.]), 'currentState': array([24.70554671,  8.27992993,  0.        ]), 'targetState': array([24,  9], dtype=int32), 'currentDistance': 1.0081156058798313}
episode index:480
target Thresh 20.97268488391677
target distance 20.0
model initialize at round 480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7.5940089, 2.3000658]), 'previousTarget': array([6.8434743 , 2.25304229]), 'currentState': array([27.24879169,  6.        ,  0.        ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6809226291098971
running average episode reward sum: 0.6641262759099188
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'previousTarget': array([6., 2.]), 'currentState': array([6.68721038, 2.7650379 , 0.        ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 1.0283681716216906}
episode index:481
target Thresh 20.994717474214447
target distance 18.0
model initialize at round 481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.4372777 , 21.48940186]), 'previousTarget': array([10.26752934, 19.54026305]), 'currentState': array([22.22340238,  7.        ,  0.        ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6811517725546841
running average episode reward sum: 0.6641615985170656
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'previousTarget': array([ 7., 23.]), 'currentState': array([ 6.83113655, 22.56752218,  0.        ]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.46427570480140473}
episode index:482
target Thresh 21.016706043367343
target distance 11.0
model initialize at round 482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([15., 17.,  0.]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 11.401754250991406}
done in step count: 29
reward sum = 0.6722763688010811
running average episode reward sum: 0.6641783992836992
{'scaleFactor': 20, 'currentTarget': array([22.,  8.]), 'previousTarget': array([22.,  8.]), 'currentState': array([21.93463399,  8.51937538,  0.        ]), 'targetState': array([22,  8], dtype=int32), 'currentDistance': 0.5234725387320731}
episode index:483
target Thresh 21.038650679329766
target distance 18.0
model initialize at round 483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 3.78641543, 15.70981108]), 'currentState': array([18.        ,  5.79022503,  0.        ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 19.536096204222694}
done in step count: 13
reward sum = 0.8330202471825844
running average episode reward sum: 0.6645272460768786
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'previousTarget': array([ 2., 17.]), 'currentState': array([ 1.42008866, 16.871784  ,  0.        ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.5939162463756907}
episode index:484
target Thresh 21.060551469880295
target distance 18.0
model initialize at round 484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([20.03413379, 24.57723665,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 16.840461317702395}
done in step count: 16
reward sum = 0.812272314571641
running average episode reward sum: 0.6648318750840844
{'scaleFactor': 20, 'currentTarget': array([23.,  8.]), 'previousTarget': array([23.,  8.]), 'currentState': array([23.17729211,  7.97628695,  0.        ]), 'targetState': array([23,  8], dtype=int32), 'currentDistance': 0.17887090483662405}
episode index:485
target Thresh 21.082408502622116
target distance 15.0
model initialize at round 485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([17.94871688, 19.95596391,  0.        ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 14.09768726090251}
done in step count: 60
reward sum = 0.4411013589703902
running average episode reward sum: 0.6643715242278834
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'previousTarget': array([ 4., 22.]), 'currentState': array([ 3.20044381, 22.33558657,  0.        ]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 0.8671265475045798}
episode index:486
target Thresh 21.104221864983387
target distance 20.0
model initialize at round 486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.042426  , 20.10373956]), 'previousTarget': array([21.61161351, 20.0776773 ]), 'currentState': array([ 1.15876939, 22.2578518 ,  0.        ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.4986539360258295
running average episode reward sum: 0.664031241705908
{'scaleFactor': 20, 'currentTarget': array([22., 20.]), 'previousTarget': array([22., 20.]), 'currentState': array([21.7310152 , 19.09424311,  0.        ]), 'targetState': array([22, 20], dtype=int32), 'currentDistance': 0.944853623784841}
episode index:487
target Thresh 21.125991644217592
target distance 6.0
model initialize at round 487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([1.90207195, 6.93896437, 0.        ]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 5.478629734753341}
done in step count: 11
reward sum = 0.8695949974142206
running average episode reward sum: 0.6644524789102282
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.25225526, 12.35576618,  0.        ]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.43612187459184193}
episode index:488
target Thresh 21.14771792740388
target distance 16.0
model initialize at round 488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([16.51827803,  4.        ,  0.        ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 14.009590005386148}
done in step count: 9
reward sum = 0.8867598028959853
running average episode reward sum: 0.664907095114698
{'scaleFactor': 20, 'currentTarget': array([16., 18.]), 'previousTarget': array([16., 18.]), 'currentState': array([15.65900829, 17.12137997,  0.        ]), 'targetState': array([16, 18], dtype=int32), 'currentDistance': 0.9424693617305409}
episode index:489
target Thresh 21.169400801447402
target distance 2.0
model initialize at round 489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.        ,  9.25399065,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.7460093498229252}
done in step count: 1
reward sum = 0.9834646057203604
running average episode reward sum: 0.665557212483281
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.73335865, 10.52325571,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5462442043662601}
episode index:490
target Thresh 21.191040353079693
target distance 11.0
model initialize at round 490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([16., 16.,  0.]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 9.219544457292912}
done in step count: 82
reward sum = 0.313288606714617
running average episode reward sum: 0.6648397611477033
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'previousTarget': array([ 7., 14.]), 'currentState': array([ 7.02782254, 14.73311275,  0.        ]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 0.7336405068910345}
episode index:491
target Thresh 21.212636668858984
target distance 15.0
model initialize at round 491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([ 6.23688138, 25.37749231,  0.        ]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 14.953332110406608}
done in step count: 55
reward sum = 0.49067182735163456
running average episode reward sum: 0.6644857612822642
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'previousTarget': array([21., 23.]), 'currentState': array([21.6246478 , 22.09595879,  0.        ]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 1.0988518481320981}
episode index:492
target Thresh 21.234189835170564
target distance 21.0
model initialize at round 492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.34071782, 27.5511113 ]), 'previousTarget': array([ 4.02633404, 27.32455532]), 'currentState': array([22.30595279, 21.20131898,  0.        ]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.653281375865685
running average episode reward sum: 0.6644630343341575
{'scaleFactor': 20, 'currentTarget': array([ 2., 28.]), 'previousTarget': array([ 2., 28.]), 'currentState': array([ 1.12163681, 28.33296786,  0.        ]), 'targetState': array([ 2, 28], dtype=int32), 'currentDistance': 0.9393558880439509}
episode index:493
target Thresh 21.25569993822713
target distance 12.0
model initialize at round 493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 3.69515336, 21.36748564,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 16.031821090704646}
done in step count: 30
reward sum = 0.6569653169460876
running average episode reward sum: 0.6644478567685946
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.33739536, 10.80139418,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0398449573364474}
episode index:494
target Thresh 21.27716706406913
target distance 15.0
model initialize at round 494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([23.6435343,  6.       ,  0.       ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 13.015918576833784}
done in step count: 40
reward sum = 0.6044284301770386
running average episode reward sum: 0.664326605401743
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'previousTarget': array([23., 19.]), 'currentState': array([23.05812013, 18.64218989,  0.        ]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.36249968882480094}
episode index:495
target Thresh 21.298591298565086
target distance 20.0
model initialize at round 495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.00124766,  3.02495322]), 'currentState': array([25.34660006, 21.        ,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 18.01185530386082}
done in step count: 9
reward sum = 0.8786216955986623
running average episode reward sum: 0.6647586519545594
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'previousTarget': array([26.,  3.]), 'currentState': array([26.23128837,  3.60603654,  0.        ]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 0.6486714146466378}
episode index:496
target Thresh 21.319972727411965
target distance 12.0
model initialize at round 496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 5., 24.,  0.]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 10.770329614269032}
done in step count: 16
reward sum = 0.8052636163690481
running average episode reward sum: 0.6650413581203833
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'previousTarget': array([ 9., 14.]), 'currentState': array([ 9.14813799, 14.44148707,  0.        ]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.46567767770320745}
episode index:497
target Thresh 21.341311436135513
target distance 19.0
model initialize at round 497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([12., 22.,  0.]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 17.262676501632093}
done in step count: 17
reward sum = 0.80369936453658
running average episode reward sum: 0.6653197878521427
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.40762922,  4.10897124,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0699698096105796}
episode index:498
target Thresh 21.362607510090598
target distance 2.0
model initialize at round 498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 4.95674825, 22.71361816,  0.        ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.2896295440919243}
done in step count: 0
reward sum = 0.994925986421036
running average episode reward sum: 0.6659803213162087
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'previousTarget': array([ 5., 23.]), 'currentState': array([ 4.95674825, 22.71361816,  0.        ]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.2896295440919243}
episode index:499
target Thresh 21.38386103446154
target distance 1.0
model initialize at round 499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.44666827,  5.07295406,  0.        ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 1.2072308886189347}
done in step count: 7
reward sum = 0.9237621637545856
running average episode reward sum: 0.6664958850010854
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'previousTarget': array([22.,  4.]), 'currentState': array([21.42664084,  3.39581972,  0.        ]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.8329312904384382}

Process finished with exit code 0
