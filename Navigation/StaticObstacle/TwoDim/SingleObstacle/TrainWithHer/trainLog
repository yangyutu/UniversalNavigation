/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/StaticObstacle/TwoDim/SingleObstacle/DDPGHER_CNN.py
episode index:0
target Thresh 3.799999999999999
target distance 2.0
model initialize at round 0
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.54965651, 2.36389169, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 2.18640038347049}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.12992867, 1.1299292 , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 3.425561733441022}
episode index:1
target Thresh 4.100980165737321
target distance 1.0
model initialize at round 1
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.        , 9.00007415, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.4141611326654413}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.        , 9.00007415, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.4141611326654413}
episode index:2
target Thresh 4.396000524884688
target distance 1.0
model initialize at round 2
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.12992866, 1.12992923, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 3.425561713384219}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.3333333333333333
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.12915094, 1.13668001, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 3.4203329607176847}
episode index:3
target Thresh 4.685179089519418
target distance 3.0
model initialize at round 3
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.1268969 ,  3.83989278,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.278503725180564}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.25
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.1265949 ,  1.16380847,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 5.901183616725204}
episode index:4
target Thresh 4.968631534923135
target distance 3.0
model initialize at round 4
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.12943625,  1.13411819,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.9626914327313503}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.12827638,  1.14529661,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.952042550626185}
episode index:5
target Thresh 5.246471245853414
target distance 2.0
model initialize at round 5
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.02416649, 10.13938771,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.860951528022126}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.3333333333333333
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.02416649, 10.13938771,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.860951528022126}
episode index:6
target Thresh 5.518809361899206
target distance 4.0
model initialize at round 6
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.12859493,  8.1230718 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.250273178181896}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2857142857142857
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.0882214 ,  1.53271579,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 9.958241902784568}
episode index:7
target Thresh 5.785754821938149
target distance 5.0
model initialize at round 7
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 8.00372959, 10.1474225 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 7.0480272408940445}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.25
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 1.12972083, 11.86835685,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.897434577527894}
episode index:8
target Thresh 6.047414407713585
target distance 4.0
model initialize at round 8
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([13.0944837 ,  4.40980314,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 5.432488583769801}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2222222222222222
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 1.12989716, 11.86981523,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 15.14449728277569}
episode index:9
target Thresh 6.303892786548666
target distance 2.0
model initialize at round 9
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.        , 10.81740619,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8174061924218936}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.3
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.        , 10.81740619,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8174061924218936}
episode index:10
target Thresh 6.555292553214675
target distance 2.0
model initialize at round 10
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.13101893, 3.91334742, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 2.6746941068057986}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2727272727272727
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.12819216, 1.14617531, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 2.057348099824099}
episode index:11
target Thresh 6.801714270970326
target distance 3.0
model initialize at round 11
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([12.        , 11.08113952,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 5.049100983396889}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.25
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 1.12991205, 11.86993615,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 15.365413152702986}
episode index:12
target Thresh 7.043256511788387
target distance 7.0
model initialize at round 12
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.12467787, 4.82193772, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.239763009409412}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.29029084134615385
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.11910591, 10.25141439,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.1560081340036867}
episode index:13
target Thresh 7.280015895785792
target distance 7.0
model initialize at round 13
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 7.02126198, 11.86147765,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 9.169669318980192}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.26955578125
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 1.12991892, 11.86997789,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 14.987198827593991}
episode index:14
target Thresh 7.512087129872971
target distance 1.0
model initialize at round 14
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.00031912,  9.78030205,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0235375154507622}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.3182520625
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.00031912,  9.78030205,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0235375154507622}
episode index:15
target Thresh 7.739563045637888
target distance 3.0
model initialize at round 15
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 2.00004447, 10.25090435,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.4012189247786324}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.29836130859375
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([16.8700715 , 11.87006923,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 14.700219648662573}
episode index:16
target Thresh 7.962534636479896
target distance 1.0
model initialize at round 16
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 4.99999964, 10.99994826,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.162260960649243}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.28081064338235295
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([16.87007143, 11.87006977,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 14.987200718121834}
episode index:17
target Thresh 8.181091094008332
target distance 8.0
model initialize at round 17
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([16.87007282, 11.87006889,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.278889915331439}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2652100520833333
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([16.87007141, 11.87006993,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.278890520610327}
episode index:18
target Thresh 8.395319843720328
target distance 6.0
model initialize at round 18
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.9999994 ,  5.99998999,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 4.123115195656098}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.25125162828947367
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.8700714 , 11.87007002,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 2.6446793640543342}
episode index:19
target Thresh 8.605306579972192
target distance 6.0
model initialize at round 19
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([11.97380298, 11.86774956,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 5.582987471821521}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.23868904687499998
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.87007138, 11.87007019,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.9666695734173394}
episode index:20
target Thresh 8.81113530025828
target distance 7.0
model initialize at round 20
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 9.97379893, 11.8677501 ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 9.336176349361297}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.22732290178571427
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([16.87007136, 11.87007034,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 8.089201077983075}
episode index:21
target Thresh 9.012888338811136
target distance 9.0
model initialize at round 21
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.86775038, 3.97379571, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.027448800385286}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.25596163352272727
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.99638495, 10.09677349,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.344842407906671}
episode index:22
target Thresh 9.21064639953625
target distance 4.0
model initialize at round 22
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.86775086,  7.97377288,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 6.258950044912785}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2448328668478261
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.87007134, 11.87007052,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.04566866533033}
episode index:23
target Thresh 9.404488588294726
target distance 4.0
model initialize at round 23
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([16.86775105,  9.97376394,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 6.626450914628138}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.23463149739583333
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([16.87007125, 11.87007098,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 8.377071463243123}
episode index:24
target Thresh 9.59449244454666
target distance 2.0
model initialize at round 24
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([4.86775032, 5.97379795, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 3.4813604027682437}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2252462375
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([16.87007107, 11.87007122,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 16.824298938515103}
episode index:25
target Thresh 9.78073397236797
target distance 2.0
model initialize at round 25
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 3.99999499, 10.99999881,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.999998807911334}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2165829206730769
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 1.12992955, 11.87007138,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.818169449122723}
episode index:26
target Thresh 9.963287670853045
target distance 9.0
model initialize at round 26
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([2.00000048, 5.        , 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 12.529963667528438}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2085613310185185
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 1.12992962, 11.87007139,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 11.901915602002175}
episode index:27
target Thresh 10.142226563915358
target distance 8.0
model initialize at round 27
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([13.00494258, 11.86740305,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 10.311935408564768}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.20111271205357142
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 1.12992956, 11.87007138,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 17.84761339324732}
episode index:28
target Thresh 10.317622229498014
target distance 1.0
model initialize at round 28
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.00494258, 11.86740305,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.0351506571752287}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.19417779094827586
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 1.12992969, 11.87007139,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 13.186205660054938}
episode index:29
target Thresh 10.48954482820589
target distance 8.0
model initialize at round 29
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.00000036,  4.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 6.082762471504595}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18770519791666668
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 1.12992976, 11.8700714 ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 13.99557128306176}
episode index:30
target Thresh 10.658063131370797
target distance 1.0
model initialize at round 30
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.00000024,  9.        ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.60555114321316}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18165019153225806
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 1.1299298 , 11.87007141,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 15.986767215366434}
episode index:31
target Thresh 10.823244548560947
target distance 5.0
model initialize at round 31
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.00000036,  6.        ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.00000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.175973623046875
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 1.12992997, 11.87007143,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 13.186205391064695}
episode index:32
target Thresh 10.985155154545662
target distance 6.0
model initialize at round 32
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.00000072, 4.        , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 4.000000000000083}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.19798957386363636
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.13095748, 7.9319506 , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8717027187821389}
episode index:33
target Thresh 11.143859715726173
target distance 7.0
model initialize at round 33
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 5.02620018, 11.8677503 ,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 8.429672732843194}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.19216635110294117
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 1.12992997, 11.87007143,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 7.918020342927956}
episode index:34
target Thresh 11.299421716043039
target distance 8.0
model initialize at round 34
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.0262003 , 11.86775031,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.096289527355397}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18667588392857143
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.12993022, 11.87007146,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.065060875974394}
episode index:35
target Thresh 11.451903382370576
target distance 5.0
model initialize at round 35
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 5.02620055, 11.86775031,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 6.60214991332382}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18149044270833334
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 1.12993069, 11.87007152,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 5.934202577217246}
episode index:36
target Thresh 11.60136570940843
target distance 2.0
model initialize at round 36
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.00000036, 8.        , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 4.123105538880157}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1765852956081081
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 1.12993092, 11.87007155,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 8.089201722238363}
episode index:37
target Thresh 11.747868484080277
target distance 2.0
model initialize at round 37
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.0000006, 7.       , 0.       ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.9999994039535687}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17193831414473684
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 1.12993138, 11.8700716 ,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 5.652865761536961}
episode index:38
target Thresh 11.891470309449378
target distance 10.0
model initialize at round 38
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.13224973,  5.9738012 ,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.18972628438623}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16752963942307691
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 1.12993218, 11.8700717 ,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 5.652865440641461}
episode index:39
target Thresh 12.032228628160603
target distance 2.0
model initialize at round 39
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.00000179, 11.        ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9999982118605732}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.1883413984375
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.00000179, 11.        ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9999982118605732}
episode index:40
target Thresh 12.170199745418232
target distance 12.0
model initialize at round 40
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([1.13224977, 8.97380132, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 14.636013835145096}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18374770579268293
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 1.12993312, 11.87007182,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 16.21903015285465}
episode index:41
target Thresh 12.305438851508809
target distance 1.0
model initialize at round 41
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.13224974,  5.97380122,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 3.5116925713206006}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17937276041666667
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 1.1305955 , 11.87015309,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 16.463292411331167}
episode index:42
target Thresh 12.438000043877988
target distance 12.0
model initialize at round 42
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([12.0001334, 11.       ,  0.       ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 10.198169831969537}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.175201300872093
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([16.87007127, 11.87007126,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 15.144514806974799}
episode index:43
target Thresh 12.567936348770218
target distance 8.0
model initialize at round 43
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.97380131, 11.86775024,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 13.345924729448297}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.171219453125
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([16.87007126, 11.87007127,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 15.630639736595416}
episode index:44
target Thresh 12.695299742439962
target distance 7.0
model initialize at round 44
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([10.97380131, 11.86775024,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 6.996975462301673}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16741457638888887
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.85875261, 11.87132503,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 4.946439483486141}
episode index:45
target Thresh 12.820141171942895
target distance 4.0
model initialize at round 45
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.55092835, 5.        , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.530884974425713}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16377512907608693
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.478483  , 11.91247042,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.572854946432658}
episode index:46
target Thresh 12.942510575515385
target distance 12.0
model initialize at round 46
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.23598123,  6.        ,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.412592786492183}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1602905518617021
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([11.09582086, 11.87016262,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 8.973320125515048}
episode index:47
target Thresh 13.062456902550478
target distance 2.0
model initialize at round 47
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.3147856,  5.       ,  0.       ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.114123642166352}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1569511653645833
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.19339338, 11.87646511,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 9.05844690340434}
episode index:48
target Thresh 13.180028133178295
target distance 6.0
model initialize at round 48
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.36385465,  7.        ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.22612109354173}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15374808035714282
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([16.85992354, 11.87120602,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.989676001754864}
episode index:49
target Thresh 13.295271297458726
target distance 4.0
model initialize at round 49
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 2.75209237, 11.88249337,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 5.03944589728118}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15067311874999997
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.00537187, 11.86728629,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.236561836778264}
episode index:50
target Thresh 13.408232494194078
target distance 2.0
model initialize at round 50
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86091616,  9.97697203,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8612240836768663}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.1673265870098039
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86091616,  9.97697203,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8612240836768663}
episode index:51
target Thresh 13.518956909369209
target distance 12.0
model initialize at round 51
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([2.9694162, 4.       , 0.       ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 12.556822008144911}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16410876802884614
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([16.85654264, 11.87154485,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 3.4150426003450844}
episode index:52
target Thresh 13.62748883422654
target distance 6.0
model initialize at round 52
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.17307945, 11.87408823,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.234656377535283}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16101237617924527
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.33389629, 11.89359199,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.396620109542207}
episode index:53
target Thresh 13.733871682983127
target distance 5.0
model initialize at round 53
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 2.71381652, 10.        ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.365226052511295}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.1723599421296296
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.20919745, 11.87550056,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9001471017147042}
episode index:54
target Thresh 13.838148010196925
target distance 6.0
model initialize at round 54
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([1.7511633, 7.       , 0.       ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 4.190416816218382}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.18563521590909088
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.20647133, 10.99872144,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7935297050316683}
episode index:55
target Thresh 13.940359527789191
target distance 9.0
model initialize at round 55
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([12.77902062, 11.88043892,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.832411825307792}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1823203013392857
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([10.83428417, 11.87216001,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 10.088732329687812}
episode index:56
target Thresh 14.0405471217298
target distance 11.0
model initialize at round 56
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([11.03742496, 11.86801488,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.350976973363677}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17912169956140347
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([10.59626906, 11.90030598,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.023160360831062}
episode index:57
target Thresh 14.13875086839218
target distance 3.0
model initialize at round 57
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 2.8803551 , 11.87109211,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 4.998113951854682}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1760333943965517
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([11.92346937, 11.86800552,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.299400233148802}
episode index:58
target Thresh 14.2350100505844
target distance 4.0
model initialize at round 58
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.56835711, 10.        ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.676525065120791}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17304977754237286
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.56941988, 11.90925127,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.64315299051273}
episode index:59
target Thresh 14.329363173262806
target distance 12.0
model initialize at round 59
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([1.1332695 , 6.99833344, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 14.009980417713175}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17016561458333332
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.83873837, 11.8741262 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 6.876017472652041}
episode index:60
target Thresh 14.421847978934528
target distance 13.0
model initialize at round 60
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([1.13946122, 8.99975574, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 16.426569736473912}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16737601434426227
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([13.38398898, 11.90122982,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 10.240989484895945}
episode index:61
target Thresh 14.512501462754983
target distance 8.0
model initialize at round 61
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 9.97200069, 11.86778887,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.739736031379664}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1646764012096774
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.53882851, 11.91129011,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 8.923215260264799}
episode index:62
target Thresh 14.601359887326431
target distance 8.0
model initialize at round 62
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.57708598, 11.90425998,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.638956703166639}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16206249007936505
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.94076244, 11.86802232,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.994829949515568}
episode index:63
target Thresh 14.68845879720349
target distance 2.0
model initialize at round 63
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.66758496, 5.        , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.3324150443077283}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.17515526367187498
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.66758496, 5.        , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.3324150443077283}
episode index:64
target Thresh 14.77383303311145
target distance 5.0
model initialize at round 64
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([10.97375184, 11.8677513 ,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 7.974504053332987}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1724605673076923
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.24598977, 11.87900987,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.026716535634083}
episode index:65
target Thresh 14.857516745883007
target distance 2.0
model initialize at round 65
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.004421, 7.      , 0.      ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.122035608019224}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1698475284090909
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 1.13028515, 11.87011516,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.065030428302173}
episode index:66
target Thresh 14.939543410119073
target distance 5.0
model initialize at round 66
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.06946027, 5.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.1849246890741476}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.1807826399253731
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.22290702, 9.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.2664412749130425}
episode index:67
target Thresh 15.019945837579046
target distance 12.0
model initialize at round 67
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([1.2967946 , 8.99999994, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 13.703205395629153}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17812407169117644
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 1.14167742, 11.87136827,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 14.152662666278095}
episode index:68
target Thresh 15.09875619030595
target distance 12.0
model initialize at round 68
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([1.2238711 , 6.99999786, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 13.920550848096699}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17554256340579708
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 1.12995995, 11.87007513,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 14.163874550942468}
episode index:69
target Thresh 15.176005993491701
target distance 3.0
model initialize at round 69
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 2.05368721, 10.        ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.188180398786415}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17303481249999997
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 1.12996275, 11.87007548,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.999024035516066}
episode index:70
target Thresh 15.251726148087581
target distance 13.0
model initialize at round 70
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.045361017187282}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1705977024647887
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([12.01931539, 11.86761505,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 9.464209748150626}
episode index:71
target Thresh 15.325946943165045
target distance 11.0
model initialize at round 71
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.00016787,  9.00007785,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.487016853881883}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1682282899305555
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([11.01413752, 11.86752771,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.144725611467274}
episode index:72
target Thresh 15.398698068031749
target distance 14.0
model initialize at round 72
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([1.13459821, 8.98045634, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 15.161243037928896}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16592379280821914
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 1.13080509, 11.87017855,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 15.985992416423132}
episode index:73
target Thresh 15.470008624107667
target distance 13.0
model initialize at round 73
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([2.38666284, 9.        , 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.650016432549801}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16368157939189187
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([13.20067058, 11.8747744 ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 3.369128113972404}
episode index:74
target Thresh 15.539907136566047
target distance 12.0
model initialize at round 74
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.13259683,  7.99505795,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.329051803045513}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1614991583333333
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([12.0012771 , 11.86736421,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.192934160488814}
episode index:75
target Thresh 15.608421565743868
target distance 8.0
model initialize at round 75
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([15.62035197,  5.        ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.5728232732906}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.17065541940789472
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.06024422, 11.14541035,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.15739610823015812}
episode index:76
target Thresh 15.675579318326335
target distance 11.0
model initialize at round 76
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.196531015704494}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16843911525974026
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([11.02497482, 11.86772463,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 7.269021014558471}
episode index:77
target Thresh 15.741407258309936
target distance 10.0
model initialize at round 77
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.0000087,  7.       ,  0.       ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.944279693554238}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1662796394230769
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([12.95742362, 11.86815981,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.011379666379509}
episode index:78
target Thresh 15.805931717748377
target distance 3.0
model initialize at round 78
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 2.02621912, 11.8677507 ,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.8678395693776744}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16417483386075948
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 1.12993747, 11.87007236,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.966669742857861}
episode index:79
target Thresh 15.869178507285772
target distance 6.0
model initialize at round 79
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.80615479, 5.        , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 4.080427127332934}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.17340389843749998
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.66354087, 9.        , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.3364591300487718}
episode index:80
target Thresh 15.931172926481239
target distance 12.0
model initialize at round 80
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.453624047073713}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1712631095679012
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([10.69667141, 11.88567863,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.166576463093154}
episode index:81
target Thresh 15.991939773929056
target distance 13.0
model initialize at round 81
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.205906925752268}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16917453506097557
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([11.01188463, 11.86749356,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 9.203347041396732}
episode index:82
target Thresh 16.051503357178444
target distance 3.0
model initialize at round 82
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.00005817, 9.        , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 5.000000000338405}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16713628765060237
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 1.12992884, 11.87007129,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 7.9180203282221955}
episode index:83
target Thresh 16.109887502456885
target distance 12.0
model initialize at round 83
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 4.40226316, 10.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.644812165279582}
done in step count: 33
reward sum = 0.18402591023557582
running average episode reward sum: 0.16733735458613777
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.14961787, 11.87113032,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.2173815345805743}
episode index:84
target Thresh 16.16711556420097
target distance 9.0
model initialize at round 84
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.11950064, 4.        , 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 7.055159752135154}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16536867982630085
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 1.58109274, 11.90648131,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.5831803417416173}
episode index:85
target Thresh 16.223210434398432
target distance 4.0
model initialize at round 85
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.00036871, 11.        ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 6.082701924908452}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16344578820041364
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([11.02758722, 11.86778002,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 7.483424363892638}
episode index:86
target Thresh 16.2781945517453
target distance 11.0
model initialize at round 86
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 5.37713779, 11.89724327,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.382099122493491}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16156710097971924
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([12.28951637, 11.88923701,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 4.740557560757472}
episode index:87
target Thresh 16.332089910621647
target distance 7.0
model initialize at round 87
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.00000346, 11.        ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.000000000000647}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15973111119585878
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([12.02906869, 11.867813  ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.062718504284314}
episode index:88
target Thresh 16.384918069889633
target distance 11.0
model initialize at round 88
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.86595846, 3.97936669, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.181255182246378}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15793637960938847
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.10947431, 11.86910598,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 8.913701635498684}
episode index:89
target Thresh 16.436700161517347
target distance 4.0
model initialize at round 89
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.4135302, 8.       , 0.       ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.042304391345091}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.16673708650261748
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.19273284, 9.99997735, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8072671577873689}
episode index:90
target Thresh 16.487456899031883
target distance 5.0
model initialize at round 90
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.30738312,  6.        ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.2724991402012478}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16490481082676453
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.11587761, 11.8693792 ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.871718064190714}
episode index:91
target Thresh 16.53720858580501
target distance 4.0
model initialize at round 91
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.91725202,  9.03054273,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.9711948613425927}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.1734384541873432
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.05344683, 11.03676781,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9472670053758322}
episode index:92
target Thresh 16.585975123174805
target distance 12.0
model initialize at round 92
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 12.20655561573373}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17157352457242553
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12.01498572, 11.86754117,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.061800279566816}
episode index:93
target Thresh 16.63377601840644
target distance 6.0
model initialize at round 93
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 1.12997398, 11.87007695,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 7.918020990522404}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16974827431101674
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 1.13011614, 11.8700944 ,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 7.918022718568868}
episode index:94
target Thresh 16.680630392495363
target distance 2.0
model initialize at round 94
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.0151782, 8.       , 0.       ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 4.119450687979928}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16796145037090077
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 1.13091879, 11.87019231,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 8.089090898271579}
episode index:95
target Thresh 16.726556987815947
target distance 11.0
model initialize at round 95
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 5.78404894, 11.8798884 ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.12544412363498}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16621185192953722
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.43568916, 11.90516632,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.9252824125955845}
episode index:96
target Thresh 16.771574175618678
target distance 1.0
model initialize at round 96
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.13857323,  4.97859623,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 3.1006598610229767}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16449832768284095
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([11.26903884, 11.88646246,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.256719200788552}
episode index:97
target Thresh 16.815699963378915
target distance 10.0
model initialize at round 97
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 9.980393886139279}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16281977331873032
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 2.45381684, 11.9094405 ,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.294772590175895}
episode index:98
target Thresh 16.858952002000116
target distance 2.0
model initialize at round 98
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 2.02105075, 11.86764431,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.989613281311043}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16117512914379364
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 1.14819858, 11.87199923,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.2920329152111485}
episode index:99
target Thresh 16.90134759287443
target distance 2.0
model initialize at round 99
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.00697815, 9.        , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.000012173621881}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1595633778523557
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 1.13120475, 11.87022677,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 4.947111676055349}
episode index:100
target Thresh 16.942903694803487
target distance 1.0
model initialize at round 100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.66439724,  8.99999999,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.200593051985367}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.16788453252708485
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.66439724,  8.99999999,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.200593051985367}
episode index:101
target Thresh 16.98363693078215
target distance 11.0
model initialize at round 101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 4.96076938, 11.86805763,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 13.38231115025456}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1662386057376036
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.42565632, 11.90366638,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.91280943936844}
episode index:102
target Thresh 17.023563594647925
target distance 6.0
model initialize at round 102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([ 2.56860173, 10.        ,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 8.127047496757768}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16462463869160746
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([ 1.1504508 , 11.87220866,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.275234030101863}
episode index:103
target Thresh 17.062699657598724
target distance 10.0
model initialize at round 103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86303194, 5.9815016 , 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.34936007940367}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16304170947341892
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([12.49913898, 11.91244309,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 8.053529626878108}
episode index:104
target Thresh 17.101060774581548
target distance 3.0
model initialize at round 104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.02625676, 11.86775149,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.343947761184709}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.17053655033557683
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.06994461, 11.86911948,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.2729382153553102}
episode index:105
target Thresh 17.138662290554674
target distance 3.0
model initialize at round 105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.40525746, 7.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.1634941704949626}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.1778899791059959
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.12963706, 9.        , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.008367873341083}
episode index:106
target Thresh 17.17551924662586
target distance 7.0
model initialize at round 106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.00494245, 11.86740305,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.079550866406478}
done in step count: 30
reward sum = 0.21463876394293727
running average episode reward sum: 0.17823342569325704
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.91054   , 11.86854555,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.2583538678977688}
episode index:107
target Thresh 17.211646386068956
target distance 3.0
model initialize at round 107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 5.54678149, 11.9117746 ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.7155689910756933}
done in step count: 54
reward sum = 0.06267216326897833
running average episode reward sum: 0.17716341400414334
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.3014515 , 11.88610371,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.1283393962601336}
episode index:108
target Thresh 17.24705816022145
target distance 14.0
model initialize at round 108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.165525060596456}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17553806158208698
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 1.13554464, 11.87072528,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 6.924893423193831}
episode index:109
target Thresh 17.28176873426516
target distance 11.0
model initialize at round 109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([11.02619884, 11.86775028,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 10.255108883531697}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17394226102224983
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 1.14814077, 11.87199381,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 4.94590616544513}
episode index:110
target Thresh 17.315791992892525
target distance 3.0
model initialize at round 110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.00283372, 11.        ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0028337240218512}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.18093377218421153
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.06480959, 11.86890069,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.2765459342460086}
episode index:111
target Thresh 17.34914154586064
target distance 12.0
model initialize at round 111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.000000000000018}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17931829207542394
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 1.13565414, 11.87073732,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.641106512907605}
episode index:112
target Thresh 17.381830733435358
target distance 12.0
model initialize at round 112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 3.65581561, 11.89693719,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.906973537165994}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17773140453493347
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.40683608, 11.90446138,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 5.934181295703143}
episode index:113
target Thresh 17.41387263172757
target distance 8.0
model initialize at round 113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.87827411, 11.87182581,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 8.942458394716516}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1761723571267323
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.71743194, 11.88665298,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 8.891144243196491}
episode index:114
target Thresh 17.44528005792383
target distance 8.0
model initialize at round 114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4.84440629, 4.99992542, 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 8.596058916609786}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.18136895347780418
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.69034942, 11.89204857,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9442638064957731}
episode index:115
target Thresh 17.476065575413383
target distance 7.0
model initialize at round 115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 7.02619884, 11.86775028,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 7.72613538943921}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17980542801678862
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 1.29511791, 11.8894711 ,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 5.931503063916919}
episode index:116
target Thresh 17.506241498813704
target distance 1.0
model initialize at round 116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.86409626, 3.97553075, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.3031970677474332}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.18681563803373916
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.86409626, 3.97553075, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.3031970677474332}
episode index:117
target Thresh 17.535819898896506
target distance 12.0
model initialize at round 117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.86833143, 6.95021976, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.182722227310359}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18523245466057187
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([13.08794264,  2.51019344,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.9564125893021456}
episode index:118
target Thresh 17.564812607416204
target distance 12.0
model initialize at round 118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([1.13277935, 9.01809771, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 14.191854086629023}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18367587941132338
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([1.12903109, 6.86038354, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 13.897627078944138}
episode index:119
target Thresh 17.59323122184278
target distance 7.0
model initialize at round 119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 7.02618485, 11.86774674,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 8.510492309330143}
done in step count: 31
reward sum = 0.2039068257457904
running average episode reward sum: 0.18384447063077725
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13267829, 5.96630296, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.2984561419320386}
episode index:120
target Thresh 17.62108711000093
target distance 13.0
model initialize at round 120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([1.12243838, 1.78631376, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 16.12301883507531}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18232509484044024
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([1.12993541, 1.12992851, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 16.380375545967933}
episode index:121
target Thresh 17.648391414617326
target distance 12.0
model initialize at round 121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.00003593,  9.00001833,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.049913201146472}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18083062684994483
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.12992866,  1.12992924,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 12.239825444421614}
episode index:122
target Thresh 17.675155057777882
target distance 12.0
model initialize at round 122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([1.13232427, 7.02590648, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 13.81436986194159}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17936045915197782
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([1.12994803, 1.12992646, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 12.899428889517147}
episode index:123
target Thresh 17.70138874529672
target distance 1.0
model initialize at round 123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([1.13259815, 6.00493528, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 3.0351519139450898}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17791400383623604
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([1.12992954, 1.12992997, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 6.534143145743293}
episode index:124
target Thresh 17.727102970998622
target distance 11.0
model initialize at round 124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([1.13225469, 3.02621157, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 14.29771592215801}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17649069180554616
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([1.12993053, 1.12992955, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 15.43751404495258}
episode index:125
target Thresh 17.75230802091674
target distance 13.0
model initialize at round 125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.13259643,  8.00494993,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.177863283294798}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17508997202931167
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.12992863,  1.12992945,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.583045721160136}
episode index:126
target Thresh 17.777013977407105
target distance 10.0
model initialize at round 126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.00983381, 9.        , 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 13.407612956777408}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1737113108322305
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([1.12993979, 1.12992752, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.987189919816599}
episode index:127
target Thresh 17.801230723181742
target distance 10.0
model initialize at round 127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.12992871,  1.12992886,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.57041830760446}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17235419121635367
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.12992871,  1.12992885,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.57041831247097}
episode index:128
target Thresh 17.824967945261843
target distance 14.0
model initialize at round 128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([1.12993444, 1.12992995, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 17.314646763439143}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17101811221467653
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([1.12993143, 1.12992929, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 17.314649680547056}
episode index:129
target Thresh 17.848235138852747
target distance 2.0
model initialize at round 129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.00000012,  8.        ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1920929665620993e-07}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.1773948959668713
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.00000012,  8.        ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1920929665620993e-07}
episode index:130
target Thresh 17.871041611142125
target distance 6.0
model initialize at round 130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([10.03066659, 10.13215125,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 7.143756084410852}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1760407364556738
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([1.12993876, 1.12992782, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 14.399863104447968}
episode index:131
target Thresh 17.89339648502298
target distance 13.0
model initialize at round 131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([1.13245309, 5.02541362, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 14.193730482392368}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17470709451282782
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([1.12997405, 1.12992325, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 13.897289422099425}
episode index:132
target Thresh 17.915308702742934
target distance 8.0
model initialize at round 132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.02619883, 10.13224973,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.725345888151082}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17339350733603962
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.12994829, 1.12992703, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.6446675187787054}
episode index:133
target Thresh 17.936787029481238
target distance 6.0
model initialize at round 133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([2.00094974, 7.        , 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.943422446437893}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17209952593800948
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([1.12994675, 1.12992812, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 13.270122967328534}
episode index:134
target Thresh 17.957840056854977
target distance 9.0
model initialize at round 134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.09637564,  8.37539884,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.511159133799444}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17082471463476498
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.12992861,  1.12992966,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 12.787260364913728}
episode index:135
target Thresh 17.978476206355804
target distance 6.0
model initialize at round 135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([13.1358482 ,  6.01824818,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 4.934539893538044}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1695686505565682
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([13.13013963,  1.12990274,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.998861077874285}
episode index:136
target Thresh 17.998703732718674
target distance 10.0
model initialize at round 136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.46932632, 9.        , 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.738261695288024}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16833092318024287
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([1.13470391, 1.12937218, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 16.21558315488233}
episode index:137
target Thresh 18.018530727223837
target distance 10.0
model initialize at round 137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([10.02620642, 10.13224958,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 10.100716529643911}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.17271824212458894
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.13283326, 4.01548424, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8673049742227638}
episode index:138
target Thresh 18.03796512093346
target distance 6.0
model initialize at round 138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([1.13263504, 2.00511049, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 8.493529306895846}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17147566484311708
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([1.13002184, 1.12994383, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.322803820286248}
episode index:139
target Thresh 18.057014687864154
target distance 1.0
model initialize at round 139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.12992876,  1.12992876,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.230466552391753}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.17739369580852338
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.12992876,  1.12992876,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.230466552391753}
episode index:140
target Thresh 18.075687048096686
target distance 2.0
model initialize at round 140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.02619916, 1.13225163, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8681437884300565}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.18322778307229273
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.02619916, 1.13225163, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8681437884300565}
episode index:141
target Thresh 18.093989670824122
target distance 12.0
model initialize at round 141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([3.99518096, 5.00000107, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.054670858833408}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18193744657178362
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86976236, 1.1299658 , 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.570701944082256}
episode index:142
target Thresh 18.111929877339588
target distance 2.0
model initialize at round 142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([1.76346335, 6.00021708, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.7417634561117468}
done in step count: 23
reward sum = 0.3073568677250236
running average episode reward sum: 0.1828145054609671
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.43233447, 8.43418447, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8014932112053209}
episode index:143
target Thresh 18.1295148439649
target distance 3.0
model initialize at round 143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.88382359, 6.65222321, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 3.757642675530351}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18154496028415484
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 6.3975061 , 11.89313338,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.210638240886995}
episode index:144
target Thresh 18.146751604921167
target distance 1.0
model initialize at round 144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.86740546,  5.99484201,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 2.1752670791570647}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1802929260752986
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.87449582,  8.81153123,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 4.890355374436636}
episode index:145
target Thresh 18.163647055142608
target distance 8.0
model initialize at round 145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.99999964,  3.99994922,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 6.000050783157345}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.18493047452683767
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.87334171,  9.84593943,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8868260298165455}
episode index:146
target Thresh 18.18020795303459
target distance 3.0
model initialize at round 146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.86757391,  6.98291485,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 5.32139763270807}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1836724440878796
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.90958983,  7.48427498,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 5.807220110077135}
episode index:147
target Thresh 18.196440923177075
target distance 9.0
model initialize at round 147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([11.26414494, 10.1159581 ,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.49593309005471}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18243141406025878
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 9.49690387, 10.09830787,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 8.210073754248155}
episode index:148
target Thresh 18.212352458974568
target distance 3.0
model initialize at round 148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.9997102 ,  6.85123551,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.1487645276347695}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.1875828810799886
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.85971104,  8.09445854,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.864884669055949}
episode index:149
target Thresh 18.227948925253543
target distance 11.0
model initialize at round 149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.11746333,  7.2482779 ,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.194177845769113}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18633232853945533
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.12383114,  5.80991896,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.125615424935802}
episode index:150
target Thresh 18.24323656080847
target distance 12.0
model initialize at round 150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.13244368,  6.04615208,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.580428033682754}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18509833960873046
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.08929562,  3.54528224,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.104019931757563}
episode index:151
target Thresh 18.258221480897404
target distance 1.0
model initialize at round 151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.2999077 ,  5.53089664,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8786241946188529}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.19045953474288355
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.2999077 ,  5.53089664,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8786241946188529}
episode index:152
target Thresh 18.272909679688183
target distance 9.0
model initialize at round 152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.42083254,  9.03656173,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 7.178576882986561}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.19355078243272786
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.73575128,  2.78665548,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8298519382242892}
episode index:153
target Thresh 18.287307032656177
target distance 4.0
model initialize at round 153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.86762582, 5.97977471, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.751242701902449}
done in step count: 22
reward sum = 0.323533544973709
running average episode reward sum: 0.19439482634533164
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.9050225 , 8.77459151, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.191242101230276}
episode index:154
target Thresh 18.301419298934526
target distance 3.0
model initialize at round 154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.99057972, 9.6074169 , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.7892420243244267}
done in step count: 39
reward sum = 0.13527595427905592
running average episode reward sum: 0.1940134142674847
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.82387163, 7.92142132, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.2360346702060745}
episode index:155
target Thresh 18.31525212361792
target distance 2.0
model initialize at round 155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.01922643,  8.77577579,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7760140026857932}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.19917999494525723
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.01922643,  8.77577579,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7760140026857932}
episode index:156
target Thresh 18.328811040020668
target distance 9.0
model initialize at round 156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.91555774,  9.06243646,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 7.317606865320928}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.19791133255707088
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.46474575,  3.27501747,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.3570770626321869}
episode index:157
target Thresh 18.342101471890118
target distance 2.0
model initialize at round 157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.1378386 , 10.10971594,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.2393256174739564}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.20298784311050713
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.1378386 , 10.10971594,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.2393256174739564}
episode index:158
target Thresh 18.355128735576226
target distance 10.0
model initialize at round 158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.09244517,  7.55394095,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 9.206717433097577}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2017111900091832
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.1127546 ,  4.71110555,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.071589353749845}
episode index:159
target Thresh 18.36789804215817
target distance 6.0
model initialize at round 159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.10086911,  6.38899057,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.06705624762523}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.20504481938803204
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.48623279, 11.87151726,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0116813193078225}
episode index:160
target Thresh 18.380414499528833
target distance 1.0
model initialize at round 160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.76468813, 10.47397783,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8996682331206763}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.2099824292054977
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.76468813, 10.47397783,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8996682331206763}
episode index:161
target Thresh 18.39268311443806
target distance 12.0
model initialize at round 161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.10544948,  4.6621271 ,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.127118121986106}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.20868624137089586
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.12364276,  3.20786608,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.154586100863343}
episode index:162
target Thresh 18.404708794495395
target distance 1.0
model initialize at round 162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.80382097, 2.81239563, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.210801253785387}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.21323417854039958
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.41550851, 3.29188657, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.5077844911049662}
episode index:163
target Thresh 18.416496350133208
target distance 4.0
model initialize at round 163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.00002944,  7.44358432,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.08022183805719}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.2157769543403033
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.54393748, 10.08478001,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.022555942676219}
episode index:164
target Thresh 18.428050496530922
target distance 6.0
model initialize at round 164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.59064937, 6.73021913, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 4.289358536585292}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.21966542734430147
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.69285374, 10.15049274,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9033279600491887}
episode index:165
target Thresh 18.43937585550115
target distance 8.0
model initialize at round 165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.87944192, 4.78528744, 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 9.451190380914522}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.2227704060387635
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.03034196, 11.85120789,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.2902680315500477}
episode index:166
target Thresh 18.450476957338495
target distance 13.0
model initialize at round 166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.        ,  6.45768356,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.013360391705554}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.22143645151158528
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.09912417,  1.61530328,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.444966912425134}
episode index:167
target Thresh 18.4613582426317
target distance 13.0
model initialize at round 167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.88889584, 7.69113942, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.132578733474196}
done in step count: 96
reward sum = 0.007268856709612724
running average episode reward sum: 0.2201616443996688
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.16935253,  6.98239494,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.1702651365672308}
episode index:168
target Thresh 18.472024064039974
target distance 12.0
model initialize at round 168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.        ,  3.02492893,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 11.649097575450371}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2188589127760021
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.12977525,  1.13118794,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 12.052841963056697}
episode index:169
target Thresh 18.48247868803409
target distance 3.0
model initialize at round 169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.4474591 ,  5.66211998,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.402287328901594}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.22189557735158444
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.25378042,  7.23312466,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8077761406239895}
episode index:170
target Thresh 18.492726296603045
target distance 4.0
model initialize at round 170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.79704922,  4.80528688,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.201153059274337}
done in step count: 14
reward sum = 0.48767497911552954
running average episode reward sum: 0.22344984285897596
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.41047807,  7.00299265,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.1582572066513275}
episode index:171
target Thresh 18.502770988926898
target distance 9.0
model initialize at round 171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 7.98125428, 10.13719438,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 9.323515773395764}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.22664944224642378
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.14337235,  4.48115631,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.5020627669235689}
episode index:172
target Thresh 18.512616783016515
target distance 12.0
model initialize at round 172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.        ,  5.01239443,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.436751748141415}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.22533932986349647
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.12985398,  1.13053746,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 11.425574351999796}
episode index:173
target Thresh 18.52226761732081
target distance 12.0
model initialize at round 173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.        ,  9.00048208,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.20683207820686}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2240442762435913
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.12992227,  1.12998099,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.171282007318988}
episode index:174
target Thresh 18.531727352302184
target distance 3.0
model initialize at round 174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.58874106, 8.1675868 , 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.8316415391107461}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.2281925946650565
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.23352122, 6.63352609, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.43455182737407383}
episode index:175
target Thresh 18.540999771980758
target distance 6.0
model initialize at round 175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 8.02544256, 10.13244535,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.181700643828101}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.23202388674082328
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.87087503, 8.16944377, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8405337073109442}
episode index:176
target Thresh 18.55008858544804
target distance 1.0
model initialize at round 176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.91039547,  3.65171182,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.1439155038618791}
done in step count: 44
reward sum = 0.10467395472325501
running average episode reward sum: 0.23130439559948107
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.99204829,  3.02642884,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9736036333107405}
episode index:177
target Thresh 18.558997428350605
target distance 11.0
model initialize at round 177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.12992382,  1.12997848,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.570398768214286}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.23000493270285477
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.12992084,  1.12999257,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.570391689727677}
episode index:178
target Thresh 18.567729864344397
target distance 7.0
model initialize at round 178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.00022602, 9.00083351, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.099881165378101}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.23282664755158183
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.80944778, 3.12497792, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8955298971274962}
episode index:179
target Thresh 18.576289386520244
target distance 12.0
model initialize at round 179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.        ,  2.00504243,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.438858693540508}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.23153316617629527
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.12992197,  1.12998339,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.916274688487288}
episode index:180
target Thresh 18.58467941880115
target distance 12.0
model initialize at round 180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.13197595,  6.03777524,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 12.187885914615887}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.23025397741289033
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.12989795,  1.13017837,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 14.87575232865634}
episode index:181
target Thresh 18.5929033173119
target distance 11.0
model initialize at round 181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.13223507,  2.02689071,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.13227074894909}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.22898884566886346
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.12992723,  1.12994084,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.16722324737643}
episode index:182
target Thresh 18.600964371721542
target distance 13.0
model initialize at round 182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.99980617, 5.7048682 , 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.00415226962801}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.23136278875968422
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.70219922,  5.77990041,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.370309512332968}
episode index:183
target Thresh 18.60886580655933
target distance 8.0
model initialize at round 183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.74686167,  9.00070492,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 6.047004374184009}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.23453204670120767
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.04266297,  3.52429163,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5260245626853495}
episode index:184
target Thresh 18.616610782504544
target distance 2.0
model initialize at round 184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.00308847, 5.0986867 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9013185964269971}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.23866971131363357
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.00308847, 5.0986867 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9013185964269971}
episode index:185
target Thresh 18.62420239765084
target distance 6.0
model initialize at round 185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 8.98025043, 11.86542219,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 4.111853861375674}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.24133864776154415
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.01109538, 11.6795328 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.199873813827333}
episode index:186
target Thresh 18.631643688745495
target distance 3.0
model initialize at round 186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.0000211 ,  6.60022998,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 4.511986086202318}
done in step count: 15
reward sum = 0.46329123015975304
running average episode reward sum: 0.24252555996688216
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.3170781 , 11.81480659,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0631519654787978}
episode index:187
target Thresh 18.638937632404165
target distance 11.0
model initialize at round 187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.13259264,  8.00528115,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.3501551270556}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.24123553039259024
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.12992516,  1.1299576 ,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.347600995075341}
episode index:188
target Thresh 18.646087146301564
target distance 6.0
model initialize at round 188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.47497718,  9.53115483,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.709875468085209}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.24426870880321147
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.98549372, 10.15651535,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8436093765109156}
episode index:189
target Thresh 18.653095090338585
target distance 12.0
model initialize at round 189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.13220761,  3.02815977,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.6328509091042}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.24298308402003665
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.12991935,  1.1300046 ,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.631285059029608}
episode index:190
target Thresh 18.659964267786282
target distance 12.0
model initialize at round 190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.8677555 , 3.97348587, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 13.1642990109226}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.24501065640592448
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.04991368, 10.39659316,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6054677414346084}
episode index:191
target Thresh 18.66669742640722
target distance 3.0
model initialize at round 191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.00026095,  3.01814401,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.018144044937958}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.24868247590381032
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.10301251,  1.37040105,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0958929631152734}
episode index:192
target Thresh 18.673297259554634
target distance 3.0
model initialize at round 192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.78207284,  4.34863806,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.6656795942164793}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.2523162454587129
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.1291856 ,  5.23315912,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1603286878935202}
episode index:193
target Thresh 18.67976640724977
target distance 12.0
model initialize at round 193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.1321784,  2.0294746,  0.       ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 13.69140866025642}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.25101564625531747
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.12991884,  1.13000875,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 14.232070763072686}
episode index:194
target Thresh 18.68610745723797
target distance 11.0
model initialize at round 194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.89297729, 6.65169426, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.81186698016357}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.24972838653093124
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.8725326 , 7.14693499, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.02535440246363}
episode index:195
target Thresh 18.692322946023772
target distance 5.0
model initialize at round 195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([5.85410784, 9.82357753, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 4.249407137334827}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.2530588539465898
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.19016338, 6.11946852, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8186012938779171}
episode index:196
target Thresh 18.698415359885573
target distance 9.0
model initialize at round 196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([3.99801409, 3.99934804, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.281282536583599}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.25570211325396747
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.53140118, 10.65510696,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.6335127631429921}
episode index:197
target Thresh 18.704387135870142
target distance 2.0
model initialize at round 197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.58637404, 10.92306995,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.5913989786579276}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.2594611934900586
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.58637404, 10.92306995,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.5913989786579276}
episode index:198
target Thresh 18.710240662767504
target distance 2.0
model initialize at round 198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.00042903, 9.03392339, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9660767077395387}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.26318249402528443
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.00042903, 9.03392339, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9660767077395387}
episode index:199
target Thresh 18.715978282066462
target distance 12.0
model initialize at round 199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 5.94310373, 10.04300438,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 10.10232662212367}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.265735486242658
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.14076858, 11.8391301 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.2010070546181015}
episode index:200
target Thresh 18.72160228889124
target distance 13.0
model initialize at round 200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.02469115,  1.13264957,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 14.148276165746319}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2644134191469234
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.12991947,  1.13000366,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 13.464475611493153}
episode index:201
target Thresh 18.727114932919555
target distance 13.0
model initialize at round 201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.        ,  8.09923053,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.163015926986414}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2631044418244139
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.12991857,  1.13001092,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 13.46447015209234}
episode index:202
target Thresh 18.732518419282524
target distance 5.0
model initialize at round 202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13387833, 8.02316473, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.144788028331353}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.26625417363808673
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13362262, 4.06311682, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.2760720395380993}
episode index:203
target Thresh 18.737814909446737
target distance 4.0
model initialize at round 203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.78470874,  6.98991227,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.1578277225660485}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.2693730257280961
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.22350845,  9.8754189 ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.1701698100063203}
episode index:204
target Thresh 18.74300652207888
target distance 4.0
model initialize at round 204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.92787004,  3.97446263,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 2.227946239490712}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.2724614499928371
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.24939418,  6.58734965,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.6381042793939735}
episode index:205
target Thresh 18.74809533389323
target distance 5.0
model initialize at round 205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.73601496,  3.87806165,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.1330794991112336}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.2755198895559787
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.11791365,  6.68824899,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9355559909707143}
episode index:206
target Thresh 18.753083380482362
target distance 8.0
model initialize at round 206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.53325081, 4.9982909 , 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 6.194462866070526}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.27833078380933146
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.19434177, 10.13736892,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.1803463732167263}
episode index:207
target Thresh 18.757972657131422
target distance 12.0
model initialize at round 207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([5.17647922, 9.14584735, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 10.981187742880712}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.28052675066902216
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.99780975, 10.50325553,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.4967492981697829}
episode index:208
target Thresh 18.76276511961626
target distance 12.0
model initialize at round 208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([3.99999464, 6.99215871, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.000008438706601}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.28270170349177803
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.34234582,  7.660461  ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.743914913684407}
episode index:209
target Thresh 18.767462684985773
target distance 4.0
model initialize at round 209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.11751848,  6.23905103,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.690830918363044}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.28135550490372196
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.08787615,  6.52758967,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.434835162575387}
episode index:210
target Thresh 18.77206723232874
target distance 11.0
model initialize at round 210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([5.17635734, 9.14563374, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 9.860724959283605}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.28368927472645317
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.98681891,  9.92238184,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.07872940508371522}
episode index:211
target Thresh 18.776580603525492
target distance 12.0
model initialize at round 211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.12992294,  1.12996238,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.58302532732819}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.28235111777019634
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.12991859,  1.13001081,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.582998879984933}
episode index:212
target Thresh 18.78100460398469
target distance 3.0
model initialize at round 212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.18731491,  5.98677862,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.0303904345788804}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.2854856195647025
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.12835614,  7.65831387,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.0923095550235205}
episode index:213
target Thresh 18.785341003365506
target distance 11.0
model initialize at round 213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.86767852, 7.97715678, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.13235005279158}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.28776737338683006
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.99917997,  8.46312851,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.4631292363293268}
episode index:214
target Thresh 18.78959153628551
target distance 8.0
model initialize at round 214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.12110515,  8.22879275,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 6.29049404416617}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.2904167111850309
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.10884414,  2.35146104,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.9579580483504356}
episode index:215
target Thresh 18.793757903014548
target distance 3.0
model initialize at round 215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.31769704,  9.73876391,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.3006336489688854}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.29325043011472984
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.33426162, 11.88293789,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9440922409359398}
episode index:216
target Thresh 18.797841770154854
target distance 3.0
model initialize at round 216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 5.9740949, 11.8675708,  0.       ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.3435625679944363}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.2962769258284868
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.67678752, 11.85941021,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9181787523246618}
episode index:217
target Thresh 18.801844771307746
target distance 7.0
model initialize at round 217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.02355349,  7.00126731,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 5.377640332899033}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.29491785736138365
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.20119381,  3.46088868,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.4746778276720331}
episode index:218
target Thresh 18.805768507727056
target distance 2.0
model initialize at round 218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.13212758,  6.03534834,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8685919921307801}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.2981374105241171
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.13212758,  6.03534834,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8685919921307801}
episode index:219
target Thresh 18.80961454895967
target distance 2.0
model initialize at round 219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.98204994, 6.48693645, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.513377454623293}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.30132769502173473
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.98204994, 6.48693645, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.513377454623293}
episode index:220
target Thresh 18.81338443347336
target distance 4.0
model initialize at round 220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.99991512, 9.99918509, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.2365084414670893}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.30426286382254136
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.99428943, 11.86682342,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8668422288195954}
episode index:221
target Thresh 18.8170796692722
target distance 8.0
model initialize at round 221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([3.99994624, 5.99998617, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.810299830770432}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.30656125745397134
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.00800717, 11.86436548,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8644025679762757}
episode index:222
target Thresh 18.820701734499778
target distance 3.0
model initialize at round 222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([13.91534434,  3.12414598,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.5621082196688676}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.3094466329810836
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.76136313,  2.0963192 ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.25734207183451396}
episode index:223
target Thresh 18.82425207803048
target distance 7.0
model initialize at round 223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.99854648, 5.99994373, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.099359942332457}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.31170136341420374
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.26933342, 11.88149628,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.921724568224513}
episode index:224
target Thresh 18.827732120049056
target distance 11.0
model initialize at round 224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 6.98086409, 11.86513737,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 9.209970143698298}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.31375505041014057
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.62872962,  9.65795166,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5048155710613343}
episode index:225
target Thresh 18.831143252618716
target distance 12.0
model initialize at round 225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.13221077,  2.02821064,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 13.53346590602918}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.31236675372690986
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.12985054,  1.1305657 ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 14.142828761100498}
episode index:226
target Thresh 18.83448684023797
target distance 2.0
model initialize at round 226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.80768776, 6.98786085, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8077789755488282}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.3153959750761305
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.80768776, 6.98786085, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8077789755488282}
episode index:227
target Thresh 18.837764220386454
target distance 4.0
model initialize at round 227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.1326031 , 8.01104101, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.1901286101469553}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.3181793260626387
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.22894468, 6.01217693, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.7711514671565141}
episode index:228
target Thresh 18.840976704059912
target distance 12.0
model initialize at round 228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.13254136,  4.00944303,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.294868446644175}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.3167898966911862
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.12992048,  1.12999544,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 12.82779251308975}
episode index:229
target Thresh 18.84412557629466
target distance 11.0
model initialize at round 229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.00003577,  9.00022424,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.486937826442995}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.31541254931426793
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.12992686,  1.12994384,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.347608970389125}
episode index:230
target Thresh 18.84721209668157
target distance 11.0
model initialize at round 230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.13224651,  6.02635157,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.574555457421384}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.3140471270228642
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.12992826,  1.12993248,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.301096984195881}
episode index:231
target Thresh 18.85023749986995
target distance 3.0
model initialize at round 231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3600449, 4.0051682, 0.       ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1915979371118042}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.316204278415007
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.31315947, 3.97644985, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.025438030868942}
episode index:232
target Thresh 18.853202996061427
target distance 14.0
model initialize at round 232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.        ,  3.00139391,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.000000080958221}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.31484717850764643
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.12992821,  1.12993287,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.285940505908847}
episode index:233
target Thresh 18.856109771494005
target distance 8.0
model initialize at round 233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.8676415 , 3.97422024, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 6.087924403012643}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.317165673471289
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.51922482, 9.9181657 , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.525634154370856}
episode index:234
target Thresh 18.858958988916616
target distance 6.0
model initialize at round 234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 9.97534995, 10.13266109,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 6.522424236030062}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.3194644365629006
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.13347819,  5.97825239,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.3068426755358507}
episode index:235
target Thresh 18.861751788054224
target distance 8.0
model initialize at round 235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.48453927,  9.00000262,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 6.022103555653856}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.3213895064821256
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.08090134,  3.00089936,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9190991007850171}
episode index:236
target Thresh 18.86448928606372
target distance 14.0
model initialize at round 236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.00432011,  1.13270111,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.361470641575837}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.3200334326151124
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.12992645,  1.12994713,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.583035545291388}
episode index:237
target Thresh 18.867172577980806
target distance 6.0
model initialize at round 237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([5.85399171, 9.8234717 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 5.428829337959988}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.3224807711335363
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.79633311, 5.89972742, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.22701275776690072}
episode index:238
target Thresh 18.86980273715803
target distance 11.0
model initialize at round 238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.86740411, 5.99496329, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.13259727516488}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.32390729690824566
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.1708562 ,  6.96333196,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.2710184535655629}
episode index:239
target Thresh 18.87238081569413
target distance 11.0
model initialize at round 239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.86740305, 5.99505671, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.35008687978923}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.32562056604873213
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.07236168,  8.14049454,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.15803458616381943}
episode index:240
target Thresh 18.874907844854896
target distance 13.0
model initialize at round 240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.99982388, 8.997785  , 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.04533610323545}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.3274801526522643
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.03150351,  7.66731811,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0240423290165168}
episode index:241
target Thresh 18.877384835485685
target distance 12.0
model initialize at round 241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([3.99999809, 4.99998641, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.198043562693634}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.3290126201871465
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.95616314,  7.28299045,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.2863656140102655}
episode index:242
target Thresh 18.87981277841578
target distance 13.0
model initialize at round 242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.87042555, 7.90187317, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 11.501664898789805}
done in step count: 82
reward sum = 0.014905125382474753
running average episode reward sum: 0.32771999675173635
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.22942238,  5.23884361,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.33118106527088836}
episode index:243
target Thresh 18.882192644854722
target distance 6.0
model initialize at round 243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.92492497,  9.00002635,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.141979850693499}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.33007565250275384
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.00295962,  5.03500942,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.035134296193459286}
episode index:244
target Thresh 18.88452538678082
target distance 2.0
model initialize at round 244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.01753294, 10.73020458,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0188381040424095}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.33281003759457933
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.01753294, 10.73020458,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0188381040424095}
episode index:245
target Thresh 18.88681193732195
target distance 12.0
model initialize at round 245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.13200292,  6.0367391 ,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 11.519652955170468}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.33145715126289405
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.12893428,  1.13872036,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.625450298674068}
episode index:246
target Thresh 18.889053211128815
target distance 12.0
model initialize at round 246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.12987801,  1.13023376,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.148613403918828}
done in step count: 76
reward sum = 0.020276547153583634
running average episode reward sum: 0.3301973107604272
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.59134184, 5.71449274, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.49851367948235314}
episode index:247
target Thresh 18.89125010474082
target distance 3.0
model initialize at round 247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.13285096,  4.0084016 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.3299703925963025}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.33269651515252224
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.15380747,  2.04403517,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.2766795040105048}
episode index:248
target Thresh 18.8934034969447
target distance 10.0
model initialize at round 248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.13175128,  4.95422338,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.338515431365}
done in step count: 88
reward sum = 0.01095663679740604
running average episode reward sum: 0.3314043871269997
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.32676708, 3.50141672, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.5984943189751063}
episode index:249
target Thresh 18.895514249126055
target distance 3.0
model initialize at round 249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.05569881, 10.04504809,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9565748845012696}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.3340787695784917
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.05569881, 10.04504809,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9565748845012696}
episode index:250
target Thresh 18.8975832056139
target distance 10.0
model initialize at round 250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.87548181, 7.17025897, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.19925748090247}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.33274777846463316
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.87498872, 7.16751718, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.199398201528576}
episode index:251
target Thresh 18.899611194018416
target distance 6.0
model initialize at round 251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.71767342,  7.00005937,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 4.010010377866485}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.3350086999786624
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.95966816,  3.0189749 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.04457246009200456}
episode index:252
target Thresh 18.901599025562
target distance 12.0
model initialize at round 252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.13259686,  6.99504909,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.181337916275181}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.33659005646343054
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.75180005, 6.86322269, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.898196315357463}
episode index:253
target Thresh 18.90354749540378
target distance 12.0
model initialize at round 253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.87162555, 4.88142226, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.495360461253378}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.3352648987608186
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.9115559 , 5.53557733, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.638466374378057}
episode index:254
target Thresh 18.90545738295767
target distance 5.0
model initialize at round 254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.86836922,  5.00038505,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 3.003271064371528}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.3374893501382272
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.02558892,  1.13340121,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8669765034132749}
episode index:255
target Thresh 18.907329452204163
target distance 4.0
model initialize at round 255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 6.98106558, 11.86497687,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 2.1964246330296007}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.33988196986424973
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.97094906, 10.73702495,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.26457481422515605}
episode index:256
target Thresh 18.90916445199591
target distance 2.0
model initialize at round 256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.53206468,  4.03808844,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.468429379963675}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.3422559699815095
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.46844661,  3.95329273,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.47076936735632247}
episode index:257
target Thresh 18.9109631163573
target distance 6.0
model initialize at round 257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.02571571, 11.86762637,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.11815037054114}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.34442745846995326
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.0587391 , 11.07132207,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.09239653709454307}
episode index:258
target Thresh 18.912726164778057
target distance 6.0
model initialize at round 258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([14.0286386 ,  9.00019193,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.459574107104556}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.34640795090829324
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.46853817,  5.01563179,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.4687988534396274}
episode index:259
target Thresh 18.91445430250106
target distance 12.0
model initialize at round 259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([12.00493006, 11.86740104,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.126105260378381}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.3480516931644152
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.08280385, 6.85293059, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.16877763499158926}
episode index:260
target Thresh 18.916148220804434
target distance 14.0
model initialize at round 260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([3.99999988, 6.9999249 , 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.000000119444291}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.3495346057983638
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.19833767,  6.90699066,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8070397946440308}
episode index:261
target Thresh 18.917808597278096
target distance 4.0
model initialize at round 261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([3.99999559, 8.99998534, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.8284406117469763}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.3518264584479884
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.99959719, 10.99698639,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.0030404119345260796}
episode index:262
target Thresh 18.91943609609477
target distance 3.0
model initialize at round 262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.32163239,  5.01554656,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.9616963746564857}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.35410088256035344
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.23744047,  3.07252395,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9573869624124275}
episode index:263
target Thresh 18.92103136827568
target distance 12.0
model initialize at round 263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.00016795,  9.00005583,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.440483414579012}
done in step count: 15
reward sum = 0.46329123015975304
running average episode reward sum: 0.35451448236186633
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.56687268, 6.99857195, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0884600230470374}
episode index:264
target Thresh 18.92259505195097
target distance 1.0
model initialize at round 264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.60143328, 10.89026511,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.4133971205596203}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.35695027676804797
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.60143328, 10.89026511,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.4133971205596203}
episode index:265
target Thresh 18.924127772614966
target distance 10.0
model initialize at round 265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12.00494025, 11.86740269,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.051798296127757}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.35867041200576205
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.10649317, 10.22854258,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7787729794795096}
episode index:266
target Thresh 18.92563014337636
target distance 6.0
model initialize at round 266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 8.97921169, 11.86602661,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 4.112996561076397}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.3607072269420701
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.97564895, 10.21890137,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7814781159381843}
episode index:267
target Thresh 18.927102765203493
target distance 11.0
model initialize at round 267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.13224951,  4.97379114,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.13228341090142}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.3617129813554378
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.58196634, 5.76859608, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8749240456234683}
episode index:268
target Thresh 18.928546227164734
target distance 3.0
model initialize at round 268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.02020454, 8.00602305, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.404308196692933}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.3638999219451945
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.93908966, 6.04013753, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.3428422633695163}
episode index:269
target Thresh 18.92996110666411
target distance 2.0
model initialize at round 269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.00045729, 8.71366227, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.286337814550448}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.3660706629750271
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.98364206, 10.2925967 ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.2930535952648769}
episode index:270
target Thresh 18.931347969672288
target distance 3.0
model initialize at round 270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([13.66534853,  3.01020765,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.6738620161582125}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.3682253837758573
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.85876869,  1.12766946,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8836893405669701}
episode index:271
target Thresh 18.93270737095296
target distance 5.0
model initialize at round 271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.86693579, 4.99752306, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.535578732977874}
done in step count: 41
reward sum = 0.12208654873684796
running average episode reward sum: 0.36732046158821385
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.43787163, 7.04673783, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.1066603225094245}
episode index:272
target Thresh 18.93403985428477
target distance 2.0
model initialize at round 272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.98628926,  5.02027673,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9798192074660679}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.3696379690549237
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.98628926,  5.02027673,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9798192074660679}
episode index:273
target Thresh 18.935345952678812
target distance 6.0
model initialize at round 273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([11.98119155, 11.86488057,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.110819839894918}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.37158272099267947
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.87186393, 10.71291099,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.3143866277112523}
episode index:274
target Thresh 18.93662618859186
target distance 11.0
model initialize at round 274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([12.        , 10.99188066,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.396771257588762}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.3729045725186152
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.63402463, 3.93122245, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.6377441371281197}
episode index:275
target Thresh 18.937881074135348
target distance 4.0
model initialize at round 275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.36525822, 9.00397503, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.586174130541935}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.3749954979805043
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.29325724, 7.01027048, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.29343702805299643}
episode index:276
target Thresh 18.939111111280226
target distance 12.0
model initialize at round 276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([3.99999952, 4.9999752 , 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.000000476867912}
done in step count: 11
reward sum = 0.5688000922764597
running average episode reward sum: 0.3756951535555799
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.12496631,  5.19736375,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8970152808498284}
episode index:277
target Thresh 18.940316792057754
target distance 11.0
model initialize at round 277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.86741607, 8.00602789, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.930528822562083}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.3743437321399124
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.90857456, 6.57938085, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.179624030212352}
episode index:278
target Thresh 18.94149859875632
target distance 5.0
model initialize at round 278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.00385535, 9.00363338, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 3.0036358542227863}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.3762367653580489
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.50594572, 5.01019416, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.1062573022704851}
episode index:279
target Thresh 18.94265700411436
target distance 8.0
model initialize at round 279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.96156037,  8.00001001,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 6.000133145883605}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.3779551161960559
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.63330984,  2.05520201,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.3708219710350695}
episode index:280
target Thresh 18.94379247150946
target distance 14.0
model initialize at round 280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.        , 5.15782726, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.192465643824596}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.3766100801953582
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.8892363 , 7.70503243, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.06591895179675}
episode index:281
target Thresh 18.94490545514372
target distance 8.0
model initialize at round 281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.1322497 ,  6.97379982,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.19019374882533}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.3781629034925377
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.99778765, 11.85651278,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.3149883388668162}
episode index:282
target Thresh 18.945996400225436
target distance 7.0
model initialize at round 282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([12.02615094, 11.86773815,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 5.100506133308096}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.37985623245546163
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.23852854, 11.09271495,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.7670950711553784}
episode index:283
target Thresh 18.94706574314718
target distance 5.0
model initialize at round 283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([13.04497072,  9.05654468,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 4.251430749878047}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.3816965274116044
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.88382006,  5.28109543,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.139281207919119}
episode index:284
target Thresh 18.94811391166039
target distance 9.0
model initialize at round 284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([3.99999976, 7.99999988, 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.615773371964005}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.3832151580171777
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.85836219, 11.86906419,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.2214983476971635}
episode index:285
target Thresh 18.949141325046437
target distance 10.0
model initialize at round 285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 5.94478272, 10.04568018,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.111550517896138}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.3847231688283064
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.8770713 , 11.26026938,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.2878395634771107}
episode index:286
target Thresh 18.950148394284376
target distance 11.0
model initialize at round 286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.00017558,  8.99790211,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.815636375093982}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.3858159009790571
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.36744882, 3.25309943, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.6813077988110736}
episode index:287
target Thresh 18.951135522215335
target distance 7.0
model initialize at round 287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.97206903, 5.        , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 5.3748540668115945}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.38745325896176874
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.52419174, 10.81553864,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9441910739460765}
episode index:288
target Thresh 18.952103103703642
target distance 8.0
model initialize at round 288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.583011083224646}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.3887900329359495
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.01951987, 11.85571727,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8559398797089939}
episode index:289
target Thresh 18.953051525794802
target distance 3.0
model initialize at round 289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.89911878,  9.0107497 ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.3527858418386332}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.390725239718929
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.09479093,  7.48602441,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.0409487848902452}
episode index:290
target Thresh 18.95398116787029
target distance 5.0
model initialize at round 290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.86728631, 7.99575235, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 3.5372676056979713}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.3923288471425753
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.04949511, 10.9085183 ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9548970890702085}
episode index:291
target Thresh 18.95489240179933
target distance 7.0
model initialize at round 291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([10.02614569, 11.86773682,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.10050072895394}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.39392147095373087
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.15297092, 10.68447525,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9038883346082923}
episode index:292
target Thresh 18.95578559208765
target distance 8.0
model initialize at round 292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.71491408,  5.        ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.042441737142326}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.3955032236125919
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.25589695, 11.03813917,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7450798228155003}
episode index:293
target Thresh 18.956661096023268
target distance 5.0
model initialize at round 293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.01895559, 9.00868297, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.008742678532965}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.3972277024438416
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.88697434, 5.29779581, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.131288740068466}
episode index:294
target Thresh 18.957519263819442
target distance 12.0
model initialize at round 294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 5.97399615, 11.86770068,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 10.198483176321954}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.3985041540880997
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.8874626 ,  9.90065266,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.15011515448056093}
episode index:295
target Thresh 18.95836043875472
target distance 13.0
model initialize at round 295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.99983213, 9.00007702, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.0455212278889}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.39964127481964323
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.79844673, 10.61700751,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0090666221954605}
episode index:296
target Thresh 18.959184957310303
target distance 1.0
model initialize at round 296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.88977989, 7.66532103, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9506409777709156}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4016626846687354
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.88977989, 7.66532103, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9506409777709156}
episode index:297
target Thresh 18.959993149304598
target distance 7.0
model initialize at round 297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 8.        , 10.84724572,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 8.478488897867747}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.40304806576045105
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.88317056, 3.95579006, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8842764035747865}
episode index:298
target Thresh 18.96078533802519
target distance 7.0
model initialize at round 298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 8.97606519, 10.13286717,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 7.927924111299502}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.4044241800890114
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.28607112,  4.98890642,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.2196845330778578}
episode index:299
target Thresh 18.961561840358115
target distance 11.0
model initialize at round 299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 6.        , 10.99999428,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.000000000001837}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.4056553692803813
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.08680872, 11.29651581,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.3089617806754772}
episode index:300
target Thresh 18.962322966914673
target distance 2.0
model initialize at round 300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.9920635 ,  8.10609055,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.10638699146998096}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4076299361598485
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.9920635 ,  8.10609055,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.10638699146998096}
episode index:301
target Thresh 18.963069022155626
target distance 4.0
model initialize at round 301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.43276709, 4.00369418, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 2.0498969999923546}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.40942586352355764
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.44493616, 2.08281302, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.5612075085415218}
episode index:302
target Thresh 18.963800304513025
target distance 1.0
model initialize at round 302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.63199282, 7.03806746, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.029924025445508}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.41137495308288585
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.63199282, 7.03806746, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.029924025445508}
episode index:303
target Thresh 18.964517106509557
target distance 3.0
model initialize at round 303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.00088847, 9.07440722, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.4683761770024195}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.4131467460003764
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.32017291, 7.38208282, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9186873892993147}
episode index:304
target Thresh 18.965219714875584
target distance 12.0
model initialize at round 304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([3.99999988, 6.        , 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 11.180339994122978}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.41420230385160467
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.95673226, 11.85139282,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8524915460230642}
episode index:305
target Thresh 18.965908410663815
target distance 7.0
model initialize at round 305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.24757385,  7.00095522,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 5.29910846795284}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.41565058063640337
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.56749433,  1.3826151 ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.7538071827732087}
episode index:306
target Thresh 18.966583469361755
target distance 12.0
model initialize at round 306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.86775042, 7.97379912, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.132280414279638}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.41669110607610566
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.05403739,  8.15374529,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.16296518916739508}
episode index:307
target Thresh 18.967245161001877
target distance 9.0
model initialize at round 307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 8.97396094, 11.86017098,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 8.016616800193376}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.4179827136862482
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.87078334,  7.43296111,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0391326776875545}
episode index:308
target Thresh 18.967893750269663
target distance 13.0
model initialize at round 308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 3.99506331, 11.86740211,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.039067802190749}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.4190089569773121
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.94405621, 11.49097762,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.49415456144890724}
episode index:309
target Thresh 18.968529496609467
target distance 1.0
model initialize at round 309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.82516901, 10.41307641,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9227870901133467}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.420883121632224
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.82516901, 10.41307641,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9227870901133467}
episode index:310
target Thresh 18.969152654328305
target distance 13.0
model initialize at round 310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([3.99999988, 4.        , 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.704700022751727}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.421775257241425
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.04045257,  8.91844103,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.3282564461663255}
episode index:311
target Thresh 18.96976347269757
target distance 3.0
model initialize at round 311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.73993534, 7.03579593, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.27294049650627}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.4234682852630871
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.55480374, 5.12327886, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0375197145888113}
episode index:312
target Thresh 18.970362196052754
target distance 3.0
model initialize at round 312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 5.99998856, 10.99910653,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0000118432347436}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.4251504952143232
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.95385885, 11.25928569,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9884714353410516}
episode index:313
target Thresh 18.970949063891183
target distance 1.0
model initialize at round 313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.70700192, 11.35866702,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7927759769663602}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.426981226121284
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.70700192, 11.35866702,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7927759769663602}
episode index:314
target Thresh 18.97152431096782
target distance 2.0
model initialize at round 314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.81426066, 9.43924785, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.47690436418039245}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.42880033333994655
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.81426066, 9.43924785, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.47690436418039245}
episode index:315
target Thresh 18.97208816738916
target distance 14.0
model initialize at round 315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.369316876853006}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.4296532984119522
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.00945598, 8.75943333, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.2407524451663807}
episode index:316
target Thresh 18.972640858705294
target distance 5.0
model initialize at round 316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 6.01836129, 11.8643726 ,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 3.5477302720395634}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.4311449283854161
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.11131924, 10.48609893,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0129391230423608}
episode index:317
target Thresh 18.97318260600012
target distance 11.0
model initialize at round 317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.972155791100475}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.4321007364427733
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.02371423,  9.49415587,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0942229778119792}
episode index:318
target Thresh 18.97371362597977
target distance 10.0
model initialize at round 318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.13224899,  4.97376646,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.184018337244828}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.4328258765520093
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.8660477 , 4.99217679, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.3169864880585123}
episode index:319
target Thresh 18.974234131059326
target distance 1.0
model initialize at round 319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.66583323, 10.69167829,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9600795525973506}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4345982956877843
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.66583323, 10.69167829,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9600795525973506}
episode index:320
target Thresh 18.974744329447756
target distance 11.0
model initialize at round 320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.310400490210087}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.4354199125114789
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.58380136, 11.05971984,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.4204613756964865}
episode index:321
target Thresh 18.975244425231217
target distance 7.0
model initialize at round 321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.15220722,  5.99990767,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 5.002408454258454}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.43687047178939353
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.09222347, 10.05423174,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.3109293743621853}
episode index:322
target Thresh 18.975734618454695
target distance 9.0
model initialize at round 322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([11.01053615, 11.85416255,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.527843389982877}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.43791353824670187
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.83694844, 4.98836506, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.29512477835124}
episode index:323
target Thresh 18.97621510520201
target distance 3.0
model initialize at round 323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.94472335, 10.0456186 ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9559808397540038}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4396483730051997
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.94472335, 10.0456186 ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9559808397540038}
episode index:324
target Thresh 18.97668607767427
target distance 2.0
model initialize at round 324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.95711696, 7.9820379 , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9572854929017149}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4413725318574914
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.95711696, 7.9820379 , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9572854929017149}
episode index:325
target Thresh 18.97714772426674
target distance 8.0
model initialize at round 325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.86733622, 4.99546156, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 6.288197387466775}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.4426486130481126
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.14140643, 11.01855218,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.1426182370086018}
episode index:326
target Thresh 18.97760022964422
target distance 5.0
model initialize at round 326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.20791449, 9.00814474, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.0153214071101395}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.4439168894608095
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.57445309, 6.23309922, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.4852065783621669}
episode index:327
target Thresh 18.978043774814886
target distance 4.0
model initialize at round 327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.99705803,  5.99995875,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.0000434101594196}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.4454598257734289
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.86437424,  7.86957192,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8741591976717208}
episode index:328
target Thresh 18.97847853720273
target distance 8.0
model initialize at round 328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([5.       , 9.9999963, 0.       ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.082763137833823}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.44671184757958865
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.92231548, 11.86510404,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8685849891010472}
episode index:329
target Thresh 18.978904690718494
target distance 12.0
model initialize at round 329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.895369891272122}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.4473685402574962
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.2363699 , 7.75248774, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8027410824372012}
episode index:330
target Thresh 18.979322405829272
target distance 7.0
model initialize at round 330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.86043312,  4.97510291,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.358246184078911}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.44860723046819856
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.33491543, 10.24092253,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7073762495326482}
episode index:331
target Thresh 18.979731849626678
target distance 5.0
model initialize at round 331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.32781452, 5.99999869, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.0178585498574217}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.44997437736437873
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.75114319, 8.74820375, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.3540212688699005}
episode index:332
target Thresh 18.98013318589369
target distance 4.0
model initialize at round 332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([14.42132735,  9.0080744 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.5543238117174587}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.4514759558107319
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.37291074,  7.11475217,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.3901672459533013}
episode index:333
target Thresh 18.98052657517016
target distance 13.0
model initialize at round 333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.205906925752268}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.45201120567274944
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.90384357, 9.82377815, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9208622781651017}
episode index:334
target Thresh 18.980912174817053
target distance 12.0
model initialize at round 334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.        ,  9.99998868,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.440303254735154}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.45285622264275616
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.89524529, 6.42512875, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0639271985662748}
episode index:335
target Thresh 18.981290139079363
target distance 11.0
model initialize at round 335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([5.        , 9.99999583, 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 9.000000000000986}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.45381135572268844
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.63072412,  9.55635155,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.771127004801414}
episode index:336
target Thresh 18.98166061914784
target distance 10.0
model initialize at round 336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 7.97410855, 11.86767184,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 8.240335653968764}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.45488166698167154
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.8891975,  9.9447307,  0.       ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.12382200752946182}
episode index:337
target Thresh 18.982023763219445
target distance 11.0
model initialize at round 337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.13224967,  5.97379847,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.13228354615126}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.4556019499080386
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.57685755, 6.09601989, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.4339001652834475}
episode index:338
target Thresh 18.98237971655665
target distance 2.0
model initialize at round 338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.82241505,  3.03253269,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8230582584700928}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.45720784386111224
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.82241505,  3.03253269,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8230582584700928}
episode index:339
target Thresh 18.98272862154554
target distance 1.0
model initialize at round 339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.11884219, 5.28531993, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9262000563570528}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4588042913791678
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.11884219, 5.28531993, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9262000563570528}
episode index:340
target Thresh 18.98307061775276
target distance 9.0
model initialize at round 340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([10.02619114, 11.86774833,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 7.270202589013807}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.4598474056273227
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.19670425, 9.38750794, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0101636445390585}
episode index:341
target Thresh 18.98340584198135
target distance 1.0
model initialize at round 341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.42918015,  2.55244711,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7253543241324003}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.46142679917812
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.42918015,  2.55244711,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7253543241324003}
episode index:342
target Thresh 18.983734428325477
target distance 14.0
model initialize at round 342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.99999928, 3.99999738, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.000000715256016}
done in step count: 12
reward sum = 0.5403600876626367
running average episode reward sum: 0.4616569253836142
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.87980279,  4.69499543,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.1211920474685233}
episode index:343
target Thresh 18.984056508224057
target distance 10.0
model initialize at round 343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.13224961,  7.97380034,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.620595993959924}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.46256426262813854
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.01010333, 10.60261695,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.39751146619308897}
episode index:344
target Thresh 18.984372210513346
target distance 1.0
model initialize at round 344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.10547935,  5.383628  ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9733127123140766}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.464122047374144
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.10547935,  5.383628  ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9733127123140766}
episode index:345
target Thresh 18.98468166147846
target distance 7.0
model initialize at round 345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.59157358,  8.0002507 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 5.035123269257799}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.46513471847999904
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.54343744,  2.24952704,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9265710512103821}
episode index:346
target Thresh 18.984984984903924
target distance 6.0
model initialize at round 346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.98137081,  4.99999857,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 4.1186284251343945}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.46639513715873104
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.64595334,  8.75261021,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.6917061721022013}
episode index:347
target Thresh 18.985282302123146
target distance 11.0
model initialize at round 347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.13224473,  5.97356424,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.883547035043529}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.4668659827695525
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.60144504, 2.09919013, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.41071247214257517}
episode index:348
target Thresh 18.985573732066978
target distance 1.0
model initialize at round 348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.10293829, 10.36629909,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0983153205975835}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4683935874034506
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.10293829, 10.36629909,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0983153205975835}
episode index:349
target Thresh 18.985859391311283
target distance 11.0
model initialize at round 349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 6.65258702, 10.10180583,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.162713103902433}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.46905056942828
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.5660047 ,  3.05658572,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.1001780889984485}
episode index:350
target Thresh 18.98613939412357
target distance 13.0
model initialize at round 350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([3.99999988, 7.        , 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.180340004785375}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.46980852191032196
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.06537077,  8.67477189,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.33173281592171455}
episode index:351
target Thresh 18.986413852508697
target distance 5.0
model initialize at round 351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86730286,  6.97546796,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 3.146427864352763}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.4710377590639858
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.87219779, 10.07388806,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.14762401383962825}
episode index:352
target Thresh 18.986682876253674
target distance 4.0
model initialize at round 352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([3.99999726, 8.9999969 , 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.828431255137324}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.4723945926077139
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.99986506, 10.99819696,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.0018080832919711198}
episode index:353
target Thresh 18.98694657297159
target distance 11.0
model initialize at round 353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.82349087,  9.14589958,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.66252578654064}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.47303284883225066
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.80003283, 4.32786401, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7012514942369487}
episode index:354
target Thresh 18.987205048144645
target distance 3.0
model initialize at round 354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.99747705, 4.99991202, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0000911587974228}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.47437641827215976
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.88773904, 6.70427498, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1331742344159823}
episode index:355
target Thresh 18.987458405166358
target distance 13.0
model initialize at round 355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.        ,  8.99919176,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 13.037970904534449}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.47490744078063424
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.87894753, 2.08053535, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.1453947853528655}
episode index:356
target Thresh 18.987706745382912
target distance 8.0
model initialize at round 356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([12.14602825,  9.82343031,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 7.0708141344277475}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.4759787784815288
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.9844064 ,  3.90706212,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.3385879309706081}
episode index:357
target Thresh 18.987950168133708
target distance 10.0
model initialize at round 357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.86775028, 6.97380079, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.189726312754734}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.47670255812438767
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.85947959,  7.66120918,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.923842181878767}
episode index:358
target Thresh 18.98818877079109
target distance 12.0
model initialize at round 358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.0233263 , 11.86694815,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 14.064982616854522}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.4773199250825196
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.69629987, 2.9779621 , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0240330253516636}
episode index:359
target Thresh 18.988422648799308
target distance 4.0
model initialize at round 359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.03372157, 9.01085901, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.0111417435368977}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.4786329252906236
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.85883771, 7.14413157, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8708479304826086}
episode index:360
target Thresh 18.988651895712675
target distance 11.0
model initialize at round 360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.86775043, 3.97379384, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.132283461751655}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.47896562339020193
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.08305141,  4.61662951,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.6221973044914983}
episode index:361
target Thresh 18.988876603233017
target distance 12.0
model initialize at round 361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.13224972,  4.97380104,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.315143220735957}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.47947516705843085
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.4456078 , 6.47834519, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7612321949443468}
episode index:362
target Thresh 18.989096861246338
target distance 13.0
model initialize at round 362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.86982119, 7.08624774, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.23727079122069}
done in step count: 93
reward sum = 0.00847803669294384
running average episode reward sum: 0.47817765430260306
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.29542749,  1.91621719,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.30707810427974025}
episode index:363
target Thresh 18.98931275785878
target distance 4.0
model initialize at round 363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.58373338, 7.99989486, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.551202069584185}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.47947386953803545
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.86281687, 9.72670794, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.30579039178514045}
episode index:364
target Thresh 18.98952437943187
target distance 3.0
model initialize at round 364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.58615206, 6.99943352, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.8753697387555048}
done in step count: 21
reward sum = 0.3405616262881148
running average episode reward sum: 0.4790932880496795
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.73012294, 7.03458439, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0024275193771683}
episode index:365
target Thresh 18.98973181061705
target distance 11.0
model initialize at round 365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.13224969,  4.97379933,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.33285889115639}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.4796923153940622
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.03463349, 7.96047228, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9610964989887725}
episode index:366
target Thresh 18.989935134389572
target distance 10.0
model initialize at round 366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([5.25013133, 9.21307013, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.185101956365505}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.4803882270431928
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.6291023 ,  4.92100494,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9928822702213449}
episode index:367
target Thresh 18.99013443208165
target distance 6.0
model initialize at round 367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.70748103, 5.99999952, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.349194784217566}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.48153526990448847
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.93467323, 9.96413589, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9353610482681197}
episode index:368
target Thresh 18.990329783415014
target distance 13.0
model initialize at round 368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.13259695,  6.99505742,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.177863144502622}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.4822224152180942
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.99034523, 8.63707025, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.1775577171629927}
episode index:369
target Thresh 18.99052126653281
target distance 3.0
model initialize at round 369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.26971965,  4.97159118,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.2613223576777892}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.4833583005823696
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.23179145,  6.18484425,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7901340211615052}
episode index:370
target Thresh 18.99070895803083
target distance 6.0
model initialize at round 370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.02584414, 11.86765967,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.11828293188387}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.4844880625754091
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.06783589, 11.83433127,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8370844489238299}
episode index:371
target Thresh 18.99089293298818
target distance 11.0
model initialize at round 371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.310400490210085}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.4849690635665748
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.71498343,  8.90062352,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7218566241063282}
episode index:372
target Thresh 18.9910732649973
target distance 3.0
model initialize at round 372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.02179915,  7.94518733,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0550379003093413}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.48554109636155385
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.96960586,  9.18817741,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9876974568013906}
episode index:373
target Thresh 18.99125002619339
target distance 6.0
model initialize at round 373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.56822586, 9.00261915, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 4.042751620739007}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.48665595974026626
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.84527266, 5.03314269, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8459221641518347}
episode index:374
target Thresh 18.991423287283293
target distance 3.0
model initialize at round 374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.79453993, 3.02278781, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0432203717368775}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.4878915438476255
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.39306274, 1.48476219, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7961424721231805}
episode index:375
target Thresh 18.991593117573753
target distance 3.0
model initialize at round 375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.35047114, 7.03426766, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.092034623967634}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.4891205556990946
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.35506161, 5.13413477, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9358372406110158}
episode index:376
target Thresh 18.991759584999148
target distance 7.0
model initialize at round 376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.91547596, 7.00141954, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.0021337217107025}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.49009735793861947
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.68072573, 1.57441498, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.5320325790600028}
episode index:377
target Thresh 18.991922756148668
target distance 14.0
model initialize at round 377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.        ,  7.99999118,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.649107851077042}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.49064825724590816
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.33143587, 4.03820405, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.3336304605909547}
episode index:378
target Thresh 18.992082696292947
target distance 2.0
model initialize at round 378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.30324495, 7.88963223, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.32270504252754173}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.49199219324262083
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.30324495, 7.88963223, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.32270504252754173}
episode index:379
target Thresh 18.99223946941018
target distance 11.0
model initialize at round 379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([5.17655938, 9.14598758, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.662513710860123}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.4926319292883639
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.97036566,  4.99731191,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9703693789101246}
episode index:380
target Thresh 18.992393138211703
target distance 2.0
model initialize at round 380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.93917739, 9.9872272 , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.939219458814994}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.49358926018262017
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.74459386, 9.04738009, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9862641585244581}
episode index:381
target Thresh 18.99254376416708
target distance 11.0
model initialize at round 381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.13224972,  3.97380079,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.310400646203082}
done in step count: 14
reward sum = 0.48767497911552954
running average episode reward sum: 0.49357377777144973
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.60184989, 8.05589709, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0246237473553668}
episode index:382
target Thresh 18.992691407528714
target distance 5.0
model initialize at round 382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.8670218 ,  8.00411324,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.1267272249428837}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.4946414702576862
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.8920238 ,  4.32946409,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.115941247357087}
episode index:383
target Thresh 18.992836127355904
target distance 6.0
model initialize at round 383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 6.97381111, 11.8677477 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 4.118638494845081}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.4957036018455568
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.90234046, 11.8636103 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8691145677396223}
episode index:384
target Thresh 18.99297798153852
target distance 5.0
model initialize at round 384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.84333248,  6.99982846,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 3.1164465279572546}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.4967602158667372
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.74197842,  9.71445054,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.795028599224868}
episode index:385
target Thresh 18.99311702682012
target distance 4.0
model initialize at round 385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.01665127,  5.99660122,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.0034679734852925}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.4979344121986886
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.36188462,  7.39161766,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7078767955713601}
episode index:386
target Thresh 18.993253318820678
target distance 11.0
model initialize at round 386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.13224954,  4.97379261,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.132283437952879}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.4982763114171018
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.74526778, 4.11901749, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.917070710881089}
episode index:387
target Thresh 18.993386912058803
target distance 13.0
model initialize at round 387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 3.99505756, 11.86740304,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.039073606697462}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.49888666084805
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.65634935, 10.26140892,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9880846403698854}
episode index:388
target Thresh 18.99351785997358
target distance 10.0
model initialize at round 388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.86775037, 7.97380131, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.18972616553829}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.49959332993970024
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.06906164,  9.63801294,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6417398374160451}
episode index:389
target Thresh 18.99364621494591
target distance 12.0
model initialize at round 389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.        ,  7.99999869,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.049875751600435}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5002963750872907
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.12998611, 8.41225753, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6019448478844742}
episode index:390
target Thresh 18.99377202831951
target distance 12.0
model initialize at round 390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4., 8., 0.]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.440306508910572}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5009958240960188
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.85568524, 10.82799879,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.22452431031189063}
episode index:391
target Thresh 18.99389535042139
target distance 7.0
model initialize at round 391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.46924633, 9.00184929, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.029930001247228}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.5019049546467942
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.94895975, 3.19905019, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.802574425247689}
episode index:392
target Thresh 18.994016230582044
target distance 7.0
model initialize at round 392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.86775027, 5.9738012 , 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.183499268386782}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.5027003777901865
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.01733292, 11.829576  ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8297570552563337}
episode index:393
target Thresh 18.994134717155145
target distance 2.0
model initialize at round 393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.9824549 ,  5.15017056,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.1511920274454066}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.503962559572445
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.9824549 ,  5.15017056,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.1511920274454066}
episode index:394
target Thresh 18.994250857536905
target distance 5.0
model initialize at round 394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.86740316, 7.99505776, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.2113008962145897}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5050917682823881
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.10361959, 10.20402176,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8026944478982169}
episode index:395
target Thresh 18.99436469818502
target distance 5.0
model initialize at round 395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.8677473 , 7.97381067, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.0290778506357214}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5060953244230891
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.47074499, 10.17380039,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9811812607744314}
episode index:396
target Thresh 18.994476284637276
target distance 12.0
model initialize at round 396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([3.99999917, 4.        , 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.4403073081831}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5065795611275492
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.17453443,  7.97975327,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9951777380265346}
episode index:397
target Thresh 18.994585661529733
target distance 11.0
model initialize at round 397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 4.97380122, 11.86775026,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.067814229286242}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5072509213696911
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.81677722, 10.02951407,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.268451012719676}
episode index:398
target Thresh 18.99469287261461
target distance 13.0
model initialize at round 398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.00493966, 11.86740259,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 13.527923728403957}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5077298345895509
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.13587876, 3.55534661, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.46495125990984015}
episode index:399
target Thresh 18.994797960777774
target distance 2.0
model initialize at round 399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.04418382, 11.86098614,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8621191003030785}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.508960510003077
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.04418382, 11.86098614,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8621191003030785}
episode index:400
target Thresh 18.99490096805588
target distance 12.0
model initialize at round 400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.99983213, 9.00007785, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 10.00016786990621}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5096209100716478
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.89479278,  8.43996108,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5698351995133009}
episode index:401
target Thresh 18.995001935653224
target distance 2.0
model initialize at round 401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.84723628,  6.04080415,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8482183004906615}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5108407585540566
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.84723628,  6.04080415,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8482183004906615}
episode index:402
target Thresh 18.99510090395819
target distance 10.0
model initialize at round 402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([5.16726711, 9.12958001, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.751228831780194}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5113060105082494
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.87957643,  3.09933621,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8851679906797533}
episode index:403
target Thresh 18.995197912559416
target distance 3.0
model initialize at round 403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.99955881,  9.04226971,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.0422698001048956}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5123918867198626
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.07566951,  7.22488165,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7788031414288321}
episode index:404
target Thresh 18.995293000261636
target distance 12.0
model initialize at round 404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.86740487, 7.99491085, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.891684752293864}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5128510111874525
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.88371968,  4.00989693,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.11670073997336787}
episode index:405
target Thresh 18.9953862051012
target distance 14.0
model initialize at round 405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4., 8., 0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.16552506059646}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5133984025161163
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.85907352,  9.08313105,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9276362089868669}
episode index:406
target Thresh 18.995477564361288
target distance 5.0
model initialize at round 406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.00282595, 11.8670126 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.1254878870255633}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5143544260971579
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.1497142 , 10.52207085,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9753984855253831}
episode index:407
target Thresh 18.99556711458682
target distance 11.0
model initialize at round 407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.13259652,  3.99501918,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.347963451090758}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.5145612459823079
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.23751993, 2.10813846, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7701102374416672}
episode index:408
target Thresh 18.99565489159908
target distance 11.0
model initialize at round 408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 6.97396067, 11.86770974,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 10.25494925292212}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.5152946078503218
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.13675884,  7.05765141,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8651641326820974}
episode index:409
target Thresh 18.995740930510046
target distance 9.0
model initialize at round 409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 8.00000039, 10.29260544,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 10.235590338249132}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5159250623128819
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.1395791 , 2.00575978, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8604401735777983}
episode index:410
target Thresh 18.995825265736425
target distance 11.0
model initialize at round 410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.86760906, 7.98089447, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.916577437265055}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.5162032237421075
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.89852604,  2.6856833 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.1302701593261584}
episode index:411
target Thresh 18.995907931013434
target distance 6.0
model initialize at round 411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.13263884,  6.99527888,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.161809937113862}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.517140837276714
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.41040055, 11.02007763,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.589941197899066}
episode index:412
target Thresh 18.99598895940829
target distance 4.0
model initialize at round 412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.32364115, 8.99991572, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 2.3984084364630243}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5181889224164797
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.5520806, 10.9831028,  0.       ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.44823799818160404}
episode index:413
target Thresh 18.996068383333423
target distance 3.0
model initialize at round 413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.00057983, 9.33044147, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0537322110099292}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5192319443430099
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.3358571, 8.8951465, 0.       ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.6723689851890896}
episode index:414
target Thresh 18.996146234559472
target distance 10.0
model initialize at round 414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([11.02617366, 11.86774391,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 9.942327806493433}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5197520887918822
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.84235185, 5.33933055, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.6792179800117953}
episode index:415
target Thresh 18.996222544227958
target distance 14.0
model initialize at round 415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.02619855, 11.8677502 ,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 13.849023231433684}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5201813801555886
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.10882779, 4.26511848, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.7428959115359249}
episode index:416
target Thresh 18.99629734286377
target distance 13.0
model initialize at round 416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.00016787,  9.00007737,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.180518892095062}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5206967530823737
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.32462554, 7.63298177, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9256330752458125}
episode index:417
target Thresh 18.996370660387356
target distance 4.0
model initialize at round 417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([5.17663764, 9.14612529, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.0284913165271172}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5216101579793059
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.76010292, 10.97825166,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.24088088154871345}
episode index:418
target Thresh 18.996442526126707
target distance 12.0
model initialize at round 418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 4.        , 10.99215472,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 11.657869366132154}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.522119660921181
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.67837301,  5.9846046 ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1956739309857278}
episode index:419
target Thresh 18.996512968829077
target distance 11.0
model initialize at round 419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([12.82336002,  9.14612873,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 9.996761514037928}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5227188544368448
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.9873677 , 10.04327452,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9568088694861054}
episode index:420
target Thresh 18.996582016672484
target distance 5.0
model initialize at round 420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.80231109, 6.99994791, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.1054815502276596}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5236209474191802
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.33975263, 10.51708901,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8386343898653177}
episode index:421
target Thresh 18.996649697276982
target distance 10.0
model initialize at round 421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([12.05563474, 10.04501133,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.505019218920278}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5241220633983409
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.94751852, 4.1820162 , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8196656635619984}
episode index:422
target Thresh 18.99671603771572
target distance 12.0
model initialize at round 422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.        ,  5.99999988,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.770329658542257}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5246208100348105
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.97673223, 9.74286722, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.2581833756552615}
episode index:423
target Thresh 18.996781064525763
target distance 12.0
model initialize at round 423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4.86775027, 3.97380117, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.315143201733205}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.5249481676321083
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.13336352,  5.9882658 ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.13387874608504055}
episode index:424
target Thresh 18.99684480371869
target distance 1.0
model initialize at round 424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.95685944, 11.85681491,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.2844110611791986}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5260659366494446
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.95685944, 11.85681491,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.2844110611791986}
episode index:425
target Thresh 18.99690728079104
target distance 3.0
model initialize at round 425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.01305521, 9.07408357, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0741629050418666}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5270610870328966
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.24750496, 7.68803549, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.39822175242542873}
episode index:426
target Thresh 18.996968520734466
target distance 12.0
model initialize at round 426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.86740305, 6.99505742, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.132598157194499}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5275482786103957
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.24020992,  6.92458697,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.25176960206163046}
episode index:427
target Thresh 18.997028548045765
target distance 12.0
model initialize at round 427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.        ,  9.99902117,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 12.20599432137478}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5279473183708707
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.67212062, 3.99998688, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0523681132732983}
episode index:428
target Thresh 18.997087386736666
target distance 7.0
model initialize at round 428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.26602292, 7.00119483, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.05476726742132}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.5287152150646449
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.90128038, 1.53904201, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.0123184262299119}
episode index:429
target Thresh 18.997145060343424
target distance 12.0
model initialize at round 429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.13224967,  4.9737986 ,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.13228050522039}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.5290284830093529
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.7385803 , 5.02419307, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.2625367843337892}
episode index:430
target Thresh 18.997201591936253
target distance 10.0
model initialize at round 430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.86775027, 5.97380117, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.620595846327653}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5295065883634495
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.09294469,  8.44763139,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.560133733268506}
episode index:431
target Thresh 18.997257004128542
target distance 2.0
model initialize at round 431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.12752271, 3.54392195, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.47357072416695617}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5305956934829786
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.12752271, 3.54392195, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.47357072416695617}
episode index:432
target Thresh 18.997311319085913
target distance 11.0
model initialize at round 432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.99989341, 8.96510713, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.38044972265357}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.5309024480737549
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.17220099,  1.99640797,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.17223845158473247}
episode index:433
target Thresh 18.997364558535068
target distance 10.0
model initialize at round 433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.86775027, 3.97380117, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 11.522389296746514}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5312882426544461
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.94219309, 10.16870677,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8333007055177893}
episode index:434
target Thresh 18.997416743772497
target distance 2.0
model initialize at round 434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.95481014,  2.35762024,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.6439672974853027}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5323657409471945
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.95481014,  2.35762024,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.6439672974853027}
episode index:435
target Thresh 18.997467895672987
target distance 10.0
model initialize at round 435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([5.59973187, 9.60065851, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.683313971597755}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5327464096516591
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.1693683 ,  3.99599489,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0102927541729547}
episode index:436
target Thresh 18.99751803469799
target distance 9.0
model initialize at round 436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 7.02370922, 10.13293432,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 9.559407667385214}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.5333911690117239
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.04104503, 2.92675165, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9276601285063952}
episode index:437
target Thresh 18.997567180903776
target distance 1.0
model initialize at round 437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.13260981, 2.23735182, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.1549883132608525}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5344564859774505
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.13260981, 2.23735182, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.1549883132608525}
episode index:438
target Thresh 18.997615353949485
target distance 12.0
model initialize at round 438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.778321394833641}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.5347502535066342
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.72888838, 11.51876335,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8946472373211888}
episode index:439
target Thresh 18.99766257310498
target distance 5.0
model initialize at round 439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.97106463,  3.99988723,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 3.1533542704217443}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5355860483850282
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.93057829,  6.08110604,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.307800465736828}
episode index:440
target Thresh 18.997708857258548
target distance 9.0
model initialize at round 440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.        ,  3.99999988,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.89949502090537}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5361261728501415
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.97441779, 11.1946057 ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9936606138888725}
episode index:441
target Thresh 18.997754224924474
target distance 11.0
model initialize at round 441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([12.82336282,  9.14612382,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.055063627461685}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5366638533131503
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.8832211 , 6.94751325, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.12803191723520754}
episode index:442
target Thresh 18.997798694250424
target distance 6.0
model initialize at round 442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.86715786, 4.97597966, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.116394347153718}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5354524224930303
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.86208   , 7.20666668, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.9897804732963558}
episode index:443
target Thresh 18.99784228302472
target distance 12.0
model initialize at round 443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.99999893, 5.99974215, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.77023485011413}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.5356659292210293
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.63095087,  2.73579929,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.9692778758721281}
episode index:444
target Thresh 18.99788500868346
target distance 2.0
model initialize at round 444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.08638763, 4.98747551, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.08729081334986552}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5367093765710944
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.08638763, 4.98747551, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.08729081334986552}
episode index:445
target Thresh 18.997926888317473
target distance 11.0
model initialize at round 445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 5.99505762, 11.86740304,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 9.196530944727249}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5371541804142647
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.57521963,  9.47589926,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6746257830723915}
episode index:446
target Thresh 18.99796793867917
target distance 6.0
model initialize at round 446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.86716577, 5.97595399, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 4.116421107388499}
done in step count: 13
reward sum = 0.5133420832795048
running average episode reward sum: 0.5371009095034487
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.67228926, 9.00955791, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0432496665635123}
episode index:447
target Thresh 18.998008176189245
target distance 6.0
model initialize at round 447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.04600179, 9.00105298, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.001317417908432}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5379165324733071
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.86048442, 5.10322225, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8666534894785581}
episode index:448
target Thresh 18.998047616943236
target distance 2.0
model initialize at round 448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.98083961,  6.90785688,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.09411415837270298}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5389456715992017
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.98083961,  6.90785688,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.09411415837270298}
episode index:449
target Thresh 18.998086276717977
target distance 10.0
model initialize at round 449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 7.99999976, 10.77471191,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 10.483163894374849}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.539467527745648
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.71013068,  4.09111566,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7159522697904919}
episode index:450
target Thresh 18.99812417097788
target distance 7.0
model initialize at round 450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.85660136, 5.9999994 , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 5.072846523015163}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.54017242236262
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.24553331, 11.04479607,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7557953943495243}
episode index:451
target Thresh 18.99816131488117
target distance 11.0
model initialize at round 451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.13259684,  3.99504795,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.186644956402871}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.5403717077328899
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.71716864, 3.99198105, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.2240740455165922}
episode index:452
target Thresh 18.998197723285895
target distance 12.0
model initialize at round 452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.13259695,  4.99505742,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.329052024865677}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5407204176409712
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.89590419, 6.92522802, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.12816702931935545}
episode index:453
target Thresh 18.9982334107559
target distance 11.0
model initialize at round 453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.86775039, 4.97379542, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.32271042108927}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.5409176180640629
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.6521877 ,  3.11038735,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.6614636546917314}
episode index:454
target Thresh 18.998268391566658
target distance 4.0
model initialize at round 454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.37309313,  7.00232899,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.4279015899507392}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5418167002221639
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.50045705,  5.20331538,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5393332022946336}
episode index:455
target Thresh 18.99830267971095
target distance 6.0
model initialize at round 455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.96794486, 7.00167859, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 4.001806971565687}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5426076723707995
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.72340319, 3.10405302, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.730848279789888}
episode index:456
target Thresh 18.998336288904497
target distance 5.0
model initialize at round 456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.99999976, 9.99999762, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.1622786402986405}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5433951829345396
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.88313482, 10.16956086,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.2122525652144784}
episode index:457
target Thresh 18.998369232591422
target distance 6.0
model initialize at round 457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.00414288, 9.00092709, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 4.125011576831536}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.5440807283866476
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.76380047, 4.83060782, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7823585321733582}
episode index:458
target Thresh 18.99840152394964
target distance 10.0
model initialize at round 458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.13259679,  7.99505713,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.669994939421464}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.5446698907431036
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.93984012, 10.23784091,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.2100355080741658}
episode index:459
target Thresh 18.998433175896125
target distance 5.0
model initialize at round 459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.96006912, 9.05998288, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.207059080821463}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5454477822849664
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.88560212, 5.27801326, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.142609278772928}
episode index:460
target Thresh 18.998464201092077
target distance 4.0
model initialize at round 460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([12.02277862, 11.86675362,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 2.2006579022893273}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5463253359025695
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.03810779, 11.39328893,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.3951308507745629}
episode index:461
target Thresh 18.998494611947986
target distance 12.0
model initialize at round 461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([12.05536158, 10.04544881,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.877870286627907}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5466543661194335
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.45665551, 2.14521914, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5624160682988343}
episode index:462
target Thresh 18.998524420628605
target distance 11.0
model initialize at round 462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.13259694,  3.99505603,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.187722197218127}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.5469065606446379
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.85592384, 5.953751  , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.2815016921831612}
episode index:463
target Thresh 18.9985536390578
target distance 13.0
model initialize at round 463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 5.        , 11.60959433,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 14.606310394649626}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.5471576681244751
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.47666009,  1.75844769,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.5763958525001531}
episode index:464
target Thresh 18.998582278923333
target distance 1.0
model initialize at round 464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.90719208,  5.49404577,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0387430705601206}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5481315226016268
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.90719208,  5.49404577,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0387430705601206}
episode index:465
target Thresh 18.99861035168153
target distance 3.0
model initialize at round 465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.08543754,  8.93955171,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0638844608635358}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5489939013084902
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.48996893,  9.05379248,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0655412762079328}
episode index:466
target Thresh 18.998637868561875
target distance 13.0
model initialize at round 466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.132598050286157}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5493923980736218
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.96045058, 7.11893163, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9677861588008279}
episode index:467
target Thresh 18.99866484057148
target distance 4.0
model initialize at round 467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.86268996,  6.97933835,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.19711343842196}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5502483972230372
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.72837283,  8.80018753,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.33720371018453726}
episode index:468
target Thresh 18.99869127849951
target distance 12.0
model initialize at round 468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([12.00492029, 11.86739945,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 14.052188513273544}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5505641518048511
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13369344, 2.13350394, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8765331451243085}
episode index:469
target Thresh 18.998717192921493
target distance 12.0
model initialize at round 469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([3.99999964, 4.        , 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 12.206555908713884}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5508785627501467
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.98340063, 10.74845416,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.2520929364207096}
episode index:470
target Thresh 18.998742594203538
target distance 2.0
model initialize at round 470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.71880698,  7.05025387,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.28564832330780837}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.551832111449191
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.71880698,  7.05025387,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.28564832330780837}
episode index:471
target Thresh 18.998767492506502
target distance 13.0
model initialize at round 471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.704699910719649}
done in step count: 21
reward sum = 0.3405616262881148
running average episode reward sum: 0.5513845044891039
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.98860468, 11.52086687,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.1174262837720792}
episode index:472
target Thresh 18.99879189779003
target distance 6.0
model initialize at round 472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.86731792,  9.00455294,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.097399737150907}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5521268205472665
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.45967899,  5.10651116,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.4718573966464259}
episode index:473
target Thresh 18.99881581981657
target distance 10.0
model initialize at round 473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.86775044, 4.9737931 , 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.343117315460015}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.5522916361362482
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.11940022,  3.06725164,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.1370372037305707}
episode index:474
target Thresh 18.998839268155248
target distance 3.0
model initialize at round 474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([5.16141975, 9.12189598, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.456009100984933}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5511289169022772
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.99857603, 8.19682902, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.8031715387168397}
episode index:475
target Thresh 18.99886225218571
target distance 8.0
model initialize at round 475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 9.02616898, 11.86774272,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 6.088324069721514}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5499710830432387
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.12721597, 8.95315716, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.050792409552909}
episode index:476
target Thresh 18.99888478110187
target distance 11.0
model initialize at round 476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.86775037, 7.97380131, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 10.18408396726371}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5504402860924144
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.06349502,  8.53405342,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0460151907747548}
episode index:477
target Thresh 18.998906863915607
target distance 6.0
model initialize at round 477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.17658877,  8.00111032,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 4.0849589720014805}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5511768126905474
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.61028838,  4.12356043,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.4088304355778733}
episode index:478
target Thresh 18.998928509460335
target distance 12.0
model initialize at round 478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.86775037, 7.97380131, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.132280455356941}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5514840370817858
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.87740946,  7.70185908,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9266797591216097}
episode index:479
target Thresh 18.99894972639456
target distance 2.0
model initialize at round 479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.69163519, 7.02619439, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6921310411905648}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5524184453378654
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.69163519, 7.02619439, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6921310411905648}
episode index:480
target Thresh 18.99897052320534
target distance 13.0
model initialize at round 480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.99983213, 9.00007785, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.000167869878663}
done in step count: 13
reward sum = 0.5133420832795048
running average episode reward sum: 0.5523372054999063
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.33321481,  9.0327478 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.33482014648499514}
episode index:481
target Thresh 18.998990908211674
target distance 5.0
model initialize at round 481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 8.01939291, 11.86530129,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.140936142665998}
done in step count: 34
reward sum = 0.174824614723797
running average episode reward sum: 0.5515539843572171
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.6619674 , 10.07958406,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9805261501498682}
episode index:482
target Thresh 18.99901088956784
target distance 2.0
model initialize at round 482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.77611829,  5.05372024,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.2302365789467381}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.552482444016933
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.77611829,  5.05372024,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.2302365789467381}
episode index:483
target Thresh 18.99903047526664
target distance 11.0
model initialize at round 483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 5.99505824, 11.86740292,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 9.450448433932092}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.5525780111558204
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.3161945 ,  8.50041705,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8468607225004282}
episode index:484
target Thresh 18.999049673142622
target distance 9.0
model initialize at round 484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.10904837, 9.00049686, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 7.056964727167815}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.5531180693802414
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.80084985, 1.73023476, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8450643564799443}
episode index:485
target Thresh 18.999068490875192
target distance 13.0
model initialize at round 485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([3.99999952, 8.99718368, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.528616148181944}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.5533450289726464
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.29999219,  2.30373026,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7581469935438264}
episode index:486
target Thresh 18.999086935991688
target distance 14.0
model initialize at round 486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([3.99999976, 9.99987507, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.99995217007736}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5537182258138217
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.03641132,  5.95131509,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9520116530614277}
episode index:487
target Thresh 18.99910501587041
target distance 12.0
model initialize at round 487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([12.00494235, 11.86740303,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 10.726400965021169}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5541691739935065
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.12685612, 7.78560417, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.2491145223894515}
episode index:488
target Thresh 18.999122737743544
target distance 12.0
model initialize at round 488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.86740306, 6.99505642, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.327137594413964}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5544639963290898
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.29866395,  5.02906091,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.7019378786316891}
episode index:489
target Thresh 18.99914010870008
target distance 2.0
model initialize at round 489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.98915684,  8.09733117,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9027339571891829}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5553732534794387
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.98915684,  8.09733117,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9027339571891829}
episode index:490
target Thresh 18.99915713568863
target distance 12.0
model initialize at round 490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 5.99607514, 11.86722881,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.726814280901472}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5557392792170059
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.63475075,  4.07327416,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6389660569784065}
episode index:491
target Thresh 18.99917382552022
target distance 3.0
model initialize at round 491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.98564851,  3.98013937,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0199615973916665}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5565406221454265
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.85566835,  4.90739249,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8606651395518172}
episode index:492
target Thresh 18.999190184870997
target distance 6.0
model initialize at round 492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.11437616, 8.00161517, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 4.003249411101753}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.557242365305375
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.31756887, 4.02702975, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.3187171060811124}
episode index:493
target Thresh 18.99920622028493
target distance 3.0
model initialize at round 493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.97162068,  9.98698616,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0134112814320502}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5580374212460523
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.35632569, 10.62857646,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.7431500995457784}
episode index:494
target Thresh 18.99922193817639
target distance 13.0
model initialize at round 494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([3.99999976, 5.        , 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.704700134783844}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5583208553366538
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.12079293,  8.13226134,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8761057710005569}
episode index:495
target Thresh 18.999237344832746
target distance 4.0
model initialize at round 495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.33653831, 8.97762871, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 2.05018137746618}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5591105310315395
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.19125602, 10.77382715,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8397743596098937}
episode index:496
target Thresh 18.999252446416868
target distance 5.0
model initialize at round 496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 7.00006522, 10.36003038,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.293945048525873}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5598970289570293
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.93239423, 9.21916168, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9578052250070298}
episode index:497
target Thresh 18.999267248969588
target distance 3.0
model initialize at round 497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.99195617, 9.93881965, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.452611714747198}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5606803682563124
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.15887091, 11.76092327,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.1342408791413952}
episode index:498
target Thresh 18.999281758412124
target distance 11.0
model initialize at round 498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.614260354881594}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5611074235052979
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.03513677, 10.1430706 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8576494602826532}
episode index:499
target Thresh 18.99929598054845
target distance 8.0
model initialize at round 499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.99998689, 5.        , 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.211109824732044}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.5616999586582871
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.06092771, 11.16104563,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9527814287015943}
episode index:500
target Thresh 18.99930992106761
target distance 10.0
model initialize at round 500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 10.73351535954344}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5619726878747253
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.0278234 , 11.08818521,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.09247038472113762}
episode index:501
target Thresh 18.999323585545994
target distance 3.0
model initialize at round 501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.90558541, 6.01679468, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.021168712588256}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5627456506478832
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.87624279, 4.22598451, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.1691455858842201}
episode index:502
target Thresh 18.999336979449577
target distance 8.0
model initialize at round 502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4.86740304, 4.99505762, 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 8.583011042279463}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5631652039020624
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.00180329, 10.41218091,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5878218537084426}
episode index:503
target Thresh 18.999350108136102
target distance 11.0
model initialize at round 503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([3.99999964, 6.        , 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.295630453610018}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5635830922623758
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.0313427 , 10.47576735,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.101415827936857}
episode index:504
target Thresh 18.999362976857213
target distance 7.0
model initialize at round 504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.86775023, 5.97380135, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.922302042752209}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.5641648584163117
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.03655923, 11.86541541,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8661872811245799}
episode index:505
target Thresh 18.999375590760575
target distance 2.0
model initialize at round 505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.15786159, 5.0699681 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.17267257265442343}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5650261926882162
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.15786159, 5.0699681 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.17267257265442343}
episode index:506
target Thresh 18.999387954891915
target distance 5.0
model initialize at round 506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.55273911,  8.00197732,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.0351128738908524}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5656918214994819
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.57991459,  4.33381331,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8832358896497009}
episode index:507
target Thresh 18.99940007419705
target distance 12.0
model initialize at round 507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.13259695,  6.99505742,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.132598157194499}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5660252862024849
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.10880729, 7.03477793, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.11423016374287548}
episode index:508
target Thresh 18.999411953523868
target distance 5.0
model initialize at round 508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.50920212, 6.99994755, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.3582742238243104}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5666863367207511
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.52391976, 10.03824019,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.47761355325800114}
episode index:509
target Thresh 18.99942359762425
target distance 6.0
model initialize at round 509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 9.99999857, 10.63533157,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.790092267769092}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5673447948840437
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.14528674,  7.98627544,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.14593355016976875}
episode index:510
target Thresh 18.999435011156002
target distance 5.0
model initialize at round 510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([3.00071263, 9.00829744, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 3.1703752588641154}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5680006759116678
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.40341412, 5.05918578, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.0236573380570801}
episode index:511
target Thresh 18.99944619868468
target distance 6.0
model initialize at round 511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 6.00000679, 11.27735052,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 4.602866468651043}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5686539949040278
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.03001618, 8.7217401 , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.27987415925347914}
episode index:512
target Thresh 18.99945716468545
target distance 13.0
model initialize at round 512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.        ,  7.99999988,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.00000000000002}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5690538524919342
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.97817398, 8.42117676, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.064994929487956}
episode index:513
target Thresh 18.99946791354485
target distance 11.0
model initialize at round 513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 6.97380867, 11.86774835,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 10.255101363182762}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.5695313863392262
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.1215841 ,  7.01427666,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8785319126952775}
episode index:514
target Thresh 18.999478449562577
target distance 11.0
model initialize at round 514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.996760623513145}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5699279874094414
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.02338591, 10.12592775,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8743850444739196}
episode index:515
target Thresh 18.99948877695318
target distance 12.0
model initialize at round 515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.        , 10.99994385,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.180314777652688}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5702480724931924
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.35948744, 5.12450441, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9464268332459103}
episode index:516
target Thresh 18.99949889984774
target distance 4.0
model initialize at round 516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.24354595,  5.02513676,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.1618051817802852}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5709826023336311
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.06449809,  3.0982008 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.11748788066128477}
episode index:517
target Thresh 18.99950882229556
target distance 12.0
model initialize at round 517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([3.99999976, 8.9999938 , 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.19803804527507}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5713741049111724
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.14276809,  7.96598253,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.976475788769692}
episode index:518
target Thresh 18.99951854826575
target distance 6.0
model initialize at round 518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([ 8.02568613, 10.13974673,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 5.774396228201954}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.571925166366064
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.84699649, 6.03002739, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8475285869846096}
episode index:519
target Thresh 18.99952808164883
target distance 10.0
model initialize at round 519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([5.17016559, 9.13472705, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.751783650680418}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5721682666155404
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.0856379 ,  3.02821184,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.09016517332756945}
episode index:520
target Thresh 18.99953742625827
target distance 10.0
model initialize at round 520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4.86740304, 4.99505756, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.109325740190874}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5724104336586848
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.89387975, 11.56708021,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.576924148848045}
episode index:521
target Thresh 18.99954658583205
target distance 11.0
model initialize at round 521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.13224972,  5.97380104,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.574514862754729}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5726516728587521
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.79325761, 8.05244165, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9698501174817217}
episode index:522
target Thresh 18.999555564034118
target distance 4.0
model initialize at round 522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.38112205, 8.03102374, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.123220985648954}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5733731801764217
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.59027576, 6.0414716 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.591730825967713}
episode index:523
target Thresh 18.999564364455875
target distance 10.0
model initialize at round 523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.86775027, 3.97380117, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 11.522389296746514}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5736116613136686
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.93267628, 10.03820935,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9641440485103613}
episode index:524
target Thresh 18.9995729906176
target distance 4.0
model initialize at round 524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.33956742,  6.00682104,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 2.1127001350950736}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5743285914825949
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.3659339 ,  4.12433672,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.38648057572197964}
episode index:525
target Thresh 18.999581445969884
target distance 11.0
model initialize at round 525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([12.82336433,  9.14612117,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.05506454052717}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5747077784522098
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.01469977, 7.04684523, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.04909744385422894}
episode index:526
target Thresh 18.99958973389497
target distance 12.0
model initialize at round 526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.329052001496866}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5749423695672792
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.19044958,  6.92163272,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.20594288583535453}
episode index:527
target Thresh 18.99959785770815
target distance 2.0
model initialize at round 527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.51807249,  5.13932395,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5016625278166295}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5757474029582502
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.51807249,  5.13932395,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5016625278166295}
episode index:528
target Thresh 18.99960582065905
target distance 13.0
model initialize at round 528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.99999976, 9.99987018, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.082992471771986}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5760486212714198
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.14270645,  5.98911991,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9993614637510213}
episode index:529
target Thresh 18.999613625932955
target distance 10.0
model initialize at round 529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.86775031, 3.97379946, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.184022529105802}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.5760914294185273
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.88586018,  2.40802919,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.6028742348320707}
episode index:530
target Thresh 18.999621276652082
target distance 2.0
model initialize at round 530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.13215613,  4.16729021,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.21319300707527497}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5768897506437278
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.13215613,  4.16729021,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.21319300707527497}
episode index:531
target Thresh 18.99962877587682
target distance 2.0
model initialize at round 531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.91373026, 9.76889217, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9425040182246676}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5776850706613148
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.91373026, 9.76889217, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9425040182246676}
episode index:532
target Thresh 18.99963612660696
target distance 1.0
model initialize at round 532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.2091562 , 11.56480613,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9718229669599532}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5784774063636388
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.2091562 , 11.56480613,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9718229669599532}
episode index:533
target Thresh 18.999643331782888
target distance 6.0
model initialize at round 533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([12.08898197,  9.87250012,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 4.336164056976076}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.5789996865015345
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.31225068,  7.70922676,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.42667266879998555}
episode index:534
target Thresh 18.99965039428678
target distance 4.0
model initialize at round 534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.40396982,  8.99869168,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.0881779066558286}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5796931450314382
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.17315573, 10.79488754,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.2684288084960872}
episode index:535
target Thresh 18.99965731694372
target distance 1.0
model initialize at round 535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.90118781, 5.51052533, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0357488040133356}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5804772996116034
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.90118781, 5.51052533, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0357488040133356}
episode index:536
target Thresh 18.99966410252287
target distance 13.0
model initialize at round 536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 3.97380215, 11.86775   ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.684884636247649}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.580837269142122
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.074238  ,  8.09373393,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9304952063597213}
episode index:537
target Thresh 18.99967075373855
target distance 7.0
model initialize at round 537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 8.97380604, 11.86774896,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.100550362246564}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.5813512797942741
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.70580992, 10.83687228,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7244158311210964}
episode index:538
target Thresh 18.999677273251333
target distance 2.0
model initialize at round 538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.26710951, 10.86053526,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.3013269066470722}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5821279935608895
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.26710951, 10.86053526,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.3013269066470722}
episode index:539
target Thresh 18.999683663669117
target distance 14.0
model initialize at round 539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 3.97380935, 11.86774815,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 13.381320181263634}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5824112600369341
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.81637168,  6.08590171,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.20272756303834813}
episode index:540
target Thresh 18.99968992754815
target distance 13.0
model initialize at round 540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([3.99999988, 6.        , 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.704700022751727}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5826934793171339
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.02673343,  9.74417301,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.006327615701391}
episode index:541
target Thresh 18.99969606739407
target distance 5.0
model initialize at round 541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.10029089, 5.01477051, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 3.0164382105883676}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.583283528248283
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.28883647, 1.8092873 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7362913118130169}
episode index:542
target Thresh 18.99970208566289
target distance 9.0
model initialize at round 542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.31010011551624}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5836343522063894
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.99695212, 11.17046708,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0114210579626226}
episode index:543
target Thresh 18.999707984762004
target distance 9.0
model initialize at round 543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.07738107, 9.00080264, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 7.001230276158769}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.5840587490773335
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13909604, 1.82548933, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8784131207425547}
episode index:544
target Thresh 18.999713767051134
target distance 13.0
model initialize at round 544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.        ,  9.99998653,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.083040399406284}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5843358741076962
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.72967216, 5.98983116, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.026081312726474}
episode index:545
target Thresh 18.999719434843264
target distance 12.0
model initialize at round 545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 4.97380104, 11.86775028,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.06368004459528}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5846828430882681
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.84618144, 10.0198432 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9921529671248065}
episode index:546
target Thresh 18.99972499040559
target distance 11.0
model initialize at round 546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.13259695,  4.99505742,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.61426060534732}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5848906208816969
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.84558512, 7.49015542, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9873984490236893}
episode index:547
target Thresh 18.999730435960412
target distance 12.0
model initialize at round 547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.        ,  5.99999988,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.19803905056446}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5851647107899876
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.00254817, 8.3486937 , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.3487030082542595}
episode index:548
target Thresh 18.999735773686027
target distance 13.0
model initialize at round 548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.0049184 , 11.86739914,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 14.132904750045368}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5853708539326173
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.70676136, 3.02131941, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.2940126098130584}
episode index:549
target Thresh 18.999741005717592
target distance 2.0
model initialize at round 549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.08380902, 9.12735026, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.15245340004429167}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5861247251072853
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.08380902, 9.12735026, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.15245340004429167}
episode index:550
target Thresh 18.999746134147987
target distance 8.0
model initialize at round 550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.00000012,  4.99999988,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.485281542825978}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.5865392106334064
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.97119722, 11.85391529,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.293211261710057}
episode index:551
target Thresh 18.999751161028662
target distance 3.0
model initialize at round 551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.9659158 ,  5.99443209,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0061453925377835}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5871976540924039
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.31650725,  7.28311431,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.42465345186853787}
episode index:552
target Thresh 18.999756088370425
target distance 5.0
model initialize at round 552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.6504221 ,  7.00260663,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 3.072245999227153}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5877678210831951
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.49475324,  3.39294159,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7831351595722076}
episode index:553
target Thresh 18.999760918144286
target distance 9.0
model initialize at round 553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.86775027, 3.97380118, 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 10.011815824146503}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5880337490065557
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.94970338, 10.36644419,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.635549143270593}
episode index:554
target Thresh 18.999765652282218
target distance 11.0
model initialize at round 554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.788874285776417}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.5881695808665243
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.11872037, 11.161662  ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.895984586132914}
episode index:555
target Thresh 18.999770292677937
target distance 13.0
model initialize at round 555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.        , 10.99999988,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.045361006394582}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5883677242392351
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.88846648, 10.18444848,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9074105674047414}
episode index:556
target Thresh 18.999774841187666
target distance 12.0
model initialize at round 556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.        ,  3.99999905,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.049875526226812}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.5883863404241528
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.87945175, 2.51707163, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0033220816133512}
episode index:557
target Thresh 18.999779299630866
target distance 13.0
model initialize at round 557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 3.99999964, 10.99935818,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 13.038060544434577}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.588583385147575
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.50807445,  4.10949232,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.5039636062029451}
episode index:558
target Thresh 18.99978366979098
target distance 7.0
model initialize at round 558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([15.52489614,  8.00016093,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 5.2275154331611215}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.5890642288235186
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.66962075,  2.22583783,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8417110676453149}
episode index:559
target Thresh 18.99978795341612
target distance 12.0
model initialize at round 559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.86740322, 6.99504216, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.891734454461464}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.5891970077564929
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.35626728,  3.06990667,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.3630610374930757}
episode index:560
target Thresh 18.9997921522198
target distance 7.0
model initialize at round 560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.74737373, 9.00208879, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.008464060673829}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.5896750433932906
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.90478497, 3.47445589, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0463424210015724}
episode index:561
target Thresh 18.9997962678816
target distance 14.0
model initialize at round 561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.        , 10.99999976,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.369316819027995}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5899337922317811
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.15204946, 7.56886514, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.45716113414131476}
episode index:562
target Thresh 18.999800302047834
target distance 4.0
model initialize at round 562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 4.00072134, 11.43424249,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.461694004817024}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5905733414462895
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.07976496, 10.54204142,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.547878956709475}
episode index:563
target Thresh 18.999804256332226
target distance 3.0
model initialize at round 563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.99976528,  9.01094937,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.4221420943570242}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5912106227557818
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.75469947,  7.16664791,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.1242984443844115}
episode index:564
target Thresh 18.999808132316538
target distance 14.0
model initialize at round 564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4., 9., 0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.041594578792312}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5914002274873534
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.13900411,  8.06518231,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.15352809490643193}
episode index:565
target Thresh 18.99981193155122
target distance 10.0
model initialize at round 565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.94134623692523}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5915891622375414
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.56555054, 10.90282215,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.44518520485386226}
episode index:566
target Thresh 18.999815655556016
target distance 7.0
model initialize at round 566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.86738487, 5.99516899, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.0794379556479194}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.5920579203288333
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.90050352, 11.64824151,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6558327549406793}
episode index:567
target Thresh 18.999819305820573
target distance 12.0
model initialize at round 567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.568784232893615}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5923097406990729
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.9635905 , 9.46665261, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0706406048819963}
episode index:568
target Thresh 18.999822883805056
target distance 5.0
model initialize at round 568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.04524239,  9.05599887,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.6276998807096557}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5928548905396721
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.76488961,  5.24763065,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.7882490341019119}
episode index:569
target Thresh 18.999826390940694
target distance 13.0
model initialize at round 569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.00474595, 11.86737089,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 14.780711815566638}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5930399473915213
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.31673927, 2.99372433, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.2059574073702446}
episode index:570
target Thresh 18.999829828630393
target distance 12.0
model initialize at round 570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([12.        , 10.99931717,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.206164050794646}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5932887248752927
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.84280069, 4.0415726 , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.16260352163056294}
episode index:571
target Thresh 18.999833198249277
target distance 8.0
model initialize at round 571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 7.98175294, 10.13617979,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.335114334671232}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5936042707015597
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.08284307,  3.99659095,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0000282492241364}
episode index:572
target Thresh 18.999836501145236
target distance 13.0
model initialize at round 572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.        , 10.99999213,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.704697221951431}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5938511949946198
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.42751873, 6.1955192 , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9110222950822673}
episode index:573
target Thresh 18.999839738639473
target distance 13.0
model initialize at round 573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 12.083045973594597}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5940972589242894
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.98419108, 11.39882891,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0619305880449337}
episode index:574
target Thresh 18.999842912027027
target distance 10.0
model initialize at round 574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([12.06526136, 10.02997392,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.699017436095838}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5942785459454537
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.89654698, 2.43334409, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.06061086662412}
episode index:575
target Thresh 18.9998460225773
target distance 11.0
model initialize at round 575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.13223697,  7.97320314,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.286938264351324}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5944592034977946
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.6033847 , 3.93841671, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1156608047161447}
episode index:576
target Thresh 18.999849071534552
target distance 8.0
model initialize at round 576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4.86740304, 4.99505757, 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 8.583011069576214}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5947699863990116
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.00521766, 10.32968734,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.6703329714535079}
episode index:577
target Thresh 18.999852060118403
target distance 1.0
model initialize at round 577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.72200799, 6.09586501, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.1570460731537118}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5954710763879406
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.72200799, 6.09586501, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.1570460731537118}
episode index:578
target Thresh 18.99985498952433
target distance 11.0
model initialize at round 578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.13259675,  3.99503925,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.347967956347802}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.5954767169109983
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.87570968, 2.71459614, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.1302721356131076}
episode index:579
target Thresh 18.99985786092413
target distance 14.0
model initialize at round 579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 3.99999988, 10.99999988,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.165525158585872}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.5956540627371756
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.90098498,  8.98913484,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.099609370911795}
episode index:580
target Thresh 18.99986067546641
target distance 2.0
model initialize at round 580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.92223608, 3.18437815, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.20010629081338943}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5963500109940824
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.92223608, 3.18437815, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.20010629081338943}
episode index:581
target Thresh 18.999863434277014
target distance 2.0
model initialize at round 581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.04003918,  6.18739104,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.19162082078846682}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5970435676762231
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.04003918,  6.18739104,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.19162082078846682}
episode index:582
target Thresh 18.999866138459513
target distance 8.0
model initialize at round 582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 8.97381454, 11.86774667,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 6.308992695181428}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.5974901052959894
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.9094206 ,  9.06760806,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9367813846150267}
episode index:583
target Thresh 18.999868789095604
target distance 4.0
model initialize at round 583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.00008738, 10.98846793,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.00012062583002}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.5980937181293867
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.05460768, 11.85757384,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8593107058339435}
episode index:584
target Thresh 18.999871387245587
target distance 11.0
model initialize at round 584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.13259692,  6.9950549 ,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.132598261943569}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.5983279030396357
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.42890182, 7.06423148, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.4336847447701614}
episode index:585
target Thresh 18.999873933948756
target distance 5.0
model initialize at round 585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.7976892 ,  5.99996257,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.104276510012898}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.5988469680515134
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.87187773,  9.25970531,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.2895896434514018}
episode index:586
target Thresh 18.99987643022382
target distance 12.0
model initialize at round 586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 3.97380105, 11.86775027,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.0636800343684}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.5991449816280866
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.83814637, 10.23239488,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.784483405774424}
episode index:587
target Thresh 18.999878877069328
target distance 2.0
model initialize at round 587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.08036494, 9.3371022 , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.34654930430060804}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5998267078498075
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.08036494, 9.3371022 , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.34654930430060804}
episode index:588
target Thresh 18.999881275464052
target distance 10.0
model initialize at round 588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.065246673699491}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.600122046100487
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.03316886, 10.24997497,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7507580949100758}
episode index:589
target Thresh 18.999883626367374
target distance 12.0
model initialize at round 589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.86740305, 4.99505681, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.565977766343572}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.6001196984617377
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.60844612,  1.59160813,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5657723566447387}
episode index:590
target Thresh 18.999885930719692
target distance 12.0
model initialize at round 590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([12.00431072, 11.86729726,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 14.051682756080961}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6002858872902183
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.14842944, 2.03460769, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8522734958939842}
episode index:591
target Thresh 18.999888189442775
target distance 9.0
model initialize at round 591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 9.00000012, 10.88496381,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 9.81848910678478}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6006477460110118
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.37074782, 4.9915907 , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.1743978994089885}
episode index:592
target Thresh 18.999890403440148
target distance 8.0
model initialize at round 592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 9.02483826, 11.86739123,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 6.672376527810142}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6010806756130168
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.06885103, 9.05070289, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.08550583290011925}
episode index:593
target Thresh 18.999892573597435
target distance 13.0
model initialize at round 593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.831072604542193}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6011856246966465
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.43965679,  8.85406593,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.4632438284003532}
episode index:594
target Thresh 18.999894700782725
target distance 1.0
model initialize at round 594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.83642127, 3.45953612, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9958422293874676}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6018559009576605
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.83642127, 3.45953612, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9958422293874676}
episode index:595
target Thresh 18.99989678584693
target distance 1.0
model initialize at round 595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.71556207,  3.58458451,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8274050437480058}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6025239279694765
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.71556207,  3.58458451,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8274050437480058}
episode index:596
target Thresh 18.999898829624094
target distance 6.0
model initialize at round 596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.4132812, 9.0116713, 0.       ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.0329031737896806}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6030264004519397
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.86609464, 5.12082416, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.874481790142193}
episode index:597
target Thresh 18.999900832931758
target distance 8.0
model initialize at round 597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.13259769,  4.995062  ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.772920247530664}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6034517325582073
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.9634518 , 11.13324248,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9726216764381558}
episode index:598
target Thresh 18.999902796571273
target distance 1.0
model initialize at round 598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.64214353, 2.06522813, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.6454479258737692}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6041137496991785
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.64214353, 2.06522813, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.6454479258737692}
episode index:599
target Thresh 18.99990472132812
target distance 8.0
model initialize at round 599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.05579578,  4.99999988,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.092184022364474}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6045358517830133
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.24324163, 10.98388729,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7569298871500477}
episode index:600
target Thresh 18.99990660797223
target distance 4.0
model initialize at round 600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 5.99999154, 10.99973059,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 2.0000084820053634}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6051106673374508
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.99166989, 11.05047986,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.05116255691883414}
episode index:601
target Thresh 18.999908457258282
target distance 13.0
model initialize at round 601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.        ,  8.99999988,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.00000000000002}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6053265829907524
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.90458357, 8.22004337, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.7857713682468505}
episode index:602
target Thresh 18.999910269926016
target distance 11.0
model initialize at round 602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.509715282311054}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6054808296128137
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.0736553, 11.011108 ,  0.       ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.07448819222515483}
episode index:603
target Thresh 18.999912046700526
target distance 11.0
model initialize at round 603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.0046602 , 11.86735665,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.324508496028065}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6057594721755409
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.4667257 , 5.97696134, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.1130296240083557}
episode index:604
target Thresh 18.99991378829254
target distance 12.0
model initialize at round 604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([12.0049391, 11.8674025,  0.       ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.126114038084097}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6060371936058294
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.03496325, 6.47933022, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5218423599662233}
episode index:605
target Thresh 18.999915495398724
target distance 1.0
model initialize at round 605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.50850025,  9.2329793 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5439221995410031}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6066872972467439
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.50850025,  9.2329793 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5439221995410031}
episode index:606
target Thresh 18.999917168701945
target distance 11.0
model initialize at round 606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([5.17664035, 9.14612938, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 9.889995316361157}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6069625750725318
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.23012414,  7.14393245,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8864585519880086}
episode index:607
target Thresh 18.999918808871534
target distance 6.0
model initialize at round 607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.88214195, 5.99999726, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.0961196699425555}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6074486563635308
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.6930908 , 9.85591114, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.33904993737487754}
episode index:608
target Thresh 18.999920416563597
target distance 12.0
model initialize at round 608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.13224914,  7.97377822,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.820231990079243}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6075978987933014
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.75587883, 3.95359873, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.2484919019308331}
episode index:609
target Thresh 18.99992199242122
target distance 8.0
model initialize at round 609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 8.97380374, 11.86774948,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 6.088352037346844}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6079370928116731
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.07504041, 11.29096643,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9696451472806619}
episode index:610
target Thresh 18.99992353707477
target distance 6.0
model initialize at round 610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([10.97385393, 10.13229298,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 5.9580190712412415}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6083453381589534
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.3060629 ,  5.02905316,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6945450161064476}
episode index:611
target Thresh 18.999925051142135
target distance 6.0
model initialize at round 611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.28338754, 9.00736225, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.070931784010926}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6088259830312427
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.6858523 , 5.11976464, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6962305291072695}
episode index:612
target Thresh 18.999926535228955
target distance 6.0
model initialize at round 612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.86709042, 5.99680064, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.417197274395682}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6093050597310286
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.74643215, 9.34666643, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9919706180380571}
episode index:613
target Thresh 18.999927989928885
target distance 2.0
model initialize at round 613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.35868752, 6.1945982 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.247285689622743}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6098599374839095
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.68194062, 4.43850648, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6453190999407128}
episode index:614
target Thresh 18.99992941582383
target distance 14.0
model initialize at round 614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.        ,  9.99999881,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.64911026370073}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6100635666760088
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.06519816, 6.0992355 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.11873704070794608}
episode index:615
target Thresh 18.99993081348416
target distance 3.0
model initialize at round 615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.83884251, 4.0291611 , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0417025952447996}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.610615411535301
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.88263541, 2.66746943, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9431976715192333}
episode index:616
target Thresh 18.999932183468964
target distance 3.0
model initialize at round 616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.23413238, 5.03664029, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0627516429136887}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6111654675944009
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.86000229, 3.85035182, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.8729252634097731}
episode index:617
target Thresh 18.999933526326252
target distance 3.0
model initialize at round 617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.942764  , 10.04258322,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9591260900537915}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6117946496856721
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.942764  , 10.04258322,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9591260900537915}
episode index:618
target Thresh 18.99993484259318
target distance 9.0
model initialize at round 618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.99534762, 4.        , 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 7.070411365879792}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6121221320771332
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.43675208, 11.69284925,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.892910014491445}
episode index:619
target Thresh 18.99993613279628
target distance 9.0
model initialize at round 619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.64583206,  9.0000099 ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 7.190890191367188}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6124485580737831
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.00979761,  1.74680591,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.25338358613051076}
episode index:620
target Thresh 18.999937397451646
target distance 9.0
model initialize at round 620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.13224973,  3.97380117,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 10.011815829387592}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6126460513629154
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.0911625 , 10.97674069,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.0940829251058889}
episode index:621
target Thresh 18.999938637065156
target distance 3.0
model initialize at round 621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.07544363, 4.02157462, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.024356596874163}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.613188421055258
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.53006753, 2.44434035, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7679383037722392}
episode index:622
target Thresh 18.999939852132677
target distance 11.0
model initialize at round 622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.184084079649052}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6133840927560119
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.1985407 ,  7.91239951,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.2144175069995369}
episode index:623
target Thresh 18.999941043140247
target distance 3.0
model initialize at round 623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.02153993, 9.11332166, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.5109695378987982}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6139235413253131
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.86424112, 7.41841257, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5972222499216778}
episode index:624
target Thresh 18.99994221056429
target distance 14.0
model initialize at round 624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.        , 9.99999988, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.16552504099858}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6140586033329427
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.92483639,  8.14915512,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.16702340886760203}
episode index:625
target Thresh 18.999943354871785
target distance 8.0
model initialize at round 625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.86766709, 4.9741271 , 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 6.088020230822072}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6144472876726664
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.5363878 , 10.97013867,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5372183619436112}
episode index:626
target Thresh 18.999944476520472
target distance 13.0
model initialize at round 626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.13259679,  6.99504316,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.827725103771067}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6145253947597739
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.53820239, 2.94566645, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.4649829774884035}
episode index:627
target Thresh 18.999945575959032
target distance 11.0
model initialize at round 627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.18772204862523}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6147789863883412
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.25634631,  8.85506784,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.133208639479593}
episode index:628
target Thresh 18.999946653627248
target distance 12.0
model initialize at round 628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4., 8., 0.]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.440306508910572}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6150317716842262
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.87246079, 10.31865542,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.6931786815262257}
episode index:629
target Thresh 18.9999477099562
target distance 5.0
model initialize at round 629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.10794416,  4.99995983,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.0019815097076643}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6154880704593305
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.38860264,  7.52096315,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.6168373467315854}
episode index:630
target Thresh 18.999948745368442
target distance 1.0
model initialize at round 630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.50773966, 4.20234263, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9455457928253862}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6160974396028182
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.50773966, 4.20234263, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9455457928253862}
episode index:631
target Thresh 18.999949760278145
target distance 3.0
model initialize at round 631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.505831 ,  3.0436393,  0.       ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.1547233394271816}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6166257664388897
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.36203629,  1.8476342 ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.3927920719866897}
episode index:632
target Thresh 18.99995075509129
target distance 11.0
model initialize at round 632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.00494207, 11.86740298,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.046622001903392}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6168740368513086
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.60775552, 10.92242769,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.39984146773644896}
episode index:633
target Thresh 18.999951730205815
target distance 3.0
model initialize at round 633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.72439384, 4.99940372, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.235289183709293}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6173994721244137
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.82916673, 6.48068929, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9584256160384667}
episode index:634
target Thresh 18.99995268601178
target distance 13.0
model initialize at round 634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.648875396109553}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.6174197082466187
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.08778673, 10.63479121,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.6408325785459323}
episode index:635
target Thresh 18.999953622891518
target distance 3.0
model initialize at round 635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.00170922, 10.99259555,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.001736588559673}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6179426332336523
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.35273048, 11.88525983,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0966507151527847}
episode index:636
target Thresh 18.999954541219793
target distance 1.0
model initialize at round 636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.90424299, 11.3116515 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9564423900902554}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6185424093196277
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.90424299, 11.3116515 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9564423900902554}
episode index:637
target Thresh 18.99995544136395
target distance 11.0
model initialize at round 637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.310400490210087}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.618667479675073
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.25995485, 10.77187481,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.34585781613976185}
episode index:638
target Thresh 18.99995632368406
target distance 1.0
model initialize at round 638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.41040587,  7.66366547,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5306165238313559}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6192642441826237
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.41040587,  7.66366547,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5306165238313559}
episode index:639
target Thresh 18.99995718853306
target distance 5.0
model initialize at round 639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.84143585,  6.00241184,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 3.0065959922862437}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6197068000510884
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.05387348,  2.27478993,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7272083596145106}
episode index:640
target Thresh 18.999958036256903
target distance 6.0
model initialize at round 640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.86348432, 6.98176639, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 4.429308659404394}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6201479750900102
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.20092982, 10.91245759,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.21917223040242598}
episode index:641
target Thresh 18.999958867194692
target distance 14.0
model initialize at round 641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.        , 10.99999917,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.649110376792548}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6203270154568871
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.1446008 , 6.56164622, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.4615879438679736}
episode index:642
target Thresh 18.999959681678806
target distance 4.0
model initialize at round 642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.86701499, 4.99714809, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.1824597989224195}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.620839726163797
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.88261894, 6.70153728, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.93171679651164}
episode index:643
target Thresh 18.999960480035057
target distance 7.0
model initialize at round 643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.98407328, 4.        , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.095919958451607}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6211404505796918
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.6323582 , 8.50607616, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.802394947678829}
episode index:644
target Thresh 18.999961262582794
target distance 2.0
model initialize at round 644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.07689297,  3.75199628,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7559172788175595}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6217278297260799
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.07689297,  3.75199628,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7559172788175595}
episode index:645
target Thresh 18.999962029635046
target distance 3.0
model initialize at round 645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([13.99971056,  9.07054198,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0027737206017049}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6222359909803739
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.7232188 ,  8.23158151,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.05523097369202}
episode index:646
target Thresh 18.99996278149865
target distance 6.0
model initialize at round 646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.00000584,  6.99999833,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.6568595599960645}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6226691656465556
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.89892948, 11.19924764,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9207463422747806}
episode index:647
target Thresh 18.999963518474352
target distance 5.0
model initialize at round 647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([12.99999297,  9.76545358,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.096120375363935}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6231010033538911
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.43224931,  8.994923  ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.4322791227005074}
episode index:648
target Thresh 18.999964240856958
target distance 5.0
model initialize at round 648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.63083005, 7.99997532, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 3.414638298406871}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6235315102824674
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.12692961, 11.77232467,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.1656488700704895}
episode index:649
target Thresh 18.999964948935435
target distance 2.0
model initialize at round 649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.00386144, 8.12992299, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.12998035619759996}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6241106925743406
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.00386144, 8.12992299, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.12998035619759996}
episode index:650
target Thresh 18.999965642993015
target distance 1.0
model initialize at round 650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.13733714, 11.80420535,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.179378419952567}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6246880955043339
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.13733714, 11.80420535,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.179378419952567}
episode index:651
target Thresh 18.999966323307333
target distance 1.0
model initialize at round 651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.32714286,  5.26417649,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8052693305327294}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6252637272596954
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.32714286,  5.26417649,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8052693305327294}
episode index:652
target Thresh 18.999966990150526
target distance 5.0
model initialize at round 652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.50728887,  3.9999398 ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.042647401300325}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6256882851046268
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.50392892,  6.94279633,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.5071652764836092}
episode index:653
target Thresh 18.99996764378934
target distance 2.0
model initialize at round 653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.95456195, 10.20518124,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.21015222618814466}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6262606271763324
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.95456195, 10.20518124,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.21015222618814466}
episode index:654
target Thresh 18.999968284485238
target distance 2.0
model initialize at round 654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.54596007, 5.96109772, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.4557034636832127}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6268312216386586
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.54596007, 5.96109772, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.4557034636832127}
episode index:655
target Thresh 18.999968912494506
target distance 8.0
model initialize at round 655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.04515254,  9.05556837,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 6.130386766633347}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.627182660325185
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.72168955,  3.13293047,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.30842700053773914}
episode index:656
target Thresh 18.99996952806836
target distance 12.0
model initialize at round 656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.8677503 , 6.97380018, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.522606953796984}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6272909626627322
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.92468143,  4.92835813,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.931408447838909}
episode index:657
target Thresh 18.999970131453033
target distance 10.0
model initialize at round 657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.13224972,  4.97380104,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 9.980393933080709}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6274547938602432
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.95416117, 9.36296928, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.0208673961891273}
episode index:658
target Thresh 18.999970722889888
target distance 12.0
model initialize at round 658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([3.99999988, 4.        , 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 12.20655571339376}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6275623545616598
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.97116733, 10.25331879,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7472376859945105}
episode index:659
target Thresh 18.999971302615513
target distance 10.0
model initialize at round 659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.13224956,  7.97379826,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.132287151313665}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6277838978691421
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.98942669, 8.278702  , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.2789024934189036}
episode index:660
target Thresh 18.999971870861796
target distance 4.0
model initialize at round 660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.38288558,  5.99976337,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.0932694023781955}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6282713654971767
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.00684148,  7.92529798,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.07501465401512904}
episode index:661
target Thresh 18.999972427856054
target distance 2.0
model initialize at round 661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.90454519, 5.23324895, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7726699135303764}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6288328891142504
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.90454519, 5.23324895, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7726699135303764}
episode index:662
target Thresh 18.99997297382108
target distance 13.0
model initialize at round 662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.00494244, 11.86740304,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.039073606697462}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.628993159101446
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.35618243, 11.84494265,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.0622754597859256}
episode index:663
target Thresh 18.99997350897528
target distance 6.0
model initialize at round 663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 9.97392915, 11.86771763,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.118516769816376}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6294050669943655
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.96856151, 10.16022938,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8403588980810552}
episode index:664
target Thresh 18.999974033532716
target distance 13.0
model initialize at round 664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([14.        ,  5.99999893,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.180339695575759}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6294562179181169
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.03049808, 3.97573512, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.0389732894264514}
episode index:665
target Thresh 18.999974547703218
target distance 12.0
model initialize at round 665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.        ,  5.99999964,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.000000000000027}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6295596429604227
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.85368128, 5.93650967, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.856038992919603}
episode index:666
target Thresh 18.99997505169246
target distance 2.0
model initialize at round 666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.74387928,  3.94572643,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7458565608510648}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.63011502580456
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.74387928,  3.94572643,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7458565608510648}
episode index:667
target Thresh 18.99997554570205
target distance 13.0
model initialize at round 667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 4.99560368, 11.86731178,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.132443404362096}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6302171549516995
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.22299815,  3.06292077,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.23170498404792772}
episode index:668
target Thresh 18.99997602992959
target distance 13.0
model initialize at round 668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.        ,  9.99974728,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.038269130991155}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6303189787800135
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.71803925, 2.99849408, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.2819647689502754}
episode index:669
target Thresh 18.999976504568785
target distance 1.0
model initialize at round 669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.63384914, 6.32358468, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.769158072304004}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6308707414982523
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.63384914, 6.32358468, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.769158072304004}
episode index:670
target Thresh 18.999976969809495
target distance 13.0
model initialize at round 670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.13259694,  6.99505654,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.309949834251409}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6309712877793187
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.31017821, 4.97913892, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6901371530769556}
episode index:671
target Thresh 18.99997742583782
target distance 3.0
model initialize at round 671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.11276007,  3.07433736,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0802386786108025}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6314460328867899
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.72663349,  1.3976859 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6614465415870383}
episode index:672
target Thresh 18.999977872836183
target distance 1.0
model initialize at round 672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.93299174, 5.37676549, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.38267785620553363}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6319936613668987
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.93299174, 5.37676549, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.38267785620553363}
episode index:673
target Thresh 18.99997831098338
target distance 14.0
model initialize at round 673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([3.99999988, 5.        , 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.165525178183744}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.6319910734564502
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.39249026,  3.05319762,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.39607902111897964}
episode index:674
target Thresh 18.999978740454683
target distance 9.0
model initialize at round 674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.86767207, 3.97410775, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 7.079266683676565}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6322614663105889
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.9246703 , 11.45091636,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.45716532089745576}
episode index:675
target Thresh 18.999979161421887
target distance 12.0
model initialize at round 675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([5.17650133, 9.14588592, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.446687799740028}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.632359211620919
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.29117581,  3.01928844,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.2918139723445626}
episode index:676
target Thresh 18.99997957405338
target distance 3.0
model initialize at round 676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.49621129,  4.99810469,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.1214264457130914}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6328284003777566
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.2712087 ,  6.31885421,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.4185954728126055}
episode index:677
target Thresh 18.999979978514222
target distance 13.0
model initialize at round 677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4., 8., 0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.04536101718728}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6329792314843159
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.63880922,  8.65899317,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7241290480502136}
episode index:678
target Thresh 18.999980374966203
target distance 4.0
model initialize at round 678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.01914372, 11.8649226 ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.196595656882592}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6334461251051048
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.05471157, 10.69262286,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.3122083606573404}
episode index:679
target Thresh 18.99998076356791
target distance 12.0
model initialize at round 679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 5.99999976, 10.99999702,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 10.440305880911144}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6336524998292149
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.92130031,  7.28703126,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7172991466865369}
episode index:680
target Thresh 18.999981144474784
target distance 10.0
model initialize at round 680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.669994727410566}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6339180706811545
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.06041612, 10.3927119 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.11875676665708}
episode index:681
target Thresh 18.9999815178392
target distance 7.0
model initialize at round 681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.99858415,  9.00006962,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 5.000069818685085}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6342457201376337
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.29024237,  3.32657397,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7333097955821963}
episode index:682
target Thresh 18.999981883810502
target distance 4.0
model initialize at round 682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.92745394,  4.99990189,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.00141334487691}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6347080250861876
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.82544979,  6.96853626,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.1773633031551087}
episode index:683
target Thresh 18.99998224253509
target distance 9.0
model initialize at round 683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([3.99999976, 9.        , 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.2801101185256964}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6349708879881084
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.72141645, 11.60873319,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9439267907492338}
episode index:684
target Thresh 18.999982594156453
target distance 8.0
model initialize at round 684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.02897894, 9.00047755, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.000547528035214}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6352955655238922
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.87638837, 3.23237601, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9066725884994379}
episode index:685
target Thresh 18.999982938815247
target distance 2.0
model initialize at round 685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.06359053, 3.45893812, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.4603233197821173}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6357543183438282
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.15996551, 1.95081395, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8414732439355137}
episode index:686
target Thresh 18.999983276649342
target distance 14.0
model initialize at round 686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 13.000000000000027}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6358454143813099
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.09354882, 11.27182933,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.28747619803986624}
episode index:687
target Thresh 18.999983607793872
target distance 9.0
model initialize at round 687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 5.99999976, 10.99999964,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.000000238418627}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6361050958284301
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.79329657, 11.12274677,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8027367023893869}
episode index:688
target Thresh 18.999983932381298
target distance 6.0
model initialize at round 688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.85406014, 5.99002412, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.417855309150597}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6364917357474019
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.4509129 , 9.77778777, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.5026934643482751}
episode index:689
target Thresh 18.99998425054147
target distance 1.0
model initialize at round 689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.65229406, 3.32642729, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.93765010817258}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6370185593187825
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.65229406, 3.32642729, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.93765010817258}
episode index:690
target Thresh 18.99998456240164
target distance 9.0
model initialize at round 690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 8.97557051, 10.13274949,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 10.746358633225432}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6371604888865193
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.51992006,  1.41636027,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.7816343155769665}
episode index:691
target Thresh 18.99998486808657
target distance 5.0
model initialize at round 691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.98863626,  4.99997997,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.0000415493888193}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6375439274863943
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.39783648,  8.33282065,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5186939846942429}
episode index:692
target Thresh 18.99998516771853
target distance 4.0
model initialize at round 692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.68641002,  8.99984834,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 2.114654903240738}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.637994802049906
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.90264018, 10.93538734,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.11684917587034203}
episode index:693
target Thresh 18.99998546141738
target distance 10.0
model initialize at round 693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([12.02801388, 11.85974912,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 11.234974994349047}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6381347113994378
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.887716  , 4.08789698, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.1425958452694999}
episode index:694
target Thresh 18.999985749300606
target distance 14.0
model initialize at round 694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4., 6., 0.]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 13.000000000000025}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6381710937302142
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.68736586, 11.56014176,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6414817969834837}
episode index:695
target Thresh 18.99998603148336
target distance 12.0
model initialize at round 695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 10.895369891272122}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6383103477487412
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.10367357, 11.43971667,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.4517731259292821}
episode index:696
target Thresh 18.999986308078523
target distance 2.0
model initialize at round 696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.53480029, 2.24591815, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.5886315399775462}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6388292712096468
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.53480029, 2.24591815, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.5886315399775462}
episode index:697
target Thresh 18.999986579196733
target distance 14.0
model initialize at round 697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.        ,  6.99999964,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.369316790115516}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6389145262596241
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.13375939, 4.88684238, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.2397024665543863}
episode index:698
target Thresh 18.99998684494644
target distance 3.0
model initialize at round 698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.44290042,  7.99556851,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.1485828456045428}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6393595698558192
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.66619922,  9.58743632,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.6756510840867762}
episode index:699
target Thresh 18.999987105433956
target distance 13.0
model initialize at round 699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.0261578 , 11.86773989,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.990073402350488}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6394438237504448
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.09655627, 4.21692682, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7890036199397757}
episode index:700
target Thresh 18.99998736076347
target distance 13.0
model initialize at round 700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.00420146, 11.86727822,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 14.780244559885224}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6394780271848793
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.95080872, 2.17565177, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.182409774844853}
episode index:701
target Thresh 18.99998761103712
target distance 6.0
model initialize at round 701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.00480203, 11.86737961,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.097656242507009}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6398527023598296
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.06410306, 10.96239443,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.07431945362160008}
episode index:702
target Thresh 18.999987856355023
target distance 8.0
model initialize at round 702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 8.97452545, 10.13254665,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 8.725171416490328}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6400432119403988
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.84065382,  3.08807831,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.1820686476318521}
episode index:703
target Thresh 18.999988096815304
target distance 8.0
model initialize at round 703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.94011343, 5.        , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.305873460739141}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6403519218666198
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.55841874, 10.96733713,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.4427876193126114}
episode index:704
target Thresh 18.999988332514153
target distance 12.0
model initialize at round 704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.86775028, 4.97380104, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.305877872538344}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.6403375920621631
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.01766548,  3.0847731 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.08659415085085745}
episode index:705
target Thresh 18.999988563545852
target distance 13.0
model initialize at round 705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 3.97380106, 11.86775027,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.183271169127469}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6404718049496458
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.43067888,  9.60776014,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5825258836326631}
episode index:706
target Thresh 18.999988790002817
target distance 4.0
model initialize at round 706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([10.9803377 , 11.86543224,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 2.197273028063607}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6409096100345827
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.97536402, 10.19833989,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8020385686466506}
episode index:707
target Thresh 18.999989011975632
target distance 1.0
model initialize at round 707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.16184092, 5.17356884, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8421288121666665}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.641416799850918
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.16184092, 5.17356884, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8421288121666665}
episode index:708
target Thresh 18.999989229553094
target distance 13.0
model initialize at round 708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.99999988, 4.        , 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.045361135906973}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6414478345920156
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.10562511,  5.9150817 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9211575207224766}
episode index:709
target Thresh 18.999989442822233
target distance 11.0
model initialize at round 709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.99983164, 9.00003774, 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.849026980864512}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6415797276286817
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.61501099,  5.07003196,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.39130679685983694}
episode index:710
target Thresh 18.999989651868358
target distance 2.0
model initialize at round 710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.08202887,  3.13071907,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.15432501079024374}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6420838349034654
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.08202887,  3.13071907,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.15432501079024374}
episode index:711
target Thresh 18.99998985677509
target distance 12.0
model initialize at round 711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([12.        , 10.99988449,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.180288228633824}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6422688027441908
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.01754279, 6.09030745, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.0919955713737639}
episode index:712
target Thresh 18.9999900576244
target distance 13.0
model initialize at round 712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([14.00485846, 11.86738938,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 13.527849989038351}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6423474401822687
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.02707177, 4.07859442, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.08312619422395742}
episode index:713
target Thresh 18.99999025449663
target distance 9.0
model initialize at round 713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 7.97381501, 11.86774666,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 7.588889667811992}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6425885589635261
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.63427114,  8.67833253,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7111749716599074}
episode index:714
target Thresh 18.999990447470527
target distance 4.0
model initialize at round 714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.39480697,  4.00867033,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.0978596512534278}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6430185050349058
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.65215359,  2.05830907,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.3526996880702332}
episode index:715
target Thresh 18.99999063662328
target distance 4.0
model initialize at round 715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.03102291,  5.99984932,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.000391253127307}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6434472501396056
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.37677244,  7.95117067,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.6251374957630054}
episode index:716
target Thresh 18.999990822030565
target distance 13.0
model initialize at round 716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.        ,  9.99999952,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.180339802199743}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6436290265515447
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.938472  , 8.60310561, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.1155563961314614}
episode index:717
target Thresh 18.999991003766535
target distance 8.0
model initialize at round 717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.45118332,  8.00002027,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 6.173020023697634}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6439267228933949
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.90405808,  2.15230322,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.18000311987747375}
episode index:718
target Thresh 18.999991181903894
target distance 13.0
model initialize at round 718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.00472245, 11.867367  ,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.132732000498663}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6439538351442929
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.72670452, 2.68245544, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7930535997985642}
episode index:719
target Thresh 18.999991356513895
target distance 1.0
model initialize at round 719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.1986721 ,  7.71669459,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7437215499955666}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6444483437065925
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.1986721 ,  7.71669459,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7437215499955666}
episode index:720
target Thresh 18.99999152766639
target distance 12.0
model initialize at round 720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4., 7., 0.]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.19803902718559}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6446277231709385
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.23902307,  8.94386326,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.76304470320065}
episode index:721
target Thresh 18.99999169542983
target distance 12.0
model initialize at round 721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 3.99999785, 10.89163429,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 13.381375235062281}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.6446078086093785
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.91799995,  1.30132601,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7034695146823711}
episode index:722
target Thresh 18.99999185987134
target distance 4.0
model initialize at round 722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.95973074,  7.00223565,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.002640560647386}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6450302044480931
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.32392105,  5.04612231,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.32718819863937676}
episode index:723
target Thresh 18.999992021056688
target distance 7.0
model initialize at round 723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([15.29113221,  4.99999881,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 5.164013391634599}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6453234983646013
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.4405765 , 10.56240177,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.793253051705394}
episode index:724
target Thresh 18.99999217905035
target distance 7.0
model initialize at round 724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.61027527, 7.00161183, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.0167724935791265}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6456159831944432
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.88920519, 1.54610612, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9983514036316576}
episode index:725
target Thresh 18.99999233391553
target distance 11.0
model initialize at round 725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.13258695,  6.99419036,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.408942398844026}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.6455948171152837
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.83752783, 1.92863429, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8405628603574931}
episode index:726
target Thresh 18.999992485714174
target distance 12.0
model initialize at round 726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.182309676547058}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6456673652294219
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.95798621, 6.98326843, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9841656143358799}
episode index:727
target Thresh 18.999992634507
target distance 5.0
model initialize at round 727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.19900665,  7.00110388,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 3.007694820318924}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6460201573101507
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.67626718,  3.46071136,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8649679368415668}
episode index:728
target Thresh 18.999992780353534
target distance 11.0
model initialize at round 728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.132249  ,  4.97376695,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.559628770105038}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.645955296928708
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.07702551, 2.01685391, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.07884784661808962}
episode index:729
target Thresh 18.999992923312114
target distance 13.0
model initialize at round 729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.648875396109553}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6459792217702975
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.62840842, 11.14097723,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6440277353191878}
episode index:730
target Thresh 18.99999306343992
target distance 7.0
model initialize at round 730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.4984715 , 9.00109565, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.026180315516371}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6462684088814189
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.87257504, 3.74565897, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9088875365837299}
episode index:731
target Thresh 18.999993200793014
target distance 2.0
model initialize at round 731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.2911281 ,  6.03238487,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.2929237954990535}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6467516487599961
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.2911281 ,  6.03238487,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.2929237954990535}
episode index:732
target Thresh 18.999993335426332
target distance 9.0
model initialize at round 732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 6.9739275 , 11.86771797,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 8.547536075608615}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6469805090618242
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.83978526,  7.03900397,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8406905404439832}
episode index:733
target Thresh 18.999993467393733
target distance 7.0
model initialize at round 733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.13227865,  5.97391454,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.459683807787627}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6473286282592877
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.96213539, 10.13936935,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.2908871435029023}
episode index:734
target Thresh 18.999993596748002
target distance 7.0
model initialize at round 734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 9.02533159, 11.86752473,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 5.36111986670728}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6476144056358057
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.64856721, 10.13055683,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.37490010664959716}
episode index:735
target Thresh 18.99999372354089
target distance 8.0
model initialize at round 735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.86671242,  8.00162931,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 6.063888582504828}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6478994064433659
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.20626128,  2.09906124,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.22881618412105706}
episode index:736
target Thresh 18.999993847823106
target distance 9.0
model initialize at round 736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.86740299, 3.99505789, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.133115762374166}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6481254672894401
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.00957933, 11.86662721,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.3160455239004925}
episode index:737
target Thresh 18.99999396964437
target distance 7.0
model initialize at round 737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.83994269, 5.        , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 5.327794019680026}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6484090032416224
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.46810861, 10.33920394,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5780873496272854}
episode index:738
target Thresh 18.999994089053413
target distance 14.0
model initialize at round 738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.        ,  8.99999976,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.369316819027995}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6485263007888259
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.11383773, 6.08237022, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.14051292296302514}
episode index:739
target Thresh 18.999994206098
target distance 11.0
model initialize at round 739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.       , 10.8455494,  0.       ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.619181600626572}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6485464279922046
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.79004257, 2.06470008, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7926874304422319}
episode index:740
target Thresh 18.999994320824943
target distance 11.0
model initialize at round 740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 5.99600252, 11.86724148,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.32391169810807}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6487154354274378
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.15166033,  5.99175016,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0032792392054073}
episode index:741
target Thresh 18.999994433280143
target distance 12.0
model initialize at round 741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.99982995, 8.99962293, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.661855609509418}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6487823112504383
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.53367632,  3.1100996 ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.5449149802576154}
episode index:742
target Thresh 18.99999454350858
target distance 11.0
model initialize at round 742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 5.99998903, 10.34677348,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.274723005448537}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6488490070577644
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.9191544 ,  2.22553468,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.2395869447798308}
episode index:743
target Thresh 18.999994651554353
target distance 10.0
model initialize at round 743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 5.99996197, 10.27340668,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.812171532438125}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6489155235752858
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.49566171,  2.46541413,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7349416053061307}
episode index:744
target Thresh 18.99999475746067
target distance 3.0
model initialize at round 744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.44890511, 4.07880485, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.2114146611062273}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6493196638120975
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.19128715, 2.17200799, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.157405393498743}
episode index:745
target Thresh 18.999994861269904
target distance 6.0
model initialize at round 745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([10.99999571, 11.30314732,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 4.61568216966847}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6496590476407675
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.96997056,  8.26731077,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7333043541011653}
episode index:746
target Thresh 18.99999496302358
target distance 3.0
model initialize at round 746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.77620006, 7.99804449, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.2674389016596876}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6499975228112618
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.7440025 , 8.06773603, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.1927513714326068}
episode index:747
target Thresh 18.999995062762395
target distance 2.0
model initialize at round 747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.85597026,  1.72406363,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.31126426489641684}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6504654405615141
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.85597026,  1.72406363,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.31126426489641684}
episode index:748
target Thresh 18.99999516052625
target distance 8.0
model initialize at round 748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([10.01928175, 11.86523732,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 7.153447581258554}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6507416883044226
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.06741612, 8.22747199, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.23725184838492552}
episode index:749
target Thresh 18.999995256354246
target distance 12.0
model initialize at round 749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([3.99999952, 4.        , 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.04987609559161}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6507585932950688
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.3589492 ,  5.91430005,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1166426086805605}
episode index:750
target Thresh 18.999995350284728
target distance 10.0
model initialize at round 750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.13224855,  3.97374532,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.343106237523653}
done in step count: 11
reward sum = 0.5688000922764597
running average episode reward sum: 0.6506494608036991
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.85031472, 1.81609076, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8699757112718544}
episode index:751
target Thresh 18.99999544235526
target distance 14.0
model initialize at round 751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 3.99505729, 11.86740305,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.036238507658497}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.650761751268887
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.81806376, 10.11040642,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9080074546331337}
episode index:752
target Thresh 18.99999553260267
target distance 14.0
model initialize at round 752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.00494241, 11.86740304,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.612511583644807}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6508737434858274
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.17430776, 8.00934587, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.17455813349937987}
episode index:753
target Thresh 18.999995621063068
target distance 11.0
model initialize at round 753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.86775027, 3.97380117, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.90287859041495}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6509366924945912
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.17084395,  7.99829183,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8291578140674033}
episode index:754
target Thresh 18.999995707771827
target distance 7.0
model initialize at round 754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.94259143,  9.00009739,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 5.088167897283568}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6512101207164527
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.63044816,  3.45995069,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8301314039116842}
episode index:755
target Thresh 18.999995792763638
target distance 11.0
model initialize at round 755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 5.97451557, 11.86756477,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.765114283966735}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6513722514264838
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.14211822,  6.08055088,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.16335859746314244}
episode index:756
target Thresh 18.9999958760725
target distance 4.0
model initialize at round 756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.49083883, 8.01180255, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.0752336297916267}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6517667398658148
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.82759076, 6.10589814, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.20233477520280568}
episode index:757
target Thresh 18.999995957731734
target distance 2.0
model initialize at round 757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.04473197,  5.98955786,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.045934603425919325}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6522261504992373
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.04473197,  5.98955786,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.045934603425919325}
episode index:758
target Thresh 18.99999603777401
target distance 1.0
model initialize at round 758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.40348148, 2.55941057, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5974247664051162}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6526843505644556
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.40348148, 2.55941057, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5974247664051162}
episode index:759
target Thresh 18.999996116231344
target distance 4.0
model initialize at round 759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([5.17663069, 9.14611407, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.6003016531916625}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6530755553663445
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.10193905, 11.19883692,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.22344504947564772}
episode index:760
target Thresh 18.999996193135114
target distance 3.0
model initialize at round 760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.13073742,  5.04723811,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0553672044254423}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6534657320347199
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.81868619,  3.42013955,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6075465733698301}
episode index:761
target Thresh 18.99999626851609
target distance 12.0
model initialize at round 761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 5.99999738, 10.90135682,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.387837974185297}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6535246186017265
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.35302665,  2.08862597,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.36398128407016966}
episode index:762
target Thresh 18.999996342404422
target distance 14.0
model initialize at round 762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.165525060596456}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6536315219726613
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.1312834 , 8.19283382, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.23328140186066035}
episode index:763
target Thresh 18.999996414829667
target distance 7.0
model initialize at round 763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 3.97381181, 11.86774659,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.100544270267158}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6538982019177232
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.83052243, 10.33470162,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0641378887761743}
episode index:764
target Thresh 18.999996485820795
target distance 2.0
model initialize at round 764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.20827499,  6.96452594,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.21127442028154822}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6543506225688112
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.20827499,  6.96452594,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.21127442028154822}
episode index:765
target Thresh 18.999996555406206
target distance 5.0
model initialize at round 765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([10.97412334, 10.13234792,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 5.121740804748944}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6546745773696351
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.99971096,  6.9260881 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.362740320021433}
episode index:766
target Thresh 18.999996623613733
target distance 13.0
model initialize at round 766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.18033988749897}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6547794239318976
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.85762987, 8.62788065, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0629031481916895}
episode index:767
target Thresh 18.99999669047066
target distance 1.0
model initialize at round 767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.20893414,  9.14847052,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.876787277417115}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6552289298903197
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.20893414,  9.14847052,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.876787277417115}
episode index:768
target Thresh 18.999996756003732
target distance 1.0
model initialize at round 768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.60219789, 11.11660255,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.4145391131917264}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6556772667825299
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.60219789, 11.11660255,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.4145391131917264}
episode index:769
target Thresh 18.99999682023916
target distance 3.0
model initialize at round 769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.90974545,  4.99007964,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0139452715817259}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6560595040983968
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.53843319,  6.53739274,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.7607241659905822}
episode index:770
target Thresh 18.999996883202645
target distance 2.0
model initialize at round 770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.10052593, 5.11363459, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.1517177706105953}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6565056007208373
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.10052593, 5.11363459, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.1517177706105953}
episode index:771
target Thresh 18.999996944919367
target distance 4.0
model initialize at round 771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([11.97832287, 11.86136621,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.19752819481137}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6568857748131678
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.97191355, 10.20891123,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7915872008794381}
episode index:772
target Thresh 18.999997005414016
target distance 3.0
model initialize at round 772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.82414819, 3.99931105, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.2963790400520252}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6572649652726591
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.87656794, 5.46164381, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9906999348060911}
episode index:773
target Thresh 18.999997064710794
target distance 6.0
model initialize at round 773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.52033299, 6.00237155, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 4.031012088742482}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6575818064027977
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.58378328, 2.17733657, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6101239077559935}
episode index:774
target Thresh 18.999997122833413
target distance 14.0
model initialize at round 774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.00000000000002}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6576818194146973
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.10212455, 9.06625948, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.12173636280039368}
episode index:775
target Thresh 18.999997179805128
target distance 2.0
model initialize at round 775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.33818865, 9.95014048, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.34184431236803514}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6581229510907093
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.33818865, 9.95014048, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.34184431236803514}
episode index:776
target Thresh 18.999997235648728
target distance 2.0
model initialize at round 776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.9951545 ,  9.20474148,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7952732836813091}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6585629472926517
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.9951545 ,  9.20474148,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7952732836813091}
episode index:777
target Thresh 18.999997290386553
target distance 1.0
model initialize at round 777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.57283354, 10.26382124,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6306662439304906}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6590018123989594
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.57283354, 10.26382124,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6306662439304906}
episode index:778
target Thresh 18.999997344040494
target distance 13.0
model initialize at round 778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.177862962124477}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.659099489007722
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.99793977,  8.2442702 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.755732612508483}
episode index:779
target Thresh 18.999997396632015
target distance 11.0
model initialize at round 779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.310400490210087}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6591497938886015
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.51717152, 10.79494067,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.5563413552614436}
episode index:780
target Thresh 18.999997448182153
target distance 13.0
model initialize at round 780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.0261987 , 11.86775024,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.393026338985132}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.659247030888264
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.4762239 , 8.79058819, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.5640874999893145}
episode index:781
target Thresh 18.999997498711533
target distance 4.0
model initialize at round 781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.00019741,  9.99982214,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.236324089540498}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6596188377541358
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.01781761, 11.85111991,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8513063939676753}
episode index:782
target Thresh 18.999997548240362
target distance 3.0
model initialize at round 782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.18017095,  7.99737418,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.2951363661192496}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6599896949217551
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.69150776,  9.6497463 ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7192619251436798}
episode index:783
target Thresh 18.999997596788457
target distance 12.0
model initialize at round 783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 10.18230950236307}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6601348368127987
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.08487501,  8.3869228 ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.1015068827752945}
episode index:784
target Thresh 18.999997644375235
target distance 6.0
model initialize at round 784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.06861222,  4.99999738,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 4.000591033495835}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6604435822436105
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.62655208,  8.8782556 ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6382704790672563}
episode index:785
target Thresh 18.99999769101973
target distance 1.0
model initialize at round 785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.68955283,  4.2491933 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0194085576468468}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6608755878641657
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.68955283,  4.2491933 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0194085576468468}
episode index:786
target Thresh 18.9999977367406
target distance 5.0
model initialize at round 786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 4.97409201, 11.8676685 ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.1478512951056827}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6611826074475657
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.85774553, 10.44844093,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0197768436520305}
episode index:787
target Thresh 18.99999778155614
target distance 2.0
model initialize at round 787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.30826235,  3.90429866,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.32277611618478785}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6616125787579115
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.30826235,  3.90429866,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.32277611618478785}
episode index:788
target Thresh 18.99999782548427
target distance 12.0
model initialize at round 788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.13259695,  6.99505729,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.1813388106091}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6617057084307468
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.93835227, 6.94826199, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.950263775433277}
episode index:789
target Thresh 18.999997868542568
target distance 3.0
model initialize at round 789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.05761778, 6.03673983, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0383396728503342}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.662070637913746
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.41445173, 4.59746408, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.710564523106361}
episode index:790
target Thresh 18.999997910748252
target distance 9.0
model initialize at round 790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.062257748298576}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6622633504448284
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.0121248 , 10.73259544,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.26767930826979786}
episode index:791
target Thresh 18.99999795211821
target distance 3.0
model initialize at round 791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.69324374,  4.0227592 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0677714087103651}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.662626654295277
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.92970483,  2.32065117,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.6829760163144025}
episode index:792
target Thresh 18.999997992668984
target distance 5.0
model initialize at round 792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.81034386,  7.00073969,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 3.108230345871543}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.662929142751399
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.73104614,  3.32468367,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9952289232885985}
episode index:793
target Thresh 18.999998032416805
target distance 5.0
model initialize at round 793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.16806556,  7.99976989,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.113437910076059}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6632906929494451
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.17509159, 10.00039471,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.2960264722701889}
episode index:794
target Thresh 18.99999807137756
target distance 1.0
model initialize at round 794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.4529953, 10.032743 ,  0.       ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.5479838006377846}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6637142266690056
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.4529953, 10.032743 ,  0.       ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.5479838006377846}
episode index:795
target Thresh 18.999998109566842
target distance 14.0
model initialize at round 795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.369316876853006}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6637577229873785
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.97626807, 8.59587234, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.1437496197314867}
episode index:796
target Thresh 18.99999814699993
target distance 5.0
model initialize at round 796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.93705678, 4.99999893, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.1429415912465135}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.664057274150506
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.2333039 , 8.65540028, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0086488114432595}
episode index:797
target Thresh 18.99999818369179
target distance 11.0
model initialize at round 797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.1322494 ,  7.97379068,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.559635837198044}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6641462899606243
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.2781506 , 5.90323904, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.1562470833366616}
episode index:798
target Thresh 18.9999982196571
target distance 4.0
model initialize at round 798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4.53827023, 6.99997675, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 2.523166323675834}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6645040543036024
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.00429809, 8.95957851, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.04064935521205285}
episode index:799
target Thresh 18.999998254910253
target distance 12.0
model initialize at round 799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.13259815856355}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.664592289099004
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.97316589, 7.08605642, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.09014309389464863}
episode index:800
target Thresh 18.999998289465346
target distance 12.0
model initialize at round 800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([3.99999964, 6.99999559, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.440305584038954}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.664634417697
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.78215796,  4.87808808,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9047064872482642}
episode index:801
target Thresh 18.9999983233362
target distance 11.0
model initialize at round 801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.86775027, 3.97380117, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 12.33004276383014}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6646329039982369
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.57463746, 10.79007041,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6117831696658901}
episode index:802
target Thresh 18.999998356536366
target distance 12.0
model initialize at round 802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([3.99999964, 7.99999988, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.000000357627872}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6647688293201569
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.08978255,  8.04080417,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.09861990633016562}
episode index:803
target Thresh 18.999998389079128
target distance 12.0
model initialize at round 803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([3.99999774, 8.99958014, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 11.180154154779583}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6648105811445022
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.07019989,  4.06148245,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.0933172867072646}
episode index:804
target Thresh 18.999998420977498
target distance 11.0
model initialize at round 804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 11.401754250991408}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6648522292376069
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.30475099, 11.00657469,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.3048219062986807}
episode index:805
target Thresh 18.999998452244238
target distance 5.0
model initialize at round 805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.0996232 , 6.99999654, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.1951826131873244}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6651470775884288
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.19010728, 10.66284209,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.6895653860033878}
episode index:806
target Thresh 18.999998482891858
target distance 13.0
model initialize at round 806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.00494236, 11.86740303,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 11.37236811239754}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6652337502192052
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.29067355, 9.77049172, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0472829189112196}
episode index:807
target Thresh 18.99999851293261
target distance 4.0
model initialize at round 807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.5042007 ,  8.99956775,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 2.0629947991156885}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6655861836966567
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.07348192, 10.91517496,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.11222691018507465}
episode index:808
target Thresh 18.999998542378517
target distance 7.0
model initialize at round 808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.95943663, 9.05875498, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.148933925238763}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6658232526908513
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.89479883, 3.59950459, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9803374512955546}
episode index:809
target Thresh 18.999998571241356
target distance 3.0
model initialize at round 809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.31456995, 8.02923417, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0762328893342947}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6661740881813565
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.96729985, 6.24974537, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.7509669175732052}
episode index:810
target Thresh 18.999998599532674
target distance 14.0
model initialize at round 810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([3.99999964, 8.99999893, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.369316963590531}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6662590669759848
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.89773695,  6.02273486,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.10475975423396261}
episode index:811
target Thresh 18.999998627263782
target distance 2.0
model initialize at round 811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.61443467,  3.94332897,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6170426006358882}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6666700779772459
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.61443467,  3.94332897,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6170426006358882}
episode index:812
target Thresh 18.999998654445783
target distance 12.0
model initialize at round 812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 4.97380038, 11.86775025,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.198684709522853}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6668018256519356
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.86662025,  9.42079421,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5943647885171774}
episode index:813
target Thresh 18.999998681089544
target distance 12.0
model initialize at round 813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([3.99999988, 8.        , 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.049875739738559}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6669332496222649
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.02571838,  8.33227609,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6682190201859586}
episode index:814
target Thresh 18.99999870720572
target distance 10.0
model initialize at round 814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.13224972,  4.97380079,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.189726312754736}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6669717821946225
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.64925605, 6.99871996, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0585191847075344}
episode index:815
target Thresh 18.999998732804762
target distance 13.0
model initialize at round 815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.13259689,  5.99505212,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.827728226157015}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.6669267792871838
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.21235798, 2.1820257 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8084016999262925}
episode index:816
target Thresh 18.999998757896908
target distance 8.0
model initialize at round 816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.17645224, 9.00038877, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 6.056640677159444}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6671598860444823
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.84573871, 3.04572424, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.1608951603592012}
episode index:817
target Thresh 18.999998782492195
target distance 1.0
model initialize at round 817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.96983794,  7.46308076,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.46406200405385317}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6675667810493179
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.96983794,  7.46308076,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.46406200405385317}
episode index:818
target Thresh 18.999998806600466
target distance 2.0
model initialize at round 818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.09572375,  5.18252999,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.20610733059541916}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.667972682415558
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.09572375,  5.18252999,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.20610733059541916}
episode index:819
target Thresh 18.99999883023136
target distance 2.0
model initialize at round 819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.66056408, 3.95206707, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6623008951900127}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6683775937784658
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.66056408, 3.95206707, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6623008951900127}
episode index:820
target Thresh 18.99999885339433
target distance 13.0
model initialize at round 820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 3.99999964, 10.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.000000357627872}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6684588535797406
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.30880101,  9.12877493,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9243328335958196}
episode index:821
target Thresh 18.999998876098644
target distance 12.0
model initialize at round 821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.86740312, 6.99505087, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.891737736562396}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6684527241122337
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.00430044,  2.97891291,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.02152113006050521}
episode index:822
target Thresh 18.999998898353383
target distance 12.0
model initialize at round 822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.86775028, 4.97380092, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.305877848675738}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.6684063045321758
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.98958749,  3.09375018,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.09432664450178774}
episode index:823
target Thresh 18.999998920167446
target distance 1.0
model initialize at round 823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.4225142 , 6.59672439, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7311622584449251}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6688087240655104
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.4225142 , 6.59672439, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7311622584449251}
episode index:824
target Thresh 18.999998941549563
target distance 7.0
model initialize at round 824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.13259697,  6.9950574 ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.510231707528154}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6690372892484614
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.95098687, 11.85265743,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.2772629837443938}
episode index:825
target Thresh 18.999998962508286
target distance 13.0
model initialize at round 825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 13.038404810405323}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6690304891782927
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.64057606, 10.88461914,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.37748948580187297}
episode index:826
target Thresh 18.999998983052
target distance 10.0
model initialize at round 826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 5.94478285, 10.04568069,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.111550330061313}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6692063969906527
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.88467096, 10.51952183,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.49412555048739043}
episode index:827
target Thresh 18.99999900318892
target distance 14.0
model initialize at round 827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 3.9788099 , 11.86619458,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.93714891356581}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6691994091093706
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.3159378,  2.3705229,  0.       ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.7043139313591243}
episode index:828
target Thresh 18.9999990229271
target distance 1.0
model initialize at round 828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.2217145 , 5.35805809, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.6791514760574673}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6695984448040516
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.2217145 , 5.35805809, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.6791514760574673}
episode index:829
target Thresh 18.99999904227444
target distance 13.0
model initialize at round 829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.045361017187279}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6696330699260874
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.31655234, 5.92122193, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.1470617000379912}
episode index:830
target Thresh 18.99999906123868
target distance 6.0
model initialize at round 830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.62754142, 6.99999952, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.3184366265779754}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6699132948720248
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.51593424, 10.98143339,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.48442169158255727}
episode index:831
target Thresh 18.999999079827397
target distance 1.0
model initialize at round 831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.19053581, 11.73315843,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.092132570151391}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6703100337003036
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.19053581, 11.73315843,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.092132570151391}
episode index:832
target Thresh 18.999999098048036
target distance 7.0
model initialize at round 832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.05552077, 9.00058126, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.000889474788871}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.670534601486978
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.90952317, 3.4781023 , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0486227241748125}
episode index:833
target Thresh 18.99999911590788
target distance 14.0
model initialize at round 833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 3.99505804, 11.8674027 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.954159197938505}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6706120083084863
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.92167665,  6.35017661,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6545265310184369}
episode index:834
target Thresh 18.99999913341408
target distance 10.0
model initialize at round 834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.86775028, 5.97380104, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.132287303698398}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6706452122459537
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.65572678,  6.0040192 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.3442966842526037}
episode index:835
target Thresh 18.99999915057363
target distance 13.0
model initialize at round 835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.045361017187282}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6706365701634694
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.4596688 , 5.96518787, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.1061398761744736}
episode index:836
target Thresh 18.999999167393398
target distance 1.0
model initialize at round 836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.69703674, 6.40555882, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.6671934097965189}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6710300748586147
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.69703674, 6.40555882, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.6671934097965189}
episode index:837
target Thresh 18.999999183880114
target distance 6.0
model initialize at round 837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.77775037, 9.00276601, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 4.0089313604430465}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6713062919530555
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.52117666, 5.11820483, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.49319790614298087}
episode index:838
target Thresh 18.99999920004037
target distance 9.0
model initialize at round 838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 7.97583664, 11.86719869,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 8.54567106658063}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6715280663369018
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.14375852,  7.96135325,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.2873808892043286}
episode index:839
target Thresh 18.99999921588063
target distance 13.0
model initialize at round 839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 4.9950762 , 11.86739949,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.471356167879092}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6716037375562923
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.66743819,  5.2039918 ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0387987274958708}
episode index:840
target Thresh 18.999999231407234
target distance 12.0
model initialize at round 840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.02517672, 11.8674836 ,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.151810532823845}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6716792288203454
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.30659256, 5.10799647, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.701767136514494}
episode index:841
target Thresh 18.99999924662639
target distance 5.0
model initialize at round 841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.43496108,  4.9999851 ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.0527624183869437}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6719533627528629
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.84605806,  8.57442761,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.5946975654374952}
episode index:842
target Thresh 18.99999926154419
target distance 4.0
model initialize at round 842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.86714342,  9.00421726,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.1837638493639373}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6722831926902854
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.22035539,  7.04311499,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.22453373832935736}
episode index:843
target Thresh 18.999999276166594
target distance 4.0
model initialize at round 843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.00031269,  9.99986577,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.2364076817468366}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6726122410401785
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.04386507, 11.86748884,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8685971678564883}
episode index:844
target Thresh 18.999999290499456
target distance 9.0
model initialize at round 844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([11.02325364, 11.86692437,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 8.54476708327415}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6727801629442729
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.92049303, 7.09676684, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.12524048660110718}
episode index:845
target Thresh 18.999999304548506
target distance 1.0
model initialize at round 845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.39010191,  9.39282775,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5536182262944498}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6731669476216438
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.39010191,  9.39282775,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5536182262944498}
episode index:846
target Thresh 18.999999318319368
target distance 8.0
model initialize at round 846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.77765095, 9.0012213 , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.005338972999355}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6733844305642392
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.60285821, 3.14849866, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.6208783109604187}
episode index:847
target Thresh 18.99999933181755
target distance 5.0
model initialize at round 847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.90686727, 7.00327492, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 3.1960286928461197}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6736546140187626
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.90038713, 3.57176394, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9970371645780484}
episode index:848
target Thresh 18.999999345048447
target distance 13.0
model initialize at round 848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.        ,  3.99999976,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 11.180339844849358}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.6735663717634265
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.86962434, 2.35359639, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.37686631312755065}
episode index:849
target Thresh 18.99999935801736
target distance 9.0
model initialize at round 849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.98063767, 4.        , 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 7.06835555408518}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6737321833848813
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.5432829 , 11.83239629,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9494598960594455}
episode index:850
target Thresh 18.99999937072947
target distance 12.0
model initialize at round 850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.        ,  3.99999881,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.198038793397005}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.6736440573635576
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.68142458, 2.70716517, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9820499194265242}
episode index:851
target Thresh 18.99999938318986
target distance 12.0
model initialize at round 851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([12.00166972, 11.8667314 ,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.36608853928131}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6736730400381236
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.3263446 , 2.09430551, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.1287577710924193}
episode index:852
target Thresh 18.999999395403517
target distance 5.0
model initialize at round 852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.99714243, 7.99999821, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 3.0000031490853623}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6739413014214318
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.81757248, 11.62469619,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6507880870661081}
episode index:853
target Thresh 18.99999940737533
target distance 13.0
model initialize at round 853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.00016787,  9.00007785,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 11.000167869878664}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6740129063268223
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.35728972, 9.36783608, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7405267602895615}
episode index:854
target Thresh 18.999999419110086
target distance 2.0
model initialize at round 854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.57798499, 9.10189176, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.5868974161664664}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6743941777814109
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.57798499, 9.10189176, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.5868974161664664}
episode index:855
target Thresh 18.999999430612476
target distance 12.0
model initialize at round 855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.00493935, 11.86740254,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.726397991927325}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6745102838091195
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.2177748 , 7.80939092, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.2894091964837918}
episode index:856
target Thresh 18.999999441887105
target distance 14.0
model initialize at round 856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.369316876853006}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6745380866239207
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.01677302, 9.02757524, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.03227581071516718}
episode index:857
target Thresh 18.999999452938482
target distance 12.0
model initialize at round 857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 3.97380067, 11.86774983,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.428262663873799}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6746537542822844
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.96523835,  8.47478905,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.526360060590545}
episode index:858
target Thresh 18.999999463771022
target distance 8.0
model initialize at round 858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.13224973,  4.97380118,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.337240408700561}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6747241129974679
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.81985649, 11.18142864,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.2556717350100155}
episode index:859
target Thresh 18.99999947438907
target distance 14.0
model initialize at round 859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.        ,  9.99999988,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.369316847940516}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6747943080877324
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.04346678, 6.92458919, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.08704108995747491}
episode index:860
target Thresh 18.999999484796863
target distance 2.0
model initialize at round 860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.04332066, 2.13854373, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.14515868200555607}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6751720150469802
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.04332066, 2.13854373, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.14515868200555607}
episode index:861
target Thresh 18.99999949499857
target distance 13.0
model initialize at round 861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.86740305, 5.99505719, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.528446664293774}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6751583821191868
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.52944101,  3.9224298 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.0635715899514848}
episode index:862
target Thresh 18.999999504998264
target distance 6.0
model initialize at round 862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.27743053,  6.00084937,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 4.010456751085113}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6754218138896165
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.2389017 ,  2.08833945,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.2547113709774087}
episode index:863
target Thresh 18.99999951479996
target distance 13.0
model initialize at round 863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.        , 10.99988544,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.529909229184312}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6754908764784305
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.37845578, 5.05972546, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6244071956933226}
episode index:864
target Thresh 18.999999524407563
target distance 12.0
model initialize at round 864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.00381042, 11.86720795,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 12.726711431877575}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6755172885242287
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.56159656, 3.45503773, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7825436548061}
episode index:865
target Thresh 18.999999533824923
target distance 13.0
model initialize at round 865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.00493233, 11.86740141,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.9718825777131}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6755436395722304
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.32453249, 4.33972549, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.7357198941803875}
episode index:866
target Thresh 18.999999543055807
target distance 8.0
model initialize at round 866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.97773254, 4.        , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.079141462259749}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6757533643247423
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.98646404, 9.9668441 , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.03581250974537886}
episode index:867
target Thresh 18.99999955210391
target distance 2.0
model initialize at round 867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.41841254,  6.03172374,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.41961344846992854}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6761269203566262
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.41841254,  6.03172374,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.41961344846992854}
episode index:868
target Thresh 18.999999560972846
target distance 3.0
model initialize at round 868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.52200028,  4.99773192,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1104165999767253}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6764420792514979
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.01422724,  6.66350782,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6636603351878876}
episode index:869
target Thresh 18.999999569666166
target distance 11.0
model initialize at round 869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.84885780179613}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6765539629966111
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.02486259, 11.26517532,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.26633831696143556}
episode index:870
target Thresh 18.999999578187346
target distance 12.0
model initialize at round 870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.        ,  6.99993634,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 11.180311419027923}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.676500800478503
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.75734685, 1.67910234, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.822526333079142}
episode index:871
target Thresh 18.999999586539797
target distance 6.0
model initialize at round 871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([15.35587943,  8.00015557,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.2237014083668925}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6767599738724498
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.08930817,  4.07744849,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.11821259599181898}
episode index:872
target Thresh 18.99999959472686
target distance 4.0
model initialize at round 872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.96331342, 9.06973177, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.8527862170247333}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6770729635931
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.18056453, 7.24024507, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.3005349335266354}
episode index:873
target Thresh 18.999999602751803
target distance 13.0
model initialize at round 873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([3.99999952, 6.99999905, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.401754460098363}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6770972934929863
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.06679258,  4.86989762,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.872458090011873}
episode index:874
target Thresh 18.999999610617845
target distance 10.0
model initialize at round 874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.86775028, 6.97380104, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.132287303698398}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6771635730325657
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.82831737,  6.9196164 ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8322086219493456}
episode index:875
target Thresh 18.99999961832813
target distance 13.0
model initialize at round 875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.86740304, 5.99505756, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.309950028859305}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6771478845145936
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.71921368,  3.96316717,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7201562118083477}
episode index:876
target Thresh 18.99999962588574
target distance 11.0
model initialize at round 876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([11.02619474, 11.86774924,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 9.470806575533956}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6772580704358997
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.48720432, 9.57333798, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7692046838250085}
episode index:877
target Thresh 18.999999633293697
target distance 3.0
model initialize at round 877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.12338912, 9.02756643, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0349481389266493}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6775687104467928
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.98825122, 7.36080158, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6393063874021967}
episode index:878
target Thresh 18.999999640554968
target distance 12.0
model initialize at round 878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.13259679,  7.99505725,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.327137610204655}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6776341520624676
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.91417439, 5.94452738, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.10219220365959229}
episode index:879
target Thresh 18.999999647672457
target distance 4.0
model initialize at round 879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.81672478,  5.00845253,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.01679731042295}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6779436587078512
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.55991512,  3.08074379,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5657070791058014}
episode index:880
target Thresh 18.99999965464901
target distance 11.0
model initialize at round 880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.99983177, 9.00007773, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.000168225339678}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.678098667324528
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.05214139,  9.1442821 ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.9587769670050756}
episode index:881
target Thresh 18.999999661487415
target distance 9.0
model initialize at round 881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.02619355, 11.86774891,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.079575123489372}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6782533244477428
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.83964658, 11.00839504,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.16057302791643369}
episode index:882
target Thresh 18.999999668190416
target distance 9.0
model initialize at round 882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.86740304, 5.99505757, 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.713402783926012}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6783615097399877
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.0193482 , 11.81584097,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8160703684101988}
episode index:883
target Thresh 18.999999674760684
target distance 6.0
model initialize at round 883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.01064885,  6.99999881,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 4.12570244054655}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6786150600683362
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.67529108, 11.10460601,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.3411426350495829}
episode index:884
target Thresh 18.999999681200855
target distance 11.0
model initialize at round 884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 10.816653826391995}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6786788756960838
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.05494855, 11.38908663,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.3929475114819871}
episode index:885
target Thresh 18.9999996875135
target distance 5.0
model initialize at round 885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.13225441,  7.97381722,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.3553185813307}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6789314954752078
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.95848841, 11.85905093,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8600533207701448}
episode index:886
target Thresh 18.99999969370115
target distance 3.0
model initialize at round 886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.14203698, 9.96631763, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.3433539301958144}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6792370969459235
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.15341758, 11.69209066,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.708890989084834}
episode index:887
target Thresh 18.999999699766274
target distance 8.0
model initialize at round 887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.86775027, 6.97380117, 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.190193109975363}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6793894270732367
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.03918249, 10.52611484,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0713250832694876}
episode index:888
target Thresh 18.999999705711296
target distance 1.0
model initialize at round 888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.90691887,  3.37385988,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6330209664620383}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6797500688875525
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.90691887,  3.37385988,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6330209664620383}
episode index:889
target Thresh 18.999999711538603
target distance 1.0
model initialize at round 889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.50868905,  7.32388616,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6030479147849441}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.680109900270825
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.50868905,  7.32388616,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6030479147849441}
episode index:890
target Thresh 18.99999971725052
target distance 13.0
model initialize at round 890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.132597896098035}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6801716084530406
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.02682106, 8.48736015, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.4880976137345308}
episode index:891
target Thresh 18.99999972284934
target distance 5.0
model initialize at round 891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.15098906, 8.00505543, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.0088462602095003}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.680420855528766
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.90084764, 4.40983566, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.076949587440847}
episode index:892
target Thresh 18.99999972833729
target distance 10.0
model initialize at round 892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 5.99999726, 10.99998093,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 8.246209285207607}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6805710071463149
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.04634079,  8.33266179,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6689452592588928}
episode index:893
target Thresh 18.99999973371657
target distance 7.0
model initialize at round 893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.1680584 ,  4.99999917,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 5.002824399167997}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6807687744761289
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.44957858, 10.34002824,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6469798578543832}
episode index:894
target Thresh 18.999999738989338
target distance 8.0
model initialize at round 894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.27800936,  4.99999973,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.00643758078722}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6809660998677758
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.66354432, 10.96883258,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.33789618320275533}
episode index:895
target Thresh 18.999999744157694
target distance 10.0
model initialize at round 895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 3.99505254, 11.86740308,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.051805512077975}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6811151402139055
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.87485708, 11.86189054,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8709282754431865}
episode index:896
target Thresh 18.99999974922371
target distance 8.0
model initialize at round 896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.30422608, 9.00037317, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 6.04057774429349}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6813116395001777
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.7100857 , 3.03512847, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.29203477533129935}
episode index:897
target Thresh 18.999999754189414
target distance 13.0
model initialize at round 897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.02619384, 11.86774901,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 12.490293390264256}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.681371528421252
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.90066274, 6.12063644, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.156272335691822}
episode index:898
target Thresh 18.999999759056788
target distance 11.0
model initialize at round 898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86740359, 7.99502333, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.968175791006983}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6813904002429123
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.98539697,  4.01244307,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.01918536747994587}
episode index:899
target Thresh 18.999999763827784
target distance 1.0
model initialize at round 899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.43432403,  6.40553546,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.5942191250551223}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.681744410909309
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.43432403,  6.40553546,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.5942191250551223}
episode index:900
target Thresh 18.999999768504306
target distance 9.0
model initialize at round 900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 9.02239443, 10.13336009,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 10.009937548436858}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6818465602174008
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.79507234, 2.9949088 , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.2049908956408822}
episode index:901
target Thresh 18.99999977308823
target distance 4.0
model initialize at round 901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.04549885,  4.008237  ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 2.0087523517163084}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6821438478446542
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.95065759,  2.11079514,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.1212857634288301}
episode index:902
target Thresh 18.999999777581383
target distance 2.0
model initialize at round 902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.35782667,  7.98512232,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.6423456494473648}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6824958480131541
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.35782667,  7.98512232,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.6423456494473648}
episode index:903
target Thresh 18.99999978198557
target distance 8.0
model initialize at round 903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 9.0256353 , 11.86760454,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 6.673187882148657}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6826892984025201
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.08438153, 9.00509993, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.08453550934884907}
episode index:904
target Thresh 18.999999786302542
target distance 10.0
model initialize at round 904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([3.99999905, 4.        , 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 10.630146530447755}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6827472018193405
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.98145062, 11.86159979,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.861799441328148}
episode index:905
target Thresh 18.999999790534037
target distance 3.0
model initialize at round 905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.16906101,  8.99836348,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.3014358708493239}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.683042182832785
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.52687874, 10.7338331 ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.9033892985658757}
episode index:906
target Thresh 18.999999794681738
target distance 6.0
model initialize at round 906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 6.01884991, 11.8648948 ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.430461281962955}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6832841429399154
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.12967211, 10.3958702 ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.4165670114465678}
episode index:907
target Thresh 18.999999798747314
target distance 3.0
model initialize at round 907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([10.99664537, 11.85033832,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.3152170078018097}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6835778828705983
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.9045801 , 10.31251208,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.136179908179911}
episode index:908
target Thresh 18.999999802732386
target distance 12.0
model initialize at round 908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.329052001496866}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6835941198488417
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.15332022,  6.96339085,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.15763032754581052}
episode index:909
target Thresh 18.999999806638545
target distance 8.0
model initialize at round 909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 4.99505002, 11.86740299,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.067273871908878}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6837850878490078
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.93667573, 10.52119341,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.4829758952609602}
episode index:910
target Thresh 18.999999810467358
target distance 9.0
model initialize at round 910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([3.99999833, 8.        , 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.615774639852697}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6839285797942888
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.54247682, 11.76610523,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9387216427051506}
episode index:911
target Thresh 18.999999814220356
target distance 1.0
model initialize at round 911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.23858142,  9.42293668,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.4855888456591181}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.684275149333988
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.23858142,  9.42293668,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.4855888456591181}
episode index:912
target Thresh 18.999999817899038
target distance 4.0
model initialize at round 912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.23795199,  4.01017666,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.0242112909287595}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6845661951726146
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.36161892,  2.11639251,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6489049351327961}
episode index:913
target Thresh 18.99999982150488
target distance 14.0
model initialize at round 913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.02617653, 11.86774463,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 14.371163050886501}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6845812620226378
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.89078592, 4.10231277, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.14965166012475664}
episode index:914
target Thresh 18.999999825039318
target distance 1.0
model initialize at round 914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.73466563,  8.14472457,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.1274882056117963}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6849259819548534
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.73466563,  8.14472457,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.1274882056117963}
episode index:915
target Thresh 18.999999828503775
target distance 11.0
model initialize at round 915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.13223295,  7.97301477,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 11.761762196389547}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6849025042794541
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.42944855, 2.23714074, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.49057292029300276}
episode index:916
target Thresh 18.999999831899625
target distance 11.0
model initialize at round 916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4.99983178, 9.00007785, 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 9.055543737396352}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6849994273254961
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.51041139,  9.60821718,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6434388544999824}
episode index:917
target Thresh 18.999999835228238
target distance 14.0
model initialize at round 917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.        ,  5.99999988,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.369316847940494}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.684975920793866
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.82128978, 3.98670101, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0027543208205933}
episode index:918
target Thresh 18.999999838490936
target distance 11.0
model initialize at round 918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 9.972155793073503}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6850304539492862
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.92088681, 9.27333392, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9605956232559597}
episode index:919
target Thresh 18.999999841689032
target distance 9.0
model initialize at round 919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.1325968 ,  7.99505764,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.739742610998298}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6851711885102109
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.0280341 , 10.94234793,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.06410672843640113}
episode index:920
target Thresh 18.999999844823797
target distance 8.0
model initialize at round 920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 8.02719078, 10.13223665,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 8.190669401191279}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6853116174586255
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.84014837, 3.98761206, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.2966213257301127}
episode index:921
target Thresh 18.99999984789649
target distance 3.0
model initialize at round 921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.00751146, 11.86669791,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.329001358083012}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6855986981338331
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.66845711, 10.27010151,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8016685732696844}
episode index:922
target Thresh 18.999999850908342
target distance 1.0
model initialize at round 922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.5338887,  6.9737224,  0.       ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5345349876099703}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6859393279300044
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.5338887,  6.9737224,  0.       ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5345349876099703}
episode index:923
target Thresh 18.999999853860555
target distance 2.0
model initialize at round 923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.91007495,  4.03521323,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.09657373587314762}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6862792204322447
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.91007495,  4.03521323,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.09657373587314762}
episode index:924
target Thresh 18.99999985675431
target distance 2.0
model initialize at round 924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.97766042, 11.10505141,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.10740044857693962}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6866183780317774
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.97766042, 11.10505141,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.10740044857693962}
episode index:925
target Thresh 18.999999859590766
target distance 3.0
model initialize at round 925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.97971439,  5.00936091,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0095647336235145}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6869028074291513
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.31263176,  3.21994197,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8403744042498084}
episode index:926
target Thresh 18.999999862371055
target distance 4.0
model initialize at round 926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.06176531, 6.00833273, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 2.0092822862019486}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6871866231708675
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.6785695, 4.0822314, 0.       ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.33178241763078686}
episode index:927
target Thresh 18.999999865096292
target distance 11.0
model initialize at round 927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.13224971,  4.97380067,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.178937666935063}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6871610130503052
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.3037152 , 4.97195811, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.1956233050514538}
episode index:928
target Thresh 18.999999867767563
target distance 6.0
model initialize at round 928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.86739694, 6.99509494, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 4.007099715277754}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6873928095916936
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.53284171, 11.06001507,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.5362108723309822}
episode index:929
target Thresh 18.999999870385942
target distance 1.0
model initialize at round 929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.8117671 , 5.60972941, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.4332929242291694}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6877289463555735
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.8117671 , 5.60972941, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.4332929242291694}
episode index:930
target Thresh 18.999999872952472
target distance 4.0
model initialize at round 930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.8325386 ,  5.00461888,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 2.0116013982687506}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6880106553283387
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.04255477,  3.07047486,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.08232626814355169}
episode index:931
target Thresh 18.99999987546818
target distance 8.0
model initialize at round 931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.00611055,  4.99999988,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 6.326490414535754}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.688192376728201
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.40091689, 11.09521222,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6066019604931339}
episode index:932
target Thresh 18.999999877934076
target distance 6.0
model initialize at round 932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.04546625,  9.05539487,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 4.166216774221019}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6884220740736157
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.06127887,  5.08175053,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.10216774796516896}
episode index:933
target Thresh 18.999999880351144
target distance 1.0
model initialize at round 933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.31416601, 11.82876746,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0757433549241584}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6887556692833869
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.31416601, 11.82876746,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0757433549241584}
episode index:934
target Thresh 18.99999988272035
target distance 12.0
model initialize at round 934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.        ,  4.99999678,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.440305584038514}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.6886593925667613
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.83398094, 1.60944115, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9209019659474266}
episode index:935
target Thresh 18.99999988504264
target distance 12.0
model initialize at round 935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.32905200177065}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6887089999364816
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.81247552, 8.03990456, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8134548781593148}
episode index:936
target Thresh 18.99999988731895
target distance 5.0
model initialize at round 936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.2682085 ,  9.00132227,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.013282457139389}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6889371653581076
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.99899098,  5.15142083,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8485797682138696}
episode index:937
target Thresh 18.999999889550185
target distance 13.0
model initialize at round 937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 4.97380045, 11.86775026,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.393027171305501}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6889471868194462
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.82646408,  9.47406909,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.50483286464817}
episode index:938
target Thresh 18.999999891737236
target distance 12.0
model initialize at round 938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.00016787,  9.00007642,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.440489263150923}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6889963292090155
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.81708083, 5.29147814, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7317531314524631}
episode index:939
target Thresh 18.999999893880982
target distance 10.0
model initialize at round 939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 5.9963952, 11.8671665,  0.       ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.923776128970774}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6890865256008144
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.62613903,  6.00841532,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.3739556705077927}
episode index:940
target Thresh 18.99999989598228
target distance 7.0
model initialize at round 940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.13224975,  6.97380113,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.335854705047768}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.689219809048635
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.07588831, 10.925551  ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.10630939782793314}
episode index:941
target Thresh 18.99999989804197
target distance 13.0
model initialize at round 941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.132598050286157}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6892685055258922
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.24028169,  6.97400665,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7601628568213394}
episode index:942
target Thresh 18.999999900060875
target distance 12.0
model initialize at round 942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.770329614269032}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6893170987232402
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.90391777, 9.2302838 , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9327904197458458}
episode index:943
target Thresh 18.9999999020398
target distance 2.0
model initialize at round 943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.40889889,  8.96356237,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.5922231193976436}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6896462119661181
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.40889889,  8.96356237,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.5922231193976436}
episode index:944
target Thresh 18.999999903979543
target distance 1.0
model initialize at round 944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.6354562 , 3.96439014, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.6364531796260009}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6899746286730323
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.6354562 , 3.96439014, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.6364531796260009}
episode index:945
target Thresh 18.999999905880873
target distance 1.0
model initialize at round 945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.37353826,  5.21105635,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8729048680804233}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6903023510528705
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.37353826,  5.21105635,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8729048680804233}
episode index:946
target Thresh 18.99999990774456
target distance 13.0
model initialize at round 946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.02619884, 11.86775028,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.060291649189557}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6903496472931789
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.73432535, 11.26738186,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.3769298061473942}
episode index:947
target Thresh 18.99999990957134
target distance 6.0
model initialize at round 947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([3.99993908, 7.        , 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.472163197771165}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6905734345850638
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.89817861, 11.0692674 ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9008455963997465}
episode index:948
target Thresh 18.999999911361947
target distance 1.0
model initialize at round 948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.86755252,  3.67736408,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.3487639192924897}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6908994899753851
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.86755252,  3.67736408,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.3487639192924897}
episode index:949
target Thresh 18.999999913117097
target distance 13.0
model initialize at round 949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.99999928, 4.        , 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.180340591217536}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.690802476764083
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.94649977,  2.26304782,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.26843328711704234}
episode index:950
target Thresh 18.999999914837495
target distance 5.0
model initialize at round 950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.903476  , 5.00361621, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 3.1975107885255514}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6910250819409873
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.1343184 , 1.53498244, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.48402764391821274}
episode index:951
target Thresh 18.999999916523826
target distance 10.0
model initialize at round 951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.669994727410566}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.691154789050293
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.94864669, 11.3732292 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0194265915175282}
episode index:952
target Thresh 18.999999918176762
target distance 10.0
model initialize at round 952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.13224971,  4.97380043,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.1322872913468}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6911256868910473
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.68529962, 5.032672  , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.3163918305002373}
episode index:953
target Thresh 18.999999919796974
target distance 5.0
model initialize at round 953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([5.88909901, 9.85421866, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.432931680797584}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6913970436133837
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.87562407, 8.16702554, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8914118227937228}
episode index:954
target Thresh 18.9999999213851
target distance 8.0
model initialize at round 954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.86774905, 4.97380601, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.392300688277613}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6915708425205949
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.01314026, 11.1118354 ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.11260472119083817}
episode index:955
target Thresh 18.99999992294178
target distance 13.0
model initialize at round 955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.401754250991404}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6916163666294907
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.97514539, 9.05673393, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9767943812789971}
episode index:956
target Thresh 18.999999924467634
target distance 13.0
model initialize at round 956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.        ,  6.99999988,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.180339866174156}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6916233895442914
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.72183088, 4.94850803, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.2828948203182985}
episode index:957
target Thresh 18.999999925963273
target distance 11.0
model initialize at round 957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.929943319783787}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6916303977974745
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.18011651, 10.05195429,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.18745987443016196}
episode index:958
target Thresh 18.9999999274293
target distance 2.0
model initialize at round 958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.87139618,  6.46253538,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.4800811602930528}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6919519510844427
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.87139618,  6.46253538,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.4800811602930528}
episode index:959
target Thresh 18.999999928866295
target distance 10.0
model initialize at round 959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([10.02202582, 11.86651291,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 9.93825296841603}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6920796118124798
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.98644185, 6.18734486, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.18783482006907293}
episode index:960
target Thresh 18.99999993027484
target distance 1.0
model initialize at round 960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.9685623 , 8.63174438, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6325261230585543}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6924000284495115
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.9685623 , 8.63174438, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6325261230585543}
episode index:961
target Thresh 18.999999931655488
target distance 10.0
model initialize at round 961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 5.9950517 , 11.86740304,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.051806343195333}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6925269579937429
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.92333721, 11.43090769,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.43767410532099194}
episode index:962
target Thresh 18.9999999330088
target distance 5.0
model initialize at round 962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 9.97717086, 11.86100608,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.1430602120835487}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6927449985358055
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.68701572, 11.87002257,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.1085710956723855}
episode index:963
target Thresh 18.999999934335317
target distance 13.0
model initialize at round 963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.1325965 ,  7.99503113,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.644172688927748}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6927145788602384
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.17205483, 2.0536401 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8296809457253944}
episode index:964
target Thresh 18.99999993563556
target distance 1.0
model initialize at round 964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.33433771, 6.2644763 , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8079460498897549}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.693033009348466
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.33433771, 6.2644763 , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8079460498897549}
episode index:965
target Thresh 18.999999936910065
target distance 12.0
model initialize at round 965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.8677503 , 5.97379994, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.820239819532313}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.6929680159741142
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.16501453,  2.07910526,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.18299573423841822}
episode index:966
target Thresh 18.999999938159327
target distance 3.0
model initialize at round 966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.71162742, 4.99913275, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.0415824451318119}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.693233819473624
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.14028794, 6.634637  , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.6499575558196052}
episode index:967
target Thresh 18.999999939383855
target distance 13.0
model initialize at round 967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 4.97380071, 11.86775013,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.684886040146262}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6933170293062958
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.05774614,  7.57957386,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0317947803414007}
episode index:968
target Thresh 18.999999940584136
target distance 5.0
model initialize at round 968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 7.97475712, 11.86748566,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.147161550052149}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6935329044050509
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.57796083, 11.90243246,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0716450292223019}
episode index:969
target Thresh 18.99999994176065
target distance 13.0
model initialize at round 969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.531018778398135}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6935018606183334
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.3828204 , 8.49160785, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7890430512487681}
episode index:970
target Thresh 18.999999942913867
target distance 6.0
model initialize at round 970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.86774727, 6.97381295, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.182365165993882}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6937171007206833
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.86754901, 11.07183717,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8705181559496877}
episode index:971
target Thresh 18.999999944044248
target distance 1.0
model initialize at round 971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.63765073,  5.08416104,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.1159567402276434}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6940322065841393
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.63765073,  5.08416104,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.1159567402276434}
episode index:972
target Thresh 18.999999945152243
target distance 12.0
model initialize at round 972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.895369891272122}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6940366311365644
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.06343141, 9.14809085, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.16110382454748837}
episode index:973
target Thresh 18.9999999462383
target distance 3.0
model initialize at round 973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.78376961, 4.03377354, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0561455002152078}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6942994272031594
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.88536752, 2.32768272, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.1117041720088483}
episode index:974
target Thresh 18.999999947302854
target distance 1.0
model initialize at round 974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.6835565 , 4.62206888, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6979299280500539}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6946129662521818
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.6835565 , 4.62206888, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6979299280500539}
episode index:975
target Thresh 18.99999994834633
target distance 12.0
model initialize at round 975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([3.99999976, 8.        , 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.049875858356245}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.694694080976821
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.99230705,  8.48147996,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.5185771027816255}
episode index:976
target Thresh 18.99999994936914
target distance 2.0
model initialize at round 976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.10450709,  6.97264647,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.1080275285248206}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6950065742409184
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.10450709,  6.97264647,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.1080275285248206}
episode index:977
target Thresh 18.9999999503717
target distance 5.0
model initialize at round 977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.31235106, 7.00404313, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 3.0817423981682768}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6952187352079522
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.93767142, 3.08324004, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9188763141940212}
episode index:978
target Thresh 18.999999951354404
target distance 8.0
model initialize at round 978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.02548595, 10.13248272,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.33695832241551}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6952989826055948
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.2833352 , 3.04993436, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.28770171430306823}
episode index:979
target Thresh 18.99999995231765
target distance 14.0
model initialize at round 979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.99999952, 8.99999285, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.99999768917557}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6953020829254806
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.67124921,  3.9930172 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6712855248319122}
episode index:980
target Thresh 18.999999953261824
target distance 14.0
model initialize at round 980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.369316876853006}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6953051769246328
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.94525305, 8.37799438, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0180290205741003}
episode index:981
target Thresh 18.999999954187302
target distance 11.0
model initialize at round 981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([11.00000238, 10.89804654,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.656037101396468}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6953456929263643
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.5960113 , 2.98096037, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.0608911868581927}
episode index:982
target Thresh 18.999999955094456
target distance 7.0
model initialize at round 982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.13263969,  5.99531758,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.440128500777225}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.695556429759603
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.94594351, 10.15945273,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.265436224587776}
episode index:983
target Thresh 18.999999955983647
target distance 12.0
model initialize at round 983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 11.661903789690628}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6955592558432759
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.14124053, 9.9426042 , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.15245709561170342}
episode index:984
target Thresh 18.999999956855227
target distance 1.0
model initialize at round 984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.16337442, 11.54509568,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5690522857658762}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.695868332740897
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.16337442, 11.54509568,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5690522857658762}
episode index:985
target Thresh 18.99999995770955
target distance 1.0
model initialize at round 985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.2256633 , 7.59118623, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.46696105748949224}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6961767827076912
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.2256633 , 7.59118623, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.46696105748949224}
episode index:986
target Thresh 18.999999958546958
target distance 12.0
model initialize at round 986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 5.9999839 , 11.63882513,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.88910622627826}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6961789716776873
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.22200073,  2.08411808,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.23740298088852296}
episode index:987
target Thresh 18.999999959367784
target distance 5.0
model initialize at round 987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.99914706,  9.00068295,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.163195343159243}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6963877986294305
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.99878271,  5.52118522,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.47881633172298954}
episode index:988
target Thresh 18.999999960172353
target distance 3.0
model initialize at round 988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 3.03116889, 11.86295422,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.3446186317518467}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6966442315934048
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.78382254, 11.08580286,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.2325829478118227}
episode index:989
target Thresh 18.999999960960995
target distance 2.0
model initialize at round 989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.73929912, 2.10286417, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7464209448228406}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6969506515614923
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.73929912, 2.10286417, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7464209448228406}
episode index:990
target Thresh 18.99999996173402
target distance 11.0
model initialize at round 990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.86740305, 6.99505743, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.186646090478567}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6969891391892052
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.88214833,  6.88751808,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.2513488808348159}
episode index:991
target Thresh 18.99999996249174
target distance 11.0
model initialize at round 991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.929943319783787}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6969904982183429
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.15561172, 10.25194085,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.29612361829966155}
episode index:992
target Thresh 18.99999996323445
target distance 11.0
model initialize at round 992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.00016787,  9.00007785,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.055543384442366}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.6970678299799558
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.53318935, 10.02787238,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.46764201118739823}
episode index:993
target Thresh 18.999999963962455
target distance 14.0
model initialize at round 993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 3.97380536, 11.86774896,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 13.3813241310715}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6971060835620936
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.85912934,  6.1186059 ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.18415184714669056}
episode index:994
target Thresh 18.999999964676046
target distance 11.0
model initialize at round 994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.99999702, 9.99879634, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.81598868428668}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6971442602526091
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.85662117,  4.99688223,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0071403445705862}
episode index:995
target Thresh 18.99999996537551
target distance 6.0
model initialize at round 995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.51426667, 3.99999881, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 4.032924465491578}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6973504407142028
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.16684759, 7.98127981, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8333626915561246}
episode index:996
target Thresh 18.99999996606112
target distance 13.0
model initialize at round 996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        ,  6.99999845,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.704699381113421}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6973164086084606
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.33689786, 2.94841998, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.34082351741157346}
episode index:997
target Thresh 18.999999966733156
target distance 13.0
model initialize at round 997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 3.97380024, 11.86775028,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.183271979301518}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.6973542597928458
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.26558212,  9.56704057,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5079249248758345}
episode index:998
target Thresh 18.999999967391883
target distance 7.0
model initialize at round 998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.13224981,  5.9738015 ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.928881325641118}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.6974715290523125
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.98642889, 11.30387702,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0321740166714959}
episode index:999
target Thresh 18.999999968037567
target distance 1.0
model initialize at round 999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.27547184, 7.49622691, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5741707603720324}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6977740575232602
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.27547184, 7.49622691, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5741707603720324}
episode index:1000
target Thresh 18.999999968670465
target distance 6.0
model initialize at round 1000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.71254635, 8.00285828, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 4.013166333529479}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6979785789443158
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.86092282, 4.09546514, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.8661995669206091}
episode index:1001
target Thresh 18.999999969290833
target distance 11.0
model initialize at round 1001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.86775027, 4.97380117, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.574514830027818}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.697978936945463
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.1137756 ,  7.31518347,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6942035451950322}
episode index:1002
target Thresh 18.999999969898912
target distance 12.0
model initialize at round 1002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.        , 10.99987686,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.661840433516996}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6979792942327494
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.87746972, 4.2526141 , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.1526225740077987}
episode index:1003
target Thresh 18.999999970494954
target distance 12.0
model initialize at round 1003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.00000000000002}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.698054793877438
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.17581858, 8.1174403 , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.21143414284503037}
episode index:1004
target Thresh 18.999999971079195
target distance 5.0
model initialize at round 1004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.00010467,  9.9999311 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.1623987440199204}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6982582219432315
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.90776355, 10.40789124,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5992498230755384}
episode index:1005
target Thresh 18.999999971651864
target distance 12.0
model initialize at round 1005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([3.99999952, 7.99999535, 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.440305629711787}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.6982583005457669
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.07300438,  4.24142626,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.7620785731271231}
episode index:1006
target Thresh 18.999999972213196
target distance 2.0
model initialize at round 1006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.67280865,  9.95446038,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.33034533164792707}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6985579447358903
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.67280865,  9.95446038,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.33034533164792707}
episode index:1007
target Thresh 18.999999972763412
target distance 6.0
model initialize at round 1007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.42711481, 7.00135839, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 4.042161106858758}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.6987602682034142
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.77730069, 3.02757265, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.22439972033709985}
episode index:1008
target Thresh 18.99999997330273
target distance 2.0
model initialize at round 1008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.19507253,  5.95466268,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.200271728289225}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.699058820960398
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.19507253,  5.95466268,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.200271728289225}
episode index:1009
target Thresh 18.999999973831372
target distance 4.0
model initialize at round 1009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.38378325,  4.00374937,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.040171836847026}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6993072775733085
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.14233163,  2.06979072,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.15852141062125913}
episode index:1010
target Thresh 18.999999974349546
target distance 4.0
model initialize at round 1010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.29187161,  4.00342369,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 2.0245729228635176}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6995552426795664
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.99522054,  2.0819068 ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.08204612342192681}
episode index:1011
target Thresh 18.99999997485746
target distance 12.0
model initialize at round 1011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([3.9999994 , 6.99998808, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.77032574036828}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.6995195363442003
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.659818  ,  3.02978592,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.3414835230138767}
episode index:1012
target Thresh 18.999999975355315
target distance 3.0
model initialize at round 1012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.33908623,  9.00389576,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.059616048662089}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.6997668023497835
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.11640093,  7.14748359,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8604262954257222}
episode index:1013
target Thresh 18.999999975843313
target distance 7.0
model initialize at round 1013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([12.14607695,  9.82339827,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 6.983171938917471}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.6999222344973676
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.58463386,  3.92980808,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.5888324465793277}
episode index:1014
target Thresh 18.999999976321646
target distance 9.0
model initialize at round 1014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.7077201,  4.       ,  0.       ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.205297214060547}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7000773603747101
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.24648984, 10.07062054,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.1964630093828745}
episode index:1015
target Thresh 18.99999997679051
target distance 3.0
model initialize at round 1015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.5839439 ,  7.99806845,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0848822528712678}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7003233472247351
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.00922504,  9.5077939 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5078776916302769}
episode index:1016
target Thresh 18.999999977250088
target distance 7.0
model initialize at round 1016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.13225056,  5.97380435,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.183497652262614}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7004356214654187
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.11231921, 10.79374839,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.23485172346588637}
episode index:1017
target Thresh 18.999999977700565
target distance 5.0
model initialize at round 1017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([12.14532404,  9.82409015,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 4.015552264607979}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.700680773114274
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.2024698 ,  7.90452278,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.2059087350429183}
episode index:1018
target Thresh 18.999999978142124
target distance 13.0
model initialize at round 1018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([3.99999988, 7.        , 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.401754366000183}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7007145426113404
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.98594856,  9.82218073,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.17837358054018282}
episode index:1019
target Thresh 18.99999997857494
target distance 1.0
model initialize at round 1019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.87582895,  5.12581801,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.882956733739023}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7010079597264274
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.87582895,  5.12581801,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.882956733739023}
episode index:1020
target Thresh 18.999999978999185
target distance 11.0
model initialize at round 1020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.414114271867186}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7010053439931926
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.2139248 , 8.70732997, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.36251836408308186}
episode index:1021
target Thresh 18.99999997941503
target distance 11.0
model initialize at round 1021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.86775027, 4.97380117, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.132283596994652}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7009685681490594
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.25123763,  4.95754263,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.2547998766903113}
episode index:1022
target Thresh 18.99999997982264
target distance 11.0
model initialize at round 1022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 6.97381382, 11.86774676,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 10.255096079425957}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7010795531753067
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.12216929,  7.07238765,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8808102635496928}
episode index:1023
target Thresh 18.999999980222174
target distance 6.0
model initialize at round 1023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.83475614,  7.00024414,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.003655669411922}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.701276252830409
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.82521459,  3.09597993,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.19940432469012423}
episode index:1024
target Thresh 18.999999980613804
target distance 8.0
model initialize at round 1024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.1325999 ,  4.99507571,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.289547150387428}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7013867211203305
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.06983709, 11.66605394,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.6697052100754106}
episode index:1025
target Thresh 18.999999980997675
target distance 13.0
model initialize at round 1025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([14.00493173, 11.86740131,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 13.527916532057064}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7013837489711818
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.99926838, 4.06349907, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.06350328107671534}
episode index:1026
target Thresh 18.99999998137395
target distance 2.0
model initialize at round 1026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.77089167,  7.61692637,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.44635863713962926}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.701674514551541
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.77089167,  7.61692637,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.44635863713962926}
episode index:1027
target Thresh 18.999999981742768
target distance 9.0
model initialize at round 1027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 8.97774389, 10.13885236,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 8.151207255067577}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.701825974167736
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.13537905,  6.97477874,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.3029823418087294}
episode index:1028
target Thresh 18.999999982104285
target distance 13.0
model initialize at round 1028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.99999976, 5.        , 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.000000238418579}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7017886509968141
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.41099371,  4.92167601,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.4183903427079718}
episode index:1029
target Thresh 18.999999982458643
target distance 7.0
model initialize at round 1029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 9.01869036, 11.86292047,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 6.333198834610825}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7019835163841959
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.85499778, 8.99461358, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.3115934508376403}
episode index:1030
target Thresh 18.999999982805985
target distance 13.0
model initialize at round 1030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.99999964, 7.99999893, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.401754313723512}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7019799797980751
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.52329713,  4.19179783,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9628243012890677}
episode index:1031
target Thresh 18.999999983146452
target distance 11.0
model initialize at round 1031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 4.97380065, 11.86775009,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.470811231177098}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7020890168816042
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.06140774,  9.13392865,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.9480993191202195}
episode index:1032
target Thresh 18.999999983480173
target distance 12.0
model initialize at round 1032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 11.315143201383691}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7021209654525077
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.98640721, 9.21607078, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0097949093501744}
episode index:1033
target Thresh 18.999999983807285
target distance 2.0
model initialize at round 1033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.14866877, 7.9969424 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.0079665434760885}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7024090496251842
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.14866877, 7.9969424 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.0079665434760885}
episode index:1034
target Thresh 18.999999984127925
target distance 6.0
model initialize at round 1034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 4.97381817, 11.8677428 ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.118630560003652}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7026023742149183
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.91220654, 10.7792765 ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.23754274092203656}
episode index:1035
target Thresh 18.999999984442212
target distance 3.0
model initialize at round 1035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 7.042815  , 11.85947452,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.3513547146928588}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7028411750120083
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.64455203, 10.85144115,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.3852440701480325}
episode index:1036
target Thresh 18.99999998475028
target distance 10.0
model initialize at round 1036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([3.99999881, 9.        , 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.246212407735307}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7029488558943496
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.93863415, 11.86018222,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8623683753371015}
episode index:1037
target Thresh 18.999999985052245
target distance 1.0
model initialize at round 1037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.29688281,  2.20586982,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.36127799008196787}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7032350323337577
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.29688281,  2.20586982,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.36127799008196787}
episode index:1038
target Thresh 18.99999998534823
target distance 14.0
model initialize at round 1038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.02619884, 11.86775028,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.170371793518289}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7032656934100727
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.25781727, 10.51829399,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5788768489871498}
episode index:1039
target Thresh 18.999999985638354
target distance 2.0
model initialize at round 1039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.8804183, 2.1483264, 0.       ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.1905269084785208}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7035510148587168
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.8804183, 2.1483264, 0.       ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.1905269084785208}
episode index:1040
target Thresh 18.999999985922734
target distance 8.0
model initialize at round 1040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.63911867, 9.00023305, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.152625024713696}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7036987804544338
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.8639325 , 3.13181407, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.873930377649734}
episode index:1041
target Thresh 18.99999998620148
target distance 2.0
model initialize at round 1041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.13409799, 7.03657806, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.13899721666808515}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7039831386305812
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.13409799, 7.03657806, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.13899721666808515}
episode index:1042
target Thresh 18.99999998647471
target distance 5.0
model initialize at round 1042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.96737242, 6.00331986, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1758856879205424}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7041734711918175
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.74591655, 2.33274019, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.000813244253005}
episode index:1043
target Thresh 18.99999998674253
target distance 4.0
model initialize at round 1043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.99374521, 6.99999332, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 2.000016456270041}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7044089372155802
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.17042416, 8.9615519 , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.17470733094098467}
episode index:1044
target Thresh 18.999999987005044
target distance 8.0
model initialize at round 1044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.09575355, 9.00013268, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.067887096696521}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7045553162230294
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.24140298, 3.07693148, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.25336505799610637}
episode index:1045
target Thresh 18.99999998726236
target distance 12.0
model initialize at round 1045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.99983189, 9.00007785, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.05003514728326}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7046214975053209
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.85427452, 10.03569721,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.15003402209996614}
episode index:1046
target Thresh 18.999999987514585
target distance 10.0
model initialize at round 1046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([12.00000036, 10.99992025,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 8.944236564418933}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7046875523668249
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.31833162, 6.28930175, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.7787342498604222}
episode index:1047
target Thresh 18.99999998776181
target distance 12.0
model initialize at round 1047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.0199455 , 11.86562617,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 14.061645976793274}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7046481753428957
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.33653259, 2.48477652, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.5901376633212772}
episode index:1048
target Thresh 18.999999988004145
target distance 13.0
model initialize at round 1048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 3.99505679, 11.86740306,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.039074378030818}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7046771969971207
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.33384019, 10.84057048,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.3699554609061205}
episode index:1049
target Thresh 18.999999988241676
target distance 4.0
model initialize at round 1049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([5.9053308 , 9.86728293, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 2.0934338145693436}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7049108377618855
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.15193493, 8.54430237, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.4803587719627685}
episode index:1050
target Thresh 18.999999988474507
target distance 1.0
model initialize at round 1050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.59832478, 8.53168082, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.6169812298227608}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7051916076593527
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.59832478, 8.53168082, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.6169812298227608}
episode index:1051
target Thresh 18.99999998870273
target distance 7.0
model initialize at round 1051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([12.14607731,  9.82339799,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 6.983171502064952}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.705336268678688
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.81151075,  3.93333925,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8142440405133851}
episode index:1052
target Thresh 18.99999998892643
target distance 4.0
model initialize at round 1052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.92577362,  5.00272739,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.0041024303645956}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7055686179012154
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.69205458,  3.07476902,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.316892387437636}
episode index:1053
target Thresh 18.9999999891457
target distance 9.0
model initialize at round 1053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.01531219,  4.        ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.284331351785665}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7057126467267362
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.01470702, 10.12537677,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8747468679673704}
episode index:1054
target Thresh 18.99999998936063
target distance 1.0
model initialize at round 1054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.9188879 , 11.77000294,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7742633283377586}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.705991592085289
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.9188879 , 11.77000294,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7742633283377586}
episode index:1055
target Thresh 18.999999989571304
target distance 13.0
model initialize at round 1055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.       , 10.9991343,  0.       ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 14.212122226745043}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7059843436989335
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.38852123, 2.99562012, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.1684030606357374}
episode index:1056
target Thresh 18.999999989777805
target distance 14.0
model initialize at round 1056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.        , 10.99999976,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.999999908300552}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7060118815862807
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.01587519, 6.12366352, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.12467833492673455}
episode index:1057
target Thresh 18.999999989980218
target distance 9.0
model initialize at round 1057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.13227156,  3.97388734,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.151191776344705}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7060759355143655
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.58284061, 10.51236756,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.64172217931342}
episode index:1058
target Thresh 18.999999990178623
target distance 9.0
model initialize at round 1058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([11.00333076, 11.85239887,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.521730359542762}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.706139868471859
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.82745658, 4.98728677, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0022506670724405}
episode index:1059
target Thresh 18.9999999903731
target distance 3.0
model initialize at round 1059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.6123381 , 4.99865639, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1737320707236645}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7063699251997158
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.64692419, 6.59873131, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8814704146910188}
episode index:1060
target Thresh 18.999999990563726
target distance 13.0
model initialize at round 1060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.17786313458049}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7063969958551589
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.96806094, 8.47924309, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0801925361412663}
episode index:1061
target Thresh 18.999999990750574
target distance 1.0
model initialize at round 1061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.47972775,  4.16408491,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9637908200514473}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7066734581942784
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.47972775,  4.16408491,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9637908200514473}
episode index:1062
target Thresh 18.999999990933727
target distance 3.0
model initialize at round 1062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.29109173, 3.99926094, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.2263888426842542}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7069023636898624
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.37886209, 4.9600731 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6224198437391133}
episode index:1063
target Thresh 18.99999999111325
target distance 13.0
model initialize at round 1063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([3.99999988, 6.        , 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.180340004785373}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7069288576061548
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.21910944,  7.95669669,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7820903026364325}
episode index:1064
target Thresh 18.999999991289222
target distance 8.0
model initialize at round 1064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4.99999452, 9.99999964, 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.082767998108547}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7070701215896232
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.9746916 , 10.89935609,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.10377722219808024}
episode index:1065
target Thresh 18.999999991461706
target distance 13.0
model initialize at round 1065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([3.99999988, 4.        , 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.038404910977585}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7070291744129811
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.39880895, 10.48332977,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6526842290258958}
episode index:1066
target Thresh 18.999999991630776
target distance 6.0
model initialize at round 1066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.00494319, 11.8673955 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.097797566042669}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7072123710630158
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.12952655, 10.59929301,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.42112138510851815}
episode index:1067
target Thresh 18.999999991796496
target distance 14.0
model initialize at round 1067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([3.99999988, 5.        , 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.041594697589797}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7072040610677263
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.18692149,  6.91820411,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.226456462604592}
episode index:1068
target Thresh 18.999999991958937
target distance 4.0
model initialize at round 1068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.39277273, 4.00755716, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 2.0973818672557853}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7074311854259416
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.16347533, 2.05044759, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8380444383258325}
episode index:1069
target Thresh 18.999999992118163
target distance 1.0
model initialize at round 1069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.66808271, 4.05967975, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1104459690165593}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7076578852526464
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.51907134, 2.38634157, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8037485446848338}
episode index:1070
target Thresh 18.99999999227423
target distance 8.0
model initialize at round 1070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.99833488, 5.        , 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 6.000000231050777}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7077976771431669
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.72808203, 10.97645543,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.2729353957205156}
episode index:1071
target Thresh 18.999999992427213
target distance 12.0
model initialize at round 1071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 3.98135061, 11.86473089,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 13.377473204805039}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7077562804586015
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.33969864,  2.44300285,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.6524116729650501}
episode index:1072
target Thresh 18.999999992577163
target distance 1.0
model initialize at round 1072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.61692873, 10.77240288,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8621773678557367}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7080286418002057
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.61692873, 10.77240288,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8621773678557367}
episode index:1073
target Thresh 18.999999992724145
target distance 12.0
model initialize at round 1073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.86775037, 7.97380132, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.315143078421064}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7080538403559086
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.95737326,  9.8834523 ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.12409836854900386}
episode index:1074
target Thresh 18.99999999286822
target distance 13.0
model initialize at round 1074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([3.99999988, 7.        , 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.045361135906973}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7080789920305774
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.1478307 ,  7.38944429,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6281975720902662}
episode index:1075
target Thresh 18.999999993009435
target distance 7.0
model initialize at round 1075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.46414948,  3.99999952,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 5.209965302552452}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7082177429673521
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.57066842,  9.81930388,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.924978087857065}
episode index:1076
target Thresh 18.999999993147856
target distance 12.0
model initialize at round 1076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 4.97380092, 11.86775028,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.06368016655211}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7082786187282922
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.83549776, 11.41022313,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.4419773801431576}
episode index:1077
target Thresh 18.99999999328354
target distance 12.0
model initialize at round 1077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 11.180339887498974}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.708303491893317
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.98556395, 10.36141192,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.049740385335756}
episode index:1078
target Thresh 18.999999993416534
target distance 8.0
model initialize at round 1078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.86766083, 4.97415137, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 6.027301668770064}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7084416489907283
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.4442232 , 10.99269585,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.4442832419074697}
episode index:1079
target Thresh 18.999999993546894
target distance 4.0
model initialize at round 1079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.82391767,  9.14550869,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.601117770473106}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7086653141305517
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.00674268, 11.18364444,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.18376818012050342}
episode index:1080
target Thresh 18.999999993674674
target distance 5.0
model initialize at round 1080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.00031543, 10.99905181,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.0003155776086863}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7088446246632709
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.71365857, 10.36320378,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6982126057411447}
episode index:1081
target Thresh 18.999999993799925
target distance 9.0
model initialize at round 1081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([11.03138791, 11.85899083,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.822737459860049}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7089046397398298
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.03956324, 5.05081483, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.0644002898318712}
episode index:1082
target Thresh 18.999999993922696
target distance 1.0
model initialize at round 1082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.17001519,  6.67150438,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.6926927844262291}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.70917342585272
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.17001519,  6.67150438,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.6926927844262291}
episode index:1083
target Thresh 18.999999994043034
target distance 11.0
model initialize at round 1083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 5.9340109, 10.0288654,  0.       ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.110113045122205}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7091634294230531
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.99259684,  2.28986619,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.2899607159471328}
episode index:1084
target Thresh 18.99999999416099
target distance 12.0
model initialize at round 1084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.182309677034608}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7091873266223175
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.21811718,  7.88811478,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.1832533969095576}
episode index:1085
target Thresh 18.99999999427661
target distance 7.0
model initialize at round 1085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.86485079, 3.98122628, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.092745484139027}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7093237793602345
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.00595264, 9.59927898, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5993085414835424}
episode index:1086
target Thresh 18.999999994389942
target distance 11.0
model initialize at round 1086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.13259678,  7.99505575,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.611174893003927}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7093474850743694
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.90925808, 5.90792478, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9124480774252306}
episode index:1087
target Thresh 18.999999994501028
target distance 9.0
model initialize at round 1087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 8.97962898, 11.86583631,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 8.014380802632806}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7094835397755879
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.1298681 ,  7.71008117,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9171599923599002}
episode index:1088
target Thresh 18.999999994609915
target distance 12.0
model initialize at round 1088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.13224972,  5.97380104,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.305877870411523}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7094412412370328
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.83721449, 4.00887662, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.16302734942995484}
episode index:1089
target Thresh 18.999999994716646
target distance 5.0
model initialize at round 1089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.9512229 , 7.99999905, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 3.147194102850521}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7096183593643383
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.68650622, 11.84675648,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9029257330683398}
episode index:1090
target Thresh 18.999999994821263
target distance 4.0
model initialize at round 1090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([12.02204356, 11.86552736,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 2.1994994369831673}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7098386908406313
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.1261724 , 10.51609732,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5000812758998335}
episode index:1091
target Thresh 18.999999994923808
target distance 3.0
model initialize at round 1091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.46634507, 9.99941444, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.1039244452958186}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.710058618779422
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.22840292, 11.83613748,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.1377556592361227}
episode index:1092
target Thresh 18.99999999502432
target distance 13.0
model initialize at round 1092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.311697890307714}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7100478947879438
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.844776  , 7.31605815, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9019641047428425}
episode index:1093
target Thresh 18.99999999512285
target distance 5.0
model initialize at round 1093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.29995108,  5.00310743,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 3.018049846459531}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7102238107890517
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.994977  ,  1.26895318,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7310640730652372}
episode index:1094
target Thresh 18.999999995219422
target distance 5.0
model initialize at round 1094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.854877  , 3.98522403, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 3.1336382772525004}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7103994054823951
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.7765952 , 7.65320906, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0147818359043077}
episode index:1095
target Thresh 18.999999995314084
target distance 14.0
model initialize at round 1095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([3.99999976, 9.99999952, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.649110716068085}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7104219351221237
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.89252693,  6.02987434,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.11154791298187164}
episode index:1096
target Thresh 18.99999999540687
target distance 13.0
model initialize at round 1096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.99999976, 8.99999845, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.704699605177606}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7104444236868483
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.10351205,  5.96079839,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9663582632434509}
episode index:1097
target Thresh 18.99999999549782
target distance 14.0
model initialize at round 1097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4., 4., 0.]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 13.892443989449829}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7104015967356664
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.88611448, 10.60977754,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.4065015091367431}
episode index:1098
target Thresh 18.999999995586972
target distance 1.0
model initialize at round 1098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.36610687,  8.18203664,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.896157519520048}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7106651075666621
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.36610687,  8.18203664,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.896157519520048}
episode index:1099
target Thresh 18.999999995674354
target distance 13.0
model initialize at round 1099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.99999988, 7.99999964, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.401754271902078}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7106539004653232
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.25198894,  4.16178832,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8752698113072837}
episode index:1100
target Thresh 18.999999995760007
target distance 2.0
model initialize at round 1100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.95233953, 11.12341616,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.13229916046139023}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7109167034621758
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.95233953, 11.12341616,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.13229916046139023}
episode index:1101
target Thresh 18.999999995843968
target distance 7.0
model initialize at round 1101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.0098387 , 8.00010788, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 5.000117564191214}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7110496057276366
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.88364547, 2.18661242, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8216676580954803}
episode index:1102
target Thresh 18.999999995926263
target distance 3.0
model initialize at round 1102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.17109537, 9.01962388, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.0338793323169784}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7112662425311475
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.72407499, 7.25806248, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.7915844188598241}
episode index:1103
target Thresh 18.999999996006927
target distance 1.0
model initialize at round 1103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.8796289 ,  4.86601663,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.874342038126398}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7115277767317533
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.8796289 ,  4.86601663,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.874342038126398}
episode index:1104
target Thresh 18.999999996085993
target distance 13.0
model initialize at round 1104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 3.99999988, 10.        ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.045361135906969}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7115491017217019
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.34899724,  8.66825086,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.4815148623640318}
episode index:1105
target Thresh 18.999999996163496
target distance 8.0
model initialize at round 1105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([3.99996233, 5.        , 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 6.324567232780537}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7116809515393134
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.91324158, 11.11777071,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9208040663690282}
episode index:1106
target Thresh 18.999999996239463
target distance 12.0
model initialize at round 1106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.02619883, 11.86775027,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 10.063679917525016}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7117370490876067
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.23792121, 11.48705879,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.5420634346842718}
episode index:1107
target Thresh 18.999999996313928
target distance 9.0
model initialize at round 1107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([11.02620133, 11.86775   ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 7.079582978476561}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7118298010739897
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.75512872, 10.80409053,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.31359601952922384}
episode index:1108
target Thresh 18.99999999638692
target distance 2.0
model initialize at round 1108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.72302365,  6.96641088,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.2790056022329336}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7120896479621106
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.72302365,  6.96641088,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.2790056022329336}
episode index:1109
target Thresh 18.99999999645846
target distance 7.0
model initialize at round 1109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.1325971 ,  6.99505836,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.510231220847321}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7122205356666492
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.93381895, 11.73156481,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.1862566744436698}
episode index:1110
target Thresh 18.99999999652859
target distance 12.0
model initialize at round 1110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.02619791, 11.86775004,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.74635449306971}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7122759455692895
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.06749846, 8.11312214, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.13172949512449558}
episode index:1111
target Thresh 18.999999996597328
target distance 2.0
model initialize at round 1111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.07119834,  9.91084886,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.11409264087843925}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7125346902225546
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.07119834,  9.91084886,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.11409264087843925}
episode index:1112
target Thresh 18.999999996664705
target distance 2.0
model initialize at round 1112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.11978513, 11.65214611,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.6630558273829428}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7127929699258586
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.11978513, 11.65214611,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.6630558273829428}
episode index:1113
target Thresh 18.999999996730747
target distance 3.0
model initialize at round 1113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.02721202,  7.00350082,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0038697068760731}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7130059026279001
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.73305675,  5.26050842,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7861974919688476}
episode index:1114
target Thresh 18.999999996795484
target distance 11.0
model initialize at round 1114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([12.8233595 ,  9.14612965,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.055061624976798}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7130604093856329
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.02203894, 6.94539123, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.05888830876875882}
episode index:1115
target Thresh 18.999999996858936
target distance 4.0
model initialize at round 1115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.15213537,  5.99987006,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.0059075105421056}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7132727208467569
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.70825469,  7.95307815,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.2954944719659296}
episode index:1116
target Thresh 18.999999996921133
target distance 12.0
model initialize at round 1116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 10.856988600866641}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7133268911391948
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.06497028, 10.47542772,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0489576189524434}
episode index:1117
target Thresh 18.9999999969821
target distance 5.0
model initialize at round 1117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([5.85600954, 9.82523508, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 4.017305531530925}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7135385844387125
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.82847183, 7.92381011, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.2408830302385168}
episode index:1118
target Thresh 18.999999997041858
target distance 14.0
model initialize at round 1118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.165525060596464}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7135578456596118
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.93881478, 9.27417916, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9780324155488085}
episode index:1119
target Thresh 18.99999999710043
target distance 3.0
model initialize at round 1119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.82299232, 5.99947095, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0160659912713534}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7137689547259871
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.96971838, 7.52432013, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5251938408732012}
episode index:1120
target Thresh 18.99999999715785
target distance 10.0
model initialize at round 1120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([12.82335935,  9.14612991,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 8.82456934421763}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7138588185041085
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.84534608, 9.23615299, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8777119302918962}
episode index:1121
target Thresh 18.999999997214125
target distance 11.0
model initialize at round 1121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([12.00494242, 11.86740304,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.19653098415899}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7139122250272777
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.47627594, 10.24682871,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.5789743547709812}
episode index:1122
target Thresh 18.999999997269292
target distance 11.0
model initialize at round 1122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([11.02611295, 11.86772837,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 10.7657304041679}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7139655364364252
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.34066129, 5.55928016, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.7930709387209051}
episode index:1123
target Thresh 18.999999997323364
target distance 1.0
model initialize at round 1123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.86518541, 6.75810067, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1503314354593337}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7142200154965352
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.86518541, 6.75810067, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1503314354593337}
episode index:1124
target Thresh 18.999999997376364
target distance 1.0
model initialize at round 1124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.1352061 , 6.07218373, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9376159792578188}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7144740421494272
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.1352061 , 6.07218373, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9376159792578188}
episode index:1125
target Thresh 18.999999997428315
target distance 13.0
model initialize at round 1125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4., 4., 0.]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.704699910719649}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7144597111138538
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.16687966,  8.01254911,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8332148482315918}
episode index:1126
target Thresh 18.99999999747924
target distance 8.0
model initialize at round 1126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([3.99999821, 9.        , 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.324557016714717}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7145865214855363
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.93725803, 10.54060298,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.4636617010758197}
episode index:1127
target Thresh 18.99999999752915
target distance 8.0
model initialize at round 1127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 5.97380081, 11.86774985,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 6.0883549898554}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7147131070161342
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.89910601, 11.8598288 ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8657281103110456}
episode index:1128
target Thresh 18.99999999757808
target distance 11.0
model initialize at round 1128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.00016802,  9.00006892,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.849039331429708}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7147311573116248
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.83927936, 5.05148406, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.1687653146026352}
episode index:1129
target Thresh 18.999999997626034
target distance 7.0
model initialize at round 1129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.00000942, 10.99998927,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.000009417545421}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7148573908007296
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.5177714 , 10.80285099,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5209723143622537}
episode index:1130
target Thresh 18.999999997673044
target distance 2.0
model initialize at round 1130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.60130713,  4.02420044,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6017939207486321}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7151095062818961
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.60130713,  4.02420044,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6017939207486321}
episode index:1131
target Thresh 18.99999999771912
target distance 6.0
model initialize at round 1131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 8.02608834, 10.13238509,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 6.523094666613716}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7152351825130958
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.85176503, 5.96140739, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9727681044714416}
episode index:1132
target Thresh 18.999999997764284
target distance 3.0
model initialize at round 1132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.07232118, 4.0123266 , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0149066429636662}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7154423888833403
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.8709653 , 2.49941843, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0045707897261418}
episode index:1133
target Thresh 18.999999997808555
target distance 7.0
model initialize at round 1133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.99999475, 8.        , 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.830956392575316}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7155675499160711
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.66475835, 10.428671  ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.8765389296141181}
episode index:1134
target Thresh 18.999999997851948
target distance 9.0
model initialize at round 1134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.86740304, 6.99505758, 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.18006743071861}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.715654720576938
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.06546887, 10.49018799,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.064545315963234}
episode index:1135
target Thresh 18.999999997894484
target distance 12.0
model initialize at round 1135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.177724080969865}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7157058880214124
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.22082446, 10.35277413,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.41618869549051274}
episode index:1136
target Thresh 18.999999997936172
target distance 13.0
model initialize at round 1136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.045361017187282}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.71565990257134
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.86740079, 5.90401218, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9136851633669001}
episode index:1137
target Thresh 18.999999997977042
target distance 1.0
model initialize at round 1137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.23694858,  8.43673408,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.4968714987387246}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7159097620594145
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.23694858,  8.43673408,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.4968714987387246}
episode index:1138
target Thresh 18.9999999980171
target distance 11.0
model initialize at round 1138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.00000024, 10.99060428,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.035354630166552}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7159266032609646
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.74587503, 3.98906711, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0211920716439753}
episode index:1139
target Thresh 18.999999998056364
target distance 11.0
model initialize at round 1139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.00000012, 10.99389672,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 12.037540735567879}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7159111740441513
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.86455718, 3.09759732, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.870048475336331}
episode index:1140
target Thresh 18.99999999809485
target distance 13.0
model initialize at round 1140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.205906925752268}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7158957718724157
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.88541417, 10.85071698,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.18818961794251216}
episode index:1141
target Thresh 18.999999998132573
target distance 7.0
model initialize at round 1141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 9.01863374, 11.86299783,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 6.333201151678173}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7160591731229653
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.85449124, 8.99459284, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.31124757190566}
episode index:1142
target Thresh 18.999999998169553
target distance 10.0
model initialize at round 1142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 5.94476724, 10.04565587,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 8.611782310859498}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7161096733542663
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.54289192,  6.33029489,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8108345857655699}
episode index:1143
target Thresh 18.999999998205794
target distance 6.0
model initialize at round 1143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.86740293, 6.99505826, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.084557282970933}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7162726019614741
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.08219665, 11.18239664,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9357518483067927}
episode index:1144
target Thresh 18.999999998241325
target distance 11.0
model initialize at round 1144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.574514830248853}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7162569379388822
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.12154707, 7.66854801, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.35303557005246633}
episode index:1145
target Thresh 18.99999999827615
target distance 7.0
model initialize at round 1145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.00033426,  5.99999988,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.40333314818213}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7163800776090927
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.01482908, 11.72679727,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.7269485389664291}
episode index:1146
target Thresh 18.999999998310283
target distance 13.0
model initialize at round 1146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.132598051545473}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7163049854836485
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.72896693,  3.95439221,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7303922631834527}
episode index:1147
target Thresh 18.999999998343743
target distance 1.0
model initialize at round 1147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.57065439,  6.64810514,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.5551284898360193}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7165521065764328
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.57065439,  6.64810514,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.5551284898360193}
episode index:1148
target Thresh 18.999999998376538
target distance 14.0
model initialize at round 1148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 3.99511797, 11.86739319,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.924739675424075}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.716505864909516
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.94033217,  2.43283215,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5702978337636909}
episode index:1149
target Thresh 18.999999998408683
target distance 5.0
model initialize at round 1149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.79507899,  3.99998069,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.007009892064943}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7166675989400295
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.76562775,  7.68627393,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7251911901594524}
episode index:1150
target Thresh 18.999999998440195
target distance 4.0
model initialize at round 1150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.00170171,  9.99982107,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.2376701375679313}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7168703204005508
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.12025824, 11.82360797,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8323413556536975}
episode index:1151
target Thresh 18.999999998471083
target distance 6.0
model initialize at round 1151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.85817267, 5.96742852, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.440094413566868}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7170314572752031
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.26294261, 9.94672524, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.738980243900964}
episode index:1152
target Thresh 18.999999998501355
target distance 2.0
model initialize at round 1152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.41521031,  5.01294518,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.4154120560407738}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7172768766531084
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.41521031,  5.01294518,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.4154120560407738}
episode index:1153
target Thresh 18.99999999853103
target distance 8.0
model initialize at round 1153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.76920617,  3.99999836,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 6.255406471074202}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7173982788397175
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.72327198,  9.96277834,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.2792200747884834}
episode index:1154
target Thresh 18.999999998560117
target distance 13.0
model initialize at round 1154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.99999988, 9.99999988, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.180339983460597}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7174470949944017
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.12995379,  7.65894797,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9345035562412874}
episode index:1155
target Thresh 18.999999998588628
target distance 6.0
model initialize at round 1155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.22236633,  6.99999714,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.006178936873282}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7176071753620537
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.33278963, 10.96348826,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.3347865636377716}
episode index:1156
target Thresh 18.999999998616577
target distance 7.0
model initialize at round 1156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.27714205,  3.9999994 ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 5.007675476223368}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7177279772848177
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.68535142,  9.71120918,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7777031704060524}
episode index:1157
target Thresh 18.99999999864397
target distance 1.0
model initialize at round 1157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.22184193, 8.34095669, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.6953789827783822}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7179717355082332
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.22184193, 8.34095669, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.6953789827783822}
episode index:1158
target Thresh 18.99999999867082
target distance 11.0
model initialize at round 1158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.929943316557791}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7179547946631819
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.99276891,  9.52166294,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.47839171171812195}
episode index:1159
target Thresh 18.999999998697138
target distance 2.0
model initialize at round 1159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.80710363,  9.03025174,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.19525413163455893}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7181979370815758
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.80710363,  9.03025174,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.19525413163455893}
episode index:1160
target Thresh 18.99999999872294
target distance 9.0
model initialize at round 1160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.00000119,  6.        ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.602326237088514}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7182808899781463
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.94143206, 10.99253629,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9414616438947853}
episode index:1161
target Thresh 18.999999998748226
target distance 4.0
model initialize at round 1161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.79991889,  6.00200617,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 2.0119794153631787}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7184803040143097
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.58331239,  4.01950622,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.5836384492125413}
episode index:1162
target Thresh 18.999999998773013
target distance 1.0
model initialize at round 1162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.25212121,  7.16892797,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8684732740301636}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7187223673814513
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.25212121,  7.16892797,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8684732740301636}
episode index:1163
target Thresh 18.99999999879731
target distance 12.0
model initialize at round 1163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 3.9999994 , 10.99993062,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 11.661868605333552}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7187364305457499
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.86653917,  5.99272811,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0016590683512245}
episode index:1164
target Thresh 18.999999998821124
target distance 4.0
model initialize at round 1164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 6.00109029, 10.52757832,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.0694688278434574}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7189349400474274
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.19378614, 10.27086562,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.3330484249489809}
episode index:1165
target Thresh 18.99999999884447
target distance 9.0
model initialize at round 1165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([10.01943359, 10.13768438,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.010943322996539}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7189819777810916
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.63309888, 3.98603939, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0520884547878562}
episode index:1166
target Thresh 18.99999999886735
target distance 12.0
model initialize at round 1166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([12.02612394, 11.8677313 ,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.152731974577057}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7189957823336571
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.66172109, 4.49390435, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6087408496827639}
episode index:1167
target Thresh 18.999999998889777
target distance 12.0
model initialize at round 1167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.315143201383691}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7189780952735202
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.87130281, 7.27304031, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9130824731977022}
episode index:1168
target Thresh 18.99999999891176
target distance 6.0
model initialize at round 1168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.95554449, 9.05626336, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 4.503046380962175}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7191350857822684
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.73620108, 5.09242317, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.27952086322696373}
episode index:1169
target Thresh 18.99999999893331
target distance 13.0
model initialize at round 1169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.02619883, 11.86775027,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.18327106888076}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7191487240770058
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.44372874, 10.38849547,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.678503085376243}
episode index:1170
target Thresh 18.99999999895443
target distance 11.0
model initialize at round 1170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.86775028, 5.97380092, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.559639024689387}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7191011337330365
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.58284774,  3.95100365,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.1154010207039697}
episode index:1171
target Thresh 18.999999998975134
target distance 5.0
model initialize at round 1171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.65203285,  5.00195527,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 3.0220550265761332}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7192576174073259
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.31995674,  1.53421385,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.565092079570056}
episode index:1172
target Thresh 18.999999998995428
target distance 11.0
model initialize at round 1172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.13224973,  3.97380117,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 12.330042763830141}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7192100153731243
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.63169165, 11.24055282,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.4399053310690052}
episode index:1173
target Thresh 18.999999999015323
target distance 2.0
model initialize at round 1173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.97822982,  8.03506148,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.04127042706800187}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7194491891249359
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.97822982,  8.03506148,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.04127042706800187}
episode index:1174
target Thresh 18.99999999903482
target distance 11.0
model initialize at round 1174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.972155793073503}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7194625020623828
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.1721579 ,  8.98942413,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8279096539655635}
episode index:1175
target Thresh 18.99999999905393
target distance 4.0
model initialize at round 1175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.8269016,  9.1422371,  0.       ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.0334820501011666}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7196585373497447
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.45652406, 11.13994314,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.5612042285741982}
episode index:1176
target Thresh 18.999999999072664
target distance 7.0
model initialize at round 1176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 9.97384699, 10.1322649 ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.928864162051238}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7197391216425657
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.51595608,  3.96275764,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.5172984334006864}
episode index:1177
target Thresh 18.999999999091028
target distance 14.0
model initialize at round 1177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.       ,  8.9999944,  0.       ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.892441166343621}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7196913129071213
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.4064844 , 2.01320934, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5936625747164441}
episode index:1178
target Thresh 18.999999999109026
target distance 14.0
model initialize at round 1178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.369316876852997}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7196732009335731
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.33816531, 4.88774858, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.1073042476079327}
episode index:1179
target Thresh 18.99999999912667
target distance 8.0
model initialize at round 1179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.04555839,  9.05529637,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 6.130054884021596}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7197898973734599
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.78122413,  3.09045453,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.23673805141845242}
episode index:1180
target Thresh 18.99999999914396
target distance 1.0
model initialize at round 1180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.0250507 , 2.61933362, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.381489746564213}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7200271624899938
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.0250507 , 2.61933362, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.381489746564213}
episode index:1181
target Thresh 18.999999999160913
target distance 12.0
model initialize at round 1181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 5.99999964, 10.99999964,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 10.198039307731932}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7200726394570073
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.8626382 ,  8.72952512,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.3033560981279418}
episode index:1182
target Thresh 18.999999999177525
target distance 11.0
model initialize at round 1182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 5.99999738, 10.99122417,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.035767982882554}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7200853353582481
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.69351087,  3.97062508,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.1929251299470707}
episode index:1183
target Thresh 18.99999999919381
target distance 9.0
model initialize at round 1183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([5.85390299, 9.82337852, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 8.721113442233134}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7201650827523712
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.45468017, 1.9403983 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.548567293846094}
episode index:1184
target Thresh 18.999999999209777
target distance 7.0
model initialize at round 1184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 7.02282669, 11.86097685,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 5.780482380057569}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7202808717120739
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.4518067 , 9.65310557, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8526797620284765}
episode index:1185
target Thresh 18.999999999225423
target distance 11.0
model initialize at round 1185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 6.97406935, 11.86768172,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.973631012516845}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7203259813796858
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.50124034,  4.99081716,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.1103875533633403}
episode index:1186
target Thresh 18.99999999924076
target distance 5.0
model initialize at round 1186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([13.99993885,  9.00041246,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.6059283931020647}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7204794557003432
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.46103108,  5.11948299,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.993911391316437}
episode index:1187
target Thresh 18.999999999255795
target distance 1.0
model initialize at round 1187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.46346098, 7.77896857, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9458679366633926}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7207147423537941
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.46346098, 7.77896857, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9458679366633926}
episode index:1188
target Thresh 18.999999999270532
target distance 6.0
model initialize at round 1188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.99908531,  9.00004017,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 4.000040278112449}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7208676315528237
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.99167305,  5.01723492,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.019141070382622055}
episode index:1189
target Thresh 18.999999999284974
target distance 6.0
model initialize at round 1189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.12629175,  3.99999714,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.155545210739907}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7210202637952163
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.76893082,  7.98073864,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.23187057755307977}
episode index:1190
target Thresh 18.999999999299135
target distance 11.0
model initialize at round 1190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.86775037, 7.97380131, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.132283497910205}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7210645632693597
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.22221294,  7.93550462,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7804565091475357}
episode index:1191
target Thresh 18.99999999931301
target distance 8.0
model initialize at round 1191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.79524225,  8.00002377,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 6.2628412093814365}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7211789176625901
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.16761716,  2.06338874,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.1792028045418715}
episode index:1192
target Thresh 18.999999999326615
target distance 8.0
model initialize at round 1192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.75534457,  4.99999613,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 6.251502296775437}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.721293080346863
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.15923976, 10.94755334,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.16765426651883303}
episode index:1193
target Thresh 18.99999999933995
target distance 10.0
model initialize at round 1193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 5.96868296, 11.85901085,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.957597021402437}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7212738543969024
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.9102553 ,  2.41542197,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5914267314640471}
episode index:1194
target Thresh 18.99999999935302
target distance 2.0
model initialize at round 1194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.18114066,  5.19507945,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.2662103103622119}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7215070980333904
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.18114066,  5.19507945,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.2662103103622119}
episode index:1195
target Thresh 18.99999999936583
target distance 8.0
model initialize at round 1195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.9991895 ,  9.00001085,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 6.000010902788292}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7216206999581116
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.69034725,  3.07158516,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.31781954055434064}
episode index:1196
target Thresh 18.99999999937839
target distance 4.0
model initialize at round 1196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([2.92682731, 8.99989903, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.001439015710433}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.721811493024145
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.52658626, 10.98626649,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.4736128949616921}
episode index:1197
target Thresh 18.999999999390695
target distance 7.0
model initialize at round 1197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.86774979, 5.97380309, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 5.459775305738886}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7219623181551766
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.03911225, 10.1396808 ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.2897497398374826}
episode index:1198
target Thresh 18.99999999940276
target distance 12.0
model initialize at round 1198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.00494245, 11.86740305,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.04247287949053}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7220055363531289
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.23876747, 11.68353033,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7240328854469645}
episode index:1199
target Thresh 18.999999999414587
target distance 14.0
model initialize at round 1199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 3.99999988, 10.        ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.369316992502965}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7220164416483554
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.88603495,  6.3296439 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6799745044258957}
episode index:1200
target Thresh 18.99999999942618
target distance 2.0
model initialize at round 1200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.06950843, 10.99290013,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.06987009671658106}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7222479017302469
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.06950843, 10.99290013,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.06987009671658106}
episode index:1201
target Thresh 18.99999999943754
target distance 10.0
model initialize at round 1201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 5.97889992, 11.86615364,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 11.234519105906822}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7222585872451344
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.08732302,  4.02961071,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.09220685791562501}
episode index:1202
target Thresh 18.99999999944868
target distance 11.0
model initialize at round 1202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.310400490210087}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7222096777223114
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.53786544, 9.57988416, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.7415079197734389}
episode index:1203
target Thresh 18.999999999459597
target distance 13.0
model initialize at round 1203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 3.97380126, 11.86775022,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 12.05288558268079}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.722220377234689
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.00713787,  6.28130481,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7187306303614294}
episode index:1204
target Thresh 18.999999999470297
target distance 12.0
model initialize at round 1204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.86740305, 5.99505742, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.891740209019883}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7221440527803238
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.27783412,  1.95979054,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.2807286960959658}
episode index:1205
target Thresh 18.999999999480785
target distance 9.0
model initialize at round 1205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([3.99999475, 4.        , 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.615775172054396}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7222561845773551
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.03909645, 10.13969667,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.289750923577353}
episode index:1206
target Thresh 18.999999999491067
target distance 10.0
model initialize at round 1206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 5.9738009 , 11.86775023,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 8.240653059203092}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7223326137947723
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.92031977,  9.61625618,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.3919288917225839}
episode index:1207
target Thresh 18.999999999501146
target distance 2.0
model initialize at round 1207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.81591949,  3.04896641,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.19048187247755627}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7225624709025581
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.81591949,  3.04896641,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.19048187247755627}
episode index:1208
target Thresh 18.999999999511022
target distance 2.0
model initialize at round 1208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.56999445,  9.04446483,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5717261524502067}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7227919477669894
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.56999445,  9.04446483,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5717261524502067}
episode index:1209
target Thresh 18.999999999520703
target distance 12.0
model initialize at round 1209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.179448465417412}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7227717373110611
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.90705272, 7.34594451, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.3582133474455803}
episode index:1210
target Thresh 18.999999999530196
target distance 12.0
model initialize at round 1210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14., 11.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.00000000000002}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7228138588636531
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.20632565, 11.37928148,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.4317692791323257}
episode index:1211
target Thresh 18.999999999539497
target distance 4.0
model initialize at round 1211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.04542546,  9.05559856,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 2.2664284630764726}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7230013061748218
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.55564311,  7.08785216,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.5625453507213578}
episode index:1212
target Thresh 18.999999999548617
target distance 13.0
model initialize at round 1212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 4.99999988, 10.        ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.0000001192093}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7230112736805516
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.48027096,  9.41286548,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.7585427688813711}
episode index:1213
target Thresh 18.999999999557556
target distance 5.0
model initialize at round 1213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 7.01927672, 10.13719997,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.6991425479367295}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.723198249567141
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.84465703, 8.99784012, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.307337146052243}
episode index:1214
target Thresh 18.999999999566317
target distance 7.0
model initialize at round 1214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.14318895,  7.00033367,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.002383417504432}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7233086831065919
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.42870183,  1.43912895,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.70594730400474}
episode index:1215
target Thresh 18.999999999574904
target distance 13.0
model initialize at round 1215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4., 9., 0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.000000000000018}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7233501898947443
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.00648   ,  8.44194281,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.1395217529461932}
episode index:1216
target Thresh 18.99999999958332
target distance 4.0
model initialize at round 1216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.37439257, 5.99998689, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.0347536086785847}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.723536426386203
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.55592144, 7.98071373, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.44449716756807367}
episode index:1217
target Thresh 18.999999999591573
target distance 12.0
model initialize at round 1217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.99999917, 9.99990964, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.66185801533221}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7235459136310624
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.95776518,  4.96445293,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9653772464323467}
episode index:1218
target Thresh 18.99999999959966
target distance 4.0
model initialize at round 1218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.94495881, 3.99999857, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.2120020043292383}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7237316840054422
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.74617803, 5.99320436, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.2539129235606827}
episode index:1219
target Thresh 18.999999999607585
target distance 13.0
model initialize at round 1219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.401754250991393}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7237409956502122
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.75486197, 6.03693666, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.2479051611344604}
episode index:1220
target Thresh 18.99999999961536
target distance 8.0
model initialize at round 1220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.13865143,  4.97837071,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 6.128339609696968}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7238504420092211
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.59078955, 11.04686245,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.4118850351051351}
episode index:1221
target Thresh 18.999999999622972
target distance 12.0
model initialize at round 1221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.        , 10.99999988,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.198039003806736}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7238913016618322
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.07809641, 8.5655294 , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.441433748765302}
episode index:1222
target Thresh 18.99999999963044
target distance 2.0
model initialize at round 1222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.90040839,  2.08155286,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.12872202152900275}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7241170651110048
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.90040839,  2.08155286,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.12872202152900275}
episode index:1223
target Thresh 18.999999999637758
target distance 7.0
model initialize at round 1223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.63281571,  7.00012076,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 5.040006284558304}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7242259359728422
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.2239641 ,  1.54772675,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.5046890237874682}
episode index:1224
target Thresh 18.999999999644928
target distance 12.0
model initialize at round 1224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.778321394833641}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7242048023892674
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.00265812,  9.70028982,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0414014716779953}
episode index:1225
target Thresh 18.99999999965196
target distance 4.0
model initialize at round 1225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.11490965,  6.99993956,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.0033586766980647}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7243889746548554
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.25044622,  8.97850562,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7498619018668659}
episode index:1226
target Thresh 18.99999999965885
target distance 1.0
model initialize at round 1226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.68431952, 7.64272321, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9388217799479684}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7246135965174023
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.68431952, 7.64272321, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9388217799479684}
episode index:1227
target Thresh 18.999999999665608
target distance 13.0
model initialize at round 1227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 4.99999988, 11.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 11.0000001192093}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7246221293301934
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.47744263, 11.07259303,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.48292981884738456}
episode index:1228
target Thresh 18.99999999967223
target distance 9.0
model initialize at round 1228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.99999809, 4.        , 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.062258694608666}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7246952653112104
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.0223586 , 11.67902524,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.679393246408611}
episode index:1229
target Thresh 18.999999999678717
target distance 12.0
model initialize at round 1229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.565977982391892}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.724618480062766
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.28559181,  2.93242295,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.9751795942997905}
episode index:1230
target Thresh 18.99999999968508
target distance 3.0
model initialize at round 1230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.64689875,  6.00699544,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.067108387401667}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7248015682186858
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.00748736,  4.36889982,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6311445911078889}
episode index:1231
target Thresh 18.99999999969132
target distance 7.0
model initialize at round 1231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.00003493,  6.99999988,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.403151586376227}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7249091765237031
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.99072952, 11.17246936,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.17271833270060089}
episode index:1232
target Thresh 18.99999999969743
target distance 2.0
model initialize at round 1232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.04362216, 11.69791502,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6992769602944581}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7251322834364982
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.04362216, 11.69791502,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6992769602944581}
episode index:1233
target Thresh 18.99999999970342
target distance 12.0
model initialize at round 1233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.        , 10.99999309,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 11.18033679540377}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7251403544309784
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.22467277, 6.0493142 , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.23002118655449527}
episode index:1234
target Thresh 18.999999999709292
target distance 1.0
model initialize at round 1234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.40666986, 2.48271239, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.658002161508766}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7253629128484432
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.40666986, 2.48271239, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.658002161508766}
episode index:1235
target Thresh 18.999999999715047
target distance 6.0
model initialize at round 1235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.82700014,  4.99999881,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 4.084597748885015}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7255062276438733
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.91247171,  8.94523346,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.10325005909460316}
episode index:1236
target Thresh 18.999999999720693
target distance 5.0
model initialize at round 1236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.00030744, 10.99972057,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.000307453769581}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7256493107258103
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.59936131, 10.50357672,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.6379243123626643}
episode index:1237
target Thresh 18.99999999972622
target distance 10.0
model initialize at round 1237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.13224972,  4.97380093,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.184022717038568}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7255990450719841
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.75557083, 4.95750929, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9882153888591909}
episode index:1238
target Thresh 18.999999999731642
target distance 1.0
model initialize at round 1238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.44409657, 10.95758134,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.1072446210929126}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7258205147692626
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.44409657, 10.95758134,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.1072446210929126}
episode index:1239
target Thresh 18.999999999736957
target distance 10.0
model initialize at round 1239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 4.99999928, 10.        ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.06225845803099}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7258920355234809
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.9502331 , 11.39493566,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.3980589442817624}
episode index:1240
target Thresh 18.999999999742165
target distance 3.0
model initialize at round 1240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.28224796, 9.01126814, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.0499176919974678}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7260726221185466
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.70076948, 7.23332107, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8230039382389676}
episode index:1241
target Thresh 18.99999999974727
target distance 2.0
model initialize at round 1241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.25112924,  7.01599932,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.25163837789797683}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7262931755628956
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.25112924,  7.01599932,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.25163837789797683}
episode index:1242
target Thresh 18.999999999752276
target distance 5.0
model initialize at round 1242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.98489392, 3.99999988, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.1575333323723984}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7264349348745909
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.78116691, 7.65463507, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.6902427094562572}
episode index:1243
target Thresh 18.99999999975718
target distance 9.0
model initialize at round 1243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([10.02618917, 11.86774732,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 7.2702004341535025}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7265057317517013
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.80562319, 10.09247185,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.21525191643863495}
episode index:1244
target Thresh 18.99999999976199
target distance 1.0
model initialize at round 1244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.43623686,  9.85551715,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0245674560746143}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7267254058627441
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.43623686,  9.85551715,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0245674560746143}
episode index:1245
target Thresh 18.9999999997667
target distance 14.0
model initialize at round 1245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.        ,  9.99998558,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 14.422197100685628}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7266745993020911
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.00146735, 2.08800604, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.08801827092489434}
episode index:1246
target Thresh 18.999999999771322
target distance 3.0
model initialize at round 1246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.51524583, 5.01662827, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.1262857719515864}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.726853689438978
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.31213489, 3.47620678, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6097438220347113}
episode index:1247
target Thresh 18.999999999775852
target distance 6.0
model initialize at round 1247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.64827156, 7.00085819, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.223036349782691}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7269944316750045
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.64056379, 3.04659617, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6422563094141706}
episode index:1248
target Thresh 18.99999999978029
target distance 11.0
model initialize at round 1248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([12.05521678, 10.045681  ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 9.105365210222166}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7270318908470021
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.71595024, 11.00301156,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.28406572174098843}
episode index:1249
target Thresh 18.999999999784638
target distance 2.0
model initialize at round 1249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.79295683,  5.2205908 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.302534585946199}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7272502653343246
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.79295683,  5.2205908 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.302534585946199}
episode index:1250
target Thresh 18.999999999788905
target distance 7.0
model initialize at round 1250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.1322501 ,  5.97380265,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.928880823590693}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7273200143228663
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.98249673, 11.09309465,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9868973796545052}
episode index:1251
target Thresh 18.999999999793083
target distance 12.0
model initialize at round 1251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.86775037, 7.97380131, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.132280455356941}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7273262218917976
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.81663591,  8.11267802,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.21521785332974022}
episode index:1252
target Thresh 18.999999999797183
target distance 11.0
model initialize at round 1252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4., 5., 0.]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.816653826391994}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7273324195523987
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.0136645 , 11.28583383,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.2861602631754511}
episode index:1253
target Thresh 18.999999999801197
target distance 11.0
model initialize at round 1253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([11.02978717, 11.85726309,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.367972619028993}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7273386073283736
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.387762  , 2.99765844, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.170537369911673}
episode index:1254
target Thresh 18.999999999805134
target distance 7.0
model initialize at round 1254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 9.02499763, 10.13263825,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 7.928420593971283}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.727408063617355
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.65679817, 4.9768982 , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0354311098826414}
episode index:1255
target Thresh 18.999999999808992
target distance 6.0
model initialize at round 1255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.84687871,  3.99999797,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.08866968065779}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.727547468025303
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.49615238,  7.98307407,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.5041318418176934}
episode index:1256
target Thresh 18.999999999812776
target distance 3.0
model initialize at round 1256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.27818346,  4.02624023,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0632756210935026}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7277244390133497
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.78101945,  2.54868078,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5016388320921314}
episode index:1257
target Thresh 18.99999999981648
target distance 3.0
model initialize at round 1257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.99359941,  9.00452542,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.004545814308309}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7279011286484743
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.25682589,  7.12654603,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9104292316538768}
episode index:1258
target Thresh 18.999999999820115
target distance 7.0
model initialize at round 1258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.98979044,  5.99999988,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.00001054271055}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7280039673072126
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.01245297, 11.48906032,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.48921884239228397}
episode index:1259
target Thresh 18.99999999982368
target distance 10.0
model initialize at round 1259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.189726279481821}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7280095926431791
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.19128596,  7.91163342,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9314858097470914}
episode index:1260
target Thresh 18.999999999827168
target distance 2.0
model initialize at round 1260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.21742439,  6.9526664 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.22251704665850083}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7282252868599569
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.21742439,  6.9526664 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.22251704665850083}
episode index:1261
target Thresh 18.99999999983059
target distance 8.0
model initialize at round 1261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.04560046,  9.05527073,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 6.130023007710338}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7283276241920806
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.83902556,  3.09968607,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.1893411807882544}
episode index:1262
target Thresh 18.999999999833946
target distance 7.0
model initialize at round 1262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([12.14610886,  9.8233766 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 6.983136248672865}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7284297994698383
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.52983793,  3.91984737,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.5358662862792475}
episode index:1263
target Thresh 18.999999999837232
target distance 4.0
model initialize at round 1263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.95630085, 4.99999893, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.216870680983188}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7286050923500046
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.2105968 , 6.98193109, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.21137052207678717}
episode index:1264
target Thresh 18.999999999840455
target distance 2.0
model initialize at round 1264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.9930948 ,  9.46433187,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.5357126396514683}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.728819633778977
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.9930948 ,  9.46433187,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.5357126396514683}
episode index:1265
target Thresh 18.999999999843617
target distance 2.0
model initialize at round 1265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.79983354, 6.08168638, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.21619268610245226}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7290338362799414
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.79983354, 6.08168638, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.21619268610245226}
episode index:1266
target Thresh 18.99999999984671
target distance 13.0
model initialize at round 1266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.831072604542193}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7290096085449879
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.93208313,  9.58999119,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.4155958705107745}
episode index:1267
target Thresh 18.99999999984975
target distance 13.0
model initialize at round 1267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.        , 10.99998915,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.529958891535681}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7290144052974169
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.48760767, 5.9905027 , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.1151867544836285}
episode index:1268
target Thresh 18.999999999852722
target distance 11.0
model initialize at round 1268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.614260567258981}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7289902310584858
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.02052516, 7.9539049 , 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9541256988410364}
episode index:1269
target Thresh 18.99999999985564
target distance 2.0
model initialize at round 1269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.07872975,  9.05334139,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.09509824963232592}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7292036245773373
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.07872975,  9.05334139,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.09509824963232592}
episode index:1270
target Thresh 18.9999999998585
target distance 7.0
model initialize at round 1270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.86718892, 4.97587527, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 5.0984160258444025}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.729304467516301
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.9160319 , 10.81974057,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8240298796200778}
episode index:1271
target Thresh 18.9999999998613
target distance 11.0
model initialize at round 1271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 4.97380202, 11.86774997,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.25510798416874}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.72933943329459
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.68021517,  7.04929991,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.6819993826179477}
episode index:1272
target Thresh 18.999999999864045
target distance 3.0
model initialize at round 1272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.50809455,  3.01719022,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1370294677109323}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7295127723100695
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.42725571,  1.61062694,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.5780647238702765}
episode index:1273
target Thresh 18.999999999866738
target distance 12.0
model initialize at round 1273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 11.180339887498974}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7294883017635889
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.11969714, 8.55980723, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.4561765914408157}
episode index:1274
target Thresh 18.999999999869377
target distance 7.0
model initialize at round 1274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([15.64457214,  9.00004375,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 5.263559158941672}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7295886050563234
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.78184994,  3.39709735,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6411560316781468}
episode index:1275
target Thresh 18.999999999871964
target distance 2.0
model initialize at round 1275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.95711827,  8.05243266,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.06773496862542416}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7298005262122353
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.95711827,  8.05243266,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.06773496862542416}
episode index:1276
target Thresh 18.999999999874497
target distance 12.0
model initialize at round 1276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.        , 10.99999762,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.770328728804863}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7298046698022218
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.03039726, 6.2436143 , 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.7569962463186444}
episode index:1277
target Thresh 18.999999999876984
target distance 9.0
model initialize at round 1277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.13259712,  3.99505861,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.310099431321131}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.729808806907717
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.46725506, 11.77322511,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9389857491604277}
episode index:1278
target Thresh 18.999999999879417
target distance 10.0
model initialize at round 1278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.13224968,  5.97379887,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.343118655970565}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7297569004373349
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.86537029, 4.12449531, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.8742796052996998}
episode index:1279
target Thresh 18.99999999988181
target distance 9.0
model initialize at round 1279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([11.02223425, 11.86657721,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.150327977125471}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7298231108666807
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.65803626, 6.98933772, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0467704247818297}
episode index:1280
target Thresh 18.99999999988415
target distance 2.0
model initialize at round 1280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.90221548, 3.43474448, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.573651129318845}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7300340217871595
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.90221548, 3.43474448, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.573651129318845}
episode index:1281
target Thresh 18.99999999988644
target distance 2.0
model initialize at round 1281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.57924202,  7.97185671,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.42169814039042813}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7302446036734409
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.57924202,  7.97185671,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.42169814039042813}
episode index:1282
target Thresh 18.99999999988869
target distance 2.0
model initialize at round 1282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.59871367,  6.98057771,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.4017560791677769}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7304548572948958
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.59871367,  6.98057771,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.4017560791677769}
episode index:1283
target Thresh 18.999999999890893
target distance 12.0
model initialize at round 1283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4., 5., 0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.049875621120906}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7304026497980065
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.92734931,  4.90195426,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.904875462899126}
episode index:1284
target Thresh 18.999999999893053
target distance 12.0
model initialize at round 1284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 11.174760518110984}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7303776962153573
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.92469604,  5.92069494,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9237693757644824}
episode index:1285
target Thresh 18.999999999895174
target distance 1.0
model initialize at round 1285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.22757845,  9.18521285,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.845972841219676}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.730587355860602
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.22757845,  9.18521285,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.845972841219676}
episode index:1286
target Thresh 18.99999999989725
target distance 8.0
model initialize at round 1286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.02521683, 10.13251803,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.336811624601657}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7306209173070972
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.09977765, 3.01960021, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.10168454834116077}
episode index:1287
target Thresh 18.999999999899284
target distance 1.0
model initialize at round 1287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.0533728 ,  7.65520024,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6573705307033624}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7308300625576352
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.0533728 ,  7.65520024,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6573705307033624}
episode index:1288
target Thresh 18.999999999901277
target distance 2.0
model initialize at round 1288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.60627976,  8.95766807,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.39598941847985125}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7310388833004143
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.60627976,  8.95766807,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.39598941847985125}
episode index:1289
target Thresh 18.99999999990323
target distance 12.0
model initialize at round 1289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.182309676547058}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7309864658957543
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.66359031, 5.92582189, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9850470250495629}
episode index:1290
target Thresh 18.999999999905146
target distance 4.0
model initialize at round 1290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 3.99480703, 11.86739452,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.184759048355609}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7311561123203123
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.9336079 , 10.49922005,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.5051618242262881}
episode index:1291
target Thresh 18.999999999907025
target distance 7.0
model initialize at round 1291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.1326   ,  5.9950762,  0.       ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.4905812530635085}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7312538049578353
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.95230822, 11.86510581,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.2865842376333676}
episode index:1292
target Thresh 18.999999999908866
target distance 2.0
model initialize at round 1292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.84559515,  9.02635002,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.1566370995743985}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7314616519764294
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.84559515,  9.02635002,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.1566370995743985}
episode index:1293
target Thresh 18.99999999991067
target distance 10.0
model initialize at round 1293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([5.17664065, 9.14612991, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.016013779339186}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7315258286364167
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.06879857, 10.73929143,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9670083066553498}
episode index:1294
target Thresh 18.99999999991244
target distance 3.0
model initialize at round 1294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.67455873,  3.01491117,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0658127009915883}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7316945345602496
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.4357973,  1.4829725,  0.       ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6761928149981369}
episode index:1295
target Thresh 18.999999999914174
target distance 5.0
model initialize at round 1295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.86774325, 7.97382912, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.2310548602799765}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7318629801354346
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.06617423, 10.17685252,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8258031246727955}
episode index:1296
target Thresh 18.999999999915875
target distance 12.0
model initialize at round 1296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 5.99505731, 11.86740305,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 10.407731667548571}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7318952993007117
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.79229818,  8.9698729 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.20987540723888154}
episode index:1297
target Thresh 18.99999999991754
target distance 3.0
model initialize at round 1297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.45862317, 9.00922132, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.1452582829051332}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7320633306571828
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.22091067, 7.31729174, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7175598207201804}
episode index:1298
target Thresh 18.999999999919172
target distance 8.0
model initialize at round 1298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 9.97439526, 10.13240312,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 7.915142083060882}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7321597984549832
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.91534431,  5.95280406,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9565574564483462}
episode index:1299
target Thresh 18.99999999992077
target distance 4.0
model initialize at round 1299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.56631637, 5.00737059, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 2.0536840517709574}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7323273678407871
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.84977861, 3.04350857, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8508916936121517}
episode index:1300
target Thresh 18.99999999992234
target distance 13.0
model initialize at round 1300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.311697890307714}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7323012417287602
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.1964747,  6.9914243,  0.       ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8035710581334059}
episode index:1301
target Thresh 18.99999999992388
target distance 12.0
model initialize at round 1301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([3.99999988, 8.99999976, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.440306554583248}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7323033850842873
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.29638522,  6.02660133,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7041174491704364}
episode index:1302
target Thresh 18.999999999925386
target distance 11.0
model initialize at round 1302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([12.82335973,  9.14612925,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.314869147592379}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.732335217434568
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.98584335, 6.8899389 , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8900514894607072}
episode index:1303
target Thresh 18.999999999926864
target distance 2.0
model initialize at round 1303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.99242032, 4.99975979, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.00758348948115767}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7325404818383758
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.99242032, 4.99975979, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.00758348948115767}
episode index:1304
target Thresh 18.999999999928313
target distance 5.0
model initialize at round 1304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.13779402,  3.99999452,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.00316834245655}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7326707190170438
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.90488711,  7.8041352 ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8097406295415721}
episode index:1305
target Thresh 18.999999999929734
target distance 6.0
model initialize at round 1305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.98515105,  4.99999964,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 4.000027918935756}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7328007567513339
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.64828122,  8.97888601,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.3523519593082546}
episode index:1306
target Thresh 18.999999999931124
target distance 12.0
model initialize at round 1306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([14.        ,  8.99998653,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 11.18033386324775}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7327743883805171
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.31254751, 4.01852042, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.3130957533576756}
episode index:1307
target Thresh 18.999999999932488
target distance 14.0
model initialize at round 1307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.02619869, 11.86775024,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.973991189374496}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7327761601712238
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.02715508, 7.34707488, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.3481355647223897}
episode index:1308
target Thresh 18.999999999933824
target distance 13.0
model initialize at round 1308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.02619575, 11.86774949,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.490295306442338}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7327779292548402
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.39225604, 5.46266848, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8112199941449225}
episode index:1309
target Thresh 18.999999999935135
target distance 1.0
model initialize at round 1309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.03354295, 2.09602422, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.10171420566371937}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7329819155683861
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.03354295, 2.09602422, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.10171420566371937}
episode index:1310
target Thresh 18.99999999993642
target distance 14.0
model initialize at round 1310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.        , 10.99999821,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 13.41640706531859}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7329835250077885
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.98135361, 5.99984001, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0000138673254044}
episode index:1311
target Thresh 18.99999999993768
target distance 10.0
model initialize at round 1311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.13224972,  3.97380093,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.132287302734124}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7329052215662617
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.34775539, 4.20286843, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.4026032923682592}
episode index:1312
target Thresh 18.99999999993891
target distance 13.0
model initialize at round 1312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.311697890307714}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7329068869653924
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.06758601,  9.61918899,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.1192814030559588}
episode index:1313
target Thresh 18.999999999940123
target distance 13.0
model initialize at round 1313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.311697890307714}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.732854005340068
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.88081385,  6.00995971,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.11960156072186406}
episode index:1314
target Thresh 18.99999999994131
target distance 11.0
model initialize at round 1314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.509715278570543}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7328277568919719
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.25787047, 11.86695939,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9044975194100242}
episode index:1315
target Thresh 18.99999999994247
target distance 12.0
model initialize at round 1315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.770329614269032}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7328294773583344
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.91353763, 9.39386828, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9948282323978829}
episode index:1316
target Thresh 18.999999999943608
target distance 12.0
model initialize at round 1316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([12.82335934,  9.14612992,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 10.824345768607158}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7328605718610994
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.93652839, 9.45647976, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.041853732312471}
episode index:1317
target Thresh 18.999999999944723
target distance 6.0
model initialize at round 1317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4.13176982, 4.99999964, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 4.157030886204403}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7329892815941336
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.08968578, 8.98254812, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9104814892420314}
episode index:1318
target Thresh 18.99999999994582
target distance 6.0
model initialize at round 1318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([5.85848763, 9.82758392, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 4.783634351855501}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7331177961645702
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.93242663, 7.3773169 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.38331997791749917}
episode index:1319
target Thresh 18.999999999946894
target distance 13.0
model initialize at round 1319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4., 7., 0.]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.401754250991392}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7330914473008802
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.04542503,  4.86270144,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8638965229316962}
episode index:1320
target Thresh 18.999999999947942
target distance 6.0
model initialize at round 1320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.99993014,  9.00005662,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.123177502445701}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7332196899600014
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.70179713,  5.0227263 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.2990676138039792}
episode index:1321
target Thresh 18.999999999948976
target distance 5.0
model initialize at round 1321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.97709179, 6.99999976, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 3.155108523509602}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7333477386060225
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.85622089, 10.81699383,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8295488870932575}
episode index:1322
target Thresh 18.999999999949985
target distance 3.0
model initialize at round 1322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.53857481, 5.01118135, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.1114859123149425}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7335114969290717
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.31099468, 3.42459142, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6540739449420339}
episode index:1323
target Thresh 18.999999999950976
target distance 13.0
model initialize at round 1323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.309950023624257}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.7334097034564957
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.0739258 , 2.25996974, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.27027632037897603}
episode index:1324
target Thresh 18.999999999951946
target distance 14.0
model initialize at round 1324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4., 4., 0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.416407864998762}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7333568813642939
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.87123942, 10.2438475 ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.2757551256764673}
episode index:1325
target Thresh 18.999999999952898
target distance 8.0
model initialize at round 1325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([10.02287784, 11.86096637,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 7.739770768217193}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7334504093572318
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.95104724, 7.97993812, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9811600711610369}
episode index:1326
target Thresh 18.999999999953832
target distance 2.0
model initialize at round 1326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.33935666, 10.97305298,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6611926826118677}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7336512756651766
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.33935666, 10.97305298,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6611926826118677}
episode index:1327
target Thresh 18.999999999954746
target distance 3.0
model initialize at round 1327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 4.99063444, 11.86736571,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.3308426355374878}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.733814188861212
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.67926361, 10.3951518 ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9095275688862119}
episode index:1328
target Thresh 18.99999999995564
target distance 2.0
model initialize at round 1328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.20516944, 4.93512154, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9573645002802339}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.734014479163047
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.20516944, 4.93512154, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9573645002802339}
episode index:1329
target Thresh 18.99999999995652
target distance 12.0
model initialize at round 1329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.132598004366178}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7340443787557815
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.77670528, 7.97197452, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7772107274345139}
episode index:1330
target Thresh 18.99999999995738
target distance 6.0
model initialize at round 1330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.75605595,  7.00018871,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.007620041999754}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7341709419573174
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.05254691,  3.05485201,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.0759599942144596}
episode index:1331
target Thresh 18.999999999958224
target distance 6.0
model initialize at round 1331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([4.00045013, 9.00054872, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 4.472828054341251}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7342973151240161
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.7680997 , 5.06652439, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.2412534875184189}
episode index:1332
target Thresh 18.99999999995905
target distance 13.0
model initialize at round 1332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.        ,  3.99999988,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 11.180339866174164}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.7341956194181753
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.56102952, 2.2428449 , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6113327807374854}
episode index:1333
target Thresh 18.99999999995986
target distance 4.0
model initialize at round 1333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([5.17483441, 9.1441616 , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.8640555872967974}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7343573918174121
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.38241557, 11.1048702 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.39653426898505495}
episode index:1334
target Thresh 18.999999999960657
target distance 5.0
model initialize at round 1334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.72071934, 6.00136089, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.2626256642568454}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.73448334133665
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.89093769, 2.30548048, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1296580560801945}
episode index:1335
target Thresh 18.999999999961435
target distance 2.0
model initialize at round 1335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.75871158, 4.99617362, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.24131876121791537}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7346820813506196
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.75871158, 4.99617362, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.24131876121791537}
episode index:1336
target Thresh 18.9999999999622
target distance 11.0
model initialize at round 1336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.186646103472004}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7346287816871481
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.91645536,  4.9214348 ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9252144630826423}
episode index:1337
target Thresh 18.99999999996295
target distance 7.0
model initialize at round 1337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.99999726, 8.        , 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.8309542459312915}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7347205202658573
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.47919818, 10.72817694,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5509252909258845}
episode index:1338
target Thresh 18.99999999996368
target distance 11.0
model initialize at round 1338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 5.97380105, 11.86775028,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 9.217416047156554}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7347496916006102
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.17568154,  9.48227232,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5467229218627346}
episode index:1339
target Thresh 18.999999999964402
target distance 14.0
model initialize at round 1339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4., 4., 0.]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.000000000000018}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7346717063156281
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.55479341,  3.93427394,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.5586731087426954}
episode index:1340
target Thresh 18.999999999965105
target distance 7.0
model initialize at round 1340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.13259928,  5.99507176,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.490584219246662}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7347632076532004
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.95215981, 11.8569292 ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.2809902232134793}
episode index:1341
target Thresh 18.999999999965798
target distance 7.0
model initialize at round 1341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.28671908,  4.99999976,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.162910844161938}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7348545726251429
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.31552477, 10.49219775,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5846490474619276}
episode index:1342
target Thresh 18.999999999966477
target distance 3.0
model initialize at round 1342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.51125669,  6.99891794,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.114017645590164}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.735014770262801
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.06146872,  8.32724202,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.3329650749648133}
episode index:1343
target Thresh 18.999999999967137
target distance 2.0
model initialize at round 1343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.89353625,  3.98605752,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.10737281686146603}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7352119318920698
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.89353625,  3.98605752,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.10737281686146603}
episode index:1344
target Thresh 18.999999999967788
target distance 11.0
model initialize at round 1344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.8674032 , 7.99505777, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.132598137603118}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7352406077326705
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.25494453,  7.88451881,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.27987964820680633}
episode index:1345
target Thresh 18.999999999968427
target distance 2.0
model initialize at round 1345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.60516691,  6.04202592,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.3970634024718902}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.735437308618456
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.60516691,  6.04202592,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.3970634024718902}
episode index:1346
target Thresh 18.999999999969052
target distance 8.0
model initialize at round 1346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.13259761,  4.99506161,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.899547034751833}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7354960086491773
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.97192352, 10.97568934,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9722275140845158}
episode index:1347
target Thresh 18.999999999969663
target distance 4.0
model initialize at round 1347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.39996007,  3.99994409,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.088126328424918}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7356551362392002
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.43775014,  5.9738592 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.5628572161110639}
episode index:1348
target Thresh 18.999999999970264
target distance 10.0
model initialize at round 1348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.86775037, 7.97380132, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.18972616623891}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7356833985084817
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.12628472,  8.36666932,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6457984026220308}
episode index:1349
target Thresh 18.999999999970854
target distance 6.0
model initialize at round 1349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86037651,  5.97486224,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.1160638608740285}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7358069663614384
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.48478388,  9.95273446,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5173796275484531}
episode index:1350
target Thresh 18.999999999971433
target distance 14.0
model initialize at round 1350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.165525060596456}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7357533863946935
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.70298895, 3.94052704, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.30290691100052625}
episode index:1351
target Thresh 18.999999999971998
target distance 13.0
model initialize at round 1351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 3.99505742, 11.86740305,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.162255834660009}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7357528971226744
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.22835219,  9.47361454,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5737825185180943}
episode index:1352
target Thresh 18.99999999997255
target distance 2.0
model initialize at round 1352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.56765175,  3.99183381,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.43242536600324893}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7359482017072105
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.56765175,  3.99183381,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.43242536600324893}
episode index:1353
target Thresh 18.999999999973095
target distance 8.0
model initialize at round 1353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 8.0236369, 10.1329575,  0.       ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 7.914003676741235}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7360378817650338
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.96113276, 5.95379232, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9545839114739061}
episode index:1354
target Thresh 18.999999999973628
target distance 12.0
model initialize at round 1354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 4.9950576 , 11.86740298,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.59850378579725}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7360657364187126
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.06755519,  6.99728324,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9995686916920518}
episode index:1355
target Thresh 18.99999999997415
target distance 1.0
model initialize at round 1355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.48667908, 11.39244309,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.6461500980867165}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7362603782060144
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.48667908, 11.39244309,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.6461500980867165}
episode index:1356
target Thresh 18.999999999974662
target distance 11.0
model initialize at round 1356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.00000012,  9.99840987,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.40077816713847}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7362324319406407
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.86069845, 3.03104586, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8612581842437179}
episode index:1357
target Thresh 18.999999999975163
target distance 2.0
model initialize at round 1357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.36271317, 6.98701976, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.6374190074345714}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7364266643177095
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.36271317, 6.98701976, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.6374190074345714}
episode index:1358
target Thresh 18.999999999975657
target distance 7.0
model initialize at round 1358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.13361923, 9.00102839, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 5.0755197392001365}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7365156623572109
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.15732066, 3.43485995, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.5866285605584612}
episode index:1359
target Thresh 18.99999999997614
target distance 11.0
model initialize at round 1359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 4.99999988, 11.        ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 9.0553852566176}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7365430632948159
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.40256438, 10.05496461,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.4062993871797915}
episode index:1360
target Thresh 18.99999999997661
target distance 11.0
model initialize at round 1360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.99983124, 8.99992371, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.40184062523281}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7364893361588822
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.32088699,  1.59217949,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.5189277728550117}
episode index:1361
target Thresh 18.999999999977074
target distance 4.0
model initialize at round 1361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 4.00176322, 10.96388718,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.2217413211575794}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7366460987608214
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.14250517, 10.1242127 ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.1890410514973424}
episode index:1362
target Thresh 18.999999999977526
target distance 13.0
model initialize at round 1362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.02619883, 11.86775027,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.183271063716607}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7366449584760556
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.92212913, 9.8785397 , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.1442791642614404}
episode index:1363
target Thresh 18.99999999997797
target distance 3.0
model initialize at round 1363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.1812228 , 7.99936092, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.016917043016154}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7368013771281993
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.20820826, 9.92269946, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.2158570880494974}
episode index:1364
target Thresh 18.999999999978407
target distance 2.0
model initialize at round 1364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.86438835,  5.50039911,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.5176790193815634}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7369941966321347
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.86438835,  5.50039911,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.5176790193815634}
episode index:1365
target Thresh 18.999999999978836
target distance 10.0
model initialize at round 1365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 5.99507859, 11.86739946,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.547034758901116}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7370211268963132
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.77704004,  5.97117894,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.996443514762127}
episode index:1366
target Thresh 18.999999999979252
target distance 13.0
model initialize at round 1366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.        ,  6.99999225,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.08304276720158}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7369672858607555
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.18240335, 2.88845563, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9069864333448014}
episode index:1367
target Thresh 18.999999999979664
target distance 1.0
model initialize at round 1367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.06079245,  7.78195989,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7843194458505062}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7371595612365883
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.06079245,  7.78195989,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7843194458505062}
episode index:1368
target Thresh 18.99999999998007
target distance 12.0
model initialize at round 1368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.305877825105556}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7371580508855207
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.89072187, 6.04242331, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.11722391890708618}
episode index:1369
target Thresh 18.999999999980464
target distance 14.0
model initialize at round 1369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.00000000000002}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7371042278055232
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.53029997, 4.97225424, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.4705188007214026}
episode index:1370
target Thresh 18.99999999998085
target distance 13.0
model initialize at round 1370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14., 11.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.045361017187279}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7371027600176454
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.84191127, 9.80653548, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.2498410777871351}
episode index:1371
target Thresh 18.999999999981227
target distance 11.0
model initialize at round 1371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 5.99505742, 11.86740305,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 9.196531139576582}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7371294933831574
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.07607507,  9.34983878,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6545968450160249}
episode index:1372
target Thresh 18.9999999999816
target distance 4.0
model initialize at round 1372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.02230835, 8.00323868, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.0033628885979082}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7372845338104093
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.50442266, 6.10286236, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5061398621257764}
episode index:1373
target Thresh 18.999999999981963
target distance 13.0
model initialize at round 1373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.99999988, 7.99999785, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.529963163289903}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7372307753660706
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.32078752,  2.07172498,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.3287082407873025}
episode index:1374
target Thresh 18.99999999998232
target distance 7.0
model initialize at round 1374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.00379008,  5.99999952,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.000001913307845}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7373181529839863
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.93350821, 11.31763771,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.3245225298171046}
episode index:1375
target Thresh 18.99999999998267
target distance 12.0
model initialize at round 1375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.02539619, 11.86754235,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 12.74389231753934}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7372898238728742
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.77831168, 3.24867257, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0817864749635415}
episode index:1376
target Thresh 18.999999999983014
target distance 1.0
model initialize at round 1376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.95888722, 4.94660223, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9474946087847349}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7374806083145061
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.95888722, 4.94660223, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9474946087847349}
episode index:1377
target Thresh 18.999999999983352
target distance 3.0
model initialize at round 1377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.05499375,  9.9993664 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0021436566609603}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7376348313853954
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.34734982, 11.43667376,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7852618844673487}
episode index:1378
target Thresh 18.999999999983682
target distance 7.0
model initialize at round 1378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.24020529,  5.99999952,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.005767009032951}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7377216625446519
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.83382438, 11.32194342,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.36230084357155995}
episode index:1379
target Thresh 18.999999999984006
target distance 3.0
model initialize at round 1379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.98737013, 7.99994028, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.0001394726763506}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7378754874268659
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.69175869, 9.66254854, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7307415920357222}
episode index:1380
target Thresh 18.999999999984322
target distance 7.0
model initialize at round 1380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([10.97456402, 11.86755174,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.099769881246068}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7379620185728276
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.38997558, 10.78241801,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.44656787820367416}
episode index:1381
target Thresh 18.99999999998463
target distance 12.0
model initialize at round 1381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.        , 10.99507165,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 13.450327653279047}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7379080811001187
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.6798691 , 1.63467883, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.771804090867981}
episode index:1382
target Thresh 18.999999999984936
target distance 10.0
model initialize at round 1382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12.00494267, 11.86740304,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.05180074650232}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7379634666163153
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.22709203, 11.74660785,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7803807225241322}
episode index:1383
target Thresh 18.999999999985235
target distance 12.0
model initialize at round 1383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.778321394833641}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7379348349902152
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.00305472, 10.05192829,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9982967665711308}
episode index:1384
target Thresh 18.999999999985526
target distance 7.0
model initialize at round 1384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.94638395,  6.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.088776138392693}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7380210733765039
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.73469061, 11.3924166 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.4736875123639054}
episode index:1385
target Thresh 18.999999999985814
target distance 5.0
model initialize at round 1385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.01653636, 9.00077295, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 3.000818516120894}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7381397450407344
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.88067922, 5.50200653, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5120888047528174}
episode index:1386
target Thresh 18.999999999986095
target distance 12.0
model initialize at round 1386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.305877894274122}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7380619582092159
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.42681083,  2.96793096,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.4280139063277895}
episode index:1387
target Thresh 18.99999999998637
target distance 7.0
model initialize at round 1387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.99999821, 6.        , 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.403125354474806}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7381479186139642
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.06892117, 11.86622262,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8689601557651145}
episode index:1388
target Thresh 18.99999999998664
target distance 6.0
model initialize at round 1388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.09685066,  7.00029719,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 4.001469436269286}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7382662426466395
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.46417189,  3.07687186,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.4704942407166599}
episode index:1389
target Thresh 18.999999999986905
target distance 1.0
model initialize at round 1389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.03760246, 7.65908468, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.6601564644180551}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7384545403138002
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.03760246, 7.65908468, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.6601564644180551}
episode index:1390
target Thresh 18.999999999987164
target distance 14.0
model initialize at round 1390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.369316876852997}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7383767508597462
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.61793314, 2.90541046, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9827223394963948}
episode index:1391
target Thresh 18.999999999987416
target distance 6.0
model initialize at round 1391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 8.02242472, 10.13338693,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 5.098824788309721}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7384946554927493
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.9669977 , 7.96788541, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9684478915812043}
episode index:1392
target Thresh 18.999999999987665
target distance 4.0
model initialize at round 1392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.32810195, 5.00529464, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 2.114864907266022}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7386464899109167
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.57054253, 3.01995079, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.429920638464178}
episode index:1393
target Thresh 18.99999999998791
target distance 8.0
model initialize at round 1393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.86735869, 3.99532759, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 6.066992820290385}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7387316610085416
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.35500912, 9.9517827 , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6467906506525672}
episode index:1394
target Thresh 18.999999999988148
target distance 8.0
model initialize at round 1394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.86775027, 4.97380118, 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 9.337240408700563}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7387290518541448
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.85424944, 11.07905553,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8578996963551125}
episode index:1395
target Thresh 18.999999999988386
target distance 8.0
model initialize at round 1395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 8.02153031, 10.13861645,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 7.306638993995538}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7388140417883468
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.99837889, 6.12158684, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.12159764763470265}
episode index:1396
target Thresh 18.999999999988617
target distance 3.0
model initialize at round 1396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.91530645,  6.99975872,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0038205093599586}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7389652128393215
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.92912678,  8.55759358,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5620797258670032}
episode index:1397
target Thresh 18.99999999998884
target distance 10.0
model initialize at round 1397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 9.620595846702555}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7389901167911532
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.02808925,  9.74331069,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.005236247346608}
episode index:1398
target Thresh 18.99999999998906
target distance 12.0
model initialize at round 1398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.174760518110984}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7389610583060229
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.91509261, 5.9113187 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9152655533861125}
episode index:1399
target Thresh 18.999999999989278
target distance 12.0
model initialize at round 1399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.        , 10.99206614,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 13.448317870835535}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7389071007152964
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.83115644, 2.7885641 , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.1457112964831107}
episode index:1400
target Thresh 18.99999999998949
target distance 6.0
model initialize at round 1400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.42442998, 6.00144529, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 4.042628511361018}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.739023869380025
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.45613182, 2.077582  , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.46268261310672054}
episode index:1401
target Thresh 18.999999999989697
target distance 10.0
model initialize at round 1401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 5.97380161, 11.86775011,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.386951138544243}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7390486604414515
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.48498012,  6.43032432,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7679686519882009}
episode index:1402
target Thresh 18.999999999989903
target distance 6.0
model initialize at round 1402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 8.02043821, 11.86585422,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 4.112618010685022}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7391651617526122
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.18125472, 11.66143876,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6858239593467216}
episode index:1403
target Thresh 18.999999999990102
target distance 12.0
model initialize at round 1403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([12.82337025,  9.14611083,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.446687179897484}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7391360820762171
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.73253625, 2.88363511, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.29168072188697897}
episode index:1404
target Thresh 18.999999999990298
target distance 12.0
model initialize at round 1404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([3.99999988, 8.        , 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.440306623092326}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.739107043794379
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.91876816,  4.5199154 ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.4869084500850735}
episode index:1405
target Thresh 18.99999999999049
target distance 12.0
model initialize at round 1405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4., 5., 0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.049875621120906}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7390532126332799
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.06965525,  4.90184783,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9045337767806229}
episode index:1406
target Thresh 18.999999999990678
target distance 10.0
model initialize at round 1406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.13224961,  7.97380036,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.343118904709433}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7390503971947523
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.38075759, 5.92528415, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.3880190771112901}
episode index:1407
target Thresh 18.999999999990862
target distance 2.0
model initialize at round 1407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.20153737,  6.91472799,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9366667496473348}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7392357307194719
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.20153737,  6.91472799,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9366667496473348}
episode index:1408
target Thresh 18.999999999991044
target distance 4.0
model initialize at round 1408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.77623165, 8.0141598 , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.026551743824232}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7393853150127868
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.35342395, 6.04246604, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.3559660876027225}
episode index:1409
target Thresh 18.99999999999122
target distance 12.0
model initialize at round 1409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.        ,  9.99894822,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.805591457221368}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7393314392087273
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.52354118, 1.55037375, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6901153032124189}
episode index:1410
target Thresh 18.999999999991395
target distance 2.0
model initialize at round 1410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.78924632,  8.61569893,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.6507705357236164}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7395161795069494
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.78924632,  8.61569893,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.6507705357236164}
episode index:1411
target Thresh 18.999999999991566
target distance 4.0
model initialize at round 1411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.02322299, 11.86635427,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.2009091319525256}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7396652473684884
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.09492366, 10.36258417,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6444450663915945}
episode index:1412
target Thresh 18.999999999991733
target distance 2.0
model initialize at round 1412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.83074236,  6.07285428,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.18427125431088623}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.739849489939353
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.83074236,  6.07285428,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.18427125431088623}
episode index:1413
target Thresh 18.999999999991896
target distance 12.0
model initialize at round 1413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.13224952,  7.97379643,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.192851609725784}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.739820131952192
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.43727884, 3.93861221, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.094371043456816}
episode index:1414
target Thresh 18.999999999992056
target distance 7.0
model initialize at round 1414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.00022495,  5.99999988,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.4032648567843875}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7399032095974555
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.97916789, 11.48958729,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.4900302999201264}
episode index:1415
target Thresh 18.999999999992212
target distance 3.0
model initialize at round 1415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.4079317 ,  6.99914753,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1628630734840235}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7400515830370054
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.0177984 ,  8.76991224,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7701179431292493}
episode index:1416
target Thresh 18.99999999999237
target distance 3.0
model initialize at round 1416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.45776129, 8.99900711, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.138424169891763}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.740199747057445
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.70906295, 10.77938652,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8319180944627704}
episode index:1417
target Thresh 18.99999999999252
target distance 1.0
model initialize at round 1417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.67225399, 3.17348163, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0653910290190636}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7403829630327219
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.67225399, 3.17348163, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0653910290190636}
episode index:1418
target Thresh 18.999999999992667
target distance 2.0
model initialize at round 1418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.17097116,  4.99101472,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.17120710024271185}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7405659207754753
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.17097116,  4.99101472,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.17120710024271185}
episode index:1419
target Thresh 18.999999999992813
target distance 14.0
model initialize at round 1419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 12.041594578792317}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7405620658246652
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.17400389, 11.38037304,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.4182834018546023}
episode index:1420
target Thresh 18.999999999992955
target distance 7.0
model initialize at round 1420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 7.02623593, 10.13225109,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 7.335876169082044}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7406442705637049
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.37301073, 4.92840423, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.1202901253836217}
episode index:1421
target Thresh 18.999999999993094
target distance 5.0
model initialize at round 1421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.85467584, 4.98647007, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.1323846215132822}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.740758093158245
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.16490791, 8.53426953, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.5591409027284026}
episode index:1422
target Thresh 18.999999999993232
target distance 13.0
model initialize at round 1422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4., 8., 0.]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.704699910719636}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7407282823381014
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.9591507,  4.0350442,  0.       ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9597906861331806}
episode index:1423
target Thresh 18.999999999993367
target distance 7.0
model initialize at round 1423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.99890125, 6.        , 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 5.098804144425692}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7408101971679202
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.7805978 , 11.87327927,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9004187955937745}
episode index:1424
target Thresh 18.999999999993495
target distance 2.0
model initialize at round 1424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([12.93046761,  9.99089479,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0695711511903505}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7409569970295568
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.26596951,  9.99146809,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.2661063253100775}
episode index:1425
target Thresh 18.999999999993626
target distance 2.0
model initialize at round 1425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 2.07892621, 10.52679598,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.5288346282713183}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7411035910007843
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.94806704, 8.75830662, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.2472098726526627}
episode index:1426
target Thresh 18.99999999999375
target distance 13.0
model initialize at round 1426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4., 8., 0.]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.045361017187277}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7410993781764145
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.91412527,  6.33823175,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6673167850163707}
episode index:1427
target Thresh 18.999999999993875
target distance 11.0
model initialize at round 1427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 5.97380104, 11.86775028,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 9.217416057507998}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7411222644224394
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.2003873 ,  9.39523637,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6370981992773631}
episode index:1428
target Thresh 18.999999999993996
target distance 9.0
model initialize at round 1428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([5.17664065, 9.14612991, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.040011553220932}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7411736178063285
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.05886748, 10.96609494,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.06793329596479035}
episode index:1429
target Thresh 18.999999999994113
target distance 2.0
model initialize at round 1429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.97983956, 5.73008156, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.7301990178370477}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7413196502414291
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.48913896, 3.77575064, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.5380935790282649}
episode index:1430
target Thresh 18.999999999994234
target distance 2.0
model initialize at round 1430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.79690953,  4.99042714,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.2033159618685215}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7415004191790661
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.79690953,  4.99042714,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.2033159618685215}
episode index:1431
target Thresh 18.999999999994348
target distance 6.0
model initialize at round 1431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.52134538,  6.00024664,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 4.028781883044002}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.741612849053941
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.15848731,  2.10664558,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.19102750186773876}
episode index:1432
target Thresh 18.999999999994458
target distance 13.0
model initialize at round 1432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.704699910719649}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7415826497846038
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.15046843, 9.43978552, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.46481399837680965}
episode index:1433
target Thresh 18.999999999994568
target distance 4.0
model initialize at round 1433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([5.87693005, 9.84367749, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 2.0578284962444617}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7417279896383105
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.09214247, 8.63128481, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.38005411366968084}
episode index:1434
target Thresh 18.999999999994674
target distance 4.0
model initialize at round 1434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.42206037, 6.99996245, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 2.044085409520662}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7418731269277612
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.10322821, 8.96989465, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8972769767841363}
episode index:1435
target Thresh 18.99999999999478
target distance 11.0
model initialize at round 1435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 5.99999988, 11.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.000000119209318}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7418953468515581
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.3819103 , 10.58959067,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.5606168842508639}
episode index:1436
target Thresh 18.999999999994884
target distance 11.0
model initialize at round 1436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.332858443941447}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7419175358499217
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.97668125, 10.84782216,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.2933323928515446}
episode index:1437
target Thresh 18.999999999994987
target distance 2.0
model initialize at round 1437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.84812486,  7.86673224,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8799379711116432}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7420970090516951
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.84812486,  7.86673224,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8799379711116432}
episode index:1438
target Thresh 18.999999999995087
target distance 13.0
model initialize at round 1438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.00494233, 11.86740303,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.66471439093847}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7421190270700747
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.94337848, 8.17049336, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9586610181686451}
episode index:1439
target Thresh 18.999999999995183
target distance 11.0
model initialize at round 1439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.184084079649052}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7420643752674491
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.31304249, 5.99669872, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.2105035967812958}
episode index:1440
target Thresh 18.99999999999528
target distance 12.0
model initialize at round 1440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 12.31822794829574}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7420097993174294
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.33246727, 10.49909543,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6011986928923493}
episode index:1441
target Thresh 18.99999999999537
target distance 1.0
model initialize at round 1441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.81507761, 2.26128166, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.1000255854432026}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7421887106909956
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.81507761, 2.26128166, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.1000255854432026}
episode index:1442
target Thresh 18.999999999995463
target distance 13.0
model initialize at round 1442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.        , 10.99999774,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.083045036340838}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7422106041260678
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.86663111, 6.97908317, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.3075371245045788}
episode index:1443
target Thresh 18.999999999995552
target distance 1.0
model initialize at round 1443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.10101929,  8.17368257,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8324694531529103}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7423891286384459
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.10101929,  8.17368257,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8324694531529103}
episode index:1444
target Thresh 18.99999999999564
target distance 8.0
model initialize at round 1444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.86740243, 4.99506136, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 6.1108154347105135}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7424687036359279
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.82449063, 11.10528096,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8311852264665965}
episode index:1445
target Thresh 18.999999999995726
target distance 9.0
model initialize at round 1445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([5.8538774 , 9.82336363, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 8.72108877848465}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7425185221327218
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.01223086, 2.02834353, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.030869880841431895}
episode index:1446
target Thresh 18.99999999999581
target distance 12.0
model initialize at round 1446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.        ,  9.99957514,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 12.206311978390422}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7424638586283379
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.86003161, 2.30262729, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.1072411905464419}
episode index:1447
target Thresh 18.999999999995897
target distance 1.0
model initialize at round 1447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.64775586,  4.63728982,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.7281581219866557}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7426417150795613
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.64775586,  4.63728982,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.7281581219866557}
episode index:1448
target Thresh 18.999999999995975
target distance 6.0
model initialize at round 1448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 4.99999356, 10.99999821,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.0000064373020106}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7427520382575603
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.9344032 , 11.03146999,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.07275507062223864}
episode index:1449
target Thresh 18.999999999996056
target distance 2.0
model initialize at round 1449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.14503062,  7.75735623,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7711175875366294}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7429294506449688
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.14503062,  7.75735623,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7711175875366294}
episode index:1450
target Thresh 18.999999999996135
target distance 4.0
model initialize at round 1450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([12.9887751,  9.0007002,  0.       ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.240485545393826}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7430721595004858
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.96618139, 10.96534297,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.04842322334880361}
episode index:1451
target Thresh 18.99999999999621
target distance 9.0
model initialize at round 1451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.00000179,  4.        ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.899496201017273}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7430933087966287
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.97952952, 10.86436881,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9888750645194726}
episode index:1452
target Thresh 18.999999999996284
target distance 11.0
model initialize at round 1452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([11.02619874, 11.86775022,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 9.217415829777398}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7431144289815589
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.87967874, 9.86447654, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.18122861718352232}
episode index:1453
target Thresh 18.99999999999636
target distance 2.0
model initialize at round 1453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.39358914, 3.08349168, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.4023472026222055}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7432911040647903
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.39358914, 3.08349168, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.4023472026222055}
episode index:1454
target Thresh 18.99999999999643
target distance 1.0
model initialize at round 1454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.99955177, 7.55045962, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5504598058273396}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7434675362956735
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.99955177, 7.55045962, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5504598058273396}
episode index:1455
target Thresh 18.999999999996504
target distance 5.0
model initialize at round 1455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.53028417,  6.00062597,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 3.037167951251145}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.743576761888877
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.88150294,  2.5959518 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.42106591458922066}
episode index:1456
target Thresh 18.99999999999657
target distance 12.0
model initialize at round 1456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4., 4., 0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.000000000000018}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7434989805902057
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.10405534,  3.96476916,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.10985774903432634}
episode index:1457
target Thresh 18.99999999999664
target distance 5.0
model initialize at round 1457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 6.01449479, 11.85482806,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 3.1333544092184327}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7436080347873316
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.64353403, 11.12480738,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.37768355738533654}
episode index:1458
target Thresh 18.999999999996707
target distance 10.0
model initialize at round 1458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.13224967,  4.9737985 ,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.343118570994038}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7435303386769392
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.46865914, 2.99832392, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.4686621332707049}
episode index:1459
target Thresh 18.99999999999677
target distance 7.0
model initialize at round 1459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.1325987 ,  5.99506819,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.490586599532358}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.743608314472366
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.9479285 , 11.83900826,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.2659001954679137}
episode index:1460
target Thresh 18.999999999996835
target distance 12.0
model initialize at round 1460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.565977982391892}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.743530724530718
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.28717728,  2.92821221,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.9716216871717834}
episode index:1461
target Thresh 18.9999999999969
target distance 10.0
model initialize at round 1461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([12.82335951,  9.14612963,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 8.897487636971952}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.743579271401764
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.74751437, 8.89717225, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.167773853431062}
episode index:1462
target Thresh 18.99999999999696
target distance 2.0
model initialize at round 1462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.80588996, 4.07447779, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.20790778606153634}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7437545418929453
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.80588996, 4.07447779, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.20790778606153634}
episode index:1463
target Thresh 18.99999999999702
target distance 5.0
model initialize at round 1463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.91173195,  7.99999166,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.0013066015414873}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7438629745829091
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.99025737, 11.33373284,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.3338750207880851}
episode index:1464
target Thresh 18.99999999999708
target distance 8.0
model initialize at round 1464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 8.97509108, 11.86741063,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 7.1593569856862445}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7439404571941153
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.96222879,  7.41283012,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5883834891708534}
episode index:1465
target Thresh 18.999999999997137
target distance 2.0
model initialize at round 1465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.88683331, 9.89637136, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9034867585969778}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7441151226394126
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.88683331, 9.89637136, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9034867585969778}
episode index:1466
target Thresh 18.999999999997193
target distance 2.0
model initialize at round 1466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.9583086 , 11.80337874,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8044598055939457}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7442895499586768
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.9583086 , 11.80337874,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8044598055939457}
episode index:1467
target Thresh 18.999999999997247
target distance 8.0
model initialize at round 1467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([10.97380762, 10.13225153,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 8.190191512599158}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7443373814982146
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.16856482,  2.95112004,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.17550883177043025}
episode index:1468
target Thresh 18.999999999997303
target distance 4.0
model initialize at round 1468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.99790037,  9.00056839,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.0005694916940135}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7444773832807209
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.38368833,  7.01566756,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.38400808252170876}
episode index:1469
target Thresh 18.999999999997357
target distance 9.0
model initialize at round 1469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.86774851, 3.97380815, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.342606374940151}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7445250219655639
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.67031156, 11.40680761,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.7840982192795805}
episode index:1470
target Thresh 18.999999999997407
target distance 6.0
model initialize at round 1470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.94769466,  5.99999988,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 4.110733039351198}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7446324148806112
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.64562517,  9.98034751,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.3549193393859944}
episode index:1471
target Thresh 18.99999999999746
target distance 9.0
model initialize at round 1471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([10.02868284, 10.14038974,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 9.333100696494704}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7446522168660863
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.05789266, 4.04019678, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.07047936587119187}
episode index:1472
target Thresh 18.99999999999751
target distance 2.0
model initialize at round 1472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.36345127, 8.97713828, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.6369591372849732}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7448255690610177
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.36345127, 8.97713828, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.6369591372849732}
episode index:1473
target Thresh 18.99999999999756
target distance 8.0
model initialize at round 1473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.00166929,  5.        ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.7089506267829515}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.744901925527055
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.0110054 , 11.14516528,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.14558186313467644}
episode index:1474
target Thresh 18.99999999999761
target distance 9.0
model initialize at round 1474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4., 4., 0.]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 9.899494936611692}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7449215045182231
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.02388326, 10.85679089,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9865661360439575}
episode index:1475
target Thresh 18.999999999997655
target distance 10.0
model initialize at round 1475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.13224913,  7.97377695,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.95936129567374}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7448899434014045
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.32026249, 4.08135977, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.330435279455622}
episode index:1476
target Thresh 18.9999999999977
target distance 13.0
model initialize at round 1476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        ,  8.99998128,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.529955124023777}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7448584250213721
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.96911669, 3.99536236, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9958413528383905}
episode index:1477
target Thresh 18.999999999997748
target distance 12.0
model initialize at round 1477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.837958482002342}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7448269492913806
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.13514173, 9.88921966, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.17474429975895292}
episode index:1478
target Thresh 18.99999999999779
target distance 13.0
model initialize at round 1478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 4.99505742, 11.86740305,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.033302689123065}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7448465260244493
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.09025684,  7.11240894,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9166615406070588}
episode index:1479
target Thresh 18.999999999997836
target distance 3.0
model initialize at round 1479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.17105663,  5.00345731,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0179326788491554}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.744985143236595
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.92003515,  3.31041622,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.694204698962801}
episode index:1480
target Thresh 18.99999999999788
target distance 6.0
model initialize at round 1480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.9998225, 5.       , 0.       ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.000000003938417}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7450915003309659
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.71977207, 8.96085942, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7208354981797284}
episode index:1481
target Thresh 18.99999999999792
target distance 7.0
model initialize at round 1481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.80446559,  3.99999988,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.064303118954849}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7451672651755469
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.5594354 ,  9.75103509,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8707186008624684}
episode index:1482
target Thresh 18.99999999999796
target distance 1.0
model initialize at round 1482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.60916123,  8.6193676 ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7323736539765189}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7453391011396902
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.60916123,  8.6193676 ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7323736539765189}
episode index:1483
target Thresh 18.999999999998003
target distance 12.0
model initialize at round 1483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.82024025163806}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.745283899879683
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.60563863,  2.92742795,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.609971190076108}
episode index:1484
target Thresh 18.99999999999804
target distance 2.0
model initialize at round 1484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.06014739, 11.79816098,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8004240466944837}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.74545542587303
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.06014739, 11.79816098,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8004240466944837}
episode index:1485
target Thresh 18.99999999999808
target distance 10.0
model initialize at round 1485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([12.00000024, 10.99994731,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 8.944248559461359}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7455018934531963
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.03255855, 7.99887638, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9994068606807107}
episode index:1486
target Thresh 18.999999999998117
target distance 13.0
model initialize at round 1486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.045361017187279}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7454948927787993
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.70721034, 6.37258175, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.6923723336919674}
episode index:1487
target Thresh 18.999999999998153
target distance 7.0
model initialize at round 1487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.99999881, 6.        , 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.403124982127456}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7455700810228997
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.06087399, 11.86552968,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8676677140104477}
episode index:1488
target Thresh 18.99999999999819
target distance 12.0
model initialize at round 1488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4., 9., 0.]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.19803902718559}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7455890271991771
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.9540637 , 11.16542662,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.17168608341668157}
episode index:1489
target Thresh 18.999999999998227
target distance 11.0
model initialize at round 1489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([12.00000012, 10.99841225,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.400779630679914}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7455819821410736
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.12044188, 4.03952257, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.1267607147497039}
episode index:1490
target Thresh 18.999999999998263
target distance 13.0
model initialize at round 1490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4., 8., 0.]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.401754250991392}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7455502955642478
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.0068625 ,  4.19483408,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8051951691542847}
episode index:1491
target Thresh 18.9999999999983
target distance 3.0
model initialize at round 1491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.33644983,  9.99880517,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.2011202782682484}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7456873261972476
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.69409028, 11.41135573,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.5126346637142791}
episode index:1492
target Thresh 18.99999999999833
target distance 11.0
model initialize at round 1492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.972155793073503}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7457061430835858
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.04493156, 10.68966544,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0042227172304028}
episode index:1493
target Thresh 18.999999999998362
target distance 13.0
model initialize at round 1493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 3.9950578 , 11.86740301,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 13.52792603402066}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.745674437028037
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.4276575 ,  4.05478369,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.4311521686612491}
episode index:1494
target Thresh 18.999999999998398
target distance 12.0
model initialize at round 1494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.536241300558661}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.745619417626205
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.02387472, 6.83389561, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.16781141838920388}
episode index:1495
target Thresh 18.99999999999843
target distance 7.0
model initialize at round 1495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.86740304, 5.99505756, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.168891149417333}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7456654649740484
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.01357763, 11.07832203,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.07949020195822051}
episode index:1496
target Thresh 18.99999999999846
target distance 13.0
model initialize at round 1496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.02465091, 11.86733918,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 14.148237760221445}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7456338496307751
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.77377543, 2.99902368, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.2262266783922374}
episode index:1497
target Thresh 18.99999999999849
target distance 1.0
model initialize at round 1497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.48173171, 8.49458826, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.716393439637062}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7458036534694727
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.48173171, 8.49458826, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.716393439637062}
episode index:1498
target Thresh 18.99999999999852
target distance 3.0
model initialize at round 1498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.02953095, 9.02008617, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0205135323365384}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7459398751816346
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.65308589, 7.42781603, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6691366736365977}
episode index:1499
target Thresh 18.99999999999855
target distance 12.0
model initialize at round 1499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.182309676547058}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7459081401289094
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.15936948,  6.89799958,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9120317252861481}
episode index:1500
target Thresh 18.99999999999858
target distance 9.0
model initialize at round 1500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([12.14612789,  9.82336103,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 8.721084106944303}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7459538417344198
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.04209258,  2.06671285,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.07888212330154529}
episode index:1501
target Thresh 18.999999999998607
target distance 3.0
model initialize at round 1501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.56244552,  3.02027261,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1650326882059403}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7460896913737444
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.12959495,  1.91696846,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.15391259920375802}
episode index:1502
target Thresh 18.999999999998632
target distance 1.0
model initialize at round 1502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.37297522, 11.70528845,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7978360200293817}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.746258627041493
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.37297522, 11.70528845,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7978360200293817}
episode index:1503
target Thresh 18.99999999999866
target distance 6.0
model initialize at round 1503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.13242383,  6.97447497,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 4.555445443189758}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7463625109330878
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.01265087, 11.14525586,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.1458057214852272}
episode index:1504
target Thresh 18.99999999999869
target distance 11.0
model initialize at round 1504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.322711635547794}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.7462644208522274
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.41972841,  2.02235539,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.420323333367482}
episode index:1505
target Thresh 18.999999999998714
target distance 7.0
model initialize at round 1505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.98666763, 5.        , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 5.380227530898209}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7463381994572392
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.68193653, 10.73287487,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.798917986929856}
episode index:1506
target Thresh 18.99999999999874
target distance 5.0
model initialize at round 1506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([5.85425847, 9.82370391, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 4.0149838308721755}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7464733433195769
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.81441977, 7.91481495, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.2248126228375646}
episode index:1507
target Thresh 18.999999999998764
target distance 5.0
model initialize at round 1507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.1330269 ,  7.97658549,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.2287436025367975}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7466083079460228
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.0154771 , 10.14930627,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8508345121018855}
episode index:1508
target Thresh 18.99999999999879
target distance 3.0
model initialize at round 1508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.4021295 ,  6.00302911,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0806366305326518}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7467430936929108
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.40725883,  4.23470855,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8669087411956378}
episode index:1509
target Thresh 18.999999999998813
target distance 11.0
model initialize at round 1509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.86740328, 7.99505095, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.924511833720217}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7466879131217825
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.87076761,  1.99928991,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.1292343435687801}
episode index:1510
target Thresh 18.999999999998835
target distance 12.0
model initialize at round 1510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.174760518110984}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7466559140370518
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.93025108, 5.90765068, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9103266873620617}
episode index:1511
target Thresh 18.99999999999886
target distance 12.0
model initialize at round 1511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.00187867, 11.86678873,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 13.36628293028988}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7466008641145995
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.8631937 , 2.41109159, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0449480790124572}
episode index:1512
target Thresh 18.99999999999888
target distance 13.0
model initialize at round 1512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4., 4., 0.]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.04536101718728}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.746545886961377
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.07279176,  5.91244654,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9153454738812176}
episode index:1513
target Thresh 18.999999999998902
target distance 4.0
model initialize at round 1513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 6.97383733, 11.867732  ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 2.2041538034661197}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7466802688061847
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.9129131 , 11.02767164,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.09137749838029455}
episode index:1514
target Thresh 18.999999999998924
target distance 2.0
model initialize at round 1514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.82568309, 11.86987655,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8871705611862442}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7468474765495469
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.82568309, 11.86987655,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8871705611862442}
episode index:1515
target Thresh 18.999999999998945
target distance 4.0
model initialize at round 1515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.99602026, 9.00006385, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.9999401146048748}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7469814821718757
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.15611301, 10.96869513,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.15922080794820068}
episode index:1516
target Thresh 18.999999999998966
target distance 10.0
model initialize at round 1516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([12.00494237, 11.867403  ,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 8.219872036803865}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7470259942139509
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.21981205, 10.57862117,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6189667186757225}
episode index:1517
target Thresh 18.999999999998987
target distance 7.0
model initialize at round 1517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 8.02602607, 10.13229568,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 7.183411236588957}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.747098687893652
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.52877139, 5.95124792, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0615691305821664}
episode index:1518
target Thresh 18.99999999999901
target distance 7.0
model initialize at round 1518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.98469579,  4.        ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.096040208158977}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7471712858608056
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.57373396,  9.72095644,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8375445873518733}
episode index:1519
target Thresh 18.999999999999027
target distance 12.0
model initialize at round 1519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([12.02608315, 11.86772094,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.616905452562454}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7471887922105682
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.02711966, 6.09767197, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.10136710552699023}
episode index:1520
target Thresh 18.999999999999048
target distance 2.0
model initialize at round 1520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.81374961, 5.99695444, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8137553105489406}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7473550060223956
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.81374961, 5.99695444, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8137553105489406}
episode index:1521
target Thresh 18.999999999999066
target distance 13.0
model initialize at round 1521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4., 8., 0.]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.045361017187277}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7473469487849466
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.90979845,  6.35724229,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6490560775654328}
episode index:1522
target Thresh 18.999999999999083
target distance 13.0
model initialize at round 1522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.045361017187282}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7472918427327497
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.49932205, 5.93833679, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0635573998212309}
episode index:1523
target Thresh 18.9999999999991
target distance 8.0
model initialize at round 1523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.86740303, 4.99505762, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.28956038528147}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7473359466745261
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.99786784, 11.48483678,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.48484146813187107}
episode index:1524
target Thresh 18.99999999999912
target distance 13.0
model initialize at round 1524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([14.        ,  9.99995136,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 12.52994079609655}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7473038164118502
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.14419564, 4.03945745, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.14949672794301133}
episode index:1525
target Thresh 18.999999999999137
target distance 8.0
model initialize at round 1525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.86774895, 4.97380641, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.3923003434556405}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.747375946938448
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.01111142, 11.11621047,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.11674047020122931}
episode index:1526
target Thresh 18.999999999999154
target distance 6.0
model initialize at round 1526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([1.75755858, 9.0007447 , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 4.189226519262945}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7474775343995229
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.72997657, 5.03571618, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.27237528677776585}
episode index:1527
target Thresh 18.999999999999172
target distance 1.0
model initialize at round 1527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.45013571,  1.85185719,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.47388653601316144}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7476427977932405
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.45013571,  1.85185719,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.47388653601316144}
episode index:1528
target Thresh 18.999999999999186
target distance 13.0
model initialize at round 1528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([3.99999988, 9.99999976, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.529964076627707}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7476105508987346
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.69997933,  4.00605963,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7000055609265329}
episode index:1529
target Thresh 18.999999999999204
target distance 5.0
model initialize at round 1529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.39727715,  3.99999165,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.0599550503116313}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7477117858327877
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.63970794,  7.80973054,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8862696655031174}
episode index:1530
target Thresh 18.99999999999922
target distance 9.0
model initialize at round 1530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 5.94478274, 10.04568074,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.11946738512096}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7477554138302843
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.82959579, 10.93684213,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8319964453897738}
episode index:1531
target Thresh 18.999999999999233
target distance 5.0
model initialize at round 1531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.19471157,  9.0002284 ,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.0065400506369824}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7478564220457997
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.77377802,  5.14834213,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8811909593209778}
episode index:1532
target Thresh 18.99999999999925
target distance 12.0
model initialize at round 1532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.        ,  7.99999976,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.198038980427853}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7478480955412852
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.02889248, 6.0300716 , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.041702236328210116}
episode index:1533
target Thresh 18.999999999999265
target distance 3.0
model initialize at round 1533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.24823648,  5.00243139,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.032710049195462}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7479798764438006
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.83915949,  3.30021191,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.718034153770049}
episode index:1534
target Thresh 18.99999999999928
target distance 11.0
model initialize at round 1534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.00467863, 11.8673597 ,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.324524998952358}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.747996684952632
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.55135531, 5.97940482, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.077272412214381}
episode index:1535
target Thresh 18.999999999999293
target distance 8.0
model initialize at round 1535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.02617563, 11.86774387,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.088330819660145}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7480678947931577
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.11173205, 10.73976768,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.28320471511651146}
episode index:1536
target Thresh 18.999999999999307
target distance 8.0
model initialize at round 1536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.00494438, 11.86740266,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.067268279617635}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7481390119728629
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.09078693, 11.61345544,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.6201369552274127}
episode index:1537
target Thresh 18.99999999999932
target distance 2.0
model initialize at round 1537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.37951201, 8.08737684, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6266099701732619}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7483027707427116
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.37951201, 8.08737684, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6266099701732619}
episode index:1538
target Thresh 18.999999999999336
target distance 5.0
model initialize at round 1538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([10.01848783, 11.86366429,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.1396153904669877}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7484029638741327
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.3712649 , 11.57229593,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8501943671954514}
episode index:1539
target Thresh 18.99999999999935
target distance 10.0
model initialize at round 1539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.13224971,  5.97380021,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.184022627559402}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7483477804114151
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.43699828, 4.30399851, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8218184491525483}
episode index:1540
target Thresh 18.99999999999936
target distance 10.0
model initialize at round 1540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.00000012, 10.99999976,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.0000001192093}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7483907125785719
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.10801503, 11.82659095,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8336185220043935}
episode index:1541
target Thresh 18.999999999999375
target distance 10.0
model initialize at round 1541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.13224973,  4.97380117,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.620595845952751}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7483820881804178
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.83783408, 8.96767936, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.2799880088051507}
episode index:1542
target Thresh 18.999999999999385
target distance 14.0
model initialize at round 1542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.       ,  5.9999994,  0.       ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.649110452187118}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7483055277925658
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.8153302 , 2.18296465, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.2599596123063615}
episode index:1543
target Thresh 18.9999999999994
target distance 4.0
model initialize at round 1543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.16026524, 8.00342771, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.1722976422039437}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7484361589274152
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.04672568, 6.03496657, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.05836051574319391}
episode index:1544
target Thresh 18.99999999999941
target distance 1.0
model initialize at round 1544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.53578401,  9.69659394,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.5545734631798219}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7485989834200188
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.53578401,  9.69659394,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.5545734631798219}
episode index:1545
target Thresh 18.99999999999942
target distance 11.0
model initialize at round 1545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.332858569546817}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.748590246620022
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.0840153 , 9.56234221, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.5685836221392453}
episode index:1546
target Thresh 18.99999999999943
target distance 11.0
model initialize at round 1546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.00000012, 10.90994048,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.664400559206923}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7485351917943395
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.70762689, 1.57488421, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.825505449536466}
episode index:1547
target Thresh 18.999999999999446
target distance 12.0
model initialize at round 1547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.32905180961646}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7485515004155964
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.09213925,  9.86222754,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9182550777708}
episode index:1548
target Thresh 18.999999999999456
target distance 4.0
model initialize at round 1548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.39947939, 9.00411749, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.092154849232188}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7486815510931848
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.94505072, 7.06393254, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.08430179824547385}
episode index:1549
target Thresh 18.999999999999467
target distance 3.0
model initialize at round 1549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.92304337,  7.99814034,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0048109827477634}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7488114339634473
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.51739982,  9.63682199,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7990276446044025}
episode index:1550
target Thresh 18.999999999999478
target distance 1.0
model initialize at round 1550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.74210143, 3.3212848 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.41198979981540007}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7489733866172426
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.74210143, 3.3212848 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.41198979981540007}
episode index:1551
target Thresh 18.99999999999949
target distance 12.0
model initialize at round 1551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.182309676547058}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.748964442354361
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.21813748,  7.88752636,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.1827983972344216}
episode index:1552
target Thresh 18.9999999999995
target distance 12.0
model initialize at round 1552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4., 6., 0.]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.440306508910561}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7489093592822005
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.93955438,  3.85809928,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8602255748617145}
episode index:1553
target Thresh 18.999999999999506
target distance 4.0
model initialize at round 1553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.99752057, 7.9999994 , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.000002132943639}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7490387612389043
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.63686502, 9.98423278, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.3634771230325517}
episode index:1554
target Thresh 18.999999999999517
target distance 11.0
model initialize at round 1554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.        , 10.99999988,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.055385124972977}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7490546726062749
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.50759174, 9.9285351 , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.49756720403120325}
episode index:1555
target Thresh 18.999999999999527
target distance 8.0
model initialize at round 1555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 6.97380105, 11.86775028,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.08835481327448}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7491242872125691
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.92938505, 10.93138666,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.09845944290233727}
episode index:1556
target Thresh 18.999999999999535
target distance 12.0
model initialize at round 1556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 5.99505742, 11.86740305,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 10.407731561505297}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7491401232114692
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.79436368,  9.24491791,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.3197984999347291}
episode index:1557
target Thresh 18.999999999999545
target distance 11.0
model initialize at round 1557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.18772204862523}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7491311063741223
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.21819399,  7.88738712,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9138186478098017}
episode index:1558
target Thresh 18.999999999999556
target distance 11.0
model initialize at round 1558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.184084079649052}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7491221011042383
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.81414319, 7.91897461, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.2277391692345931}
episode index:1559
target Thresh 18.999999999999563
target distance 5.0
model initialize at round 1559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 4.99504946, 11.86740299,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.127637392657075}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7492204202701971
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.53098439, 10.66292889,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6289366864077923}
episode index:1560
target Thresh 18.99999999999957
target distance 3.0
model initialize at round 1560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.71759009,  7.00579178,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.0446877364292262}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7493490426787364
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.03354731,  5.14128089,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8593741518468804}
episode index:1561
target Thresh 18.99999999999958
target distance 13.0
model initialize at round 1561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.201857023352762}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7492940307636341
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.83936586,  2.88260349,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.2179999847192378}
episode index:1562
target Thresh 18.999999999999588
target distance 1.0
model initialize at round 1562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.37977338, 11.2406541 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.6652784801125655}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7494544312557879
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.37977338, 11.2406541 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.6652784801125655}
episode index:1563
target Thresh 18.9999999999996
target distance 4.0
model initialize at round 1563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.14304035, 6.03241222, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 2.205692475057293}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.74958265732276
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.71881349, 4.04147665, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.28422906920486435}
episode index:1564
target Thresh 18.999999999999606
target distance 13.0
model initialize at round 1564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4., 6., 0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.401754250991402}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7495733980469147
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.07787709,  8.46674122,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0652115182663162}
episode index:1565
target Thresh 18.999999999999613
target distance 4.0
model initialize at round 1565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.85291708, 3.99996018, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.1743106499051232}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7497013843827725
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.88649362, 5.98728287, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.1142165614124132}
episode index:1566
target Thresh 18.99999999999962
target distance 2.0
model initialize at round 1566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.85278678, 4.2582233 , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.7562436195332394}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7498611154712327
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.85278678, 4.2582233 , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.7562436195332394}
episode index:1567
target Thresh 18.999999999999627
target distance 14.0
model initialize at round 1567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.000000000000018}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7498516963227337
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.88247218,  9.71414753,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.30907025765233864}
episode index:1568
target Thresh 18.999999999999634
target distance 4.0
model initialize at round 1568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.0136393 ,  8.99996564,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.2422314926636684}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7499792605698193
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.33469912, 11.00558145,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6653242869844994}
episode index:1569
target Thresh 18.99999999999964
target distance 13.0
model initialize at round 1569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.00016812,  9.00000667,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.038550226164057}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7499241275575387
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.39293022, 1.42873292, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8335945054428634}
episode index:1570
target Thresh 18.99999999999965
target distance 3.0
model initialize at round 1570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.0182832 , 11.85329517,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.32853803906783}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7500514833006593
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.43428458, 10.81699337,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5945799861277994}
episode index:1571
target Thresh 18.999999999999655
target distance 6.0
model initialize at round 1571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.05928552, 7.00018656, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.1093109233978025}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.750148460728585
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.64262342, 3.02465618, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6430962546909971}
episode index:1572
target Thresh 18.999999999999662
target distance 6.0
model initialize at round 1572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86713044,  5.99659877,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.096234438844374}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7502453148539959
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.2476466,  9.9766002,  0.       ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.7527172005698101}
episode index:1573
target Thresh 18.99999999999967
target distance 10.0
model initialize at round 1573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 5.94477747, 10.04567222,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.50502067825659}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.75023568751967
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.77805998,  4.50266289,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5446113968383618}
episode index:1574
target Thresh 18.999999999999677
target distance 1.0
model initialize at round 1574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.56925786, 8.9562999 , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0488318720321068}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7503942680355306
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.56925786, 8.9562999 , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0488318720321068}
episode index:1575
target Thresh 18.999999999999684
target distance 9.0
model initialize at round 1575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 3.99999952, 10.        ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.071068283910194}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7504349482271323
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.66516127, 11.5523415 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8645927586983492}
episode index:1576
target Thresh 18.99999999999969
target distance 12.0
model initialize at round 1576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.        ,  5.99999988,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.049875609259141}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7504019123031416
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.16203379, 5.90850744, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9228438226545314}
episode index:1577
target Thresh 18.999999999999694
target distance 13.0
model initialize at round 1577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.311697890307714}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7503922101347777
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.09114669,  9.89015297,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.272158266782975}
episode index:1578
target Thresh 18.9999999999997
target distance 14.0
model initialize at round 1578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 3.99505825, 11.86740293,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 15.539764062246563}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7503371298441852
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.11188608,  2.13045442,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.17186288472935948}
episode index:1579
target Thresh 18.999999999999705
target distance 7.0
model initialize at round 1579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.98677444,  6.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.09644226931657}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7504048753316256
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.90249592, 11.53025103,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.539141165007632}
episode index:1580
target Thresh 18.999999999999712
target distance 8.0
model initialize at round 1580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.86740299, 4.99505789, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.372385718774736}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7504725351195246
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.03401103, 11.13831737,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.14243751138283786}
episode index:1581
target Thresh 18.99999999999972
target distance 12.0
model initialize at round 1581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.13224971,  5.97380055,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.522607055851553}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7504175085052197
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.83239898, 3.94485555, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9596051806740303}
episode index:1582
target Thresh 18.999999999999723
target distance 5.0
model initialize at round 1582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.24206692,  7.99999762,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.0097525976039115}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7505135808308638
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.03508133, 11.59621511,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5972463096234348}
episode index:1583
target Thresh 18.99999999999973
target distance 9.0
model initialize at round 1583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.86775027, 3.97380117, 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 10.01181582938759}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7505038449153298
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.97043985, 11.55301246,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.5538019366014844}
episode index:1584
target Thresh 18.999999999999734
target distance 9.0
model initialize at round 1584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([10.0000118 , 10.78897174,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 9.083631377170454}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7505442249816293
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.36135811, 5.97611922, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.166478550619286}
episode index:1585
target Thresh 18.99999999999974
target distance 5.0
model initialize at round 1585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.8316679 ,  9.00029695,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.005015388659835}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7506400356846673
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.57260382,  5.19336927,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.912863970575097}
episode index:1586
target Thresh 18.999999999999744
target distance 10.0
model initialize at round 1586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.354328771680768}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7506546172233034
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.16768761,  9.04784819,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8336866132928266}
episode index:1587
target Thresh 18.99999999999975
target distance 5.0
model initialize at round 1587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.44686657,  5.99999726,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.0331017430963443}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7507502377414247
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.74173844,  9.75962198,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8023244875108437}
episode index:1588
target Thresh 18.999999999999755
target distance 12.0
model initialize at round 1588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.305877894274122}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7507172528819862
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.29977987, 4.95287115, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.7018043642728763}
episode index:1589
target Thresh 18.99999999999976
target distance 1.0
model initialize at round 1589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.06456387, 6.88726526, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.1299138708650232}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7508740344839473
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.06456387, 6.88726526, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.1299138708650232}
episode index:1590
target Thresh 18.999999999999766
target distance 12.0
model initialize at round 1590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.        ,  9.99999988,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.198039003806734}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7508884322859687
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.18600388, 7.52333222, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5116733488911789}
episode index:1591
target Thresh 18.99999999999977
target distance 10.0
model initialize at round 1591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 6.97380133, 11.86775023,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 9.38695144201365}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7509283932267439
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.98210555,  7.03231901,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.03694225391764591}
episode index:1592
target Thresh 18.999999999999773
target distance 9.0
model initialize at round 1592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([11.02641792, 11.86009444,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.819951349848555}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7509427388289243
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.06881457, 5.01078324, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.06965431671984773}
episode index:1593
target Thresh 18.99999999999978
target distance 8.0
model initialize at round 1593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 9.03199979, 11.8587886 ,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 7.160675310410438}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7510095093817292
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.26539583, 7.42145733, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6365112505312321}
episode index:1594
target Thresh 18.999999999999783
target distance 11.0
model initialize at round 1594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.13224973,  3.97380117,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.788874284299988}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.750954594599226
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.86912748, 9.80255358, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.23688120983469496}
episode index:1595
target Thresh 18.999999999999787
target distance 4.0
model initialize at round 1595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.98118711,  8.99999714,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 2.0000913401920686}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7510793097655172
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.41071528, 10.97010493,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.590042535716917}
episode index:1596
target Thresh 18.99999999999979
target distance 10.0
model initialize at round 1596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.13259696,  6.99505756,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.065246669080075}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7510935249362966
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.01888416, 11.85734583,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.857553783256835}
episode index:1597
target Thresh 18.999999999999794
target distance 12.0
model initialize at round 1597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 5.94478322, 10.045681  ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 10.109442771971299}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7511077223158733
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.80761962,  9.15381722,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.24631270081908}
episode index:1598
target Thresh 18.9999999999998
target distance 12.0
model initialize at round 1598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.        ,  9.99938571,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.206203356522499}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7510747201731454
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.02557632, 3.02680423, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.03704881446720947}
episode index:1599
target Thresh 18.999999999999805
target distance 11.0
model initialize at round 1599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.332858569546817}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7510647309046777
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.16625022, 9.16652302, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.23530629776063533}
episode index:1600
target Thresh 18.999999999999808
target distance 14.0
model initialize at round 1600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.165525060596464}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7510317968417103
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.98917504, 8.76281795, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7628947570362384}
episode index:1601
target Thresh 18.99999999999981
target distance 8.0
model initialize at round 1601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 9.02596629, 11.86769046,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 6.088116004991822}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.751098178366778
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.10966647, 11.38951299,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.40465676964676256}
episode index:1602
target Thresh 18.999999999999815
target distance 12.0
model initialize at round 1602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 10.895369891272122}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7510881931592035
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.06120852, 11.42960357,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.4339420626475861}
episode index:1603
target Thresh 18.99999999999982
target distance 1.0
model initialize at round 1603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.28655338, 6.51900625, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5598819755747672}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7512433750836678
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.28655338, 6.51900625, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5598819755747672}
episode index:1604
target Thresh 18.999999999999822
target distance 4.0
model initialize at round 1604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.03305513,  4.00199175,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.252814652170869}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7513672109870425
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.02222735,  2.06982541,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.0732778476039171}
episode index:1605
target Thresh 18.999999999999826
target distance 5.0
model initialize at round 1605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.75060773, 5.99999881, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.092477827783193}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7514613160860543
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.38821639, 9.81700216, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.020672189034537}
episode index:1606
target Thresh 18.99999999999983
target distance 4.0
model initialize at round 1606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.47091031,  8.99998331,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.054707589475178}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7515848622490374
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.37659597, 10.98061419,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6237053707341936}
episode index:1607
target Thresh 18.999999999999833
target distance 13.0
model initialize at round 1607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.00452662, 11.86733422,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 14.13255894447994}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7515517480909807
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.56121872, 2.98216202, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.4391437213958905}
episode index:1608
target Thresh 18.999999999999837
target distance 9.0
model initialize at round 1608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 8.97391757, 10.13227991,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.121642448629984}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7515655636219993
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.48441602,  2.96763398,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.0821157042683103}
episode index:1609
target Thresh 18.99999999999984
target distance 12.0
model initialize at round 1609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.00093335, 11.86644143,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 13.365345168283913}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7515108150926001
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.88824014, 2.41350325, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0644007640696975}
episode index:1610
target Thresh 18.99999999999984
target distance 7.0
model initialize at round 1610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.86733918, 3.97534909, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.098960088387722}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7515765284289796
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.02270895, 9.71964702, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7200052330182665}
episode index:1611
target Thresh 18.999999999999847
target distance 2.0
model initialize at round 1611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.24576664,  2.2893132 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.37960949307292685}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7517306372823115
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.24576664,  2.2893132 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.37960949307292685}
episode index:1612
target Thresh 18.999999999999847
target distance 9.0
model initialize at round 1612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.13570656,  3.99959953,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.091927528835582}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7517961328574619
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.02335244, 10.11310559,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8872017956762398}
episode index:1613
target Thresh 18.99999999999985
target distance 2.0
model initialize at round 1613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.07503093, 11.8373521 ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8407069488448662}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7519499146834487
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.07503093, 11.8373521 ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8407069488448662}
episode index:1614
target Thresh 18.999999999999854
target distance 5.0
model initialize at round 1614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.99981403, 8.        , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.16233647285785}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7520431345505177
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.34507538, 11.7724226 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0127008140039258}
episode index:1615
target Thresh 18.999999999999858
target distance 1.0
model initialize at round 1615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.9827204 , 9.77348626, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.2271718747540316}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.75219657320488
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.9827204 , 9.77348626, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.2271718747540316}
episode index:1616
target Thresh 18.99999999999986
target distance 12.0
model initialize at round 1616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.        ,  9.99999988,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.198039003806734}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7522099216058047
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.24347542, 7.60602896, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.4631343910245218}
episode index:1617
target Thresh 18.99999999999986
target distance 11.0
model initialize at round 1617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 5.99505771, 11.86740298,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.324760899004389}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7522232535068518
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.94839682,  5.99415881,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9954971805383735}
episode index:1618
target Thresh 18.999999999999865
target distance 10.0
model initialize at round 1618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.1322496 ,  7.97379962,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.343118732455732}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7522126720597352
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.33112594, 5.91944295, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.3407841402290342}
episode index:1619
target Thresh 18.99999999999987
target distance 1.0
model initialize at round 1619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.60725474, 10.83869684,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9261000062361296}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.752365627200439
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.60725474, 10.83869684,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9261000062361296}
episode index:1620
target Thresh 18.999999999999872
target distance 13.0
model initialize at round 1620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.205906925752268}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7523107566292414
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.67058264, 9.95508704, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.3324649940970845}
episode index:1621
target Thresh 18.999999999999872
target distance 12.0
model initialize at round 1621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.31514307807156}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7523001408055643
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.08199286, 9.83710142, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.18236989249257157}
episode index:1622
target Thresh 18.999999999999876
target distance 14.0
model initialize at round 1622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.041594578792317}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7522668919794941
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.82443299, 6.92568381, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.2395887518623776}
episode index:1623
target Thresh 18.99999999999988
target distance 2.0
model initialize at round 1623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.50279999,  4.99685729,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.49720994438273464}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7524194369967482
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.50279999,  4.99685729,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.49720994438273464}
episode index:1624
target Thresh 18.999999999999883
target distance 4.0
model initialize at round 1624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.43003333,  4.9999826 ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 2.0796469945184133}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7525410250355195
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.27271808,  6.99650556,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7272903158114608}
episode index:1625
target Thresh 18.999999999999883
target distance 3.0
model initialize at round 1625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.88055396,  7.00850749,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.01555635687318}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7526624635195075
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.40914303,  5.14719427,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9458729507301278}
episode index:1626
target Thresh 18.999999999999886
target distance 5.0
model initialize at round 1626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.12563944,  7.99999154,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.002638181960689}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7527545578873504
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.06386233, 11.63199758,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.6352159813612424}
episode index:1627
target Thresh 18.99999999999989
target distance 4.0
model initialize at round 1627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([10.02289464, 11.85592113,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 2.19652081740665}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7528757160213262
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.05321172, 10.74978427,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.2558112614482298}
episode index:1628
target Thresh 18.99999999999989
target distance 6.0
model initialize at round 1628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([11.02576664, 11.86759398,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 4.118193339622836}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7529675664105089
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.04640876, 11.73490234,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.7363662279456354}
episode index:1629
target Thresh 18.999999999999893
target distance 5.0
model initialize at round 1629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.12221527,  7.00042582,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 3.002913859450071}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7530593040998276
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.9043341 ,  3.25593448,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7501902875424652}
episode index:1630
target Thresh 18.999999999999893
target distance 14.0
model initialize at round 1630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.00413644, 11.86726677,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 15.539055475987588}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7530043446437817
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.80169854, 2.13620254, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.24057140091431886}
episode index:1631
target Thresh 18.999999999999897
target distance 3.0
model initialize at round 1631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([2.43323588, 9.99953938, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.1498447830362373}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7531250527659363
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.41376377, 11.90080702,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.074767977291143}
episode index:1632
target Thresh 18.9999999999999
target distance 2.0
model initialize at round 1632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.17310643,  5.65864426,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6810125525042368}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7532762315456265
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.17310643,  5.65864426,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6810125525042368}
episode index:1633
target Thresh 18.9999999999999
target distance 3.0
model initialize at round 1633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.05098844,  9.99946237,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0018360032568183}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7533966255287687
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.22656325, 11.72425815,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0596009968240934}
episode index:1634
target Thresh 18.999999999999904
target distance 1.0
model initialize at round 1634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.59689784, 4.49926305, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7781713306598236}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7535474532807389
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.59689784, 4.49926305, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7781713306598236}
episode index:1635
target Thresh 18.999999999999904
target distance 6.0
model initialize at round 1635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([14.99988163,  9.00010729,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.123238421684671}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.753638500069687
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.07445377,  5.01987088,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.07705981959774988}
episode index:1636
target Thresh 18.999999999999908
target distance 5.0
model initialize at round 1636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.58676648, 8.00474548, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.0330277547193805}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.753729435622485
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.50252691, 4.3539809 , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8184582905830375}
episode index:1637
target Thresh 18.999999999999908
target distance 13.0
model initialize at round 1637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 4.99505743, 11.86740305,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.47137440374082}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7537180573898857
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.2679878 ,  5.13742376,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9032470470506375}
episode index:1638
target Thresh 18.999999999999908
target distance 14.0
model initialize at round 1638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4., 7., 0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.369316876853004}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7537066930416461
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.06539613, 10.76017911,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.2047226478729598}
episode index:1639
target Thresh 18.99999999999991
target distance 13.0
model initialize at round 1639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.648875396109553}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7536516404430165
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.07219433, 9.86393996, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.15402712491056803}
episode index:1640
target Thresh 18.999999999999915
target distance 12.0
model initialize at round 1640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.440306508910576}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7536639069250743
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.91317925, 10.79028446,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.207661328866303}
episode index:1641
target Thresh 18.999999999999915
target distance 1.0
model initialize at round 1641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.67282063, 8.717659  , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.432160599634721}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7538139289062405
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.67282063, 8.717659  , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.432160599634721}
episode index:1642
target Thresh 18.99999999999992
target distance 3.0
model initialize at round 1642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.00850999,  9.00373924,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.422879075876321}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7539333361314955
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.58101022,  7.38570905,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7435763634298223}
episode index:1643
target Thresh 18.99999999999992
target distance 13.0
model initialize at round 1643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 11.309950023624257}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7538995185888935
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.32496334,  4.87052533,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.34980688745987976}
episode index:1644
target Thresh 18.99999999999992
target distance 5.0
model initialize at round 1644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([2.96280925, 5.00074148, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 3.174935288338869}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7539898532280491
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.59177284, 1.21704739, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9814325717037921}
episode index:1645
target Thresh 18.999999999999922
target distance 8.0
model initialize at round 1645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 9.98040002, 11.86543476,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 6.302017992595909}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7540526631592593
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.92042567,  9.64374066,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.36503806873062866}
episode index:1646
target Thresh 18.999999999999925
target distance 4.0
model initialize at round 1646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.85885046, 7.99978719, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.176803941048629}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7541716354342082
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.81869041, 9.98720981, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.18176016558964073}
episode index:1647
target Thresh 18.999999999999925
target distance 2.0
model initialize at round 1647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.86918151,  4.99687994,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.13085568821871293}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7543208031311535
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.86918151,  4.99687994,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.13085568821871293}
episode index:1648
target Thresh 18.999999999999925
target distance 7.0
model initialize at round 1648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.04567295,  9.05523071,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 5.41984794381757}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7543832980959011
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.82951757,  3.28723595,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.73286891973658}
episode index:1649
target Thresh 18.99999999999993
target distance 12.0
model initialize at round 1649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.00016793,  9.00006107,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.180517396724868}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7543493308219604
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.67597948, 3.25955244, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0026020391726167}
episode index:1650
target Thresh 18.99999999999993
target distance 1.0
model initialize at round 1650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.68723297,  5.28210855,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.783065358087216}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7544981198402391
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.68723297,  5.28210855,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.783065358087216}
episode index:1651
target Thresh 18.99999999999993
target distance 1.0
model initialize at round 1651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.20304549, 10.8493942 ,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8733258159134535}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7546467287265343
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.20304549, 10.8493942 ,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8733258159134535}
episode index:1652
target Thresh 18.999999999999932
target distance 14.0
model initialize at round 1652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.041594578792312}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7546348988184268
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.82788479,  9.67981288,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.363515387472417}
episode index:1653
target Thresh 18.999999999999932
target distance 13.0
model initialize at round 1653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.        ,  6.99996936,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.083033296033937}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7545797510146002
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.72411739, 2.87176084, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9143731075779274}
episode index:1654
target Thresh 18.999999999999936
target distance 5.0
model initialize at round 1654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.00029266, 10.99930406,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.0002927395209276}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.754669128808549
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.52275682, 10.64722109,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5934762163775925}
episode index:1655
target Thresh 18.999999999999936
target distance 11.0
model initialize at round 1655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.00000012, 10.99946368,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.816356436540207}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7546806697558265
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.63536416, 5.98083112, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0464172093415474}
episode index:1656
target Thresh 18.999999999999936
target distance 11.0
model initialize at round 1656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 5.94478126, 10.04567799,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.887938687467472}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7546688479217101
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.63046595,  4.02758612,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.6310691808022046}
episode index:1657
target Thresh 18.99999999999994
target distance 6.0
model initialize at round 1657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.99828959,  7.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.000000365689888}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7547580102571011
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.5062674 , 10.95721431,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.4955829887979924}
episode index:1658
target Thresh 18.99999999999994
target distance 10.0
model initialize at round 1658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 6.97380118, 11.86775027,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 8.909509505240662}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7547940248681577
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.89927425,  7.35067084,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6570951467003492}
episode index:1659
target Thresh 18.99999999999994
target distance 1.0
model initialize at round 1659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.28747535,  5.24343395,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8093418683818259}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7549417393110082
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.28747535,  5.24343395,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8093418683818259}
episode index:1660
target Thresh 18.999999999999943
target distance 12.0
model initialize at round 1660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.891740190748873}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7549076607780658
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.26172187,  3.85700638,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.2982373456438274}
episode index:1661
target Thresh 18.999999999999943
target distance 11.0
model initialize at round 1661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.13259618,  5.99498879,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.96816173617416}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7548326558135331
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.87406717, 2.24330716, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9072991726771777}
episode index:1662
target Thresh 18.999999999999947
target distance 13.0
model initialize at round 1662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.        , 10.99984133,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 13.038319626365741}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7548207852391564
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.13538963, 4.98100808, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.3076421284342894}
episode index:1663
target Thresh 18.999999999999947
target distance 5.0
model initialize at round 1663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.86731194, 3.99560393, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 3.127079426561528}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7549095347672578
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.95830906, 7.79520559, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.7962977224397366}
episode index:1664
target Thresh 18.999999999999947
target distance 3.0
model initialize at round 1664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.15909123, 7.00566953, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.3109152360103926}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7550267062178481
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.90117657, 5.15528291, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8504781166973494}
episode index:1665
target Thresh 18.999999999999947
target distance 8.0
model initialize at round 1665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 7.02621311, 10.13224946,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.190199901855822}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7550624082249202
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.11301059, 2.97033354, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.11683960527224317}
episode index:1666
target Thresh 18.99999999999995
target distance 14.0
model initialize at round 1666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4., 9., 0.]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 13.416407864998746}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7550283799632939
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.86122311,  3.99511822,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.3160416110948194}
episode index:1667
target Thresh 18.99999999999995
target distance 3.0
model initialize at round 1667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.26359332, 5.01081169, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.044615576122666}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7551452694237475
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.87610757, 3.3999604 , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6126963819880245}
episode index:1668
target Thresh 18.99999999999995
target distance 5.0
model initialize at round 1668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.21520329, 4.99999118, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.00771763684203}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7552335586571665
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.47242286, 8.88046563, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0264294211744973}
episode index:1669
target Thresh 18.99999999999995
target distance 1.0
model initialize at round 1669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.76847221, 10.70752239,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7444414371243457}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7553801253885094
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.76847221, 10.70752239,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7444414371243457}
episode index:1670
target Thresh 18.99999999999995
target distance 11.0
model initialize at round 1670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.00000012, 10.89318097,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.652615138638325}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7553250926571513
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.81780232, 1.51420392, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9512089432738206}
episode index:1671
target Thresh 18.999999999999954
target distance 2.0
model initialize at round 1671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.05006519,  6.02335334,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.05524401883201429}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7554714293242224
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.05006519,  6.02335334,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.05524401883201429}
episode index:1672
target Thresh 18.999999999999954
target distance 14.0
model initialize at round 1672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 13.000000000000009}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7554592478904512
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.77143907,  5.99843086,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0242579163959988}
episode index:1673
target Thresh 18.999999999999957
target distance 2.0
model initialize at round 1673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.53369761, 8.98967838, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.46641661552603747}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7556053295822729
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.53369761, 8.98967838, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.46641661552603747}
episode index:1674
target Thresh 18.999999999999957
target distance 13.0
model initialize at round 1674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.        , 10.99999142,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 12.083042421897934}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7555930827530446
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.97965403, 6.01703235, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.026534120238596765}
episode index:1675
target Thresh 18.999999999999957
target distance 7.0
model initialize at round 1675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 7.02613583, 10.1322658 ,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 6.506739738537504}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.755680736044958
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.82933776, 6.92717156, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.243964715069844}
episode index:1676
target Thresh 18.999999999999957
target distance 7.0
model initialize at round 1676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 7.02583874, 11.86021583,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 5.098924033238575}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7557413766316934
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.67368284, 11.1763657 ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.37092822997088726}
episode index:1677
target Thresh 18.999999999999957
target distance 8.0
model initialize at round 1677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([10.02558829, 11.86759183,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 6.308376454353836}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7558019449412097
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.21584828, 10.32784019,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.39251709359736375}
episode index:1678
target Thresh 18.99999999999996
target distance 7.0
model initialize at round 1678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.00017746,  9.00007675,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.38530107208124}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7558624411026503
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.2296573 , 11.77585353,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.093332784015}
episode index:1679
target Thresh 18.99999999999996
target distance 7.0
model initialize at round 1679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.99999988, 6.        , 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.071067896159179}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7559228652448511
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.06937401, 11.86773693,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.2724119272480898}
episode index:1680
target Thresh 18.99999999999996
target distance 2.0
model initialize at round 1680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.82092476,  8.73620552,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7576717673679477}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.756068062826502
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.82092476,  8.73620552,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7576717673679477}
episode index:1681
target Thresh 18.99999999999996
target distance 10.0
model initialize at round 1681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([12.0049424 , 11.86740296,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 8.50300549936098}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7561028061006836
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.16110703, 8.60667476, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.4250414305372145}
episode index:1682
target Thresh 18.99999999999996
target distance 12.0
model initialize at round 1682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([12.00494225, 11.86740301,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 10.726400864617231}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7561133100409091
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.14522266, 8.72171947, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7361851727081453}
episode index:1683
target Thresh 18.999999999999964
target distance 12.0
model initialize at round 1683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.179448465417412}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7560790012440284
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.0310635 , 6.84547502, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.15761633756639074}
episode index:1684
target Thresh 18.999999999999964
target distance 12.0
model initialize at round 1684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.13224932,  6.97378173,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.192845425359716}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7560240109947969
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.41703437, 2.90512336, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.0766137795691244}
episode index:1685
target Thresh 18.999999999999964
target distance 5.0
model initialize at round 1685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.13226967,  7.97387862,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.3552868849846424}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7561108888055947
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.94749549, 11.73856837,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.7404322777618928}
episode index:1686
target Thresh 18.999999999999964
target distance 1.0
model initialize at round 1686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.33547326,  8.44684441,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8007906838373486}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.756255458521774
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.33547326,  8.44684441,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8007906838373486}
episode index:1687
target Thresh 18.999999999999964
target distance 12.0
model initialize at round 1687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.00494242, 11.86740304,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.17772405729475}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7562658409145336
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.26613765, 10.3473366 ,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.4375750979244947}
episode index:1688
target Thresh 18.999999999999964
target distance 2.0
model initialize at round 1688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.87724686, 8.57693058, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.5898450908868145}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7564101476990721
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.87724686, 8.57693058, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.5898450908868145}
episode index:1689
target Thresh 18.999999999999968
target distance 2.0
model initialize at round 1689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.12491107,  5.99209738,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.12516080382476094}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7565542837063508
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.12491107,  5.99209738,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.12516080382476094}
episode index:1690
target Thresh 18.999999999999968
target distance 2.0
model initialize at round 1690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.31559357,  8.01029813,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.3157615449411125}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7566982492393453
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.31559357,  8.01029813,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.3157615449411125}
episode index:1691
target Thresh 18.999999999999968
target distance 8.0
model initialize at round 1691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 8.97388502, 10.13227262,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 8.72531630097636}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7567083453907996
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.26348117,  3.01385138,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.2638450089048836}
episode index:1692
target Thresh 18.99999999999997
target distance 13.0
model initialize at round 1692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.827730117166249}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7566336502132058
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.85547857,  1.97071111,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.147459428530813}
episode index:1693
target Thresh 18.99999999999997
target distance 7.0
model initialize at round 1693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.00001025, 10.99998677,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.0000102520164305}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7566931197231155
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.20097677, 11.86986928,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.1811480418167184}
episode index:1694
target Thresh 18.99999999999997
target distance 8.0
model initialize at round 1694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([10.02564928, 11.86760808,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 6.30843952078669}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7567525190625118
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.21118937, 10.17108748,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.2717938090993759}
episode index:1695
target Thresh 18.99999999999997
target distance 6.0
model initialize at round 1695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.0552355 , 10.04566756,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.1660155254825355}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7568384550772156
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.08833159, 10.74290072,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.27185015931856166}
episode index:1696
target Thresh 18.99999999999997
target distance 1.0
model initialize at round 1696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.95317614,  5.08009601,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0811104739196449}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7569522803835932
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.91158334,  3.31725299,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.6884482407870589}
episode index:1697
target Thresh 18.99999999999997
target distance 12.0
model initialize at round 1697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.        ,  5.99996638,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.770317129266585}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7568776614962793
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.8663842 , 2.18395897, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8856988700581818}
episode index:1698
target Thresh 18.99999999999997
target distance 10.0
model initialize at round 1698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([10.02196495, 10.14402796,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 10.104504002246301}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7568876104521379
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.01775093, 4.02597748, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.031463070831647094}
episode index:1699
target Thresh 18.99999999999997
target distance 12.0
model initialize at round 1699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.182309676547058}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7568747894404749
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.79337664, 7.89074469, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.1928422302374133}
episode index:1700
target Thresh 18.999999999999975
target distance 11.0
model initialize at round 1700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.02619774, 11.86775   ,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.217414803880416}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.756884728387012
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.77735997, 9.66962719, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.3983902268377332}
episode index:1701
target Thresh 18.999999999999975
target distance 10.0
model initialize at round 1701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.13259696,  4.99505757,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 10.10932573657811}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7568719241345079
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.0508187 , 11.86034908,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8618486367446063}
episode index:1702
target Thresh 18.999999999999975
target distance 3.0
model initialize at round 1702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 4.97241761, 11.86704057,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.3445017367357965}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7569853287592088
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.55254768, 10.36747829,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8398765693616423}
episode index:1703
target Thresh 18.999999999999975
target distance 5.0
model initialize at round 1703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.80195379, 3.99999976, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 3.4995769603019533}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7570707246930354
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.55120614, 7.82622635, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9402477943056696}
episode index:1704
target Thresh 18.999999999999975
target distance 6.0
model initialize at round 1704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4.35610336, 4.99999964, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 4.223626307546299}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7571560204556788
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.80725211, 8.98475373, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.1933499373620502}
episode index:1705
target Thresh 18.999999999999975
target distance 1.0
model initialize at round 1705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.90174365, 11.14453623,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9132537062691429}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7572983674542393
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.90174365, 11.14453623,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9132537062691429}
episode index:1706
target Thresh 18.999999999999975
target distance 5.0
model initialize at round 1706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4.00034857, 9.0007683 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 3.6063838962134556}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7573834299220458
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.74898106, 5.46898496, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5873563484340015}
episode index:1707
target Thresh 18.999999999999975
target distance 6.0
model initialize at round 1707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.00004578,  8.99999225,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.472180363928662}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.757468392785089
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.03302014, 11.19246669,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.1952786643731446}
episode index:1708
target Thresh 18.999999999999975
target distance 3.0
model initialize at round 1708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.83062232, 9.04042435, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.054121255695944}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.757581050249814
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.16697511, 7.2270838 , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.7907465690107459}
episode index:1709
target Thresh 18.99999999999998
target distance 1.0
model initialize at round 1709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.10633279, 11.64739382,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6560681509179901}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7577228157175042
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.10633279, 11.64739382,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6560681509179901}
episode index:1710
target Thresh 18.99999999999998
target distance 4.0
model initialize at round 1710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.20853901, 7.001001  , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 2.1518400274519847}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7578351927977395
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.12248975, 5.03895831, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.12853594420133121}
episode index:1711
target Thresh 18.99999999999998
target distance 6.0
model initialize at round 1711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.0008198 , 9.00009584, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 4.123397511759046}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.757919693269236
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.05253102, 5.01508265, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.0546534032763204}
episode index:1712
target Thresh 18.99999999999998
target distance 5.0
model initialize at round 1712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.99982184, 9.00007781, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.6056563574280918}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7580040950828558
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.3955154 , 11.74533421,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8437745703045871}
episode index:1713
target Thresh 18.99999999999998
target distance 7.0
model initialize at round 1713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.13267214, 8.00450451, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 5.079106520189511}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7580620711067282
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.82569234, 2.22951268, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7899581485082336}
episode index:1714
target Thresh 18.999999999999982
target distance 11.0
model initialize at round 1714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.470810794869452}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.758094983164392
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.03431113,  9.14484169,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.9764907107505033}
episode index:1715
target Thresh 18.999999999999982
target distance 10.0
model initialize at round 1715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 5.99505739, 11.86740303,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.368602805234486}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7581278568630141
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.10975567,  7.99352203,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9995660717943198}
episode index:1716
target Thresh 18.999999999999982
target distance 5.0
model initialize at round 1716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.71803669, 3.99999845, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.457117209278211}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7582119408135889
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.02121916, 7.7507404 , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.7510402123457176}
episode index:1717
target Thresh 18.999999999999982
target distance 12.0
model initialize at round 1717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.13259815856355}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7581984832756443
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.94556168,  6.93169624,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.08734376905315389}
episode index:1718
target Thresh 18.999999999999982
target distance 3.0
model initialize at round 1718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.02173996, 11.85479678,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.3321524222818424}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7583100606559378
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.30371487, 10.19693689,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.062884438949146}
episode index:1719
target Thresh 18.999999999999982
target distance 8.0
model initialize at round 1719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.99992824,  9.00002027,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 6.082794318444127}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7583676565509053
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.82586879,  3.01850353,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8260760499874853}
episode index:1720
target Thresh 18.999999999999982
target distance 13.0
model initialize at round 1720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.529964086141694}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.758312486751218
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.65835046, 9.6465272 , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.49159681833162333}
episode index:1721
target Thresh 18.999999999999982
target distance 12.0
model initialize at round 1721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.        ,  6.99999988,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.049875609259141}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.758299002084478
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.75789153, 6.86763655, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.900782825193436}
episode index:1722
target Thresh 18.999999999999982
target distance 4.0
model initialize at round 1722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.26751739, 7.00178945, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.131593718744845}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7584102620948759
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.11460459, 5.02463746, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.11722293739605016}
episode index:1723
target Thresh 18.999999999999982
target distance 1.0
model initialize at round 1723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.53915915, 6.88934207, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.001650441431527}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7585503953535215
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.53915915, 6.88934207, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.001650441431527}
episode index:1724
target Thresh 18.999999999999982
target distance 2.0
model initialize at round 1724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.21767294,  4.0358628 ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.22060745964621573}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7586903661388239
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.21767294,  4.0358628 ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.22060745964621573}
episode index:1725
target Thresh 18.999999999999986
target distance 1.0
model initialize at round 1725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.30962282,  2.37470078,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.6977574090874458}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7588301747331815
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.30962282,  2.37470078,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.6977574090874458}
episode index:1726
target Thresh 18.999999999999986
target distance 12.0
model initialize at round 1726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.        , 10.86601102,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 13.364361244339555}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7587557214818738
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.85047381, 1.90649818, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8555982038360702}
episode index:1727
target Thresh 18.999999999999986
target distance 3.0
model initialize at round 1727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.99998701,  9.00360227,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.4167722106039666}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7588663952541643
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.28355932,  7.17102385,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8761320412695954}
episode index:1728
target Thresh 18.999999999999986
target distance 6.0
model initialize at round 1728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.86740129, 6.99506838, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 4.007126114529373}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7589494684784245
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.98336298, 11.09634066,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.0977666302759476}
episode index:1729
target Thresh 18.999999999999986
target distance 1.0
model initialize at round 1729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.38513756, 6.29325008, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8048766256040341}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7590888040457779
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.38513756, 6.29325008, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8048766256040341}
episode index:1730
target Thresh 18.999999999999986
target distance 11.0
model initialize at round 1730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.        ,  9.99999726,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.486832113467907}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7590972917023084
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.31808191, 6.98086018, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.31865723663231127}
episode index:1731
target Thresh 18.999999999999986
target distance 1.0
model initialize at round 1731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.88892126, 9.64402771, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.37290046603682325}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7592363810258059
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.88892126, 9.64402771, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.37290046603682325}
episode index:1732
target Thresh 18.999999999999986
target distance 3.0
model initialize at round 1732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([12.8154068 ,  9.99957323,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.5505208066165033}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7593464581285032
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.99534678, 11.6280755 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.6280927359774653}
episode index:1733
target Thresh 18.999999999999986
target distance 8.0
model initialize at round 1733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.99998808, 5.        , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 6.08276449009825}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7594029913129735
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.77921786, 11.07873785,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.23440222148792017}
episode index:1734
target Thresh 18.999999999999986
target distance 3.0
model initialize at round 1734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 5.94437379, 10.04546026,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.4231980889069247}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7595128454966548
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.56706622, 11.63028488,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8478343720906979}
episode index:1735
target Thresh 18.999999999999986
target distance 1.0
model initialize at round 1735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.38357508,  9.82511079,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0299453872218771}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7596513749635346
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.38357508,  9.82511079,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0299453872218771}
episode index:1736
target Thresh 18.999999999999986
target distance 4.0
model initialize at round 1736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.99979308, 9.00007771, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.828518493506649}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7597609596641889
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.94106929, 11.04481488,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.07403514497980297}
episode index:1737
target Thresh 18.999999999999986
target distance 1.0
model initialize at round 1737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.30357805, 6.84105349, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8941647487435327}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7598991869601244
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.30357805, 6.84105349, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8941647487435327}
episode index:1738
target Thresh 18.999999999999986
target distance 3.0
model initialize at round 1738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([12.99995337,  9.35023367,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.192597811807985}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7600085031263347
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.05870259, 10.87753631,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8794975655596884}
episode index:1739
target Thresh 18.999999999999986
target distance 5.0
model initialize at round 1739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.29909211, 8.00224388, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.0829758608236033}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7600903947912047
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.46231252, 4.25747776, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8746840241824356}
episode index:1740
target Thresh 18.999999999999986
target distance 11.0
model initialize at round 1740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.559639095810898}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7600158163965657
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.91592772,  2.94341739,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.3148992218924036}
episode index:1741
target Thresh 18.99999999999999
target distance 6.0
model initialize at round 1741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.9999516, 7.       , 0.       ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 4.123117364359851}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.760097609842951
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.90687599, 11.11018953,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.14426993274063465}
episode index:1742
target Thresh 18.99999999999999
target distance 3.0
model initialize at round 1742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.70506883,  6.01017177,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.052345667556017}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7602065613002987
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.46835262,  4.13977087,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9794632901252927}
episode index:1743
target Thresh 18.99999999999999
target distance 2.0
model initialize at round 1743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.89700447,  7.01055646,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.103535105101636}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7603440575380853
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.89700447,  7.01055646,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.103535105101636}
episode index:1744
target Thresh 18.99999999999999
target distance 14.0
model initialize at round 1744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.02517807, 11.86036678,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 15.550940190221102}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7602885139127277
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.72853683, 2.18497264, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.32849220868855217}
episode index:1745
target Thresh 18.99999999999999
target distance 5.0
model initialize at round 1745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 8.97411718, 11.86766241,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.147825422931424}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7603699637902117
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.83010216, 11.48429223,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9610455553475161}
episode index:1746
target Thresh 18.99999999999999
target distance 5.0
model initialize at round 1746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.90593505,  8.00020242,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.1339963079628426}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7604513204222723
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.14954594,  4.18086267,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8326763842018476}
episode index:1747
target Thresh 18.99999999999999
target distance 12.0
model initialize at round 1747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.536241300558661}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7604157860834115
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.84168135, 7.44331812, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.009119521394046}
episode index:1748
target Thresh 18.99999999999999
target distance 2.0
model initialize at round 1748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.94559836,  2.06560385,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.08522560322960533}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7605527696248161
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.94559836,  2.06560385,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.08522560322960533}
episode index:1749
target Thresh 18.99999999999999
target distance 9.0
model initialize at round 1749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.0005554 , 9.00000215, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 7.0000021678004325}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7605836001850306
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.16955944, 1.84699461, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.22838794444666075}
episode index:1750
target Thresh 18.99999999999999
target distance 13.0
model initialize at round 1750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.       , 10.9992162,  0.       ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.601009513895747}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7605480511821229
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.63752277, 2.98639289, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.362732538858043}
episode index:1751
target Thresh 18.999999999999993
target distance 11.0
model initialize at round 1751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.055061527404254}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7605556041994277
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.99479576,  6.89046298,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.10966057627646021}
episode index:1752
target Thresh 18.999999999999993
target distance 2.0
model initialize at round 1752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.09212375,  6.76117659,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.25597540456096035}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.760692195412092
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.09212375,  6.76117659,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.25597540456096035}
episode index:1753
target Thresh 18.999999999999993
target distance 5.0
model initialize at round 1753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.99555612,  6.        ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.000003291348893}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7607730436473188
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.48640648,  9.73161936,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8938932786972328}
episode index:1754
target Thresh 18.999999999999993
target distance 12.0
model initialize at round 1754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.042472879490532}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7607804555526481
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.83560927, 11.83051593,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8466292151888606}
episode index:1755
target Thresh 18.999999999999993
target distance 10.0
model initialize at round 1755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.980393886139277}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7607448956668513
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.13647896,  7.99109009,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.13676949247430342}
episode index:1756
target Thresh 18.999999999999993
target distance 4.0
model initialize at round 1756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([5.97369441, 9.92032607, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.9753018914569844}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7608526105810991
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.11263234, 10.64321703,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6530039792189738}
episode index:1757
target Thresh 18.999999999999993
target distance 11.0
model initialize at round 1757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4., 7., 0.]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.848857801796129}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7608599645782088
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.06265647, 11.35029655,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.3558560216927723}
episode index:1758
target Thresh 18.999999999999993
target distance 1.0
model initialize at round 1758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.25323603,  6.62225556,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.671811336051909}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7609959168439404
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.25323603,  6.62225556,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.671811336051909}
episode index:1759
target Thresh 18.999999999999993
target distance 1.0
model initialize at round 1759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.84577651,  8.09555674,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.91749795731844}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7611317146184609
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.84577651,  8.09555674,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.91749795731844}
episode index:1760
target Thresh 18.999999999999993
target distance 13.0
model initialize at round 1760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.827730117166249}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.761057391901315
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.89072226,  1.99497327,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.10939329425346711}
episode index:1761
target Thresh 18.999999999999993
target distance 13.0
model initialize at round 1761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 11.311697890307714}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7610426555214761
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.86234873, 9.77968848, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.1625658912131296}
episode index:1762
target Thresh 18.999999999999993
target distance 4.0
model initialize at round 1762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.82372907,  9.14572425,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.6008318460359394}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7611498349568014
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.91976697, 11.19990294,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.21540316255864783}
episode index:1763
target Thresh 18.999999999999993
target distance 14.0
model initialize at round 1763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.369316876853006}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7611142269415729
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.97139142, 9.0779665 , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.08304954266045503}
episode index:1764
target Thresh 18.999999999999993
target distance 10.0
model initialize at round 1764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.95937150162196}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7610400825692121
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.00780812,  2.02152884,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.022901046387902108}
episode index:1765
target Thresh 18.999999999999993
target distance 2.0
model initialize at round 1765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.36783803, 10.99477327,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.6321835797916129}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7611753939607357
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.36783803, 10.99477327,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.6321835797916129}
episode index:1766
target Thresh 18.999999999999993
target distance 12.0
model initialize at round 1766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.327137713346806}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7611606324987461
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.10615721,  5.88017463,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.16008582976743999}
episode index:1767
target Thresh 18.999999999999993
target distance 11.0
model initialize at round 1767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.132283595738201}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7610865876894847
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.82304103,  3.97455379,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8234343048445665}
episode index:1768
target Thresh 18.999999999999993
target distance 11.0
model initialize at round 1768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.322711635547794}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7610126265939704
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.79311261,  2.97173632,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.20880907331890655}
episode index:1769
target Thresh 18.999999999999993
target distance 13.0
model initialize at round 1769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.401754250991392}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7610198403289457
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.12867181,  7.01638414,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8714822211911977}
episode index:1770
target Thresh 18.999999999999993
target distance 1.0
model initialize at round 1770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.04025817,  6.08926717,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.09792521722072961}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7611547811305668
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.04025817,  6.08926717,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.09792521722072961}
episode index:1771
target Thresh 18.999999999999993
target distance 9.0
model initialize at round 1771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([12.82335936,  9.1461299 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.040011563489749}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7611848891829761
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.97375531, 11.85987338,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8602737972320206}
episode index:1772
target Thresh 18.999999999999993
target distance 1.0
model initialize at round 1772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.63361595, 4.71188877, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9530240307486226}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7613195846769508
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.63361595, 4.71188877, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9530240307486226}
episode index:1773
target Thresh 18.999999999999993
target distance 10.0
model initialize at round 1773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.13224973,  5.97380117,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.424042391130124}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7613048001819948
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.15115307, 11.86977986,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8828160958044011}
episode index:1774
target Thresh 18.999999999999993
target distance 12.0
model initialize at round 1774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.536241300558661}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7612496540586748
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.64097839,  8.1689595 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.90527611072015}
episode index:1775
target Thresh 18.999999999999993
target distance 1.0
model initialize at round 1775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.40533926, 6.84035665, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.6157169808923781}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7613840855597679
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.40533926, 6.84035665, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.6157169808923781}
episode index:1776
target Thresh 18.999999999999993
target distance 5.0
model initialize at round 1776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([3.99999237, 8.        , 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 4.242646081919346}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7614634980045851
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.58134657, 11.64210831,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8661794958077773}
episode index:1777
target Thresh 18.999999999999993
target distance 3.0
model initialize at round 1777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.26272753, 8.99942402, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.2428688560403662}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7615695365321415
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.78428301, 10.88588813,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9117738729636665}
episode index:1778
target Thresh 18.999999999999993
target distance 11.0
model initialize at round 1778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 5.99999988, 10.99999154,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.401749148784418}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7615546530886862
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.79854169,  3.98025627,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7987857309468932}
episode index:1779
target Thresh 18.999999999999993
target distance 8.0
model initialize at round 1779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.99990726, 5.        , 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 6.0000000007168195}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7616084847442544
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.70891255, 11.01008252,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.2912620143390184}
episode index:1780
target Thresh 18.999999999999996
target distance 4.0
model initialize at round 1780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.91746581, 4.00084877, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 2.0025502975344422}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7616875928381655
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.13887574, 1.22948153, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7829337058993526}
episode index:1781
target Thresh 18.999999999999996
target distance 3.0
model initialize at round 1781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.18288386, 6.99954057, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0170376502608562}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7617932675896593
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.33062648, 8.85389413, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0849866810006135}
episode index:1782
target Thresh 18.999999999999996
target distance 10.0
model initialize at round 1782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([12.00000036, 10.99972069,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.433833405694838}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7617999909042472
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.00098429, 5.99383934, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.006238795838505922}
episode index:1783
target Thresh 18.999999999999996
target distance 2.0
model initialize at round 1783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.80240238, 4.99925971, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8024027186210332}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.761933511088718
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.80240238, 4.99925971, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8024027186210332}
episode index:1784
target Thresh 18.999999999999996
target distance 13.0
model initialize at round 1784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.704699910719636}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7619184737663293
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.12862924,  6.28131667,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7301035563668937}
episode index:1785
target Thresh 18.999999999999996
target distance 10.0
model initialize at round 1785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.13224973,  5.97380117,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 9.620595846327655}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7619034532830474
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.10536571, 8.50399561, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5070722703267846}
episode index:1786
target Thresh 18.999999999999996
target distance 10.0
model initialize at round 1786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 5.94478322, 10.045681  ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 8.310916198291961}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7619328896550212
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.08033494,  8.03951597,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.08952772979589665}
episode index:1787
target Thresh 18.999999999999996
target distance 2.0
model initialize at round 1787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.88042632,  7.02691531,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.12256548366480724}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.762066036808458
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.88042632,  7.02691531,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.12256548366480724}
episode index:1788
target Thresh 18.999999999999996
target distance 3.0
model initialize at round 1788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.63571435, 6.9999609 , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.1849940676341377}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7621710865363459
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.39733633, 8.81357062, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0124725421976577}
episode index:1789
target Thresh 18.999999999999996
target distance 7.0
model initialize at round 1789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([12.00494554, 11.86740254,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 5.0795538239812625}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7622242730801804
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.20260016, 11.80296769,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.1316375838042911}
episode index:1790
target Thresh 18.999999999999996
target distance 11.0
model initialize at round 1790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.13259465,  6.99485462,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.409267870661452}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7621691062226755
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.41085088, 2.94932697, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.0344177751271637}
episode index:1791
target Thresh 18.999999999999996
target distance 1.0
model initialize at round 1791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.59184662,  4.9309628 ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0165042658211922}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7623018243553638
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.59184662,  4.9309628 ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0165042658211922}
episode index:1792
target Thresh 18.999999999999996
target distance 12.0
model initialize at round 1792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 3.99505789, 11.86740299,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 12.72772157365068}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.762266149771838
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.04057414,  4.06132321,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.07353092215199589}
episode index:1793
target Thresh 18.999999999999996
target distance 1.0
model initialize at round 1793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.65272987, 5.86684489, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0851158230328353}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7623986658533477
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.65272987, 5.86684489, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0851158230328353}
episode index:1794
target Thresh 18.999999999999996
target distance 9.0
model initialize at round 1794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 4.99999988, 11.        ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.000000119209301}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7624276951481369
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.83949746, 11.40981816,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9341878325803502}
episode index:1795
target Thresh 18.999999999999996
target distance 6.0
model initialize at round 1795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 4.97380087, 11.86775024,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.118649040297557}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7625056864091903
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.84595181, 10.91657409,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.17518769417928493}
episode index:1796
target Thresh 18.999999999999996
target distance 6.0
model initialize at round 1796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([15.58148241,  6.        ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 4.301288948036711}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7625835908686175
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.21290743,  9.99192619,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7871339749875462}
episode index:1797
target Thresh 18.999999999999996
target distance 2.0
model initialize at round 1797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.47358572, 10.99387097,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5264499540002581}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7627156355900476
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.47358572, 10.99387097,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5264499540002581}
episode index:1798
target Thresh 18.999999999999996
target distance 4.0
model initialize at round 1798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.09180462, 7.99997675, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.002129134871482}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7628197402951115
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.41245387, 9.99517202, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5875659675419639}
episode index:1799
target Thresh 18.999999999999996
target distance 6.0
model initialize at round 1799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 7.02123805, 11.85586272,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 4.111308360810865}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.762897340439392
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.18741028, 11.87315818,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8930441311900092}
episode index:1800
target Thresh 18.999999999999996
target distance 2.0
model initialize at round 1800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.08537474, 7.03405511, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.09191624558386775}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7630289909999476
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.08537474, 7.03405511, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.09191624558386775}
episode index:1801
target Thresh 18.999999999999996
target distance 9.0
model initialize at round 1801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 8.97485954, 10.13252619,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 10.011270063262819}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7630349576739209
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.79207733,  2.9823757 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.7922733850295617}
episode index:1802
target Thresh 18.999999999999996
target distance 1.0
model initialize at round 1802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.76560163, 6.28542602, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.7520362793962464}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7631663858726598
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.76560163, 6.28542602, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.7520362793962464}
episode index:1803
target Thresh 18.999999999999996
target distance 1.0
model initialize at round 1803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.43063741, 11.88902049,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9878289414477492}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7632976683638611
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.43063741, 11.88902049,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9878289414477492}
episode index:1804
target Thresh 18.999999999999996
target distance 1.0
model initialize at round 1804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.36574957, 9.89051483, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0932933107465055}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7634288053896984
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.36574957, 9.89051483, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0932933107465055}
episode index:1805
target Thresh 18.999999999999996
target distance 6.0
model initialize at round 1805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.86740318, 7.99505791, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.10960208362387}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7635058104808446
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.09118747, 11.86838561,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.2569939462370778}
episode index:1806
target Thresh 18.999999999999996
target distance 1.0
model initialize at round 1806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.88011181, 5.86877298, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8770060856114599}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7636366871767601
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.88011181, 5.86877298, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8770060856114599}
episode index:1807
target Thresh 18.999999999999996
target distance 14.0
model initialize at round 1807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 3.97380134, 11.86775023,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.942103208248842}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7635812578316895
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.81125423,  2.57885079,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.4615101531724123}
episode index:1808
target Thresh 18.999999999999996
target distance 12.0
model initialize at round 1808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.063679927751902}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7635868961289081
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.75778156, 11.8579695 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.891505149889905}
episode index:1809
target Thresh 18.999999999999996
target distance 12.0
model initialize at round 1809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.181338838126695}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7635315555405987
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.90023284,  4.91940257,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9247997482426005}
episode index:1810
target Thresh 18.999999999999996
target distance 9.0
model initialize at round 1810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.00016799,  9.00007785,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.28025002783733}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7635597028042428
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.32898393, 11.88355591,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.1094744740718454}
episode index:1811
target Thresh 18.999999999999996
target distance 7.0
model initialize at round 1811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.44419014,  9.00004506,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 5.030842395521139}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7636114772508189
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.23285236,  3.23694956,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7977883118962682}
episode index:1812
target Thresh 18.999999999999996
target distance 1.0
model initialize at round 1812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.18050098, 10.67043328,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6943064107800219}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7637418625363949
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.18050098, 10.67043328,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6943064107800219}
episode index:1813
target Thresh 18.999999999999996
target distance 12.0
model initialize at round 1813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.174760518110984}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7637260687260798
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.78897237, 6.87978607, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.181736409230356}
episode index:1814
target Thresh 18.999999999999996
target distance 7.0
model initialize at round 1814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.21068205, 9.00006587, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.3105854161279025}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7637776659333932
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.09406093, 3.18848731, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8169457186276227}
episode index:1815
target Thresh 18.999999999999996
target distance 10.0
model initialize at round 1815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([5.17664028, 9.14612926, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.214417409637557}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7637618698016155
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.15754185,  4.8832686 ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.2206142492925}
episode index:1816
target Thresh 18.999999999999996
target distance 5.0
model initialize at round 1816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.67421317,  8.00021172,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.0178481401420787}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7638382254043664
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.63810671,  4.29130983,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.795743996457887}
episode index:1817
target Thresh 18.999999999999996
target distance 4.0
model initialize at round 1817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([5.95865063, 9.90930744, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.9607492050930895}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7639406246203156
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.06496588, 10.60873575,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6121926037584516}
episode index:1818
target Thresh 18.999999999999996
target distance 1.0
model initialize at round 1818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.38705921,  7.85416006,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6300521401680124}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7640703988783583
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.38705921,  7.85416006,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6300521401680124}
episode index:1819
target Thresh 18.999999999999996
target distance 12.0
model initialize at round 1819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([5.        , 9.99999809, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.206554521941195}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7640342817889162
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.65839089,  2.99754935,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.6583954475646402}
episode index:1820
target Thresh 18.999999999999996
target distance 4.0
model initialize at round 1820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.09972848, 7.00307953, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.196091170145579}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7641364046435077
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.94794293, 5.01920402, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.055486333477269376}
episode index:1821
target Thresh 18.999999999999996
target distance 2.0
model initialize at round 1821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.9040072 , 10.05981356,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.11310296275866132}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7642658577693895
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.9040072 , 10.05981356,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.11310296275866132}
episode index:1822
target Thresh 18.999999999999996
target distance 2.0
model initialize at round 1822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.0885787 , 7.90180099, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9061408345439942}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7643951688731913
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.0885787 , 7.90180099, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9061408345439942}
episode index:1823
target Thresh 18.999999999999996
target distance 12.0
model initialize at round 1823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([12.        , 10.99916756,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.20607826266851}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7643791034794147
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.60098573, 4.00709053, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.39907726560403695}
episode index:1824
target Thresh 18.999999999999996
target distance 5.0
model initialize at round 1824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([2.77999306, 5.00019753, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 3.2387655299713547}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7644547861624397
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.33673535, 2.00731814, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.33681486200115723}
episode index:1825
target Thresh 18.999999999999996
target distance 12.0
model initialize at round 1825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.301281784550701}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7644185772412629
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.07507279,  8.17180756,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.2415285158593259}
episode index:1826
target Thresh 18.999999999999996
target distance 9.0
model initialize at round 1826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([10.00077841, 11.8473486 ,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 9.121534084382532}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7644459924972886
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.54179751, 5.96239685, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.45974288105981503}
episode index:1827
target Thresh 18.999999999999996
target distance 5.0
model initialize at round 1827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.07410038,  3.99999774,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.0009172690138044}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7645215143832309
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.60073513,  7.91064327,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9943257009989617}
episode index:1828
target Thresh 18.999999999999996
target distance 3.0
model initialize at round 1828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.26962543, 8.00690925, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.2439104708499558}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7646229241621357
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.46125317, 6.28671634, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8494280782606685}
episode index:1829
target Thresh 18.999999999999996
target distance 13.0
model initialize at round 1829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4., 9., 0.]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.04536101718728}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7646067869853395
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.16368594,  9.99315421,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.1638290275377861}
episode index:1830
target Thresh 18.999999999999996
target distance 10.0
model initialize at round 1830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.354328643351877}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7646117974443862
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.06814636, 10.43265758,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.43799144501887094}
episode index:1831
target Thresh 18.999999999999996
target distance 14.0
model initialize at round 1831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 3.97380116, 11.86775028,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.363391530761419}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7645956839581312
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.81452392,  9.01883289,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.1864297538374847}
episode index:1832
target Thresh 18.999999999999996
target distance 2.0
model initialize at round 1832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.87760267, 8.03215683, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.12655104422267852}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7647241096624638
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.87760267, 8.03215683, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.12655104422267852}
episode index:1833
target Thresh 18.999999999999996
target distance 12.0
model initialize at round 1833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([12.        , 10.99927759,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.206141356126793}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7647079525092264
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.73793021, 4.01040424, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.26227623156806346}
episode index:1834
target Thresh 18.999999999999996
target distance 2.0
model initialize at round 1834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.1458    , 8.98292789, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8543705829630319}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7648361770582677
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.1458    , 8.98292789, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8543705829630319}
episode index:1835
target Thresh 18.999999999999996
target distance 1.0
model initialize at round 1835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.74457556, 6.86824965, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9050409424494794}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.764964261929151
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.74457556, 6.86824965, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9050409424494794}
episode index:1836
target Thresh 18.999999999999996
target distance 2.0
model initialize at round 1836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.04377991, 10.98784983,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9562972800633924}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7650922073499843
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.04377991, 10.98784983,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9562972800633924}
episode index:1837
target Thresh 18.999999999999996
target distance 1.0
model initialize at round 1837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.15716708, 1.90172577, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.18536265692744508}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7652200135483793
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.15716708, 1.90172577, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.18536265692744508}
episode index:1838
target Thresh 18.999999999999996
target distance 8.0
model initialize at round 1838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.42335245,  5.        ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.166517023190474}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7652468141119746
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.88099668, 10.85129996,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.893457804578475}
episode index:1839
target Thresh 18.999999999999996
target distance 3.0
model initialize at round 1839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.44711447,  9.99989223,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0955030301942044}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7653472234521311
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.35236904, 11.76581594,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.00294562025278}
episode index:1840
target Thresh 18.999999999999996
target distance 5.0
model initialize at round 1840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.13210785,  5.99999774,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.002909601523768}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7654217225159811
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.63759027,  9.87962283,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.9513554201507348}
episode index:1841
target Thresh 18.999999999999996
target distance 5.0
model initialize at round 1841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 7.02612466, 10.13228383,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 4.355299350201126}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.765496140690511
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.42588018, 6.94200111, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.42981135317250657}
episode index:1842
target Thresh 18.999999999999996
target distance 1.0
model initialize at round 1842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.71371698,  1.57933974,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5088349609374687}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7656233809831368
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.71371698,  1.57933974,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5088349609374687}
episode index:1843
target Thresh 18.999999999999996
target distance 5.0
model initialize at round 1843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.35667908,  6.00011933,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 3.0683183975714874}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7656976090845559
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.64921165,  2.28576875,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.795725299463359}
episode index:1844
target Thresh 18.999999999999996
target distance 2.0
model initialize at round 1844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.34109616, 9.99260879, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6589452911524799}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7658246022503638
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.34109616, 9.99260879, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6589452911524799}
episode index:1845
target Thresh 18.999999999999996
target distance 7.0
model initialize at round 1845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.47001708,  4.        ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 5.022043015675792}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7658741961819724
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.416269  ,  9.92305515,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0921413304931002}
episode index:1846
target Thresh 18.999999999999996
target distance 12.0
model initialize at round 1846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.180339887498956}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7658575300717629
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.64130947,  6.02794698,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.35977761349807486}
episode index:1847
target Thresh 18.999999999999996
target distance 5.0
model initialize at round 1847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.8013128 ,  7.00047016,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 3.00704136842939}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7659314708022434
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.17190039,  3.1621691 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8552837912295307}
episode index:1848
target Thresh 18.999999999999996
target distance 5.0
model initialize at round 1848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 3.99505549, 11.86740304,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.1276316225507483}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7660053315535673
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.66205144, 10.57527762,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.7865756242800592}
episode index:1849
target Thresh 18.999999999999996
target distance 12.0
model initialize at round 1849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.180339887498956}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7660095345838086
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.97791009,  6.98444369,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9846914956116927}
episode index:1850
target Thresh 18.999999999999996
target distance 2.0
model initialize at round 1850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.92341256, 11.86048498,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8638865867469157}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7661359475851139
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.92341256, 11.86048498,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8638865867469157}
episode index:1851
target Thresh 18.999999999999996
target distance 2.0
model initialize at round 1851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.12457085, 5.02115726, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.1263547611373013}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7662622240712991
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.12457085, 5.02115726, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.1263547611373013}
episode index:1852
target Thresh 18.999999999999996
target distance 8.0
model initialize at round 1852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.00147593, 9.00000846, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.00000864539009}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7663113944846444
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.71054628, 3.015421  , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.7107136006712577}
episode index:1853
target Thresh 18.999999999999996
target distance 9.0
model initialize at round 1853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.00004578,  4.        ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.602351874096318}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7663373895523441
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.96421769, 11.86461782,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.295098348520137}
episode index:1854
target Thresh 18.999999999999996
target distance 3.0
model initialize at round 1854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.86135164,  3.02475573,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.338675075150766}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7664363990458469
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.76491238,  1.8851477 ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.2616433414325138}
episode index:1855
target Thresh 18.999999999999996
target distance 13.0
model initialize at round 1855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.       ,  9.9990648,  0.       ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.037902749994776}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7663808947528745
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.94308236, 2.32690486, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1586463708330987}
episode index:1856
target Thresh 18.999999999999996
target distance 2.0
model initialize at round 1856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.94627404, 5.014328  , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.055603688879917776}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7665066993329753
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.94627404, 5.014328  , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.055603688879917776}
episode index:1857
target Thresh 18.999999999999996
target distance 12.0
model initialize at round 1857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.32905180961646}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.766510614423485
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.83136231, 10.83456487,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1779905890776956}
episode index:1858
target Thresh 18.999999999999996
target distance 8.0
model initialize at round 1858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 7.97380134, 11.86775023,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 6.67375918864073}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7665594925222352
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.05318465,  8.27569322,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7262567825508731}
episode index:1859
target Thresh 18.999999999999996
target distance 3.0
model initialize at round 1859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.87023479,  6.00143802,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.009810438453896}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7666581164509867
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.41269225,  4.1429348 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9512495207196472}
episode index:1860
target Thresh 18.999999999999996
target distance 13.0
model initialize at round 1860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.        , 10.99882042,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.600776749019843}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7666214045647121
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.56265912, 2.97315447, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.4381640397388349}
episode index:1861
target Thresh 19.0
target distance 12.0
model initialize at round 1861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 4.97380126, 11.86775025,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.152804398407474}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7666044714208131
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.95173703,  5.04486442,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.0658948474586321}
episode index:1862
target Thresh 19.0
target distance 9.0
model initialize at round 1862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 9.02618527, 10.13225332,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 10.747130948453577}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7666083235228417
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.81871116, 2.97409679, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9908229919984647}
episode index:1863
target Thresh 19.0
target distance 11.0
model initialize at round 1863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.132283595738201}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7665716974351652
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.91544609, 5.9196108 , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.1166695658127089}
episode index:1864
target Thresh 19.0
target distance 11.0
model initialize at round 1864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.02600195, 11.86770022,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.25491182746356}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.766575562979436
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.39828367, 7.0414093 , 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.40043053246925564}
episode index:1865
target Thresh 19.0
target distance 1.0
model initialize at round 1865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.91005516, 8.79625154, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8013155319849571}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7667006564612261
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.91005516, 8.79625154, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8013155319849571}
episode index:1866
target Thresh 19.0
target distance 7.0
model initialize at round 1866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([12.14612869,  9.82336033,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 6.1113308162146875}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7667492233297526
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.98138796,  3.92101364,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.08114956924797284}
episode index:1867
target Thresh 19.0
target distance 1.0
model initialize at round 1867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.2535094 ,  9.88247252,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.1558572370364681}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7668740899125525
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.2535094 ,  9.88247252,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.1558572370364681}
episode index:1868
target Thresh 19.0
target distance 8.0
model initialize at round 1868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.00000048, 10.99999762,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.00000047683769}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.766922512015328
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.03847971, 11.3316176 ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.33384265566148236}
episode index:1869
target Thresh 19.0
target distance 7.0
model initialize at round 1869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 9.97487629, 10.13253072,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 5.921538396909185}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7669950133458011
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.16110236,  7.94086694,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.2605474357981736}
episode index:1870
target Thresh 19.0
target distance 9.0
model initialize at round 1870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.0261972, 11.8677498,  0.       ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.079578858093278}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7670204068448145
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.33306517, 11.88422195,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.1075425556322558}
episode index:1871
target Thresh 19.0
target distance 12.0
model initialize at round 1871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.        ,  6.99999952,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.198038933670144}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7669837171489006
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.56481486, 4.87602547, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.5782607616712888}
episode index:1872
target Thresh 19.0
target distance 3.0
model initialize at round 1872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.42065632,  8.00189829,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.1573414692872797}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7670814300601931
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.92232072,  6.14223802,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8612721305064452}
episode index:1873
target Thresh 19.0
target distance 9.0
model initialize at round 1873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.00000012,  9.        ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.280110003903117}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7671067367944193
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.18676754, 11.86958037,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.1905952552467824}
episode index:1874
target Thresh 19.0
target distance 12.0
model initialize at round 1874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 3.99506741, 11.86740145,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 14.05219866596289}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7670514374314832
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.97438519,  2.06405828,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.06898972615099928}
episode index:1875
target Thresh 19.0
target distance 8.0
model initialize at round 1875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.99880552,  4.        ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 6.000000118897976}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7670767331737904
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.16657187,  9.84212536,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.2295007439558727}
episode index:1876
target Thresh 19.0
target distance 3.0
model initialize at round 1876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.85536566,  4.01932747,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.3306685952215993}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7671741882972994
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.70556637,  2.65977984,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.44993434679873345}
episode index:1877
target Thresh 19.0
target distance 6.0
model initialize at round 1877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 3.97380115, 11.86775025,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.118648763168526}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7672462467699845
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.90358313, 10.74336046,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.2741533617917415}
episode index:1878
target Thresh 19.0
target distance 8.0
model initialize at round 1878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.00019073, 8.00000083, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 6.000000837496679}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7672942131101814
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.77031994, 2.07843199, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.24270250602470553}
episode index:1879
target Thresh 19.0
target distance 1.0
model initialize at round 1879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.34242225, 1.79936481, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.3968721161512941}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.767417992784059
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.34242225, 1.79936481, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.3968721161512941}
episode index:1880
target Thresh 19.0
target distance 5.0
model initialize at round 1880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.86560926, 4.99944999, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.533241892985956}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7674898067166565
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.34780114, 8.8416316 , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0647568259585765}
episode index:1881
target Thresh 19.0
target distance 13.0
model initialize at round 1881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.        , 10.99999726,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.083044839024286}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7674931495066584
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.87260704, 6.96626171, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.301961880250368}
episode index:1882
target Thresh 19.0
target distance 6.0
model initialize at round 1882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.00001025, 10.99997914,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.000010252053324}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7675648472498836
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.03981981, 10.64008768,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.36210840071288386}
episode index:1883
target Thresh 19.0
target distance 12.0
model initialize at round 1883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([12.00494181, 11.86740294,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.126116668305981}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7675681466608445
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.00894027, 6.46585545, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5342193646917087}
episode index:1884
target Thresh 19.0
target distance 2.0
model initialize at round 1884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.94246424, 11.86007364,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8619959602844911}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7676914526838361
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.94246424, 11.86007364,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8619959602844911}
episode index:1885
target Thresh 19.0
target distance 7.0
model initialize at round 1885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.86710211,  9.00327765,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.077859127328291}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7677390049358594
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.81902643,  3.13958399,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.187905715697736}
episode index:1886
target Thresh 19.0
target distance 8.0
model initialize at round 1886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 9.02595809, 11.8676889 ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 6.088107662068027}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7677865067880397
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.10168394, 11.2615112 ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.28058462265980183}
episode index:1887
target Thresh 19.0
target distance 7.0
model initialize at round 1887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.80051208,  8.0000456 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 5.063721510739627}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7678339583204614
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.72067618,  2.57092863,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.511980504240483}
episode index:1888
target Thresh 19.0
target distance 2.0
model initialize at round 1888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.95190391, 11.86805557,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8693869731677305}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7679568625246327
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.95190391, 11.86805557,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8693869731677305}
episode index:1889
target Thresh 19.0
target distance 12.0
model initialize at round 1889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 5.97380116, 11.86775028,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 10.746355444500667}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7679599440457837
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.9389827 ,  7.99512178,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.061211987457383786}
episode index:1890
target Thresh 19.0
target distance 11.0
model initialize at round 1890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([11.02619873, 11.86775025,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 9.217415823466206}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7679630223077902
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.66133142, 9.61221759, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5148510523434675}
episode index:1891
target Thresh 19.0
target distance 1.0
model initialize at round 1891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.6612182 , 4.78784262, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6944208082174408}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.768085663416507
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.6612182 , 4.78784262, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6944208082174408}
episode index:1892
target Thresh 19.0
target distance 4.0
model initialize at round 1892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.35858209,  3.99998712,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.100349599806085}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7681817618510467
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.15766769,  5.98549471,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8424571927870016}
episode index:1893
target Thresh 19.0
target distance 2.0
model initialize at round 1893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.14774198, 6.9919376 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.852296151357535}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7683041579641137
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.14774198, 6.9919376 , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.852296151357535}
episode index:1894
target Thresh 19.0
target distance 4.0
model initialize at round 1894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.14165541, 8.03366778, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.207387618852642}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7684000396749506
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.90468319, 6.04827738, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.10684568218789131}
episode index:1895
target Thresh 19.0
target distance 14.0
model initialize at round 1895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4., 7., 0.]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.369316876852995}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7683630867511209
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.3185013 ,  4.86355574,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9204192456455282}
episode index:1896
target Thresh 19.0
target distance 6.0
model initialize at round 1896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.51034075,  7.00006354,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 4.029922392968021}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7684337967739193
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.80702208,  3.03089476,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8076132248674455}
episode index:1897
target Thresh 19.0
target distance 11.0
model initialize at round 1897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.972155793073503}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7684366140240385
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.95123255, 11.78176973,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.231262470616025}
episode index:1898
target Thresh 19.0
target distance 12.0
model initialize at round 1898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.        , 10.99999809,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.770328905897669}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7684394283070696
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.07850715, 7.00482546, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.07865530479726929}
episode index:1899
target Thresh 19.0
target distance 7.0
model initialize at round 1899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.86731241,  3.9956014 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 5.078999528632376}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7684862365026975
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.65745004,  9.76909773,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8419333690894706}
episode index:1900
target Thresh 19.0
target distance 5.0
model initialize at round 1900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.02142835,  5.0003221 ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 3.0003986233152844}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7685567329590348
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.85942904,  1.71367815,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.31896770827927945}
episode index:1901
target Thresh 19.0
target distance 3.0
model initialize at round 1901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 6.9745776 , 11.86744392,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.3431120753513273}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7686521289984886
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.4993329 , 10.74666899,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.5599195901937382}
episode index:1902
target Thresh 19.0
target distance 12.0
model initialize at round 1902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 6., 11.,  0.]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.180339887498956}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7686548241159356
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.9305169 ,  6.01613513,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.07133192962963499}
episode index:1903
target Thresh 19.0
target distance 8.0
model initialize at round 1903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 8.02625546, 10.13224866,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 8.190220023329964}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7686789057471771
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.73417694, 3.94363893, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9803654046578821}
episode index:1904
target Thresh 19.0
target distance 11.0
model initialize at round 1904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([12.02619859, 11.86775021,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.21741567854731}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7686815839790684
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.44409477, 9.70246622, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.630521191718526}
episode index:1905
target Thresh 19.0
target distance 14.0
model initialize at round 1905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.00001628, 11.81658129,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 15.503730487762592}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7686263577709415
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.70972479, 2.41734504, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5083665772541331}
episode index:1906
target Thresh 19.0
target distance 13.0
model initialize at round 1906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.00482588, 11.86738405,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.971783084926313}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.768608772837986
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.10121386, 5.99428193, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.3403033497327894}
episode index:1907
target Thresh 19.0
target distance 2.0
model initialize at round 1907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.2191813 ,  2.05591881,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.22620202373714962}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7687300470660584
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.2191813 ,  2.05591881,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.22620202373714962}
episode index:1908
target Thresh 19.0
target distance 8.0
model initialize at round 1908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.86740303, 4.99505765, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.372385933301653}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7687764823478468
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.03662272, 11.13925249,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.14398777103014973}
episode index:1909
target Thresh 19.0
target distance 6.0
model initialize at round 1909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.04567194,  9.05523351,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.5015904924563594}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7688464946607536
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.89225518,  5.06289857,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.12476048003423325}
episode index:1910
target Thresh 19.0
target distance 3.0
model initialize at round 1910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.15354234,  6.99981368,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.011903124349176}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7689412897969856
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.35346901,  8.74590302,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9871036539450049}
episode index:1911
target Thresh 19.0
target distance 12.0
model initialize at round 1911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.1322477 ,  7.97370955,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.633770005862003}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7688861010634563
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.59437905, 2.12126954, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.4233611403927293}
episode index:1912
target Thresh 19.0
target distance 12.0
model initialize at round 1912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.        ,  8.99953258,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.661663311165768}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7688309700285507
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.8874363 , 2.32163027, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.1170177592871597}
episode index:1913
target Thresh 19.0
target distance 6.0
model initialize at round 1913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.5550766 ,  3.99999988,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.291650519649526}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7689008075572714
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.16402226,  7.99245739,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8360117697715057}
episode index:1914
target Thresh 19.0
target distance 1.0
model initialize at round 1914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.98924935, 5.00909305, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.0091503120796141}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.768995376326171
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.61946675, 3.17351544, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9098803671313033}
episode index:1915
target Thresh 19.0
target distance 5.0
model initialize at round 1915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.05421549,  5.99999917,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.0004906809112954}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7690650551485477
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.05868918,  9.84676421,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.848795641815565}
episode index:1916
target Thresh 19.0
target distance 6.0
model initialize at round 1916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86723797,  5.99602485,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.0968181210427375}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7691346612752308
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.08990053,  9.99095623,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9101444048196374}
episode index:1917
target Thresh 19.0
target distance 9.0
model initialize at round 1917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.13259726,  3.99505944,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.684051294120314}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7691370837341592
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.00543997, 10.44868197,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.5513448652991384}
episode index:1918
target Thresh 19.0
target distance 4.0
model initialize at round 1918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.68122911,  5.99999774,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.1128374680504867}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.769231332257487
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.16543853,  7.98911321,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8346324727969582}
episode index:1919
target Thresh 19.0
target distance 8.0
model initialize at round 1919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13262017, 8.00480173, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 6.067123832333728}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7692772404177696
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.61619873, 2.04004504, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.38588472132522}
episode index:1920
target Thresh 19.0
target distance 10.0
model initialize at round 1920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.13228730466267}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7692048677833639
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.85165516,  3.97641003,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.1502087821441735}
episode index:1921
target Thresh 19.0
target distance 11.0
model initialize at round 1921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.414114271867186}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7691871190959767
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.04369237, 11.37600867,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.37853869285289}
episode index:1922
target Thresh 19.0
target distance 6.0
model initialize at round 1922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.86587511,  4.97954956,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 4.11263436879804}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7692564445670655
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.86854154,  8.97621877,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.13359219031360572}
episode index:1923
target Thresh 19.0
target distance 11.0
model initialize at round 1923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.350086702751616}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.769201436244156
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.0779412 , 5.94891707, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.09318957172443922}
episode index:1924
target Thresh 19.0
target distance 11.0
model initialize at round 1924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.470810794869452}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7692249712123409
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.06272855,  9.06054272,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.9392247803801097}
episode index:1925
target Thresh 19.0
target distance 4.0
model initialize at round 1925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([10.99949768, 11.84294248,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 2.1708435153428534}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7693188315595827
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.99897226, 10.09707051,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9029300771805194}
episode index:1926
target Thresh 19.0
target distance 3.0
model initialize at round 1926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.74096191, 6.99996102, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.2446294691271542}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.769412594490792
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.34896863, 8.88823617, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.1012744145043207}
episode index:1927
target Thresh 19.0
target distance 12.0
model initialize at round 1927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.315143201383691}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7693576193024094
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.93702919, 5.99589932, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.06310419143894518}
episode index:1928
target Thresh 19.0
target distance 12.0
model initialize at round 1928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 5.        , 10.99999952,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.206555342285537}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7693398558349769
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.64627823,  4.98763807,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0490700880240698}
episode index:1929
target Thresh 19.0
target distance 10.0
model initialize at round 1929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 9.980393886139279}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7693221107752826
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.02370086,  9.21623383,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7841244424960246}
episode index:1930
target Thresh 19.0
target distance 7.0
model initialize at round 1930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13262448, 7.00477681, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.079382955735809}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.769367710407196
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.15524136, 1.50933259, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5146400544703211}
episode index:1931
target Thresh 19.0
target distance 12.0
model initialize at round 1931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.132280553149382}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.769330945182396
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.92529652,  5.93376758,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.09983658723031641}
episode index:1932
target Thresh 19.0
target distance 2.0
model initialize at round 1932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.04895627, 11.48579302,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.4882536014415081}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7694502773369835
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.04895627, 11.48579302,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.4882536014415081}
episode index:1933
target Thresh 19.0
target distance 4.0
model initialize at round 1933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.32590228, 4.99999797, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.0263811103603904}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.769543632932983
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.13981903, 6.98223943, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8603643090556022}
episode index:1934
target Thresh 19.0
target distance 10.0
model initialize at round 1934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([10.0261074 , 11.86772707,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 8.909417084907012}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7695668694275913
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.9890007 , 8.71809209, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7181763279896637}
episode index:1935
target Thresh 19.0
target distance 11.0
model initialize at round 1935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 5.97380127, 11.86775025,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.34188066608457}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7695490621038297
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.90008065,  4.12472497,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8809598481902143}
episode index:1936
target Thresh 19.0
target distance 3.0
model initialize at round 1936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.04361546,  9.00177324,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.0027222592139973}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7696422221130688
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.84867765,  7.10982168,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9029484481131377}
episode index:1937
target Thresh 19.0
target distance 12.0
model initialize at round 1937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.86775037, 7.97380131, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.174760434410567}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7696243942846436
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.08002249,  6.22417197,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7799440547280377}
episode index:1938
target Thresh 19.0
target distance 4.0
model initialize at round 1938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.81989741,  7.00122929,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.009317196878673}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7697174193520574
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.50993609,  5.00629842,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.5099749897141566}
episode index:1939
target Thresh 19.0
target distance 13.0
model initialize at round 1939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.68488566552728}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7697195139490409
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.03800012,  7.41037984,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.1283154256192442}
episode index:1940
target Thresh 19.0
target distance 8.0
model initialize at round 1940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4.86356774, 3.99964533, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 6.648629659146833}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7697646739109425
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.15846902, 9.98145641, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8417352659522168}
episode index:1941
target Thresh 19.0
target distance 2.0
model initialize at round 1941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.06513119,  8.80307734,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8057141464570823}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7698832296916268
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.06513119,  8.80307734,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8057141464570823}
episode index:1942
target Thresh 19.0
target distance 13.0
model initialize at round 1942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.        ,  9.99997962,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.083037538323365}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7698653237013713
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.18959482, 5.97081119, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.2646070268673042}
episode index:1943
target Thresh 19.0
target distance 6.0
model initialize at round 1943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13262744, 6.00476313, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 4.097616732317867}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7699335514155166
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.35880583, 2.02052325, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.64152254232153}
episode index:1944
target Thresh 19.0
target distance 11.0
model initialize at round 1944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.132283595738201}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7698787888858886
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.81493665, 4.94972904, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.19176968540048944}
episode index:1945
target Thresh 19.0
target distance 2.0
model initialize at round 1945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.03461468, 10.99697232,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.03474684237307255}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7699970423345598
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.03461468, 10.99697232,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.03474684237307255}
episode index:1946
target Thresh 19.0
target distance 2.0
model initialize at round 1946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.2073806 , 6.01334012, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.7927316534314042}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7701151743107618
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.2073806 , 6.01334012, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.7927316534314042}
episode index:1947
target Thresh 19.0
target distance 7.0
model initialize at round 1947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.86759864, 3.97439047, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.099948877861421}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.770159968882471
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.87396412, 9.89181226, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9006742755617656}
episode index:1948
target Thresh 19.0
target distance 11.0
model initialize at round 1948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.574514830248853}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7701419760254892
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.89628904,  9.98174843,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.10530471218899903}
episode index:1949
target Thresh 19.0
target distance 13.0
model initialize at round 1949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.52844675707443}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7701051531127037
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.84844748,  4.88505107,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8979329415032392}
episode index:1950
target Thresh 19.0
target distance 8.0
model initialize at round 1950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.24073529,  8.00001442,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 6.047863754107722}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7701498839414517
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.2807886 ,  2.01282275,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.28108123556426895}
episode index:1951
target Thresh 19.0
target distance 8.0
model initialize at round 1951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.00000167,  6.        ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.810250958014311}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7701726074896373
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.94278914, 10.62911636,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.3752702483043859}
episode index:1952
target Thresh 19.0
target distance 11.0
model initialize at round 1952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.18772204862523}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7701358254561526
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.88100969, 6.92064114, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9282988720561114}
episode index:1953
target Thresh 19.0
target distance 3.0
model initialize at round 1953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.22294235,  3.9998616 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.266528882806229}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7702278746754687
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.61730972,  5.92331338,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9994795882336039}
episode index:1954
target Thresh 19.0
target distance 12.0
model initialize at round 1954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.174760518110984}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.7701401555269075
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.66833447, 2.23209369, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8364700394228857}
episode index:1955
target Thresh 19.0
target distance 6.0
model initialize at round 1955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.56230366, 8.00048327, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 4.024356409821203}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7702078241590512
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.62504621, 4.009004  , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6251110603137513}
episode index:1956
target Thresh 19.0
target distance 1.0
model initialize at round 1956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.6237716 ,  9.12333537,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0759330348674103}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7703252447905489
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.6237716 ,  9.12333537,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0759330348674103}
episode index:1957
target Thresh 19.0
target distance 3.0
model initialize at round 1957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.77196227, 4.99995557, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.2633347175476473}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7704170092211973
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.26734085, 6.96319584, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.999608605815011}
episode index:1958
target Thresh 19.0
target distance 9.0
model initialize at round 1958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.13225001,  3.9738023 ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.325874992317592}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7703989770013931
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.19015668, 11.87077666,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.189158612720359}
episode index:1959
target Thresh 19.0
target distance 14.0
model initialize at round 1959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4., 5., 0.]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.64911064067354}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7703622108376648
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.133359 ,  9.7275594,  0.       ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.1315517265235968}
episode index:1960
target Thresh 19.0
target distance 2.0
model initialize at round 1960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.58737683,  5.0118413 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.41279304051145016}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7704793132288745
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.58737683,  5.0118413 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.41279304051145016}
episode index:1961
target Thresh 19.0
target distance 13.0
model initialize at round 1961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.17786313458049}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7704612768259164
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.20527993,  7.54529624,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9156066314541388}
episode index:1962
target Thresh 19.0
target distance 13.0
model initialize at round 1962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4., 7., 0.]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.401754250991402}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.770443258799324
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.93030249, 10.29233332,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.3005270589755486}
episode index:1963
target Thresh 19.0
target distance 12.0
model initialize at round 1963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4., 5., 0.]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.440306508910572}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7704252591210274
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.23629727,  8.86747043,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1557451329896098}
episode index:1964
target Thresh 19.0
target distance 8.0
model initialize at round 1964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.088354702269999}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.770469508353027
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.92779422, 10.60082979,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.40564828292200933}
episode index:1965
target Thresh 19.0
target distance 3.0
model initialize at round 1965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.66542602,  8.999982  ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.2011776655036064}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.770560825998829
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.06256342, 10.86967993,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.278722221445675}
episode index:1966
target Thresh 19.0
target distance 11.0
model initialize at round 1966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 4.9950577 , 11.86740302,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 11.324760926290534}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7705427940032145
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.88472324,  5.02076157,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.11713143460332245}
episode index:1967
target Thresh 19.0
target distance 3.0
model initialize at round 1967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.22287419,  3.99985493,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.2665759681385413}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7706339816078877
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.16122523,  5.89535669,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.226868668528102}
episode index:1968
target Thresh 19.0
target distance 10.0
model initialize at round 1968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 8.824569335970134}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7706562631052936
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.25123752,  8.88895926,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7569513145475538}
episode index:1969
target Thresh 19.0
target distance 4.0
model initialize at round 1969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.41615773,  7.00035524,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.0838169049873345}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7707473005351894
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.16440301,  5.03822215,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8364707203056417}
episode index:1970
target Thresh 19.0
target distance 4.0
model initialize at round 1970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.99999976,  9.00010908,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.236165645638679}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7708382455881903
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.46671128,  7.00629687,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.46675375953310994}
episode index:1971
target Thresh 19.0
target distance 2.0
model initialize at round 1971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.99111569, 9.99989307, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.008884954203204197}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7709544533744032
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.99111569, 9.99989307, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.008884954203204197}
episode index:1972
target Thresh 19.0
target distance 5.0
model initialize at round 1972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.92752469, 5.99999988, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.5658592414528125}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7710211262312838
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.42833568, 9.89362252, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0608304737367293}
episode index:1973
target Thresh 19.0
target distance 11.0
model initialize at round 1973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.800295654223671}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7710225243119672
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.2359826 ,  7.15388749,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8784043258537158}
episode index:1974
target Thresh 19.0
target distance 10.0
model initialize at round 1974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.06844788,  8.50462156,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 11.160055938833725}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7709680422395505
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.89122103, 2.33342366, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9515493970883256}
episode index:1975
target Thresh 19.0
target distance 8.0
model initialize at round 1975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([12.14612966,  9.82335953,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 7.0707192374635595}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7710117704570406
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.69063233,  3.91875544,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.969443090114714}
episode index:1976
target Thresh 19.0
target distance 10.0
model initialize at round 1976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.000000000000018}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7710337706996016
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.07755306, 11.50099169,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.5069587272436066}
episode index:1977
target Thresh 19.0
target distance 11.0
model initialize at round 1977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.184084079649052}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7709625955929408
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.3408234 , 4.09316357, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9687687586561278}
episode index:1978
target Thresh 19.0
target distance 3.0
model initialize at round 1978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.99993908, 9.99999988, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.414256721399332}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7710530642156832
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.30133537, 11.88165581,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.931729563009624}
episode index:1979
target Thresh 19.0
target distance 12.0
model initialize at round 1979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([12.82335934,  9.14612992,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 10.980980905198429}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7710544419294632
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.93126054, 11.49302627,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.0537177489129423}
episode index:1980
target Thresh 19.0
target distance 2.0
model initialize at round 1980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.30804831, 2.02982736, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.30948898394536123}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7711700126301549
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.30804831, 2.02982736, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.30948898394536123}
episode index:1981
target Thresh 19.0
target distance 3.0
model initialize at round 1981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.10433817,  5.00282896,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0082422168414573}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.771260239667173
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.16692619,  3.27365338,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.1052562498171323}
episode index:1982
target Thresh 19.0
target distance 11.0
model initialize at round 1982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.        , 10.93639612,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.999432628460383}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.771223465615951
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.84085902, 3.27551603, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.31817438224264194}
episode index:1983
target Thresh 19.0
target distance 7.0
model initialize at round 1983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([4.84544735, 3.99992066, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 5.753030851637565}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7712668887683624
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.4660358 , 9.85050636, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0042304633660104}
episode index:1984
target Thresh 19.0
target distance 12.0
model initialize at round 1984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([12.82335934,  9.14612992,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.034082698457302}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7712681552916529
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.72798831, 6.85114369, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.743051266185111}
episode index:1985
target Thresh 19.0
target distance 8.0
model initialize at round 1985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.86693265, 4.97672319, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 6.305973440545154}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7713115122124526
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.30127864, 10.97548209,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.6991513883299525}
episode index:1986
target Thresh 19.0
target distance 5.0
model initialize at round 1986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.99682188,  9.000049  ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.1613205838120373}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7713775356084202
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.1491394 ,  6.06227688,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8531366663403462}
episode index:1987
target Thresh 19.0
target distance 7.0
model initialize at round 1987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.00000143,  8.99999964,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.385166268150739}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7714207938903074
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.28810662, 10.41277913,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9228328848536477}
episode index:1988
target Thresh 19.0
target distance 11.0
model initialize at round 1988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.132283595738201}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7714025289816772
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.07433894,  6.89435714,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.12917697981037984}
episode index:1989
target Thresh 19.0
target distance 12.0
model initialize at round 1989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 3.99505755, 11.86740305,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.72640106542608}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7714037241618372
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.0591379 ,  8.04799738,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.07616455805003998}
episode index:1990
target Thresh 19.0
target distance 12.0
model initialize at round 1990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 10.568784232893615}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7713854861741241
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.0959501 ,  8.13006583,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.2546281032943907}
episode index:1991
target Thresh 19.0
target distance 12.0
model initialize at round 1991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.182309676547058}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7713672664976435
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.2498065 , 7.48745556, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5701798719917986}
episode index:1992
target Thresh 19.0
target distance 12.0
model initialize at round 1992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([12.05521678, 10.045681  ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 10.100401435738299}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7713684775719047
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.1600945 , 10.90192155,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.1877488569240641}
episode index:1993
target Thresh 19.0
target distance 13.0
model initialize at round 1993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.        ,  9.99999726,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.083044839024286}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7713502846998149
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13515782, 4.9632737 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8656216347985057}
episode index:1994
target Thresh 19.0
target distance 12.0
model initialize at round 1994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.049875621120908}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7713321100661935
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.38808001, 6.23248668, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8600481323444576}
episode index:1995
target Thresh 19.0
target distance 4.0
model initialize at round 1995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.86725394, 6.99593568, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 2.18366737353026}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7714216230371023
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.7194226 , 8.99289733, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7194576629824887}
episode index:1996
target Thresh 19.0
target distance 7.0
model initialize at round 1996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 7.02614887, 10.13226224,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 5.922266390327694}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7714872606820511
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.83381305, 6.94599095, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8355603959141921}
episode index:1997
target Thresh 19.0
target distance 12.0
model initialize at round 1997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.327137713346806}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7714690447811215
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.39086784, 5.86523522, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.6238617888869084}
episode index:1998
target Thresh 19.0
target distance 13.0
model initialize at round 1998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.176978770806391}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7713983996410233
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.85974798,  3.93380076,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.2693110158064047}
episode index:1999
target Thresh 19.0
target distance 12.0
model initialize at round 1999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.13259688,  5.99505098,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.891737780154259}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7713278251460651
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.71581738, 2.18248389, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.3377278986420205}
episode index:2000
target Thresh 19.0
target distance 1.0
model initialize at round 2000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.41652286,  7.15346241,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0281398074270063}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7714421040940181
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.41652286,  7.15346241,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0281398074270063}
episode index:2001
target Thresh 19.0
target distance 1.0
model initialize at round 2001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.26770222,  6.35840595,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9736030833040769}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.771556268877188
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.26770222,  6.35840595,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9736030833040769}
episode index:2002
target Thresh 19.0
target distance 4.0
model initialize at round 2002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.00024331, 9.00054526, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.2366644857894498}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7716453571103996
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.14462002, 7.02023851, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.855619370796929}
episode index:2003
target Thresh 19.0
target distance 11.0
model initialize at round 2003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.055061527404254}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7716464227692766
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.13388506,  6.82819617,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.21781130531857784}
episode index:2004
target Thresh 19.0
target distance 1.0
model initialize at round 2004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.82008563,  4.0872345 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9303279227967822}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7717603148277459
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.82008563,  4.0872345 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9303279227967822}
episode index:2005
target Thresh 19.0
target distance 10.0
model initialize at round 2005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12.0261988 , 11.86775027,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.072970813439131}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7717816238682106
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.17229396, 11.87420185,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8910185671669228}
episode index:2006
target Thresh 19.0
target distance 10.0
model initialize at round 2006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.94134623692523}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7717276322426107
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.93816935, 9.47005195, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.533542846540965}
episode index:2007
target Thresh 19.0
target distance 7.0
model initialize at round 2007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.86775027, 5.97380119, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.922302161264586}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.771770285314203
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.02022994, 11.8667486 ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8669846559941794}
episode index:2008
target Thresh 19.0
target distance 5.0
model initialize at round 2008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 9.96778308, 11.85793536,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.1512525300893683}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7718353573473965
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.90532025, 11.23503174,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9353313158133152}
episode index:2009
target Thresh 19.0
target distance 7.0
model initialize at round 2009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.99999475,  9.00000668,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.099027088346584}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.771877914383542
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.40089533,  3.09775722,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.987298889421284}
episode index:2010
target Thresh 19.0
target distance 12.0
model initialize at round 2010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 4.97380354, 11.86774967,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 14.067590377537611}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7718239822686268
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.19466319,  1.2928051 ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7334973695783837}
episode index:2011
target Thresh 19.0
target distance 12.0
model initialize at round 2011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([12.02541027, 11.86754609,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.152038528710957}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7718249549103919
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.89777727, 5.99019917, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9954616476154999}
episode index:2012
target Thresh 19.0
target distance 11.0
model initialize at round 2012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.82343494,  9.14599752,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.587629595031187}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7717884483734736
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.84901497, 2.87236857, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.19770245712042386}
episode index:2013
target Thresh 19.0
target distance 14.0
model initialize at round 2013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4., 6., 0.]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 13.000000000000025}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7717519780893228
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.96812495, 11.86545173,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8660385174222923}
episode index:2014
target Thresh 19.0
target distance 2.0
model initialize at round 2014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.51501989,  3.01479542,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5152323702756341}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.771865252541884
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.51501989,  3.01479542,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5152323702756341}
episode index:2015
target Thresh 19.0
target distance 13.0
model initialize at round 2015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4.99983213, 9.00007785, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.401936689188009}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7718470117869649
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.14330447,  5.10673742,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9046845939019357}
episode index:2016
target Thresh 19.0
target distance 12.0
model initialize at round 2016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4., 5., 0.]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 11.661903789690625}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.771828789119061
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.02557493, 11.02932165,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9748661362971021}
episode index:2017
target Thresh 19.0
target distance 5.0
model initialize at round 2017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([10.97381386, 10.1322528 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 5.1218469072213475}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7718935419490317
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.20745705,  6.91933238,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9424491730549786}
episode index:2018
target Thresh 19.0
target distance 5.0
model initialize at round 2018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.48268783,  8.000103  ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.044376762427304}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7719582306355354
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.55864421,  4.65635305,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.559364062128567}
episode index:2019
target Thresh 19.0
target distance 12.0
model initialize at round 2019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.179448465417412}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7719399799721638
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.81102862, 7.45095341, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9793975602325109}
episode index:2020
target Thresh 19.0
target distance 7.0
model initialize at round 2020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.13509769, 7.018842  , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 5.354123237367995}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7719610419563439
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.91948696, 2.82586583, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8297811238261604}
episode index:2021
target Thresh 19.0
target distance 5.0
model initialize at round 2021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 6.00000119, 10.40729362,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 3.0579908450072284}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.772025601282775
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.52638399, 10.66017732,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.5829164412835888}
episode index:2022
target Thresh 19.0
target distance 3.0
model initialize at round 2022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.24767363,  9.99990821,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.2514705576456977}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7721135767640983
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.14295686, 11.84051587,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8525864141440637}
episode index:2023
target Thresh 19.0
target distance 4.0
model initialize at round 2023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 5.94477865, 10.04567855,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 2.265979747807491}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7722014653131279
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.92717414, 11.86890309,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8719496462772856}
episode index:2024
target Thresh 19.0
target distance 7.0
model initialize at round 2024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([1.76874241, 7.00004363, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.475303345974951}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7722435263179116
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.46087665, 2.25072841, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.5246637178934739}
episode index:2025
target Thresh 19.0
target distance 5.0
model initialize at round 2025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([1.13313136, 7.0022279 , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 3.5353317891052187}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7723078187530953
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.87444499, 3.22115147, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.171007713734848}
episode index:2026
target Thresh 19.0
target distance 13.0
model initialize at round 2026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4., 6., 0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.401754250991402}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7722894586504173
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.16496082,  8.01355663,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.2924244470557984}
episode index:2027
target Thresh 19.0
target distance 12.0
model initialize at round 2027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 5.94478322, 10.045681  ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 10.261198524446938}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7722901940936371
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.00857392,  7.76799296,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.23216541040662012}
episode index:2028
target Thresh 19.0
target distance 11.0
model initialize at round 2028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.450449133057385}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7723110004297171
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.08028852,  9.05100962,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.921124953874341}
episode index:2029
target Thresh 19.0
target distance 11.0
model initialize at round 2029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.99983213, 9.00007785, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.487016853881881}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7722926658928675
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.64126721,  5.19791832,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8786490910377175}
episode index:2030
target Thresh 19.0
target distance 14.0
model initialize at round 2030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4., 5., 0.]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.369316876852995}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7722227282975114
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.86332236,  2.930634  ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.269411341039592}
episode index:2031
target Thresh 19.0
target distance 14.0
model initialize at round 2031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.369316876852997}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7721863673564663
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.13039256, 4.86494128, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.2265156003939615}
episode index:2032
target Thresh 19.0
target distance 8.0
model initialize at round 2032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.94685364,  5.        ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 6.074251543294612}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7722282702746383
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.57792503, 10.99095392,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.42217189456207815}
episode index:2033
target Thresh 19.0
target distance 5.0
model initialize at round 2033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.02664578,  6.00020504,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 3.000323362486186}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7722923173394
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.61923079,  2.23451245,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8549598713437642}
episode index:2034
target Thresh 19.0
target distance 6.0
model initialize at round 2034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([12.13178268,  9.83629749,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.9575874871485084}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.772334127011469
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.51629585,  8.09757089,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0238886696841512}
episode index:2035
target Thresh 19.0
target distance 12.0
model initialize at round 2035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.214321868139344}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7722643408045502
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.72750203,  8.34527439,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7091690645417212}
episode index:2036
target Thresh 19.0
target distance 13.0
model initialize at round 2036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.045361017187279}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7722460921790325
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.61894649, 6.26009647, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8322613876982342}
episode index:2037
target Thresh 19.0
target distance 9.0
model initialize at round 2037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.86740304, 3.99505761, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.322375823831532}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7722878629875806
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.01791366, 10.11833963,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.3197797449064368}
episode index:2038
target Thresh 19.0
target distance 13.0
model initialize at round 2038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4., 4., 0.]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.000000000000018}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7722182021473339
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.24552452,  3.91504636,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.2598064878349524}
episode index:2039
target Thresh 19.0
target distance 2.0
model initialize at round 2039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.33530736, 10.99744534,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.6646975495197827}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7723298598913794
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.33530736, 10.99744534,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.6646975495197827}
episode index:2040
target Thresh 19.0
target distance 10.0
model initialize at round 2040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.620595846702553}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7722936067978969
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.1451902 ,  7.95565373,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.966620008096941}
episode index:2041
target Thresh 19.0
target distance 8.0
model initialize at round 2041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.8608441 ,  3.97671367,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 6.304182656837172}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7723142790031869
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.33961322,  9.32243246,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7579148473211177}
episode index:2042
target Thresh 19.0
target distance 1.0
model initialize at round 2042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.40144348, 5.9695437 , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.049367355210497}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7724257257584473
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.40144348, 5.9695437 , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.049367355210497}
episode index:2043
target Thresh 19.0
target distance 13.0
model initialize at round 2043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.99983213, 9.00007773, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.038588167637691}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7723723963580219
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.66689045,  1.67944631,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.4622949703786881}
episode index:2044
target Thresh 19.0
target distance 11.0
model initialize at round 2044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 5.99505755, 11.86740305,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 9.800295654223671}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7723929997094361
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.10615467,  8.00008808,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8938453305476246}
episode index:2045
target Thresh 19.0
target distance 4.0
model initialize at round 2045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.02268603, 11.86661268,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.200517282796074}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7724798066499495
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.07717835, 10.43646432,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5687960634032501}
episode index:2046
target Thresh 19.0
target distance 12.0
model initialize at round 2046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.179448465417412}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7724615419132496
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.20457419,  7.40248431,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9948503505238597}
episode index:2047
target Thresh 19.0
target distance 13.0
model initialize at round 2047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4., 8., 0.]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.045361017187277}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7724432950132064
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.38785683,  6.12552769,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9566267516964004}
episode index:2048
target Thresh 19.0
target distance 4.0
model initialize at round 2048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.38984823,  8.99999738,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.091003507909403}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7725299503109062
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.38398237, 10.99829328,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.6160199946111873}
episode index:2049
target Thresh 19.0
target distance 10.0
model initialize at round 2049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.367486508257754}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7725305605485594
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.00287109,  6.86126003,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.861264818236214}
episode index:2050
target Thresh 19.0
target distance 5.0
model initialize at round 2050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.55992579,  8.00016391,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.0519666439222926}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.772593929363504
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.94742758,  4.06550574,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9359718887249542}
episode index:2051
target Thresh 19.0
target distance 10.0
model initialize at round 2051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([11.02610652, 11.86772682,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 9.386860509572386}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7726143544710267
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.44217152, 6.97021403, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.4431736185826077}
episode index:2052
target Thresh 19.0
target distance 8.0
model initialize at round 2052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.00038898, 9.00000107, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.000001085492408}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7726556407084982
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.9174497 , 3.00814613, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9174858609808625}
episode index:2053
target Thresh 19.0
target distance 6.0
model initialize at round 2053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.13770346,  6.99970211,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 4.002667295178643}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7726968867451542
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.92348094, 10.54666423,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0287518525381587}
episode index:2054
target Thresh 19.0
target distance 3.0
model initialize at round 2054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.98057473,  9.00081253,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.401125446413207}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7727831656323829
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.36199332,  7.13386501,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.075751993024728}
episode index:2055
target Thresh 19.0
target distance 12.0
model initialize at round 2055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.895369891272122}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7727648333001809
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.9488884 , 11.70711117,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7089559932144164}
episode index:2056
target Thresh 19.0
target distance 5.0
model initialize at round 2056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.02523628, 11.86747953,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.147153519370616}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7728279033860825
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.18505196, 11.4609377 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9362712561254503}
episode index:2057
target Thresh 19.0
target distance 7.0
model initialize at round 2057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.86740302, 5.99505773, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 5.131493261111593}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7728909121793838
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.05683755, 10.15984893,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8420714502839817}
episode index:2058
target Thresh 19.0
target distance 4.0
model initialize at round 2058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.11180687, 9.00045812, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.188771283649707}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7729769292205788
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.97511637, 7.03223169, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.04071948772482908}
episode index:2059
target Thresh 19.0
target distance 2.0
model initialize at round 2059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13350948, 5.00756451, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.866523542078761}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7730871345947435
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13350948, 5.00756451, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.866523542078761}
episode index:2060
target Thresh 19.0
target distance 10.0
model initialize at round 2060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 8.824569335970134}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7731072312058087
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.26304912,  8.85444467,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7511876911835687}
episode index:2061
target Thresh 19.0
target distance 11.0
model initialize at round 2061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.13259829055323}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.773037950012074
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.40806421,  3.94072319,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.4123471092121971}
episode index:2062
target Thresh 19.0
target distance 9.0
model initialize at round 2062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([10.02615099, 11.86773814,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 7.588854965932573}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.773078830792485
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.89896529, 8.65971254, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9612149299119315}
episode index:2063
target Thresh 19.0
target distance 13.0
model initialize at round 2063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.176978770806391}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7730426188086194
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.77759122, 5.8663019 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8943962450139028}
episode index:2064
target Thresh 19.0
target distance 3.0
model initialize at round 2064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.46460304, 9.99999166, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.1026661648147515}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7731283124556854
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.26226567, 11.88442245,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.1517182866497728}
episode index:2065
target Thresh 19.0
target distance 4.0
model initialize at round 2065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 5.94478004, 10.04567929,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 2.265978178695521}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7732139231466556
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.92835258, 11.8690311 ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8719795905243019}
episode index:2066
target Thresh 19.0
target distance 1.0
model initialize at round 2066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.21433292,  2.20799117,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.1155943515580722}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7733236406487617
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.21433292,  2.20799117,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.1155943515580722}
episode index:2067
target Thresh 19.0
target distance 9.0
model initialize at round 2067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([11.00000226, 11.45798421,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.523948327986316}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7733238617787671
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.12002634, 4.99405032, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.12017370817353837}
episode index:2068
target Thresh 19.0
target distance 7.0
model initialize at round 2068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.13232556,  5.97409853,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.459532746931168}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7733862958716725
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.96260169, 10.13936289,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.2912390329998327}
episode index:2069
target Thresh 19.0
target distance 7.0
model initialize at round 2069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.99999285, 6.        , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.099020916329539}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7734486696417827
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.93868587, 10.13181733,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.278621251570496}
episode index:2070
target Thresh 19.0
target distance 8.0
model initialize at round 2070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 9.97380356, 10.13225033,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 9.337239329628332}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7734684946443701
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.86027512,  3.9402265 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.2744014851639534}
episode index:2071
target Thresh 19.0
target distance 13.0
model initialize at round 2071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.311697890307714}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7734322344134094
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.11093592,  8.4776787 ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.4903913929719597}
episode index:2072
target Thresh 19.0
target distance 11.0
model initialize at round 2072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.614260354881594}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7734324026252215
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.08916355, 11.44710545,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.45590944035791237}
episode index:2073
target Thresh 19.0
target distance 11.0
model initialize at round 2073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.00397421, 11.8672378 ,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.323890962890028}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7734139163609977
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.8877712 , 5.02313551, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.11458863002192522}
episode index:2074
target Thresh 19.0
target distance 4.0
model initialize at round 2074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([5.85388541, 9.82337135, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 3.377620008812257}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7734990180880527
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.815631  , 7.90047388, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.919154574619107}
episode index:2075
target Thresh 19.0
target distance 7.0
model initialize at round 2075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 7.02619849, 10.13224968,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 7.928881216955233}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7735394207768349
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.13245066, 4.92828525, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.270572842458771}
episode index:2076
target Thresh 19.0
target distance 11.0
model initialize at round 2076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 5.94478322, 10.045681  ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 9.115393552020812}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7735591448159409
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.05339272,  8.09646335,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.3086037678836977}
episode index:2077
target Thresh 19.0
target distance 11.0
model initialize at round 2077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.13224972,  4.97380104,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.322711609661402}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7734901795921242
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.89723336, 2.95118063, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.11377307595410402}
episode index:2078
target Thresh 19.0
target distance 4.0
model initialize at round 2078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.04543671,  9.05567362,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.2214329707322937}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7735750809006416
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.95879541,  7.16443469,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.836580667313974}
episode index:2079
target Thresh 19.0
target distance 9.0
model initialize at round 2079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 7.97380116, 11.86775028,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 7.270210533952001}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.773594759347324
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.46450532, 10.0986115 ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.47485725870276463}
episode index:2080
target Thresh 19.0
target distance 10.0
model initialize at round 2080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 6.        , 10.99968827,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 12.041361588953242}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7735585952611859
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.59285421,  2.00697439,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.4072055209547554}
episode index:2081
target Thresh 19.0
target distance 1.0
model initialize at round 2081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.27918541, 10.74707806,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0381229678221426}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7736673567428087
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.27918541, 10.74707806,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0381229678221426}
episode index:2082
target Thresh 19.0
target distance 1.0
model initialize at round 2082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.94021211, 9.360443  , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.642345508970444}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7737760137967008
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.94021211, 9.360443  , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.642345508970444}
episode index:2083
target Thresh 19.0
target distance 13.0
model initialize at round 2083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.531018778398135}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7737230600622922
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.89141205, 6.27447664, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7336044488438298}
episode index:2084
target Thresh 19.0
target distance 13.0
model initialize at round 2084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4., 4., 0.]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.401754250991402}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7736869038205806
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.19483521,  7.90592163,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.2120166362805935}
episode index:2085
target Thresh 19.0
target distance 7.0
model initialize at round 2085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.86773634, 5.97385622, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.100498766442044}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7737270227545113
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.28603309, 11.87969099,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.1329629291461991}
episode index:2086
target Thresh 19.0
target distance 10.0
model initialize at round 2086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.94134623692523}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7736908992630591
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.46524791, 10.35623565,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7942846777880712}
episode index:2087
target Thresh 19.0
target distance 14.0
model initialize at round 2087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4., 6., 0.]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.041594578792312}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7736548103726524
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.90250878,  5.85859649,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8641136941024494}
episode index:2088
target Thresh 19.0
target distance 12.0
model initialize at round 2088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.181338838126695}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7736363503823471
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.067298  ,  6.89019523,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8927354463853175}
episode index:2089
target Thresh 19.0
target distance 8.0
model initialize at round 2089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.00000012, 10.99999952,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.000000119209251}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7736764167218771
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.03906565, 10.84827916,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.15666952165849066}
episode index:2090
target Thresh 19.0
target distance 7.0
model initialize at round 2090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 3.99505755, 11.86740305,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.07955086640648}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7737164447387486
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.76324528, 10.98002277,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.76350667944495}
episode index:2091
target Thresh 19.0
target distance 3.0
model initialize at round 2091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.56454491,  7.99998128,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.148367706679053}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.773800710300537
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.67921351,  9.90355694,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9588113079731938}
episode index:2092
target Thresh 19.0
target distance 13.0
model initialize at round 2092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4., 6., 0.]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 11.704699910719649}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7737822158811984
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.07087105,  9.11624383,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.2823047868929898}
episode index:2093
target Thresh 19.0
target distance 1.0
model initialize at round 2093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.1897974 ,  9.81002975,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8319682935898569}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.773890247296728
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.1897974 ,  9.81002975,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8319682935898569}
episode index:2094
target Thresh 19.0
target distance 3.0
model initialize at round 2094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.01824284, 11.85517476,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.3297151347143348}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7739743092311925
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.23383994, 10.04772019,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.2222266891369087}
episode index:2095
target Thresh 19.0
target distance 13.0
model initialize at round 2095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.        ,  8.99999785,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.70469917741875}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7739382228699628
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.43397367, 4.02660811, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0657508720387296}
episode index:2096
target Thresh 19.0
target distance 10.0
model initialize at round 2096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([12.        , 10.99999988,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 8.062257733512485}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7739575686148985
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.10034329, 10.68796226,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6952415718107106}
episode index:2097
target Thresh 19.0
target distance 10.0
model initialize at round 2097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.73351535954344}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7739390435062283
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.00357726, 11.81896349,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.2897904736733352}
episode index:2098
target Thresh 19.0
target distance 1.0
model initialize at round 2098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.58343434,  2.20532882,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8972342098014786}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7740467428661587
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.58343434,  2.20532882,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8972342098014786}
episode index:2099
target Thresh 19.0
target distance 4.0
model initialize at round 2099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.00026059, 10.99949682,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.000260654796911}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7741305301314605
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.04944205, 11.85540685,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8568345172354376}
episode index:2100
target Thresh 19.0
target distance 9.0
model initialize at round 2100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.86774994, 3.97380247, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.116842128783118}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7741701514878949
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.99380083, 10.09579531,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9042259373130083}
episode index:2101
target Thresh 19.0
target distance 3.0
model initialize at round 2101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.83543897,  4.00311565,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.016524150695445}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7742538003216305
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.55112987,  2.11406231,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.0433742036527305}
episode index:2102
target Thresh 19.0
target distance 10.0
model initialize at round 2102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 8.824569335970134}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7742729408112541
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.2644729 ,  8.85210262,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7502491256532582}
episode index:2103
target Thresh 19.0
target distance 10.0
model initialize at round 2103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.669994727410566}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7742920631064959
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.91206047, 11.86231926,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.2551688366357594}
episode index:2104
target Thresh 19.0
target distance 5.0
model initialize at round 2104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.13552264, 5.01844463, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 3.5478562284951565}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7743315324351864
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.94949605, 2.63641835, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6384191160847015}
episode index:2105
target Thresh 19.0
target distance 14.0
model initialize at round 2105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4., 7., 0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.041594578792314}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7743129001266346
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.25363779,  7.26443973,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0479052711703432}
episode index:2106
target Thresh 19.0
target distance 11.0
model initialize at round 2106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.310400490210087}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7742768414631163
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.8891909 , 8.54455311, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9990457120432995}
episode index:2107
target Thresh 19.0
target distance 11.0
model initialize at round 2107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.86775037, 7.97380131, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.332858443941447}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.774276606214557
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.07057812,  9.2043262 ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.2234875708616115}
episode index:2108
target Thresh 19.0
target distance 8.0
model initialize at round 2108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86731925,  3.99556145,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 6.0667557202609945}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7743160080134122
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.76157045,  9.9755775 ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.23967709012161906}
episode index:2109
target Thresh 19.0
target distance 1.0
model initialize at round 2109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.50676417, 8.20036077, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9395235338744482}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7744229672513204
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.50676417, 8.20036077, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9395235338744482}
episode index:2110
target Thresh 19.0
target distance 2.0
model initialize at round 2110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.20351675,  8.00311744,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.20354062640617593}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7745298251540911
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.20351675,  8.00311744,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.20354062640617593}
episode index:2111
target Thresh 19.0
target distance 5.0
model initialize at round 2111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([1.95458364, 7.00007725, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 3.6310042078646125}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7745904170929385
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.86122093, 3.05741883, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.276777487983167}
episode index:2112
target Thresh 19.0
target distance 11.0
model initialize at round 2112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.        ,  9.99999964,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.219544379712541}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7746093076906229
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.83467375, 8.9360224 , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.254120484643329}
episode index:2113
target Thresh 19.0
target distance 2.0
model initialize at round 2113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.86704206, 4.99782157, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.13297578022999876}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.774715925804298
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.86704206, 4.99782157, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.13297578022999876}
episode index:2114
target Thresh 19.0
target distance 7.0
model initialize at round 2114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.99527407, 4.        , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 5.3834114279092375}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7747550081088823
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.11081317, 9.88095665, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.88789874286781}
episode index:2115
target Thresh 19.0
target distance 12.0
model initialize at round 2115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4., 7., 0.]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.198039027185583}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7747188938782514
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.76759278,  4.86796873,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.26729267574444426}
episode index:2116
target Thresh 19.0
target distance 9.0
model initialize at round 2116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 9.32587565885833}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7747001754071823
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.99161995, 11.86457905,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8646196583319115}
episode index:2117
target Thresh 19.0
target distance 3.0
model initialize at round 2117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.32042772, 9.99992273, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.2091207651920513}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7747829420854603
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.54508356, 11.90979711,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.0171920905183143}
episode index:2118
target Thresh 19.0
target distance 10.0
model initialize at round 2118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([12.81758975,  9.10816293,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.72658116096081}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.774746865801368
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.83907029, 3.2765903 , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.32000088422102746}
episode index:2119
target Thresh 19.0
target distance 5.0
model initialize at round 2119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 3.99999917, 10.        ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.1622784518114333}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7748071267137258
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.7740458 , 10.38416865,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9891385910536301}
episode index:2120
target Thresh 19.0
target distance 13.0
model initialize at round 2120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.311697701223368}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7747884019442356
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.98363292, 9.71616426, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.28430724766584675}
episode index:2121
target Thresh 19.0
target distance 8.0
model initialize at round 2121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 8.0261983 , 10.13224989,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 9.337240194796635}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7748071191205107
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.28518588, 3.93971908, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.1806909707244422}
episode index:2122
target Thresh 19.0
target distance 1.0
model initialize at round 2122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.39015126, 5.0846805 , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9950014064564355}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7749131920742928
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.39015126, 5.0846805 , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9950014064564355}
episode index:2123
target Thresh 19.0
target distance 5.0
model initialize at round 2123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.24514721,  4.99999878,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.093510961515791}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7749520159009998
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.21667666,  7.10066456,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9250691892471816}
episode index:2124
target Thresh 19.0
target distance 8.0
model initialize at round 2124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13236181, 8.02576197, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 6.0879063226023495}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7749908031876347
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.00701393, 2.04085675, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.041454425953804046}
episode index:2125
target Thresh 19.0
target distance 4.0
model initialize at round 2125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.96787727,  9.00022459,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.222090191484025}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7750731217185907
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.54352403,  7.00431049,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.45649632458219863}
episode index:2126
target Thresh 19.0
target distance 14.0
model initialize at round 2126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.        ,  9.99919152,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.892036638665044}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7750206286812472
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.34365955, 2.1761399 , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8926630640680193}
episode index:2127
target Thresh 19.0
target distance 14.0
model initialize at round 2127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.165525060596453}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7749845932805952
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.75994729,  8.14466441,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8883829505347802}
episode index:2128
target Thresh 19.0
target distance 3.0
model initialize at round 2128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.00201023, 10.99900496,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0020107203086541}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7750667987323188
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.23625064, 11.13559069,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7756918982394941}
episode index:2129
target Thresh 19.0
target distance 11.0
model initialize at round 2129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.13259829055323}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7750143825973689
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.94594774, 4.9287462 , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.08943573574833141}
episode index:2130
target Thresh 19.0
target distance 11.0
model initialize at round 2130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 4.99505757, 11.86740304,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.747809387596405}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7749956484387709
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.45568911,  5.15091891,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0085697998358747}
episode index:2131
target Thresh 19.0
target distance 3.0
model initialize at round 2131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.99869931,  9.99999964,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0000012035277384}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7750777330314357
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.30193038, 11.79397731,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0572138739058456}
episode index:2132
target Thresh 19.0
target distance 11.0
model initialize at round 2132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.18408396726371}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7750771250635353
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.79913291, 8.60712159, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8904868634989789}
episode index:2133
target Thresh 19.0
target distance 5.0
model initialize at round 2133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.08852386,  3.99999952,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.0013062715512464}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7751368358765327
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.38544071,  7.93796342,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.1213645720070036}
episode index:2134
target Thresh 19.0
target distance 5.0
model initialize at round 2134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([2.18559194, 5.00001323, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 3.5060171157405575}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7751964907543423
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.86129909, 1.13419532, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.2212509474226867}
episode index:2135
target Thresh 19.0
target distance 14.0
model initialize at round 2135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4., 6., 0.]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 13.000000000000025}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7751605079853063
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.99374074, 10.21038448,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.7896403243873908}
episode index:2136
target Thresh 19.0
target distance 7.0
model initialize at round 2136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 9.02581844, 11.86765294,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 5.361620883183996}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7752200959553647
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.9390635 , 9.23901452, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.2086931586737135}
episode index:2137
target Thresh 19.0
target distance 12.0
model initialize at round 2137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([12.00494232, 11.86740303,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 10.726400934127554}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7752194228223174
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.9308175 , 8.35485709, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.3615380694770697}
episode index:2138
target Thresh 19.0
target distance 11.0
model initialize at round 2138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 9.055385138137432}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7752187503186604
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.42681315, 10.74377422,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8575369131311354}
episode index:2139
target Thresh 19.0
target distance 12.0
model initialize at round 2139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.198039027185583}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7752180784435115
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.05391484,  8.14199472,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8596975482102829}
episode index:2140
target Thresh 19.0
target distance 2.0
model initialize at round 2140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.92861199,  5.17797673,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8251172692326487}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7753230676642291
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.92861199,  5.17797673,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8251172692326487}
episode index:2141
target Thresh 19.0
target distance 11.0
model initialize at round 2141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 3.97380116, 11.86775028,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.067814290739564}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7753223477155063
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.85934143, 10.13593895,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.2186341507453846}
episode index:2142
target Thresh 19.0
target distance 11.0
model initialize at round 2142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 5.        , 10.99997044,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.041574937624008}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7752864237530137
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.44020946,  3.01520968,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.44047214148434805}
episode index:2143
target Thresh 19.0
target distance 9.0
model initialize at round 2143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.00102222,  4.        ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.616175835869399}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7753247113352185
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.96225271, 10.13958946,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.2908278660529386}
episode index:2144
target Thresh 19.0
target distance 14.0
model initialize at round 2144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.165525060596453}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7753059547754468
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.06054704,  8.23565157,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.24330557884290555}
episode index:2145
target Thresh 19.0
target distance 13.0
model initialize at round 2145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.183271074044923}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7752872156961595
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.25768475, 10.40114371,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.4767784679724896}
episode index:2146
target Thresh 19.0
target distance 14.0
model initialize at round 2146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.        , 10.99999857,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.649110188306171}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7752684940729313
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.95884299, 6.15567   , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8453325124480963}
episode index:2147
target Thresh 19.0
target distance 2.0
model initialize at round 2147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.08566582, 8.90010357, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9041709285560395}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7753731176790426
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.08566582, 8.90010357, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9041709285560395}
episode index:2148
target Thresh 19.0
target distance 1.0
model initialize at round 2148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.0866591 ,  2.06296802,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9410306788872252}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7754776439155809
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.0866591 ,  2.06296802,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9410306788872252}
episode index:2149
target Thresh 19.0
target distance 1.0
model initialize at round 2149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.30841252, 8.82637966, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8820553389291307}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7755820729184109
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.30841252, 8.82637966, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8820553389291307}
episode index:2150
target Thresh 19.0
target distance 13.0
model initialize at round 2150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.        ,  8.99999893,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.70469954406915}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7755632490307802
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.07222445, 5.93472098, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.3169930842915831}
episode index:2151
target Thresh 19.0
target distance 14.0
model initialize at round 2151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4., 7., 0.]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.041594578792312}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7755444426374689
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.22934941,  6.86079857,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.1553685555867068}
episode index:2152
target Thresh 19.0
target distance 13.0
model initialize at round 2152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.045361017187279}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7755256537141003
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.37991025, 6.13294424, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0659723185752137}
episode index:2153
target Thresh 19.0
target distance 5.0
model initialize at round 2153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.99988139, 6.        , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.0000000023448687}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7755846018785785
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.30373622, 9.90867512, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9580950711409396}
episode index:2154
target Thresh 19.0
target distance 2.0
model initialize at round 2154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.14574474,  6.00180709,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.1457559436376039}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7756887389542728
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.14574474,  6.00180709,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.1457559436376039}
episode index:2155
target Thresh 19.0
target distance 11.0
model initialize at round 2155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 9.217415937331797}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.775687854074192
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.41251165, 10.45118847,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6113402458694346}
episode index:2156
target Thresh 19.0
target distance 6.0
model initialize at round 2156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 3.99505744, 11.86740305,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.097798553179071}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7757466450551498
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.93535588, 10.52693525,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.47746111721577383}
episode index:2157
target Thresh 19.0
target distance 2.0
model initialize at round 2157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.83266723,  5.99674416,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.167337308919429}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7758273926709722
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.11582434,  5.8027391 ,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.22875126522405834}
episode index:2158
target Thresh 19.0
target distance 2.0
model initialize at round 2158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.22356546, 10.99401665,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.776457594834763}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7759312243557008
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.22356546, 10.99401665,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.776457594834763}
episode index:2159
target Thresh 19.0
target distance 11.0
model initialize at round 2159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 9.000000000000018}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7759490831638695
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.05258122,  9.26756667,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.1975228228147108}
episode index:2160
target Thresh 19.0
target distance 2.0
model initialize at round 2160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.99998152,  9.00639176,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.006391790394545812}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7760527624405174
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.99998152,  9.00639176,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.006391790394545812}
episode index:2161
target Thresh 19.0
target distance 3.0
model initialize at round 2161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.00249767, 10.99891961,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0024982552057993}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7761332190721361
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.17332053, 11.3276566 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8892456289239057}
episode index:2162
target Thresh 19.0
target distance 12.0
model initialize at round 2162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.32905200177065}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7761142448102557
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.95515882, 9.5961694 , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.5978534003226318}
episode index:2163
target Thresh 19.0
target distance 14.0
model initialize at round 2163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4., 4., 0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.416407864998762}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7760621681866322
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.98421571, 10.78693873,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.7870970157418105}
episode index:2164
target Thresh 19.0
target distance 11.0
model initialize at round 2164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.414114271867186}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7760432442708993
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.91127121, 9.24121883, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.1858178934602088}
episode index:2165
target Thresh 19.0
target distance 6.0
model initialize at round 2165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 8.97399329, 11.86770138,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 4.118450648992628}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7761016268912728
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.97156563, 11.24231086,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.24397349105622046}
episode index:2166
target Thresh 19.0
target distance 12.0
model initialize at round 2166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.02543337, 11.86755221,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.61625943755608}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7760827022321743
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.70480096, 5.10072183, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9464902044914528}
episode index:2167
target Thresh 19.0
target distance 5.0
model initialize at round 2167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.49319553,  5.99999976,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.040270261839407}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7761410127938754
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.33267009,  9.8721112 ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9334063091215954}
episode index:2168
target Thresh 19.0
target distance 11.0
model initialize at round 2168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 9.824446174815863}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7761399247001484
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.15585341,  8.29122889,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7257043297168602}
episode index:2169
target Thresh 19.0
target distance 9.0
model initialize at round 2169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([10.01818397, 10.13612331,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 9.322387866465508}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7761388376092727
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.29637312, 3.97739382, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.2972340211305418}
episode index:2170
target Thresh 19.0
target distance 13.0
model initialize at round 2170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.        , 10.99999988,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.401754219625372}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7761377515198628
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.89929535, 7.43756175, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.060692655427339}
episode index:2171
target Thresh 19.0
target distance 13.0
model initialize at round 2171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 3.99505762, 11.86740304,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.971891970871566}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7761019317890036
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.90826435,  4.12047995,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8842912156109215}
episode index:2172
target Thresh 19.0
target distance 13.0
model initialize at round 2172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.16225570998853}
done in step count: 13
reward sum = 0.5133420832795048
running average episode reward sum: 0.775981011472156
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.61165452, 10.82101779,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9082303790355117}
episode index:2173
target Thresh 19.0
target distance 8.0
model initialize at round 2173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.99997532, 5.        , 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 6.000000000050762}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7760184512092895
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.69039392, 11.07250602,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.31798278170489236}
episode index:2174
target Thresh 19.0
target distance 12.0
model initialize at round 2174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 3.97380121, 11.86775026,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 11.145387151608139}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7759996343998253
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.55830756,  6.09304757,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.0087888368907174}
episode index:2175
target Thresh 19.0
target distance 12.0
model initialize at round 2175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.440306508910576}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7759986147780884
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.87384339, 9.3587748 , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0838690130000153}
episode index:2176
target Thresh 19.0
target distance 12.0
model initialize at round 2176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.895369891272122}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7759629412279349
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.53787148, 10.56729109,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7817447866554347}
episode index:2177
target Thresh 19.0
target distance 13.0
model initialize at round 2177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.132598050286157}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7758960387800454
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.86119651, 3.92297385, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.15874330805963419}
episode index:2178
target Thresh 19.0
target distance 13.0
model initialize at round 2178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4., 4., 0.]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.18033988749897}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.775844420786704
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.1990383 ,  5.90885315,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.21891549678421504}
episode index:2179
target Thresh 19.0
target distance 2.0
model initialize at round 2179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13389557, 5.00260177, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8661083363945996}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7759472444468936
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13389557, 5.00260177, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8661083363945996}
episode index:2180
target Thresh 19.0
target distance 14.0
model initialize at round 2180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.       , 8.9999994, 0.       ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.892443689119302}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7758956503097282
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.26516017,  1.99764095,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.2651706633371299}
episode index:2181
target Thresh 19.0
target distance 13.0
model initialize at round 2181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.083045973594597}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7758441034632475
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.50020183, 9.54007556, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.679211824697785}
episode index:2182
target Thresh 19.0
target distance 14.0
model initialize at round 2182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4., 4., 0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.416407864998762}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7757926038424623
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.00384801, 10.88648937,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8864977213435095}
episode index:2183
target Thresh 19.0
target distance 11.0
model initialize at round 2183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.86775037, 7.97380131, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.574514693014823}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.775773967984762
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.24917696, 11.78279498,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8214968902617082}
episode index:2184
target Thresh 19.0
target distance 1.0
model initialize at round 2184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.337062  ,  6.79813676,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6929903026483057}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7758765885943799
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.337062  ,  6.79813676,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6929903026483057}
episode index:2185
target Thresh 19.0
target distance 10.0
model initialize at round 2185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.189726279481821}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7758099704887671
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.21898387, 4.18027107, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8484747785953114}
episode index:2186
target Thresh 19.0
target distance 3.0
model initialize at round 2186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.14433628,  9.99998975,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0103729344914862}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7757434133050615
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.36826301, 11.83098   ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0438483475853217}
episode index:2187
target Thresh 19.0
target distance 7.0
model initialize at round 2187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.99941564,  6.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.000000034148139}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7757807220741177
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.29832842, 11.7519702 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0284951070809094}
episode index:2188
target Thresh 19.0
target distance 12.0
model initialize at round 2188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4., 7., 0.]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.000000000000018}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7757621342114183
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.42900907,  6.86275738,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.45042682127021527}
episode index:2189
target Thresh 19.0
target distance 13.0
model initialize at round 2189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.99983213, 9.00007785, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.18051897564113}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7757612295553857
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.07432928,  7.05107856,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9270789082004752}
episode index:2190
target Thresh 19.0
target distance 4.0
model initialize at round 2190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.74073558,  3.99999641,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 2.132768989009018}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7758407543251004
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.27115173,  5.99948421,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.7288484484254683}
episode index:2191
target Thresh 19.0
target distance 14.0
model initialize at round 2191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.00000000000002}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7758053969080241
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.29725478, 8.50274278, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8608807065007683}
episode index:2192
target Thresh 19.0
target distance 8.0
model initialize at round 2192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.00005352,  5.        ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.2111322413774515}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7758425923494705
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.9421299, 11.1602314,  0.       ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9556583338500307}
episode index:2193
target Thresh 19.0
target distance 14.0
model initialize at round 2193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.        ,  9.99746633,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 14.42079982635031}
done in step count: 11
reward sum = 0.5688000922764597
running average episode reward sum: 0.7757482247560006
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.67241222, 1.70654663, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.4398052260584134}
episode index:2194
target Thresh 19.0
target distance 2.0
model initialize at round 2194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.96157899, 5.00332165, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.03856432620122259}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7758503895738793
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.96157899, 5.00332165, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.03856432620122259}
episode index:2195
target Thresh 19.0
target distance 12.0
model initialize at round 2195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([12.00004555, 11.83591644,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.704428264546936}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7758318292373817
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.51700078, 3.97924506, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.48344494249673203}
episode index:2196
target Thresh 19.0
target distance 11.0
model initialize at round 2196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 5.98181965, 11.86375105,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 13.364922802574084}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7757965563501975
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.57733637,  2.00148398,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5773382802486589}
episode index:2197
target Thresh 19.0
target distance 10.0
model initialize at round 2197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.051800518890262}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7758141676757888
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.15914824, 11.87119998,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8856170547520232}
episode index:2198
target Thresh 19.0
target distance 6.0
model initialize at round 2198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.86710563,  3.99672497,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 4.096105854932822}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7758717783316887
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.6813706 ,  7.99482365,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.31867144871861464}
episode index:2199
target Thresh 19.0
target distance 12.0
model initialize at round 2199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.00000000000002}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7758708279494926
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.92668787, 8.02122245, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.07632208191648308}
episode index:2200
target Thresh 19.0
target distance 2.0
model initialize at round 2200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.02402973, 10.99674237,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.024249539705090004}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7759726585592384
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.02402973, 10.99674237,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.024249539705090004}
episode index:2201
target Thresh 19.0
target distance 7.0
model initialize at round 2201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 8.02071946, 11.86168425,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 5.354763501497711}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7760301187506283
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.99667424, 10.9721956 ,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.3923088097691125}
episode index:2202
target Thresh 19.0
target distance 1.0
model initialize at round 2202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.54284334, 3.04884624, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0951585996820912}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7761317846068468
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.54284334, 3.04884624, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0951585996820912}
episode index:2203
target Thresh 19.0
target distance 8.0
model initialize at round 2203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.8665305 ,  4.99891427,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 6.284661175095639}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7761686463198202
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.12636714, 10.99482302,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8736482037139102}
episode index:2204
target Thresh 19.0
target distance 13.0
model initialize at round 2204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 4.99505758, 11.86740304,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.471374269286796}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7761500174056728
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.35274455,  5.05527524,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0084312481933921}
episode index:2205
target Thresh 19.0
target distance 2.0
model initialize at round 2205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.5758574, 5.9991852, 0.       ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.4241433817330458}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7762514906525424
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.5758574, 5.9991852, 0.       ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.4241433817330458}
episode index:2206
target Thresh 19.0
target distance 1.0
model initialize at round 2206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.26790155, 11.88173792,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9215383838442005}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7763528719435925
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.26790155, 11.88173792,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9215383838442005}
episode index:2207
target Thresh 19.0
target distance 11.0
model initialize at round 2207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.902878591090003}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7763175388023562
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.8562615 , 7.97570541, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.856606084128504}
episode index:2208
target Thresh 19.0
target distance 13.0
model initialize at round 2208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.99983212, 9.00007511, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.038586769447924}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7762664310126264
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.093804  ,  1.61601164,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.3952799630638552}
episode index:2209
target Thresh 19.0
target distance 2.0
model initialize at round 2209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.02775383,  8.93438476,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9347968553936941}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7763676679216704
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.02775383,  8.93438476,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9347968553936941}
episode index:2210
target Thresh 19.0
target distance 3.0
model initialize at round 2210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.51834643,  3.00061667,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.1269058256635964}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7764461990533206
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.95295684,  1.20894155,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.792456015287388}
episode index:2211
target Thresh 19.0
target distance 13.0
model initialize at round 2211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4., 7., 0.]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7764108876143696
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.26807123,  4.86208158,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.7448095116650196}
episode index:2212
target Thresh 19.0
target distance 2.0
model initialize at round 2212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.40053876,  5.005561  ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5994870313836917}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7765119220076754
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.40053876,  5.005561  ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5994870313836917}
episode index:2213
target Thresh 19.0
target distance 8.0
model initialize at round 2213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.1326857 ,  4.97543769,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.390907554138622}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7765484455298037
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.01464188, 11.11430258,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.11523655434681687}
episode index:2214
target Thresh 19.0
target distance 13.0
model initialize at round 2214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.401754250991392}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7765471960905127
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.08626526,  8.01624569,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9138791487390593}
episode index:2215
target Thresh 19.0
target distance 11.0
model initialize at round 2215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.614260567258981}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7765119028143409
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.17076086,  7.92982   ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9453700362390267}
episode index:2216
target Thresh 19.0
target distance 3.0
model initialize at round 2216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.64970374, 7.00316393, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0625654553967692}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7765901563538924
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.29695225, 5.08482325, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.962148179698166}
episode index:2217
target Thresh 19.0
target distance 10.0
model initialize at round 2217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.354328771680768}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7765714465857549
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.19397727,  7.91050441,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.21362734101062703}
episode index:2218
target Thresh 19.0
target distance 2.0
model initialize at round 2218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.41406628, 5.00510263, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.4140977241725133}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7766721354336208
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.41406628, 5.00510263, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.4140977241725133}
episode index:2219
target Thresh 19.0
target distance 10.0
model initialize at round 2219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 5.99999869, 10.42878455,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.620861842319597}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7766368494699543
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.75772053,  2.13950913,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.2795749286012491}
episode index:2220
target Thresh 19.0
target distance 1.0
model initialize at round 2220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.42955852,  3.12108186,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.44629736144035476}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7767374182004946
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.42955852,  3.12108186,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.44629736144035476}
episode index:2221
target Thresh 19.0
target distance 12.0
model initialize at round 2221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.13259815856355}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7767186758388495
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.99104221, 6.85150734, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.14876260652368567}
episode index:2222
target Thresh 19.0
target distance 4.0
model initialize at round 2222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([12.82337319,  9.14611478,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.3776214404167333}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7767966251524622
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.89814683, 11.20346283,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9209043666526425}
episode index:2223
target Thresh 19.0
target distance 13.0
model initialize at round 2223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.401754250991404}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7767778730236279
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.7880929 , 8.51407612, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9258576810406675}
episode index:2224
target Thresh 19.0
target distance 12.0
model initialize at round 2224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 4.9950597, 11.8674027,  0.       ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.135075207033575}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7767591377506398
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.71526202,  4.9972182 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.28475156625760684}
episode index:2225
target Thresh 19.0
target distance 3.0
model initialize at round 2225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.92252004, 3.99999762, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.3605322470574646}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7768369638343097
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.59269357, 5.90921736, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9962804492531564}
episode index:2226
target Thresh 19.0
target distance 10.0
model initialize at round 2226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.354328771680768}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7768182188530751
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.8519804 , 7.91865777, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.16889748524658432}
episode index:2227
target Thresh 19.0
target distance 8.0
model initialize at round 2227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.99007761,  8.00000036,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 6.081139529301314}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7768543753975756
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.26433063,  2.00326526,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.26435079256348126}
episode index:2228
target Thresh 19.0
target distance 7.0
model initialize at round 2228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 8.02605765, 10.13228576,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 6.506692021572686}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.776871267221085
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.37126272, 5.65318765, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.5080500114817644}
episode index:2229
target Thresh 19.0
target distance 6.0
model initialize at round 2229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 8.01838138, 11.86440606,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 4.11030251058499}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7769276029756943
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.11300455, 11.87023674,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8775431652623451}
episode index:2230
target Thresh 19.0
target distance 12.0
model initialize at round 2230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.13224959,  5.97379481,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.820237989687168}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7768618574834258
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.75057982, 2.28624798, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.37966871326880236}
episode index:2231
target Thresh 19.0
target distance 11.0
model initialize at round 2231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.414114271867186}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7768431433405681
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.93628171, 9.34611885, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.1420087608200975}
episode index:2232
target Thresh 19.0
target distance 13.0
model initialize at round 2232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.86740321, 7.99505687, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.64418515355199}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7767923494704152
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.10069612,  2.05527527,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.11486976625986926}
episode index:2233
target Thresh 19.0
target distance 2.0
model initialize at round 2233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.31831129,  5.00514939,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6817081599167266}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7768922633694884
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.31831129,  5.00514939,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6817081599167266}
episode index:2234
target Thresh 19.0
target distance 12.0
model initialize at round 2234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.568784232893615}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7768571157331233
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.17013143,  7.93042035,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.2467412968144884}
episode index:2235
target Thresh 19.0
target distance 11.0
model initialize at round 2235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.132598136344583}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7768557399825718
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.26830233,  7.8387677 ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.3130207538026362}
episode index:2236
target Thresh 19.0
target distance 7.0
model initialize at round 2236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.0150224 ,  9.00001884,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 5.000041402173151}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7768917342874523
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.78095104,  3.19018473,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8389178904667952}
episode index:2237
target Thresh 19.0
target distance 2.0
model initialize at round 2237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.01951921, 10.99790823,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.01963097134961976}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.776991425201533
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.01951921, 10.99790823,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.01963097134961976}
episode index:2238
target Thresh 19.0
target distance 3.0
model initialize at round 2238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.9941535 ,  8.00078237,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0007994478441442}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7770686956681692
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.86899835,  6.036322  ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9725413743731863}
episode index:2239
target Thresh 19.0
target distance 7.0
model initialize at round 2239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.168891158489505}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7770854088620673
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.04637255, 10.35353257,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6481284949381919}
episode index:2240
target Thresh 19.0
target distance 6.0
model initialize at round 2240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.99999988, 7.        , 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.656854333786085}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7771413725350428
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.10392119, 11.20439488,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9190943885897446}
episode index:2241
target Thresh 19.0
target distance 12.0
model initialize at round 2241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 4.9953435 , 11.86735604,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 13.368737962301553}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7771062235268175
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.3832047 ,  3.01288697,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.3834213280299268}
episode index:2242
target Thresh 19.0
target distance 12.0
model initialize at round 2242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.        ,  9.99999905,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.44030623487433}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.777104741009641
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.98522192, 6.93375267, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.06787562665514932}
episode index:2243
target Thresh 19.0
target distance 6.0
model initialize at round 2243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.99977243,  9.00000441,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.000004417217211}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7771606212498328
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.93064946,  5.00278962,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.06940662394502688}
episode index:2244
target Thresh 19.0
target distance 10.0
model initialize at round 2244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4., 7., 0.]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.944271909999184}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.777177256273775
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.0880596 , 10.30072303,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.149183878643596}
episode index:2245
target Thresh 19.0
target distance 3.0
model initialize at round 2245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.02036701, 11.86256109,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.336098978712299}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7772542031765917
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.19039004, 10.04576575,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.2514117240943028}
episode index:2246
target Thresh 19.0
target distance 6.0
model initialize at round 2246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.86754818, 3.97458185, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.117843017467212}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.777309942294003
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.2783004 , 7.95319706, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.2822084863973331}
episode index:2247
target Thresh 19.0
target distance 12.0
model initialize at round 2247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4., 7., 0.]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.770329614269032}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7773083724520128
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.07299707, 11.74832573,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.1913546220826632}
episode index:2248
target Thresh 19.0
target distance 8.0
model initialize at round 2248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.99974966, 4.        , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.082721379748873}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7773439734424744
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.33735395, 9.97731273, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.663034308399303}
episode index:2249
target Thresh 19.0
target distance 12.0
model initialize at round 2249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.        ,  6.99985051,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 11.180273035031659}
done in step count: 11
reward sum = 0.5688000922764597
running average episode reward sum: 0.7772512872730672
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.83261052, 1.97690135, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8329308681885944}
episode index:2250
target Thresh 19.0
target distance 8.0
model initialize at round 2250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 8.97393453, 10.13228357,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 7.306793590156201}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7772868819921819
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.00903085,  6.96099472,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9610371508404247}
episode index:2251
target Thresh 19.0
target distance 11.0
model initialize at round 2251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 6.97384535, 11.86773909,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 10.76577121023113}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7773034092426294
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.10545813,  6.98770519,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.3325789618754045}
episode index:2252
target Thresh 19.0
target distance 4.0
model initialize at round 2252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.31885767, 5.00004113, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 2.6127388014877164}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7773800610805155
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.0614326 , 3.00634027, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.061758912791032745}
episode index:2253
target Thresh 19.0
target distance 5.0
model initialize at round 2253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.01251972,  7.0000329 ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 3.000059025250235}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7774355712574984
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.98618491,  3.24517653,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.7549498869286182}
episode index:2254
target Thresh 19.0
target distance 7.0
model initialize at round 2254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.99960554, 4.        , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.098942167599415}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7774710211150339
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.92115239, 9.93141364, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9347450524741598}
episode index:2255
target Thresh 19.0
target distance 13.0
model initialize at round 2255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.531018574596514}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.777452236039462
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.03309864, 11.0928203 ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.098545057784931}
episode index:2256
target Thresh 19.0
target distance 5.0
model initialize at round 2256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 6.00000191, 11.5843521 ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 3.0563832914379288}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7775076404541543
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.40501761, 10.99186661,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.5950379794558484}
episode index:2257
target Thresh 19.0
target distance 14.0
model initialize at round 2257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.165525060596453}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.777472578299876
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.40693965,  8.15467823,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0326129340176498}
episode index:2258
target Thresh 19.0
target distance 5.0
model initialize at round 2258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4.95432076, 9.05521986, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 4.2499858249764095}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7775279246574236
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.33635271, 5.1887103 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.0481501346504962}
episode index:2259
target Thresh 19.0
target distance 4.0
model initialize at round 2259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.86729591, 3.99569696, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.1839031267004363}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7776042397350088
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.02970664, 5.99556364, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.030036075856674855}
episode index:2260
target Thresh 19.0
target distance 12.0
model initialize at round 2260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 4.99505956, 11.86740272,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.13507532837546}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7775854372807364
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.69633512,  4.97642252,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.30457882260278885}
episode index:2261
target Thresh 19.0
target distance 11.0
model initialize at round 2261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.12611515,  7.81911818,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.320362285093124}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7775349664558063
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.69853686, 2.73223607, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.7480984384932582}
episode index:2262
target Thresh 19.0
target distance 4.0
model initialize at round 2262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.82349826,  9.14597932,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.6004881416782872}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7776111772527767
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.92489642, 11.19970852,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.21336363644199377}
episode index:2263
target Thresh 19.0
target distance 5.0
model initialize at round 2263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.99994147, 7.        , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.000000000571014}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7776464086232481
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.18527464, 9.35754191, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6686397337827238}
episode index:2264
target Thresh 19.0
target distance 3.0
model initialize at round 2264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.00200331,  9.99998975,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.4156380687110863}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7777225029240767
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.30301371, 11.89021945,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.1306107007682074}
episode index:2265
target Thresh 19.0
target distance 4.0
model initialize at round 2265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.82348836,  9.14599004,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.600473559112242}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7777985300631217
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.9177354 , 11.20005646,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.21631008284034975}
episode index:2266
target Thresh 19.0
target distance 7.0
model initialize at round 2266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.99997151, 4.        , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.000000000081193}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7778336321671963
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.26651136, 9.91015955, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9483768799533328}
episode index:2267
target Thresh 19.0
target distance 10.0
model initialize at round 2267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 8.897487499533781}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7778498017517785
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.24012468,  8.86120176,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1485116390332752}
episode index:2268
target Thresh 19.0
target distance 2.0
model initialize at round 2268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.47605968,  9.99797034,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5239442560455398}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7779477084059205
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.47605968,  9.99797034,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5239442560455398}
episode index:2269
target Thresh 19.0
target distance 3.0
model initialize at round 2269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.86379263, 4.98181663, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.3352284806065604}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7780235023669752
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.39897601, 6.97087475, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.049656916747281}
episode index:2270
target Thresh 19.0
target distance 9.0
model initialize at round 2270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.99999845, 4.        , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.071068031029275}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7780584435812565
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.95229779, 10.10723822,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.3053331681459226}
episode index:2271
target Thresh 19.0
target distance 6.0
model initialize at round 2271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.11827341,  9.0000211 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.001769283661257}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7781132153930604
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.00668898,  5.007429  ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.009996628417753752}
episode index:2272
target Thresh 19.0
target distance 4.0
model initialize at round 2272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([5.17615534, 9.14560413, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.8627438751864989}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7781888365037543
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.19026643, 11.14852842,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.24137523899264485}
episode index:2273
target Thresh 19.0
target distance 13.0
model initialize at round 2273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 4.97380118, 11.86775027,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.052885674161136}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7781868981136911
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.05885847,  7.05080038,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9425115710091517}
episode index:2274
target Thresh 19.0
target distance 2.0
model initialize at round 2274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.93284273, 4.9980787 , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.06718474599494491}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7782843983782565
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.93284273, 4.9980787 , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.06718474599494491}
episode index:2275
target Thresh 19.0
target distance 4.0
model initialize at round 2275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.8672779 , 7.99580143, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.183800095119865}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7783598446004102
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.876533  , 9.99460719, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.1235847138750843}
episode index:2276
target Thresh 19.0
target distance 1.0
model initialize at round 2276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.83511374, 11.63510873,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0491797048235907}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7784571832720832
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.83511374, 11.63510873,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0491797048235907}
episode index:2277
target Thresh 19.0
target distance 13.0
model initialize at round 2277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.00412926, 11.86726549,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.971129327775607}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7784220121188004
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.41444657, 4.0920527 , 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9980652575615014}
episode index:2278
target Thresh 19.0
target distance 12.0
model initialize at round 2278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.13259695,  6.99505729,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.56597790809592}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.77837155069676
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.356508  , 3.02325935, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0397693275031683}
episode index:2279
target Thresh 19.0
target distance 2.0
model initialize at round 2279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.22535181,  4.99858642,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.22535624396608425}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7784687561569806
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.22535181,  4.99858642,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.22535624396608425}
episode index:2280
target Thresh 19.0
target distance 10.0
model initialize at round 2280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([12.05521693, 10.04568075,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 8.310916285874487}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7784845551459517
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.9279669 , 8.02547332, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.07640457056990585}
episode index:2281
target Thresh 19.0
target distance 2.0
model initialize at round 2281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.8829366 ,  1.61164883,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.40561123419874273}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7785816258930395
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.8829366 ,  1.61164883,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.40561123419874273}
episode index:2282
target Thresh 19.0
target distance 7.0
model initialize at round 2282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.86672596, 4.97736122, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 5.358317471586994}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7786161389785003
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.93053251, 10.93088493,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9334733444202569}
episode index:2283
target Thresh 19.0
target distance 14.0
model initialize at round 2283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.041594578792314}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.7785373827614512
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.6186604 , 2.08762944, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9888578953284513}
episode index:2284
target Thresh 19.0
target distance 2.0
model initialize at round 2284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.61996025,  3.9986465 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.38004216175476024}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7786343029440501
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.61996025,  3.9986465 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.38004216175476024}
episode index:2285
target Thresh 19.0
target distance 12.0
model initialize at round 2285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.063679927751902}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7786321798620536
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.89081763, 10.00529533,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0006788575599335}
episode index:2286
target Thresh 19.0
target distance 7.0
model initialize at round 2286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.86767793, 3.974085  , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.100263385775692}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.778666610478642
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.59327087, 9.89351142, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.0725357705690066}
episode index:2287
target Thresh 19.0
target distance 5.0
model initialize at round 2287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13326161, 5.02268496, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 3.144496751409426}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7787207334635727
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.65331093, 1.49615873, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.6115957268341082}
episode index:2288
target Thresh 19.0
target distance 10.0
model initialize at round 2288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.354328643351877}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7787185754050479
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.97981708, 10.38538271,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.3859108486700426}
episode index:2289
target Thresh 19.0
target distance 8.0
model initialize at round 2289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([12.82335934,  9.14612992,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.070718982839989}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7787529231887138
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.9251256 , 10.40155667,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.1018129564953159}
episode index:2290
target Thresh 19.0
target distance 10.0
model initialize at round 2290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.86775037, 7.97380131, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.184022662051223}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7787507529636206
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.2457813 ,  7.85499994,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8896253928507732}
episode index:2291
target Thresh 19.0
target distance 8.0
model initialize at round 2291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.00015152, 9.00000012, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082737740808291}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7787850567363241
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.08144448, 3.02829899, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.08622085924843383}
episode index:2292
target Thresh 19.0
target distance 2.0
model initialize at round 2292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.26660919,  9.9972769 ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7333958635519567}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7788815307630419
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.26660919,  9.9972769 ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7333958635519567}
episode index:2293
target Thresh 19.0
target distance 10.0
model initialize at round 2293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.94134623692523}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.778846419937118
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.39421459, 10.48993434,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7919235686974834}
episode index:2294
target Thresh 19.0
target distance 2.0
model initialize at round 2294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.00952411, 8.00681126, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.01170905173690408}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7789427831528317
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.00952411, 8.00681126, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.01170905173690408}
episode index:2295
target Thresh 19.0
target distance 4.0
model initialize at round 2295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4.79509068, 6.99999439, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 2.687447301062593}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7790172854249777
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.18151344, 8.99935083, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.18151460046403162}
episode index:2296
target Thresh 19.0
target distance 12.0
model initialize at round 2296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.        , 7.99999952, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.180339674250918}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7789669602816882
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.48259189,  2.4918754 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7251908434359434}
episode index:2297
target Thresh 19.0
target distance 4.0
model initialize at round 2297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.00019276,  9.99998915,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.2362452412582594}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7790413871919225
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.03565587, 11.86760643,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8683387920197574}
episode index:2298
target Thresh 19.0
target distance 12.0
model initialize at round 2298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.305877894274122}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7789910953450748
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.63843216, 3.9337489 , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.36758741499640973}
episode index:2299
target Thresh 19.0
target distance 13.0
model initialize at round 2299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.00340224, 11.86713066,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 13.526514917975339}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7789720087343269
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.1332531 , 4.98582557, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.3126698890447608}
episode index:2300
target Thresh 19.0
target distance 13.0
model initialize at round 2300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 12.648875396109553}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7789217907519518
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.34772101, 10.99149721,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.6523344077410865}
episode index:2301
target Thresh 19.0
target distance 13.0
model initialize at round 2301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4., 5., 0.]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.401754250991402}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7788716163994484
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.60994315,  8.85131041,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9364153764511712}
episode index:2302
target Thresh 19.0
target distance 2.0
model initialize at round 2302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.31702375,  7.99856007,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6829777637903367}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7789676339346635
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.31702375,  7.99856007,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6829777637903367}
episode index:2303
target Thresh 19.0
target distance 11.0
model initialize at round 2303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.322711569229998}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7789485906432965
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.0128607 , 5.84790123, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.998788283396561}
episode index:2304
target Thresh 19.0
target distance 13.0
model initialize at round 2304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.045361017187277}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7789295638753927
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.34756686, 9.50460284, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8191992120823982}
episode index:2305
target Thresh 19.0
target distance 12.0
model initialize at round 2305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.182309676547058}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7788946149301275
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.85066094, 6.90028954, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9125915946365831}
episode index:2306
target Thresh 19.0
target distance 2.0
model initialize at round 2306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.9985745 , 11.14437342,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.1443804542559773}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7789904560159835
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.9985745 , 11.14437342,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.1443804542559773}
episode index:2307
target Thresh 19.0
target distance 13.0
model initialize at round 2307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 3.99505772, 11.86740302,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 12.471374134825226}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7789714358403375
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.25400061,  6.0191278 ,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.2547198116666947}
episode index:2308
target Thresh 19.0
target distance 2.0
model initialize at round 2308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.03001475,  8.01085722,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.03191809426069471}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7790671606407531
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.03001475,  8.01085722,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.03191809426069471}
episode index:2309
target Thresh 19.0
target distance 13.0
model initialize at round 2309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.00000000000002}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7790322126474427
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.28860479, 5.84890298, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.727264359953235}
episode index:2310
target Thresh 19.0
target distance 10.0
model initialize at round 2310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.        , 10.99999988,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.00000000000002}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7790475627285126
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.08026283, 11.62712592,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.6322412822841786}
episode index:2311
target Thresh 19.0
target distance 11.0
model initialize at round 2311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.00490259, 11.86739657,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.236201446902358}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7790452847764242
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.00297461, 6.99472025, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.006060040981389346}
episode index:2312
target Thresh 19.0
target distance 12.0
model initialize at round 2312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.02372308, 11.86706988,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 11.614539741046475}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7790262820119834
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.20371975, 6.0170779 , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.20443432126320443}
episode index:2313
target Thresh 19.0
target distance 1.0
model initialize at round 2313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.4450556 ,  2.89549755,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.4571599851839479}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7791217762721339
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.4450556 ,  2.89549755,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.4571599851839479}
episode index:2314
target Thresh 19.0
target distance 6.0
model initialize at round 2314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.49537086,  7.        ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.030557318009548}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7791750714011739
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.23259997, 10.99139822,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.23275897073007246}
episode index:2315
target Thresh 19.0
target distance 5.0
model initialize at round 2315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([12.14612841,  9.82336073,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 4.771024034743322}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7792283205067865
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.26428404,  5.89996167,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.28258401004941713}
episode index:2316
target Thresh 19.0
target distance 12.0
model initialize at round 2316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 11.837958482002342}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7792092715512915
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.00428035, 10.04045088,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.3828203500849536}
episode index:2317
target Thresh 19.0
target distance 2.0
model initialize at round 2317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.96317959,  4.99915481,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.03683011090267603}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7793045220812521
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.96317959,  4.99915481,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.03683011090267603}
episode index:2318
target Thresh 19.0
target distance 7.0
model initialize at round 2318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.79867923,  6.        ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.313684877914479}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.779338187660346
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.67497055, 11.83453437,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8955957567526733}
episode index:2319
target Thresh 19.0
target distance 2.0
model initialize at round 2319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.20664477, 2.25947285, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7688189167552151}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7794333005104924
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.20664477, 2.25947285, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7688189167552151}
episode index:2320
target Thresh 19.0
target distance 2.0
model initialize at round 2320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.72368982, 5.99910319, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.27631163103024386}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7795283314021295
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.72368982, 5.99910319, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.27631163103024386}
episode index:2321
target Thresh 19.0
target distance 11.0
model initialize at round 2321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 4.9950577 , 11.86740302,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.236239443823022}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7795258562109572
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.2313593 ,  7.01092792,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.2316172335190633}
episode index:2322
target Thresh 19.0
target distance 3.0
model initialize at round 2322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.83348835,  9.9999702 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0137976803756237}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7795992415505134
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.80108366, 11.84930615,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8722893160217626}
episode index:2323
target Thresh 19.0
target distance 9.0
model initialize at round 2323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.1334257 ,  3.99881315,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.092338909032678}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7796327078837533
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.99263381, 10.12282861,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8772023182432772}
episode index:2324
target Thresh 19.0
target distance 3.0
model initialize at round 2324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.03848851,  8.99995124,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0007891290217386}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.779705984138427
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.78558406, 10.64515543,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6798527215327502}
episode index:2325
target Thresh 19.0
target distance 6.0
model initialize at round 2325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.523056363717578}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7797393758047475
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.06006472, 11.6658696 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.1518943751105986}
episode index:2326
target Thresh 19.0
target distance 8.0
model initialize at round 2326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([12.00494229, 11.86740302,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 6.067266262438294}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7797727387717417
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.06125634, 11.34704734,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.3524119689802615}
episode index:2327
target Thresh 19.0
target distance 2.0
model initialize at round 2327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.09940922,  4.01977646,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.1013572989350528}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7798673381107573
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.09940922,  4.01977646,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.1013572989350528}
episode index:2328
target Thresh 19.0
target distance 2.0
model initialize at round 2328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.88648586,  8.00379694,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.11357762434699795}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7799618562137582
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.88648586,  8.00379694,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.11357762434699795}
episode index:2329
target Thresh 19.0
target distance 11.0
model initialize at round 2329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.05525993, 10.04561178,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.366095247377615}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7799425987177974
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.56790238, 4.09367431, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0695510103025205}
episode index:2330
target Thresh 19.0
target distance 12.0
model initialize at round 2330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.174760518110984}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.779907590007963
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.79955568, 5.89723725, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9193544539369294}
episode index:2331
target Thresh 19.0
target distance 5.0
model initialize at round 2331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([13.04567455,  9.05522883,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 4.24999553885738}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7799601596520418
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.39856795,  5.08792866,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.995354479603855}
episode index:2332
target Thresh 19.0
target distance 12.0
model initialize at round 2332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.182309676547058}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.779940927646458
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.06165332, 7.17267922, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.829614849431371}
episode index:2333
target Thresh 19.0
target distance 9.0
model initialize at round 2333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.86740302, 3.99505769, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.095913804143926}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7799741041984518
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.01516459, 10.11727225,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8828579990937291}
episode index:2334
target Thresh 19.0
target distance 6.0
model initialize at round 2334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.9261235 ,  3.99999988,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 4.105813645391885}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7800265778154974
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.27397382,  7.9965409 ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.27399565838068346}
episode index:2335
target Thresh 19.0
target distance 1.0
model initialize at round 2335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.29129085,  5.89869689,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1445194399945233}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7801207445201996
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.29129085,  5.89869689,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1445194399945233}
episode index:2336
target Thresh 19.0
target distance 14.0
model initialize at round 2336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.369316876852997}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7800857494631066
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.98260576, 4.06837606, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9317863109816665}
episode index:2337
target Thresh 19.0
target distance 9.0
model initialize at round 2337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([10.02611855, 10.13227187,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.747101356828923}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7800665048699338
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.40259999, 1.98967409, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.40273238309073195}
episode index:2338
target Thresh 19.0
target distance 10.0
model initialize at round 2338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 7.96795883, 11.85876959,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 10.562026588348703}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7800812290020971
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.85955325,  5.97333151,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9834121770595827}
episode index:2339
target Thresh 19.0
target distance 2.0
model initialize at round 2339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.71411484, 3.99638927, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7141239730850049}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7801752113828655
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.71411484, 3.99638927, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7141239730850049}
episode index:2340
target Thresh 19.0
target distance 10.0
model initialize at round 2340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 7.97380118, 11.86775027,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 8.072970833959266}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7801898764997459
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.89497613, 10.32647114,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6816679081218376}
episode index:2341
target Thresh 19.0
target distance 11.0
model initialize at round 2341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([5.17664043, 9.14612952, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.662494227989162}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7801706203144877
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.46113153,  4.86077762,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5565627558261766}
episode index:2342
target Thresh 19.0
target distance 7.0
model initialize at round 2342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.13684406, 8.01840749, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.353107868418666}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7802035713941657
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.15799994, 2.13557047, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8787504705185099}
episode index:2343
target Thresh 19.0
target distance 10.0
model initialize at round 2343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86775028, 6.97380104, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.604242688027984}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7801686455087986
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.33673688,  4.89700455,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.1155873461169743}
episode index:2344
target Thresh 19.0
target distance 10.0
model initialize at round 2344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 7.9739608 , 10.13229199,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.425912579665836}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7801494230120465
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.90365469,  1.99994239,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.09634532381556146}
episode index:2345
target Thresh 19.0
target distance 9.0
model initialize at round 2345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 6.96683223, 10.14283011,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.338083883118315}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7801302169027595
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.87867774,  3.26707475,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.742898720085264}
episode index:2346
target Thresh 19.0
target distance 6.0
model initialize at round 2346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 9.98147353, 10.13700053,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 4.02086111635779}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7801823557110669
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.98081936,  9.20689942,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7933324785019668}
episode index:2347
target Thresh 19.0
target distance 13.0
model initialize at round 2347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4., 7., 0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.18033988749897}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7801631519354765
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.13499654,  8.24549196,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7664896970966489}
episode index:2348
target Thresh 19.0
target distance 14.0
model initialize at round 2348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.041594578792312}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7801439645104826
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.89517065, 10.797333  ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8041946933481973}
episode index:2349
target Thresh 19.0
target distance 7.0
model initialize at round 2349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.00003302, 9.00000036, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.000000357736871}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7801768287809037
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.86070288, 3.0772578 , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.2618488879843215}
episode index:2350
target Thresh 19.0
target distance 13.0
model initialize at round 2350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4., 4., 0.]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.04536101718728}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7801271663404563
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.69713161,  5.88981484,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9399466497098764}
episode index:2351
target Thresh 19.0
target distance 1.0
model initialize at round 2351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.85310283,  4.04862475,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9626492800889934}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7802206496881007
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.85310283,  4.04862475,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9626492800889934}
episode index:2352
target Thresh 19.0
target distance 3.0
model initialize at round 2352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.99999905,  9.0006938 ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.414704910868522}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7802928041081227
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.38598132,  7.03524029,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0391067671515746}
episode index:2353
target Thresh 19.0
target distance 2.0
model initialize at round 2353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.99554837, 6.99992192, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.0044523172308800485}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7803861376662756
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.99554837, 6.99992192, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.0044523172308800485}
episode index:2354
target Thresh 19.0
target distance 8.0
model initialize at round 2354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.07699358,  9.00000882,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 6.070588661796869}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7804188293275638
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.37487817,  3.00487328,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.3749098421474942}
episode index:2355
target Thresh 19.0
target distance 13.0
model initialize at round 2355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.045361017187277}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7804160118862109
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.13416557,  9.01304765,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8659327398198168}
episode index:2356
target Thresh 19.0
target distance 2.0
model initialize at round 2356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.36793971,  9.99664795,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.3679549793948977}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7805091743758646
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.36793971,  9.99664795,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.3679549793948977}
episode index:2357
target Thresh 19.0
target distance 13.0
model initialize at round 2357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 3.99505769, 11.86740302,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 12.471374164705427}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7804899134412798
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.25926828,  6.00898282,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.25942384220083253}
episode index:2358
target Thresh 19.0
target distance 10.0
model initialize at round 2358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4., 8., 0.]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.544003745317555}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7805043332533014
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.90601118, 11.8686259 ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8736960906416715}
episode index:2359
target Thresh 19.0
target distance 7.0
model initialize at round 2359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.00000811, 7.        , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.00000000000659}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7805369055697194
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.41983334, 1.43505492, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8097878157756357}
episode index:2360
target Thresh 19.0
target distance 14.0
model initialize at round 2360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4., 7., 0.]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.64911064067354}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7805176573634743
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.04617064, 10.03503952,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.3568121416033867}
episode index:2361
target Thresh 19.0
target distance 8.0
model initialize at round 2361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4., 6., 0.]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.810249675906681}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7805320471148023
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.06851371, 10.47791085,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5265654790056292}
episode index:2362
target Thresh 19.0
target distance 2.0
model initialize at round 2362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.99793637, 4.99995255, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.0020641773506097495}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7806249239463237
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.99793637, 4.99995255, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.0020641773506097495}
episode index:2363
target Thresh 19.0
target distance 13.0
model initialize at round 2363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.        ,  9.43867946,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 13.27907948841679}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.7805479831744506
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.36977518, 1.44024192, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6708671904138224}
episode index:2364
target Thresh 19.0
target distance 6.0
model initialize at round 2364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.99983201, 9.00007785, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.472251395016942}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7805995485092605
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.93533439, 11.80036513,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.8029732153899203}
episode index:2365
target Thresh 19.0
target distance 14.0
model initialize at round 2365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4., 8., 0.]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 13.000000000000009}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7805500222551522
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.06035413,  2.19609655,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8061658482342491}
episode index:2366
target Thresh 19.0
target distance 13.0
model initialize at round 2366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.18033988749897}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7805152893754896
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.85284457, 6.87171694, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.19522106619000862}
episode index:2367
target Thresh 19.0
target distance 9.0
model initialize at round 2367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 8.97380137, 10.13224978,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.560133843017958}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7804961071969633
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.64495447,  1.37100853,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7222794487851408}
episode index:2368
target Thresh 19.0
target distance 11.0
model initialize at round 2368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.347972064798812}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7804466873253264
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.85677746,  4.17033959,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.22254948176658546}
episode index:2369
target Thresh 19.0
target distance 4.0
model initialize at round 2369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.66070843,  8.99999297,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.028582486530773}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7805182288074676
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.2334168 , 10.99248481,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.23353774564391538}
episode index:2370
target Thresh 19.0
target distance 1.0
model initialize at round 2370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.0926944 , 3.82777405, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.370835850819775}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.780589709942513
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.49329728, 2.02105057, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0962135682242566}
episode index:2371
target Thresh 19.0
target distance 4.0
model initialize at round 2371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([13.04546814,  9.05561483,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.221377374029264}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7806611308067868
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.97053558,  7.14027344,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8602313146341984}
episode index:2372
target Thresh 19.0
target distance 14.0
model initialize at round 2372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.369316876853006}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7806419275871569
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.78557838, 10.91710059,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.2075623725414064}
episode index:2373
target Thresh 19.0
target distance 8.0
model initialize at round 2373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.13260427,  4.99510262,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.772887274480946}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7806742498586029
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.96146653, 11.13917566,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9714873957726307}
episode index:2374
target Thresh 19.0
target distance 2.0
model initialize at round 2374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.44137663,  3.00361681,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.44139144500826394}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.780766597542873
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.44137663,  3.00361681,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.44139144500826394}
episode index:2375
target Thresh 19.0
target distance 1.0
model initialize at round 2375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.45930517,  3.9521904 ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0949965563826876}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.780858867493402
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.45930517,  3.9521904 ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0949965563826876}
episode index:2376
target Thresh 19.0
target distance 12.0
model initialize at round 2376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.        , 10.99999893,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.440306200619775}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7808558898198669
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.99046672, 7.16858209, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8314725606488055}
episode index:2377
target Thresh 19.0
target distance 12.0
model initialize at round 2377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.00017976,  8.9967605 ,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.204845460877364}
done in step count: 43
reward sum = 0.11018311023500528
running average episode reward sum: 0.7805738575324047
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.39732334, 2.02013535, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.3978332220523535}
episode index:2378
target Thresh 19.0
target distance 8.0
model initialize at round 2378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.00000203, 8.        , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 6.00000000000036}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7806061404842616
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.07108957, 2.03863388, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.08090922848810123}
episode index:2379
target Thresh 19.0
target distance 5.0
model initialize at round 2379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.89521581,  9.00005221,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.1307706128109336}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7806573563916211
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.13727871,  5.22731306,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.158159373670378}
episode index:2380
target Thresh 19.0
target distance 9.0
model initialize at round 2380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4., 8., 0.]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.615773105863933}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7806715726426117
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.76570938, 11.87989038,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9105488304086055}
episode index:2381
target Thresh 19.0
target distance 1.0
model initialize at round 2381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.181458  , 7.13112998, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8876159732391081}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7807636500680345
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.181458  , 7.13112998, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8876159732391081}
episode index:2382
target Thresh 19.0
target distance 2.0
model initialize at round 2382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.3691721 ,  7.00288892,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.36918339954313095}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7808556502148797
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.3691721 ,  7.00288892,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.36918339954313095}
episode index:2383
target Thresh 19.0
target distance 13.0
model initialize at round 2383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.393026484038709}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.780852682634043
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.08069027,  8.98101244,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9195057941756173}
episode index:2384
target Thresh 19.0
target distance 12.0
model initialize at round 2384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 11.180339887498974}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7808180849876948
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.84296907, 8.08505546, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.928322263519625}
episode index:2385
target Thresh 19.0
target distance 1.0
model initialize at round 2385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.92947006,  9.05933917,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.4250884719259003}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7808889910711032
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.33322549, 10.60548627,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6911243404865017}
episode index:2386
target Thresh 19.0
target distance 11.0
model initialize at round 2386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 4.97399501, 11.867701  ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 11.341696661198243}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7808698050214818
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.45933991,  5.01431008,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5408494363276296}
episode index:2387
target Thresh 19.0
target distance 6.0
model initialize at round 2387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.7432842 , 6.99999784, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.850322205437785}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.780920738938977
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.39493249, 10.9986825 ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.605068942702466}
episode index:2388
target Thresh 19.0
target distance 5.0
model initialize at round 2388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 6.97381354, 11.86774709,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.1481406367869793}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7809716302161059
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.19088388, 11.87213589,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8927808567257001}
episode index:2389
target Thresh 19.0
target distance 2.0
model initialize at round 2389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.16078842, 5.99780405, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.16080341169431744}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7810632738854716
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.16078842, 5.99780405, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.16080341169431744}
episode index:2390
target Thresh 19.0
target distance 4.0
model initialize at round 2390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.13263723, 5.00471644, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 2.1843090861520045}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7811339291452434
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.91140769, 3.00582055, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.08878331503835436}
episode index:2391
target Thresh 19.0
target distance 6.0
model initialize at round 2391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.02622175,  6.99999988,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 4.129543805106647}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7811846674691793
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.24076137, 11.12991268,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7702730746510055}
episode index:2392
target Thresh 19.0
target distance 13.0
model initialize at round 2392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.831072604542193}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7811354555025348
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.896005  , 8.98843006, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9938857758656953}
episode index:2393
target Thresh 19.0
target distance 6.0
model initialize at round 2393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.98342133, 5.        , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 4.464746370755741}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7811861508009882
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.39133258, 8.99921763, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.6086679220645933}
episode index:2394
target Thresh 19.0
target distance 4.0
model initialize at round 2394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.36794794,  8.00022268,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 2.0977084146593437}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7812566367505493
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.9527297 ,  6.00395572,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.0474355226472641}
episode index:2395
target Thresh 19.0
target distance 8.0
model initialize at round 2395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 8.01868118, 10.13715936,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 6.7872153331815985}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7812884056834581
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.85135116, 6.16927831, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8439164685277574}
episode index:2396
target Thresh 19.0
target distance 11.0
model initialize at round 2396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 9.219544457292903}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7813022637745373
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.88464795, 8.4520422 , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0406054681520753}
episode index:2397
target Thresh 19.0
target distance 10.0
model initialize at round 2397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.86775028, 5.97380104, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.959371450511448}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7812392725926983
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.79699941,  2.24247216,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.3162309090056804}
episode index:2398
target Thresh 19.0
target distance 12.0
model initialize at round 2398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.895369891272122}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7812047157037867
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.3031064 , 9.28855111, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9959017096140842}
episode index:2399
target Thresh 19.0
target distance 2.0
model initialize at round 2399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.95058334,  7.0747124 ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.926606257879095}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7812958804055767
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.95058334,  7.0747124 ,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.926606257879095}
episode index:2400
target Thresh 19.0
target distance 14.0
model initialize at round 2400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 4.        , 10.99996388,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.422185065886122}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7812467860910759
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.88159765,  2.3574327 ,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.6533849171569734}
episode index:2401
target Thresh 19.0
target distance 3.0
model initialize at round 2401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.04344487, 9.00038803, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.3841148493275743}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.781317041384127
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.46498656, 7.14504373, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9732228551943444}
episode index:2402
target Thresh 19.0
target distance 12.0
model initialize at round 2402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 3.97380116, 11.86775028,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.0636799277519}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.781313905260996
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.85276607, 10.09658805,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9153310773847341}
episode index:2403
target Thresh 19.0
target distance 13.0
model initialize at round 2403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.176978770806391}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7812946781334436
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.74239758, 6.84211702, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.122637621834135}
episode index:2404
target Thresh 19.0
target distance 12.0
model initialize at round 2404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.00494245, 11.86740305,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.04247287949053}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7812915539169639
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.18965293, 11.87531199,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8956223033615693}
episode index:2405
target Thresh 19.0
target distance 13.0
model initialize at round 2405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4., 8., 0.]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.401754250991392}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7812570758380681
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.3753161 ,  4.07235085,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0006973162574295}
episode index:2406
target Thresh 19.0
target distance 7.0
model initialize at round 2406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.00006652, 7.00000012, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 5.099006585497826}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7812886994044005
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.14599712, 1.26959151, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7448568412469989}
episode index:2407
target Thresh 19.0
target distance 1.0
model initialize at round 2407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.82005233,  4.40789649,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.44582587535690094}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.781379526356475
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.82005233,  4.40789649,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.44582587535690094}
episode index:2408
target Thresh 19.0
target distance 7.0
model initialize at round 2408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([10.97979319, 11.86190263,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.093658076255882}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7814110728378547
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.44715726, 10.02285464,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0745988442837255}
episode index:2409
target Thresh 19.0
target distance 10.0
model initialize at round 2409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([10.0261973 , 11.86774989,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 8.909507972402698}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7814248052765111
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.92071986, 8.18168775, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.19823162713595333}
episode index:2410
target Thresh 19.0
target distance 11.0
model initialize at round 2410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([12.00494245, 11.86740305,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.196531015704494}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7814385263236798
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.99155982, 10.9447802 ,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.3695986644753066}
episode index:2411
target Thresh 19.0
target distance 4.0
model initialize at round 2411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.99481523,  7.99999988,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.0000068396566415}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7815084108484213
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.71633244,  9.99700236,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.28368340260673064}
episode index:2412
target Thresh 19.0
target distance 6.0
model initialize at round 2412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4.75574695, 5.99999922, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.857380725900891}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7815585524104401
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.42355448, 9.99917481, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5764461132651555}
episode index:2413
target Thresh 19.0
target distance 9.0
model initialize at round 2413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.560133908571554}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7815553305318527
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.97557532, 11.83850454,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.2864047105457013}
episode index:2414
target Thresh 19.0
target distance 9.0
model initialize at round 2414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 6.97380128, 11.86775024,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 7.270210409626688}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.781586725840121
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.02811476,  9.41796627,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.1328389906733665}
episode index:2415
target Thresh 19.0
target distance 5.0
model initialize at round 2415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.00492094, 11.86739816,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.1276076215208986}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.781636772725121
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.06187419, 10.70907055,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9822016026921196}
episode index:2416
target Thresh 19.0
target distance 1.0
model initialize at round 2416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.72712653, 11.81912116,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8633767470800796}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7817271174612712
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.72712653, 11.81912116,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8633767470800796}
episode index:2417
target Thresh 19.0
target distance 7.0
model initialize at round 2417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.99958718, 6.        , 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 5.098938568651575}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7817584027724949
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.98812145, 11.85666236,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8567447100360108}
episode index:2418
target Thresh 19.0
target distance 6.0
model initialize at round 2418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 7.97381804, 11.86774595,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.118631355018754}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7818083166200465
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.94856968, 11.86649481,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8680197817467383}
episode index:2419
target Thresh 19.0
target distance 10.0
model initialize at round 2419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([12.02168321, 11.86638747,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 11.235099171002092}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.781789012311784
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.90106764, 4.0070786 , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.09918527305331092}
episode index:2420
target Thresh 19.0
target distance 10.0
model initialize at round 2420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([10.0261984 , 11.86775016,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 8.90950907887722}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.781785704556802
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.77378156, 7.3784924 , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6613973652097848}
episode index:2421
target Thresh 19.0
target distance 12.0
model initialize at round 2421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.32905200177065}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7817664255254511
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.77415052, 7.88306624, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7829320142747662}
episode index:2422
target Thresh 19.0
target distance 1.0
model initialize at round 2422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.4169271 , 7.89360881, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9860856497944811}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7818564930345202
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.4169271 , 7.89360881, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9860856497944811}
episode index:2423
target Thresh 19.0
target distance 14.0
model initialize at round 2423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.00000000000002}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7818220379202707
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.76520132, 5.86290687, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.2718914226002901}
episode index:2424
target Thresh 19.0
target distance 1.0
model initialize at round 2424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.01585184, 7.11559188, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8845501665878402}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7819120082139118
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.01585184, 7.11559188, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8845501665878402}
episode index:2425
target Thresh 19.0
target distance 1.0
model initialize at round 2425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.50265913, 11.90168855,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.0297524844485526}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7820019043358352
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.50265913, 11.90168855,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.0297524844485526}
episode index:2426
target Thresh 19.0
target distance 6.0
model initialize at round 2426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.98925996,  5.        ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 4.000014418536084}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.782051553324572
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.84512715,  8.99197828,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.15508045687439492}
episode index:2427
target Thresh 19.0
target distance 4.0
model initialize at round 2427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 4.99505911, 11.86740237,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.184530808572528}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7821207248429721
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.97744364, 10.49089433,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.5096051119782407}
episode index:2428
target Thresh 19.0
target distance 3.0
model initialize at round 2428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.30641723,  7.99989867,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.217070141508443}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7821898394066432
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.87774479,  9.83176005,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8406968028431141}
episode index:2429
target Thresh 19.0
target distance 7.0
model initialize at round 2429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 9.97639896, 10.1329671 ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.149774934579373}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7822393497607969
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.13127776,  8.01921189,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.3101998495655685}
episode index:2430
target Thresh 19.0
target distance 12.0
model initialize at round 2430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.536241300558661}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7822048363697369
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.31462423, 9.51429793, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.840027646395912}
episode index:2431
target Thresh 19.0
target distance 12.0
model initialize at round 2431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.02500954, 11.85643566,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 14.058810056314933}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.782155994097911
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.02216229, 1.35566266, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6447183637692953}
episode index:2432
target Thresh 19.0
target distance 8.0
model initialize at round 2432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86681292,  3.99802899,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 6.0642411448174345}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7821869102532346
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.60623752,  9.98366129,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.39410130799406584}
episode index:2433
target Thresh 19.0
target distance 2.0
model initialize at round 2433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.87608314,  3.09078884,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9176166522472505}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7822763979647165
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.87608314,  3.09078884,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9176166522472505}
episode index:2434
target Thresh 19.0
target distance 2.0
model initialize at round 2434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.10782909, 4.00439215, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.10791850095091397}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7823658121749979
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.10782909, 4.00439215, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.10791850095091397}
episode index:2435
target Thresh 19.0
target distance 1.0
model initialize at round 2435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.1905086 , 11.51931256,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.961749372418131}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7824551529745977
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.1905086 , 11.51931256,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.961749372418131}
episode index:2436
target Thresh 19.0
target distance 14.0
model initialize at round 2436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.0041017 , 11.86726061,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 15.539024731311486}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7824063081975416
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.01865324, 2.00793797, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.020272016923614397}
episode index:2437
target Thresh 19.0
target distance 9.0
model initialize at round 2437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.677059552831764}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7824194747036132
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.93136905, 11.80620973,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.2318370176110116}
episode index:2438
target Thresh 19.0
target distance 2.0
model initialize at round 2438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.98307884,  8.01044929,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.01988751897992722}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7825086836110738
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.98307884,  8.01044929,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.01988751897992722}
episode index:2439
target Thresh 19.0
target distance 4.0
model initialize at round 2439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.21124559,  8.00014767,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.1500521453302146}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7825773275932004
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.80189855,  6.00508905,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.19816680929673314}
episode index:2440
target Thresh 19.0
target distance 2.0
model initialize at round 2440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.55505705, 6.99852717, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5550590028550141}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7826663987412573
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.55505705, 6.99852717, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5550590028550141}
episode index:2441
target Thresh 19.0
target distance 8.0
model initialize at round 2441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 7.9738302, 11.8677429,  0.       ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 6.67372997895758}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.782696991944066
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.11372803,  8.05732094,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.9495145485948038}
episode index:2442
target Thresh 19.0
target distance 6.0
model initialize at round 2442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.54685766,  9.0000391 ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 4.037247342136701}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7827460312433111
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.15068445,  5.06102275,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8515049479221337}
episode index:2443
target Thresh 19.0
target distance 4.0
model initialize at round 2443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 3.97386196, 11.86773357,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.204131784791733}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.782814465764079
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.94857802, 10.42381683,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.5784732150354088}
episode index:2444
target Thresh 19.0
target distance 3.0
model initialize at round 2444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.86668648, 3.99846979, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.3244652611619705}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7828828443056888
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.3290542 , 5.99141576, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0445965115263682}
episode index:2445
target Thresh 19.0
target distance 5.0
model initialize at round 2445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.99804413,  8.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 3.000000637569116}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.782931747476455
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.7484571, 11.8204202,  0.       ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8581160392566}
episode index:2446
target Thresh 19.0
target distance 2.0
model initialize at round 2446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.5602572 ,  9.01156652,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5603765792093669}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7830204553851283
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.5602572 ,  9.01156652,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5603765792093669}
episode index:2447
target Thresh 19.0
target distance 3.0
model initialize at round 2447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.86719219, 5.99627355, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.3264573403258322}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7830886659834188
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.52584975, 7.98486505, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.116457397021063}
episode index:2448
target Thresh 19.0
target distance 8.0
model initialize at round 2448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.99653947, 5.        , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.323461857983977}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7831189993170311
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.50079253, 10.99951959,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.4992076980041649}
episode index:2449
target Thresh 19.0
target distance 7.0
model initialize at round 2449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.86740292, 5.99505836, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.006697797337735}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7831677262560853
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.98727521, 10.13424253,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.313106372397886}
episode index:2450
target Thresh 19.0
target distance 3.0
model initialize at round 2450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.86666765, 4.99853969, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.3244000788901789}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7832357932792366
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.37395933, 6.99023235, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0584921785539991}
episode index:2451
target Thresh 19.0
target distance 11.0
model initialize at round 2451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.8233601 ,  9.14612859,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.587635662846392}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7832011690960452
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.9913037, 2.8661526, 0.       ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.13412960753832745}
episode index:2452
target Thresh 19.0
target distance 5.0
model initialize at round 2452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.13290676,  7.97619955,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.2290628468826736}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7832691669887905
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.9459982 , 10.17410858,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8276550227317231}
episode index:2453
target Thresh 19.0
target distance 12.0
model initialize at round 2453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.13259815856355}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7832495348468329
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.82025519, 6.87153258, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.22093455060010123}
episode index:2454
target Thresh 19.0
target distance 9.0
model initialize at round 2454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([12.05521678, 10.045681  ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.119466869678083}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7832622667063658
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.45047025, 10.33435346,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8631733667981961}
episode index:2455
target Thresh 19.0
target distance 13.0
model initialize at round 2455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.00000000000002}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7832584062303046
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.88991604, 8.32067658, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.1195672739334006}
episode index:2456
target Thresh 19.0
target distance 11.0
model initialize at round 2456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([12.00494245, 11.86740305,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.196531015704494}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7832545488966741
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.31527636, 9.3585194 , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9382663875016183}
episode index:2457
target Thresh 19.0
target distance 12.0
model initialize at round 2457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.02536009, 11.86753274,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.384355859443286}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7832200016009854
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.02825628, 3.00287847, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.028402513832498758}
episode index:2458
target Thresh 19.0
target distance 3.0
model initialize at round 2458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.05467451,  7.99993348,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0015599550794403}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7832878259191631
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.69314685,  9.83378077,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8884532727112556}
episode index:2459
target Thresh 19.0
target distance 10.0
model initialize at round 2459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([11.02619822, 11.86775012,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 9.386950996262781}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7833005163354562
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.89971556, 6.98908929, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.10087622685961899}
episode index:2460
target Thresh 19.0
target distance 3.0
model initialize at round 2460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.47053993,  9.00179911,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.1068013703170334}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7833682528180504
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.19409955,  7.05418813,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.2425923030929635}
episode index:2461
target Thresh 19.0
target distance 10.0
model initialize at round 2461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.669994727410566}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7833809002580107
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.90908985, 11.53746554,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0560840699830927}
episode index:2462
target Thresh 19.0
target distance 13.0
model initialize at round 2462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7833770025873822
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.8689499 , 7.97850817, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8692156406522228}
episode index:2463
target Thresh 19.0
target distance 4.0
model initialize at round 2463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.99352467, 8.99999988, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.2331797392797683}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7834446255571114
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.41771901, 10.99848139,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.5822829737458096}
episode index:2464
target Thresh 19.0
target distance 5.0
model initialize at round 2464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.00794518,  9.0001688 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.000179320724247}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7834929238834573
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.11261869,  5.38183647,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0814674018464083}
episode index:2465
target Thresh 19.0
target distance 4.0
model initialize at round 2465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.98880741, 9.0000474 , 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.2310424700017997}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7835604450011039
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.62222742, 10.9979626 ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.3777780723891255}
episode index:2466
target Thresh 19.0
target distance 12.0
model initialize at round 2466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 5.99505909, 11.8674028 ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.598502411919734}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.783556480871594
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.96548624,  6.00090197,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.0345255391591141}
episode index:2467
target Thresh 19.0
target distance 3.0
model initialize at round 2467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.25347745,  9.99990463,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.2479930510092743}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7836239215195391
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.26084659, 11.68500621,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7329900716867197}
episode index:2468
target Thresh 19.0
target distance 5.0
model initialize at round 2468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.02930993, 7.99999988, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 3.1716682751871352}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7836720689794339
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.91241644, 11.8685709 ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.872975536457347}
episode index:2469
target Thresh 19.0
target distance 12.0
model initialize at round 2469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.214321868139345}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7836375204883871
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.96534095, 11.7937385 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.7944948392683493}
episode index:2470
target Thresh 19.0
target distance 10.0
model initialize at round 2470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.343119182645125}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7836029999605058
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.99054059, 4.91597658, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.08455421689141171}
episode index:2471
target Thresh 19.0
target distance 2.0
model initialize at round 2471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.71095337, 3.99817336, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.710955714441239}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7836905392000039
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.71095337, 3.99817336, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.710955714441239}
episode index:2472
target Thresh 19.0
target distance 8.0
model initialize at round 2472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.00065327, 9.00000036, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.324349108854196}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7837203347765506
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.02329504, 3.01074481, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.02565365589981438}
episode index:2473
target Thresh 19.0
target distance 11.0
model initialize at round 2473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.13259829055323}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7836583012579362
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.96126324, 3.92564725, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.08383834535720312}
episode index:2474
target Thresh 19.0
target distance 6.0
model initialize at round 2474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.13304362, 7.02335915, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 4.1157055508707465}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7837063181059128
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.71597287, 3.0248486 , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.2851120198386348}
episode index:2475
target Thresh 19.0
target distance 2.0
model initialize at round 2475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.88697278,  5.6718272 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6812686210747284}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7837936741971463
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.88697278,  5.6718272 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6812686210747284}
episode index:2476
target Thresh 19.0
target distance 8.0
model initialize at round 2476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.85034582,  3.99989152,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 6.060065159272234}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7838233800210473
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.68630857,  9.99339691,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.3137609222266244}
episode index:2477
target Thresh 19.0
target distance 4.0
model initialize at round 2477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.02441041, 11.86725935,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.202356981652301}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7838904408039283
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.0392497 , 10.30287758,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.698226476088208}
episode index:2478
target Thresh 19.0
target distance 8.0
model initialize at round 2478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4., 9., 0.]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.324555320336782}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7839200836273232
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.97639252, 10.24406893,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.7562996030233562}
episode index:2479
target Thresh 19.0
target distance 9.0
model initialize at round 2479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 5.97380121, 11.86775026,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.0795804962225235}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7839324167589252
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.88184398, 10.1487306 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.225686910734185}
episode index:2480
target Thresh 19.0
target distance 2.0
model initialize at round 2480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.82609325, 5.99893116, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8260939423173975}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7840195056679301
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.82609325, 5.99893116, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8260939423173975}
episode index:2481
target Thresh 19.0
target distance 6.0
model initialize at round 2481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 5.99999976, 10.99999881,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.000000238418796}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7840672415641154
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.97705936, 10.23765009,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.7626949918854395}
episode index:2482
target Thresh 19.0
target distance 5.0
model initialize at round 2482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.76374936,  9.00009072,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.244820481964939}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7841149390101226
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.67221802,  5.04227173,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.012266988584616}
episode index:2483
target Thresh 19.0
target distance 10.0
model initialize at round 2483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.604242683135503}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7840952034833976
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.42724062, 5.84254696, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0187927612362984}
episode index:2484
target Thresh 19.0
target distance 10.0
model initialize at round 2484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 5.97380719, 11.86774875,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 8.909503430373336}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7841074413290783
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.1159399 ,  7.99102606,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.11628667747871388}
episode index:2485
target Thresh 19.0
target distance 10.0
model initialize at round 2485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([12.05525287, 10.04562311,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.70177105843886}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7840729400638992
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.86435817, 2.1228555 , 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.2314615386196746}
episode index:2486
target Thresh 19.0
target distance 3.0
model initialize at round 2486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.86685261, 4.99785674, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.3250375748875227}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7841396578202063
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.28682339, 6.98654318, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0273923845943522}
episode index:2487
target Thresh 19.0
target distance 2.0
model initialize at round 2487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.01853896, 11.86739721,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8675953071514783}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7842264184078992
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.01853896, 11.86739721,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8675953071514783}
episode index:2488
target Thresh 19.0
target distance 1.0
model initialize at round 2488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.88251265, 6.16556895, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8426615322736043}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.784313109280375
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.88251265, 6.16556895, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8426615322736043}
episode index:2489
target Thresh 19.0
target distance 13.0
model initialize at round 2489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.52844675707443}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7842645580040732
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.50293811, 3.87951619, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0102570225090295}
episode index:2490
target Thresh 19.0
target distance 11.0
model initialize at round 2490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.18772204862523}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7842300629169956
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.21621733, 6.897519  , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1915769523221669}
episode index:2491
target Thresh 19.0
target distance 12.0
model initialize at round 2491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.177724088861572}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7842103445493022
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.28813691, 9.20614061, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0662841036356605}
episode index:2492
target Thresh 19.0
target distance 11.0
model initialize at round 2492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.02619884, 11.86775028,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.06781429073956}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7842061610727482
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.45637799, 11.59654749,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8070897082465525}
episode index:2493
target Thresh 19.0
target distance 11.0
model initialize at round 2493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 5.94474922, 10.04562646,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.366094397008867}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7841864681014379
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.42082584,  4.07699756,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0144101250176298}
episode index:2494
target Thresh 19.0
target distance 12.0
model initialize at round 2494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.00016787,  9.00007785,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.050050404146477}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7841822975480907
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.96869698, 7.27302044, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.727653186017181}
episode index:2495
target Thresh 19.0
target distance 12.0
model initialize at round 2495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.86775028, 6.97380079, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.192853567208825}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7841339153901343
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.86096752,  2.89833551,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.244295688734428}
episode index:2496
target Thresh 19.0
target distance 1.0
model initialize at round 2496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.00969338, 3.18180501, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8182524034877837}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7842203655641872
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.00969338, 3.18180501, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8182524034877837}
episode index:2497
target Thresh 19.0
target distance 9.0
model initialize at round 2497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4., 4., 0.]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.602325267042653}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7842324896172039
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.02785035, 11.86697824,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.3025844318032735}
episode index:2498
target Thresh 19.0
target distance 14.0
model initialize at round 2498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4., 7., 0.]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.369316876852995}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7841981177910641
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.55642844,  4.8589319 ,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9667056132481918}
episode index:2499
target Thresh 19.0
target distance 5.0
model initialize at round 2499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([2.02487397, 9.0000211 , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.5918309296839666}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7842454385439476
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.4159736 , 5.08272994, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0071834043291692}
episode index:2500
target Thresh 19.0
target distance 8.0
model initialize at round 2500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.99999893,  9.0000006 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 6.000000596046528}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7842746786724786
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.2460858 ,  3.05601796,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.25238112184435507}
episode index:2501
target Thresh 19.0
target distance 7.0
model initialize at round 2501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.99708593,  9.00000489,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 5.098453611192004}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7843038954276055
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.17869684,  3.19341309,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8261446673415157}
episode index:2502
target Thresh 19.0
target distance 14.0
model initialize at round 2502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4., 6., 0.]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.165525060596453}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7842555999964674
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.02110385,  3.85697417,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.14457440619539927}
episode index:2503
target Thresh 19.0
target distance 11.0
model initialize at round 2503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.219544457292901}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7842676809269801
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.13898619,  8.96221515,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.2912020649164568}
episode index:2504
target Thresh 19.0
target distance 12.0
model initialize at round 2504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.02619869, 11.86775024,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 11.145387055226259}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7842480498729674
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.86948395, 6.11690485, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8926877864611161}
episode index:2505
target Thresh 19.0
target distance 7.0
model initialize at round 2505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.3209793,  4.       ,  0.       ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 5.010292178027769}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7842772306192272
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.61818115,  9.82455766,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.908669890026024}
episode index:2506
target Thresh 19.0
target distance 9.0
model initialize at round 2506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.899494936611692}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7842730438250033
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.97462868, 10.64909406,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0358745261802826}
episode index:2507
target Thresh 19.0
target distance 12.0
model initialize at round 2507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.00163685, 11.86672199,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 14.049375165819617}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7842248569778997
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.85957154, 2.01275834, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8596662199422008}
episode index:2508
target Thresh 19.0
target distance 12.0
model initialize at round 2508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4., 5., 0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.049875621120906}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7841767085419934
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.14049798,  4.93967104,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.2734698166650813}
episode index:2509
target Thresh 19.0
target distance 11.0
model initialize at round 2509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.86740305, 5.99505742, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.96818994990224}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7841153829249347
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.66903117,  2.19604747,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.38467515752931314}
episode index:2510
target Thresh 19.0
target distance 13.0
model initialize at round 2510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.531018778398135}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7840812219982795
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.16991705,  7.19726123,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.1547411968782642}
episode index:2511
target Thresh 19.0
target distance 12.0
model initialize at round 2511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.       , 7.9999994, 0.       ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.770329392902951}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7840470882698143
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.45828745,  4.43411632,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.631256182403899}
episode index:2512
target Thresh 19.0
target distance 14.0
model initialize at round 2512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14., 11.,  0.]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.000000000000012}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7840276074908071
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.91161965, 6.01283889, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.08930802389695285}
episode index:2513
target Thresh 19.0
target distance 12.0
model initialize at round 2513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 5.        , 10.99963713,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.206347525308008}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7839935222436325
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.29647149,  3.16475754,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8862986538444804}
episode index:2514
target Thresh 19.0
target distance 3.0
model initialize at round 2514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.00037241, 9.36902356, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1827409793782087}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7840595287954243
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.43962741, 10.97405124,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1237407411291322}
episode index:2515
target Thresh 19.0
target distance 12.0
model initialize at round 2515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.568784232893615}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.784040066300126
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.03414479, 10.95428182,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9548924878804448}
episode index:2516
target Thresh 19.0
target distance 2.0
model initialize at round 2516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.84015082, 4.00418174, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.15990387076205625}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7841258668300027
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.84015082, 4.00418174, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.15990387076205625}
episode index:2517
target Thresh 19.0
target distance 2.0
model initialize at round 2517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.95062172, 11.03547033,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.06079768620284452}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7842115992101338
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.95062172, 11.03547033,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.06079768620284452}
episode index:2518
target Thresh 19.0
target distance 5.0
model initialize at round 2518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.9768641,  4.9999994,  0.       ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.000089806350932}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7842585576860329
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.5219894 ,  8.67640209,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8282595764262101}
episode index:2519
target Thresh 19.0
target distance 11.0
model initialize at round 2519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.611175540405096}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7842106060485737
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.7622668 , 5.79398595, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.100665417890904}
episode index:2520
target Thresh 19.0
target distance 7.0
model initialize at round 2520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.45805871,  5.        ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 5.020937938898022}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.784239628021581
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.72602241, 10.66767764,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7217043393368163}
episode index:2521
target Thresh 19.0
target distance 13.0
model initialize at round 2521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 5.        , 10.99999952,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.083045776277952}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7842354810388207
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.1003337 ,  6.98948365,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.3373396530027413}
episode index:2522
target Thresh 19.0
target distance 10.0
model initialize at round 2522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.13228730466267}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.78421600280243
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.07477477, 6.89820572, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.12630653995443053}
episode index:2523
target Thresh 19.0
target distance 2.0
model initialize at round 2523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.00558257, 6.25607562, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7439453254700861}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7843014956697825
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.00558257, 6.25607562, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7439453254700861}
episode index:2524
target Thresh 19.0
target distance 1.0
model initialize at round 2524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.32998085, 5.09212148, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9659869383938247}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7843869208200123
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.32998085, 5.09212148, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9659869383938247}
episode index:2525
target Thresh 19.0
target distance 12.0
model initialize at round 2525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 3.97384693, 11.86773869,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.616974770469753}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7843674057645115
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.15154781,  6.04909811,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8498716061369062}
episode index:2526
target Thresh 19.0
target distance 3.0
model initialize at round 2526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.00539625,  9.00570154,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.0057160189944117}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7844329509145849
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.35068516,  7.10567551,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1051814635369912}
episode index:2527
target Thresh 19.0
target distance 14.0
model initialize at round 2527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.        , 10.99999261,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 14.4222010020815}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7843850820381507
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.8527437 , 2.04856884, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.96275940411085}
episode index:2528
target Thresh 19.0
target distance 13.0
model initialize at round 2528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7843808890193535
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.12834986,  9.13107297,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.881449993403945}
episode index:2529
target Thresh 19.0
target distance 9.0
model initialize at round 2529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([5.85387008, 9.82335934, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 8.721081695109405}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7843927962766581
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.1163318 , 1.92754614, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.13704980848607215}
episode index:2530
target Thresh 19.0
target distance 5.0
model initialize at round 2530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.16615123,  6.99999964,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 3.004597872515473}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7844394605215113
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.59910202, 10.51013112,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7868652985824706}
episode index:2531
target Thresh 19.0
target distance 1.0
model initialize at round 2531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.51106977, 4.15215492, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9787206142451664}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.784524595015776
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.51106977, 4.15215492, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9787206142451664}
episode index:2532
target Thresh 19.0
target distance 12.0
model initialize at round 2532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([12.00494245, 11.86740305,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 10.04247287949053}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7845203535402466
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.11949689, 11.3200837 ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.34166223316691985}
episode index:2533
target Thresh 19.0
target distance 5.0
model initialize at round 2533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.48234904,  5.99999714,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.044335672070539}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7845491043873105
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.3672407,  8.0545702,  0.       ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0142500882244478}
episode index:2534
target Thresh 19.0
target distance 14.0
model initialize at round 2534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 3.97385848, 11.86773576,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 13.848966549051045}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7845150957844333
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.88836103,  4.05695594,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9496290602379888}
episode index:2535
target Thresh 19.0
target distance 12.0
model initialize at round 2535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 4.99506763, 11.86740141,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.598494349116624}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7845108630721761
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.04354769,  6.97513146,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9761033613852343}
episode index:2536
target Thresh 19.0
target distance 2.0
model initialize at round 2536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.0032084 , 8.00725973, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.007937093434673578}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7845958016361997
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.0032084 , 8.00725973, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.007937093434673578}
episode index:2537
target Thresh 19.0
target distance 12.0
model initialize at round 2537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.86775039, 7.97380046, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.820239906929407}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7845618148333855
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.06226444,  3.95401848,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.07740259155858405}
episode index:2538
target Thresh 19.0
target distance 13.0
model initialize at round 2538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([14.00494086, 11.86740279,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 13.527924824566878}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7845278548023734
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.16247471, 4.01856989, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.1635324768256495}
episode index:2539
target Thresh 19.0
target distance 11.0
model initialize at round 2539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.614260567258981}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7845083918243508
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.18335072,  8.91884207,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.2293033817703776}
episode index:2540
target Thresh 19.0
target distance 12.0
model initialize at round 2540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.        ,  8.99999928,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.661903421694566}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7844744795474006
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.72808828, 3.96445967, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0020571078157698}
episode index:2541
target Thresh 19.0
target distance 12.0
model initialize at round 2541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 12.214321868139345}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7844405939520215
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.047738  , 11.27402642,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.27815354444031504}
episode index:2542
target Thresh 19.0
target distance 13.0
model initialize at round 2542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.02619884, 11.86775028,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.060291649189555}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7844211882487864
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.35930361, 10.48719767,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8206449230641034}
episode index:2543
target Thresh 19.0
target distance 3.0
model initialize at round 2543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.59001741,  6.99987829,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0808927633293088}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7844862742596949
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.77731951,  8.75920951,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7911925733541428}
episode index:2544
target Thresh 19.0
target distance 9.0
model initialize at round 2544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.9999404, 4.       , 0.       ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 7.000000000253784}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7844980699279622
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.49218344, 11.84384151,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9848584426491112}
episode index:2545
target Thresh 19.0
target distance 5.0
model initialize at round 2545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4.95432   , 9.05521865, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 4.249984429362191}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7845444178973542
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.16593495, 5.15462352, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.1875714296476796}
episode index:2546
target Thresh 19.0
target distance 5.0
model initialize at round 2546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.19817963, 7.00006926, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 3.1053713900656508}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7845907294725809
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.22845381, 3.13993967, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8898847794237926}
episode index:2547
target Thresh 19.0
target distance 12.0
model initialize at round 2547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 3.9976118 , 11.86691529,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 12.725412599035465}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.784556878046608
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.13384168,  4.48776137,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.5057912143905052}
episode index:2548
target Thresh 19.0
target distance 3.0
model initialize at round 2548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.86459991,  9.99991429,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0092098957934763}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7846217831552598
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.1362699 , 11.85166756,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8625005032219226}
episode index:2549
target Thresh 19.0
target distance 2.0
model initialize at round 2549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.1614383 , 8.99846053, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.16144564119308707}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7847062452010812
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.1614383 , 8.99846053, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.16144564119308707}
episode index:2550
target Thresh 19.0
target distance 2.0
model initialize at round 2550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.00096917, 9.0121367 , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.012175332692882797}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.784790641028129
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.00096917, 9.0121367 , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.012175332692882797}
episode index:2551
target Thresh 19.0
target distance 13.0
model initialize at round 2551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.083045973594581}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7847567643255685
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.29556242, 4.06219097, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9832817066759946}
episode index:2552
target Thresh 19.0
target distance 11.0
model initialize at round 2552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.18772204862523}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7847373107910207
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.7822581 , 7.88038821, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.906915063346824}
episode index:2553
target Thresh 19.0
target distance 12.0
model initialize at round 2553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.837958482002342}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7847034814978738
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.20595454, 11.48341241,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9296212977352003}
episode index:2554
target Thresh 19.0
target distance 13.0
model initialize at round 2554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 5.        , 10.99999976,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.083045874936266}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7846992065295773
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.10075381,  6.99048101,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.3377953276055659}
episode index:2555
target Thresh 19.0
target distance 9.0
model initialize at round 2555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.701003238415892}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7846949349063264
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.9815978 , 10.31753581,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6827122475239424}
episode index:2556
target Thresh 19.0
target distance 13.0
model initialize at round 2556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.311697701223368}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7846755359840419
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.93150169, 9.27939556, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.7238527309059155}
episode index:2557
target Thresh 19.0
target distance 12.0
model initialize at round 2557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.305877894274122}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7846417837401443
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13219959, 4.87350899, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8769706580482568}
episode index:2558
target Thresh 19.0
target distance 6.0
model initialize at round 2558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([12.14612886,  9.82336032,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 6.173906994813866}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.784687840096635
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.04718396,  5.89823779,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8994762120213375}
episode index:2559
target Thresh 19.0
target distance 3.0
model initialize at round 2559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.94419822, 10.04536521,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9562643088360198}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7847719464090972
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.94419822, 10.04536521,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9562643088360198}
episode index:2560
target Thresh 19.0
target distance 7.0
model initialize at round 2560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 9.02459577, 10.13267673,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 7.928195665398346}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7847835568361143
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.57882933, 4.96011124, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.048426595727879}
episode index:2561
target Thresh 19.0
target distance 5.0
model initialize at round 2561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([3.9999975, 8.       , 0.       ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.605552664098355}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.784848044128528
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.94409638, 10.0453118 ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9563235715787507}
episode index:2562
target Thresh 19.0
target distance 10.0
model initialize at round 2562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 5.99509354, 11.8673972 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.924962332391559}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7848437261001907
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.30408229,  5.98579292,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6960627128140973}
episode index:2563
target Thresh 19.0
target distance 2.0
model initialize at round 2563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.86157131,  3.00412393,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.13849010232516307}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7849276404035838
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.86157131,  3.00412393,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.13849010232516307}
episode index:2564
target Thresh 19.0
target distance 5.0
model initialize at round 2564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.32375133, 8.00005329, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.4365868713843173}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7849734775808144
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.382743  , 4.03949344, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.033956021099074}
episode index:2565
target Thresh 19.0
target distance 6.0
model initialize at round 2565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.00000274,  8.        ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.0000021934512535}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7850192790314843
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.05893725, 11.8657172 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8677210830323745}
episode index:2566
target Thresh 19.0
target distance 12.0
model initialize at round 2566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.132280553149382}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7849589869125491
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.07199917, 3.96043404, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.08215440048536243}
episode index:2567
target Thresh 19.0
target distance 12.0
model initialize at round 2567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 5.99505755, 11.86740305,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 10.042472879490532}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7849546340895691
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.88687652, 11.79950215,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8074655460678832}
episode index:2568
target Thresh 19.0
target distance 13.0
model initialize at round 2568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.531018778398135}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7849209177260051
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.16360589,  7.19282634,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.1623615696734377}
episode index:2569
target Thresh 19.0
target distance 3.0
model initialize at round 2569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.01442838, 4.00113714, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.001241102819283}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7849851508319483
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.8275895 , 2.23501801, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.784170146760587}
episode index:2570
target Thresh 19.0
target distance 12.0
model initialize at round 2570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.174760518110984}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7849657446630619
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.18974668,  6.90052616,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.2113867303573713}
episode index:2571
target Thresh 19.0
target distance 10.0
model initialize at round 2571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 8.897487499533781}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7849772300850436
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.2410558 ,  8.86043413,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1473200041240843}
episode index:2572
target Thresh 19.0
target distance 5.0
model initialize at round 2572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([12.02614527, 11.86773575,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 3.1480979184819273}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7850229054717187
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.06865187, 11.72600275,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.180884973724795}
episode index:2573
target Thresh 19.0
target distance 10.0
model initialize at round 2573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.189726279481821}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7850035072530525
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.80838763, 7.90566604, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9257139246822982}
episode index:2574
target Thresh 19.0
target distance 4.0
model initialize at round 2574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.08133113,  9.0002749 ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.201147915907138}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7850675835609153
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.44702041,  7.00292051,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.4470299516354974}
episode index:2575
target Thresh 19.0
target distance 12.0
model initialize at round 2575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.        ,  9.99998558,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.806239464066913}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7850339149710601
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.93786814, 2.97306665, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9750482447180903}
episode index:2576
target Thresh 19.0
target distance 2.0
model initialize at round 2576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.11714673, 2.27986917, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.30339760744569405}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7851173321557823
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.11714673, 2.27986917, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.30339760744569405}
episode index:2577
target Thresh 19.0
target distance 7.0
model initialize at round 2577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.13224973,  5.9738012 ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.928881459969503}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7851287320463347
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.97478538, 10.69756548,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0206239198148026}
episode index:2578
target Thresh 19.0
target distance 5.0
model initialize at round 2578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.00197852, 7.00000322, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 3.1616556095321293}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7851742424255335
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.88609466, 3.0963496 , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9108010087073458}
episode index:2579
target Thresh 19.0
target distance 11.0
model initialize at round 2579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 12.33004276564912}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7851270510258682
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.26275315, 10.38618745,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.9593220389016638}
episode index:2580
target Thresh 19.0
target distance 3.0
model initialize at round 2580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.22309206, 8.99991417, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0246666471981505}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7851909305101666
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.63305818, 10.90709674,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9785043713894135}
episode index:2581
target Thresh 19.0
target distance 3.0
model initialize at round 2581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.99947429, 8.99999774, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0000024031632193}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.785254760513842
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.98369025, 10.79310954,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.7932772178817697}
episode index:2582
target Thresh 19.0
target distance 5.0
model initialize at round 2582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([10.02554286, 11.86758024,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.1474759847429103}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7853001516247542
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.25033416, 11.87560773,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.1526871923632798}
episode index:2583
target Thresh 19.0
target distance 12.0
model initialize at round 2583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([12.        , 10.99999714,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.206553975045065}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7852807211831908
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.69164885, 3.97265425, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.30956133397481655}
episode index:2584
target Thresh 19.0
target distance 12.0
model initialize at round 2584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.        , 7.99998856, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.180334769550202}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7852335798718197
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.6545648,  2.5468333,  0.       ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5698118447157345}
episode index:2585
target Thresh 19.0
target distance 1.0
model initialize at round 2585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.37214815, 11.89197359,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0907863387826033}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7853166295315754
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.37214815, 11.89197359,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0907863387826033}
episode index:2586
target Thresh 19.0
target distance 8.0
model initialize at round 2586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.99523795,  9.00000322,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 6.081984642696379}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.785344483559588
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.39767486,  3.00517511,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.3977085301328138}
episode index:2587
target Thresh 19.0
target distance 13.0
model initialize at round 2587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.401754250991392}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7853400154196887
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.13008958,  6.98655501,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8700143149084004}
episode index:2588
target Thresh 19.0
target distance 9.0
model initialize at round 2588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([10.02619864, 11.86775023,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 7.588903657435095}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7853678388977036
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.89477858, 8.64108883, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9640777614432291}
episode index:2589
target Thresh 19.0
target distance 4.0
model initialize at round 2589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.00701071,  8.9999907 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.000021585757908}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7854314034386697
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.95234385, 10.99022186,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.048648951458324055}
episode index:2590
target Thresh 19.0
target distance 2.0
model initialize at round 2590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.43170547, 8.00726974, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.4317666802101422}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7855142164824989
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.43170547, 8.00726974, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.4317666802101422}
episode index:2591
target Thresh 19.0
target distance 2.0
model initialize at round 2591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.01714039, 11.26382777,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.26438397387347945}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7855969656273745
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.01714039, 11.26382777,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.26438397387347945}
episode index:2592
target Thresh 19.0
target distance 5.0
model initialize at round 2592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.96982861,  8.00003994,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.152904555581644}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7856420497131333
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.19226706,  4.10921848,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9112948710729135}
episode index:2593
target Thresh 19.0
target distance 5.0
model initialize at round 2593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([ 7.02617137, 10.13225991,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 5.121843922019087}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7856870990386101
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.91219587, 6.91923293, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9234168886860251}
episode index:2594
target Thresh 19.0
target distance 11.0
model initialize at round 2594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 5.94478312, 10.04568084,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 9.553696899767507}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7856825109224103
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.34787369,  6.05758869,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.0045671657893587}
episode index:2595
target Thresh 19.0
target distance 2.0
model initialize at round 2595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.03027844,  4.00334251,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.030462375666627222}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7857650677363847
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.03027844,  4.00334251,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.030462375666627222}
episode index:2596
target Thresh 19.0
target distance 12.0
model initialize at round 2596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.180339887498958}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7857604531309799
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.86820515, 5.96821482, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.13557352067962472}
episode index:2597
target Thresh 19.0
target distance 10.0
model initialize at round 2597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 6.66741471, 10.10454283,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.623923265758286}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.785726802955061
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.97877219,  1.43265275,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.567744238217401}
episode index:2598
target Thresh 19.0
target distance 8.0
model initialize at round 2598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.86671264,  4.99839545,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 6.0638641158948605}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7857543705568483
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.83612372, 10.96395606,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.1677933286744734}
episode index:2599
target Thresh 19.0
target distance 12.0
model initialize at round 2599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.895369891272122}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7857348849876438
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.04621784, 10.03626464,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9648429558939448}
episode index:2600
target Thresh 19.0
target distance 8.0
model initialize at round 2600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 8.02245378, 10.13333664,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 7.304411084771071}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7857624282844574
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.84143455, 5.93821066, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.1701790991948311}
episode index:2601
target Thresh 19.0
target distance 12.0
model initialize at round 2601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.82024025163806}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7857026615594153
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.67465701, 1.91118853, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.33724699368494987}
episode index:2602
target Thresh 19.0
target distance 2.0
model initialize at round 2602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.46269655, 8.00688267, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.46274773970397565}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7857849886198995
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.46269655, 8.00688267, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.46274773970397565}
episode index:2603
target Thresh 19.0
target distance 6.0
model initialize at round 2603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.04568015,  9.05521837,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 4.16599596813431}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7858298100528411
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.39404529,  5.06147044,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6090646343631814}
episode index:2604
target Thresh 19.0
target distance 1.0
model initialize at round 2604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.32272959,  3.02293725,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.323543666703555}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7859120250969668
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.32272959,  3.02293725,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.323543666703555}
episode index:2605
target Thresh 19.0
target distance 5.0
model initialize at round 2605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([12.82335983,  9.14612934,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.249107762038276}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7859567633835758
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.89984945, 10.95839942,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.10844694521942576}
episode index:2606
target Thresh 19.0
target distance 12.0
model initialize at round 2606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([12.02619884, 11.86775028,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.145387202335618}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7859520929478705
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.9047283 , 6.10080072, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9042322949743056}
episode index:2607
target Thresh 19.0
target distance 11.0
model initialize at round 2607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([11.02619879, 11.86775026,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 10.76581430769815}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7859630416277217
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.88239306, 6.98268995, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.3207183891330394}
episode index:2608
target Thresh 19.0
target distance 5.0
model initialize at round 2608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.65494812, 3.99999988, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 3.4262011003080857}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7860077089172474
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.6424273 , 7.90416884, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9723062983985096}
episode index:2609
target Thresh 19.0
target distance 9.0
model initialize at round 2609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.86740304, 3.99505761, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.322375823831532}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7860350527069342
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.01795443, 10.1183552 ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.3197390072994857}
episode index:2610
target Thresh 19.0
target distance 1.0
model initialize at round 2610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.09313166, 10.32390189,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.1311581832631394}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7861170002164298
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.09313166, 10.32390189,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.1311581832631394}
episode index:2611
target Thresh 19.0
target distance 5.0
model initialize at round 2611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.13267227,  7.9955123 ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.2109021827036215}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7861797425593791
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.90976402, 10.1997839 ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8052877335858782}
episode index:2612
target Thresh 19.0
target distance 6.0
model initialize at round 2612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.00000274,  8.        ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.0000021934512535}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.78622425854003
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.05786191, 11.85980051,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8617452762561749}
episode index:2613
target Thresh 19.0
target distance 2.0
model initialize at round 2613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.02836668, 6.00456727, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.028732015718258126}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7863060396193949
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.02836668, 6.00456727, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.028732015718258126}
episode index:2614
target Thresh 19.0
target distance 6.0
model initialize at round 2614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 5.97380825, 11.8677484 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.118641440419465}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7863504732562518
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.97142765, 11.22621772,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.2280149909217974}
episode index:2615
target Thresh 19.0
target distance 6.0
model initialize at round 2615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.00016834,  9.00007785,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.47225171308674}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7863948729224383
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.05790034, 10.9236935 ,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.09578690304414351}
episode index:2616
target Thresh 19.0
target distance 7.0
model initialize at round 2616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.00000095, 9.00000012, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 5.000000119209325}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7864219956305306
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.25508704, 3.21325168, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8270685033859909}
episode index:2617
target Thresh 19.0
target distance 10.0
model initialize at round 2617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 6.99999819, 11.73957252,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.131083132714366}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.786417167113292
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.02621646,  4.97398201,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.3772782361108573}
episode index:2618
target Thresh 19.0
target distance 7.0
model initialize at round 2618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4.86775037, 7.97380131, 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.838301249140137}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7864442605966394
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.0697995 , 10.16169584,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.252208781824743}
episode index:2619
target Thresh 19.0
target distance 13.0
model initialize at round 2619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 3.99505755, 11.86740305,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.039073622432332}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7864246604554288
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.50847667, 11.55162039,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.750222352090317}
episode index:2620
target Thresh 19.0
target distance 2.0
model initialize at round 2620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.84653008, 4.00343621, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.15350838386641658}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7865061466589941
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.84653008, 4.00343621, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.15350838386641658}
episode index:2621
target Thresh 19.0
target distance 1.0
model initialize at round 2621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.13548721,  5.10975607,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.2409337657893493}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7865875707067976
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.13548721,  5.10975607,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.2409337657893493}
episode index:2622
target Thresh 19.0
target distance 9.0
model initialize at round 2622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.99999332, 4.        , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.071068755957991}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7866145579082057
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.97764454, 10.11658314,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.3176548099919603}
episode index:2623
target Thresh 19.0
target distance 1.0
model initialize at round 2623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.0991689 , 9.90390146, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9093251986687914}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7866958785797346
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.0991689 , 9.90390146, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9093251986687914}
episode index:2624
target Thresh 19.0
target distance 2.0
model initialize at round 2624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.99425876, 3.99890459, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.005844805623748718}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7867771372926566
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.99425876, 3.99890459, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.005844805623748718}
episode index:2625
target Thresh 19.0
target distance 6.0
model initialize at round 2625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.13280465,  6.99625759,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.160913322898291}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7868212054048833
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.2138395, 11.1473972,  0.       ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7998589023112851}
episode index:2626
target Thresh 19.0
target distance 5.0
model initialize at round 2626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([13.04564368,  9.05528656,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 3.5989476375162988}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7868833214287109
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.04027891,  7.08597358,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9635642269155467}
episode index:2627
target Thresh 19.0
target distance 6.0
model initialize at round 2627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.13313043, 8.0230907 , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 4.115424839362129}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7869273155986392
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.84607428, 4.02370751, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.155740722641881}
episode index:2628
target Thresh 19.0
target distance 1.0
model initialize at round 2628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.31162107,  6.26630229,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.0060705167168433}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7870083626448169
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.31162107,  6.26630229,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.0060705167168433}
episode index:2629
target Thresh 19.0
target distance 1.0
model initialize at round 2629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.7090168 , 6.87580311, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9228771941254225}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7870893480582599
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.7090168 , 6.87580311, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9228771941254225}
episode index:2630
target Thresh 19.0
target distance 13.0
model initialize at round 2630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.311697890307714}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7870556148572091
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.17451044,  6.94336346,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.827430180382644}
episode index:2631
target Thresh 19.0
target distance 9.0
model initialize at round 2631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 7.97380349, 11.86774969,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 7.270208135643034}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7870660444298316
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.52565646, 10.46968842,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7049268945756433}
episode index:2632
target Thresh 19.0
target distance 12.0
model initialize at round 2632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.        , 10.99999058,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 12.206550215135081}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7870323457027765
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.2070022 , 3.00651597, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.01482039230296}
episode index:2633
target Thresh 19.0
target distance 2.0
model initialize at round 2633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.22932956, 8.99809682, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.22933745319768278}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7871131990263518
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.22932956, 8.99809682, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.22933745319768278}
episode index:2634
target Thresh 19.0
target distance 12.0
model initialize at round 2634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.00000000000002}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7870662567995065
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.02871361, 4.90180384, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.10230814386398812}
episode index:2635
target Thresh 19.0
target distance 13.0
model initialize at round 2635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.00494244, 11.86740304,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.971892021826257}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7870465396651459
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13355054, 5.00463023, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8664618310888544}
episode index:2636
target Thresh 19.0
target distance 10.0
model initialize at round 2636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([5.5277342 , 9.53379284, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.337430152751354}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7869870792442355
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.04827563,  2.4446076 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.44722081473995623}
episode index:2637
target Thresh 19.0
target distance 2.0
model initialize at round 2637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.01034188,  6.74423218,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.2559768220843269}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7870678271292831
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.01034188,  6.74423218,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.2559768220843269}
episode index:2638
target Thresh 19.0
target distance 3.0
model initialize at round 2638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.23344779,  3.99996924,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.260025318543855}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7871295672478397
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.57899937,  5.90472591,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9978830096833072}
episode index:2639
target Thresh 19.0
target distance 6.0
model initialize at round 2639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.00000036, 10.99999738,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.000000357628693}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7871732681693367
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.02729722, 10.51314615,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.48761850646266897}
episode index:2640
target Thresh 19.0
target distance 2.0
model initialize at round 2640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.30138075,  4.99822402,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6986215038639572}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7872538538307645
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.30138075,  4.99822402,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6986215038639572}
episode index:2641
target Thresh 19.0
target distance 12.0
model initialize at round 2641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.77032961426902}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7872069827397192
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.82164184, 2.88388377, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.212825308729659}
episode index:2642
target Thresh 19.0
target distance 1.0
model initialize at round 2642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.42779183,  4.07301921,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0893647584031754}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7872874946645244
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.42779183,  4.07301921,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0893647584031754}
episode index:2643
target Thresh 19.0
target distance 3.0
model initialize at round 2643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.01713967, 7.00010407, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.402220586135648}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7873490349464213
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.53199196, 5.03906298, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0983694308015506}
episode index:2644
target Thresh 19.0
target distance 3.0
model initialize at round 2644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.79678161,  4.99988759,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.020550123018774}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7874105286950237
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.29374324,  6.5336163 ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.6091235097280856}
episode index:2645
target Thresh 19.0
target distance 8.0
model initialize at round 2645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.86412977,  4.98176003,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 6.079961561105658}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7874369702941564
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.74000779, 10.93891471,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.267071829379448}
episode index:2646
target Thresh 19.0
target distance 13.0
model initialize at round 2646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.00016787,  9.00007785,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.180491123664936}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7874171950468314
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.36851671, 11.33644203,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7155168635959015}
episode index:2647
target Thresh 19.0
target distance 11.0
model initialize at round 2647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4., 7., 0.]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.848857801796129}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7874120454027427
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.08811144, 11.67346377,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.6792032605759905}
episode index:2648
target Thresh 19.0
target distance 1.0
model initialize at round 2648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.54004502, 6.44882745, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.7716474659162594}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7874922975562335
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.54004502, 6.44882745, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.7716474659162594}
episode index:2649
target Thresh 19.0
target distance 1.0
model initialize at round 2649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.57090303,  6.03761232,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.053714503851868}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7875724891420613
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.57090303,  6.03761232,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.053714503851868}
episode index:2650
target Thresh 19.0
target distance 3.0
model initialize at round 2650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.82457232, 4.000723  , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.5437217207627354}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7876337594215248
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.13786864, 2.15980613, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8514302660595006}
episode index:2651
target Thresh 19.0
target distance 1.0
model initialize at round 2651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13094782, 5.10608797, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.2467278808598188}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7877138371894654
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13094782, 5.10608797, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.2467278808598188}
episode index:2652
target Thresh 19.0
target distance 9.0
model initialize at round 2652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([4.954319  , 9.05521678, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 7.320891100821988}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.787723936101192
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.81249015, 1.31287174, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7122536015819518}
episode index:2653
target Thresh 19.0
target distance 6.0
model initialize at round 2653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 9.97383431, 10.13225811,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 5.101083318951515}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7877671825457657
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.16128644,  7.93970481,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9534455620585078}
episode index:2654
target Thresh 19.0
target distance 2.0
model initialize at round 2654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.04234965, 11.86778992,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8688226726282091}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.787847119576822
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.04234965, 11.86778992,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8688226726282091}
episode index:2655
target Thresh 19.0
target distance 13.0
model initialize at round 2655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4., 5., 0.]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7877877830896788
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.11943986,  2.90756018,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.15103311192184646}
episode index:2656
target Thresh 19.0
target distance 2.0
model initialize at round 2656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.55634946,  8.99384511,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.44369323597113053}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7878676521965324
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.55634946,  8.99384511,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.44369323597113053}
episode index:2657
target Thresh 19.0
target distance 2.0
model initialize at round 2657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.00614011, 9.25459707, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.745428220735808}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7879474612062404
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.00614011, 9.25459707, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.745428220735808}
episode index:2658
target Thresh 19.0
target distance 14.0
model initialize at round 2658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4., 6., 0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.64911064067354}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7879137605048064
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.0534289 , 10.57151063,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5740026519634108}
episode index:2659
target Thresh 19.0
target distance 5.0
model initialize at round 2659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.16035008,  5.00022868,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 3.1155070443423254}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7879568380384513
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.59255219,  1.29751852,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8120923234446932}
episode index:2660
target Thresh 19.0
target distance 12.0
model initialize at round 2660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.214321868139345}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7879100374346372
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.604735  , 10.08646011,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.610884412839302}
episode index:2661
target Thresh 19.0
target distance 2.0
model initialize at round 2661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.68386543,  4.9965508 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.31615338780989455}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7879897105986362
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.68386543,  4.9965508 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.31615338780989455}
episode index:2662
target Thresh 19.0
target distance 4.0
model initialize at round 2662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.81876433,  7.00025249,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.0084462585064724}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.788050548108738
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.86379675,  5.00251985,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.13622655542927917}
episode index:2663
target Thresh 19.0
target distance 3.0
model initialize at round 2663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.03410137,  7.99993122,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0006500264740226}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7881113399450335
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.3625471 ,  9.82375896,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0415973425668559}
episode index:2664
target Thresh 19.0
target distance 1.0
model initialize at round 2664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.46152985,  8.62659442,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7782225747338577}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7881908478850166
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.46152985,  8.62659442,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7782225747338577}
episode index:2665
target Thresh 19.0
target distance 7.0
model initialize at round 2665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.86767528, 3.9740953 , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.100252778645149}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7882167984296959
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.3383528 , 9.85751405, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9218530007449516}
episode index:2666
target Thresh 19.0
target distance 7.0
model initialize at round 2666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 9.02612233, 11.86773089,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.10047670291172}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7882427295138993
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.42143416, 10.73735426,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.635390601498073}
episode index:2667
target Thresh 19.0
target distance 7.0
model initialize at round 2667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.04317898,  8.00002837,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 5.0907553655309465}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7882686411595088
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.53958061,  2.05205047,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.090759160988895}
episode index:2668
target Thresh 19.0
target distance 7.0
model initialize at round 2668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.86741451, 5.97507726, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 5.360698231564491}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7882945333883737
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.04984636, 11.83651801,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8380018158259069}
episode index:2669
target Thresh 19.0
target distance 2.0
model initialize at round 2669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.15842284,  3.99768033,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8415803563234058}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7883738238253069
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.15842284,  3.99768033,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8415803563234058}
episode index:2670
target Thresh 19.0
target distance 10.0
model initialize at round 2670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([10.02619644, 11.86774967,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 9.942349592966071}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7883836075865104
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.86826425, 5.96801942, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.13556203709270853}
episode index:2671
target Thresh 19.0
target distance 2.0
model initialize at round 2671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.36833632, 9.99703658, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6316706314275633}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7884628053381622
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.36833632, 9.99703658, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6316706314275633}
episode index:2672
target Thresh 19.0
target distance 1.0
model initialize at round 2672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.55197096, 4.93615055, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0867611481375459}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7885419438322369
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.55197096, 4.93615055, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0867611481375459}
episode index:2673
target Thresh 19.0
target distance 12.0
model initialize at round 2673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.568784025117028}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7885219550314863
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.46292754, 11.40391787,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6143708591661685}
episode index:2674
target Thresh 19.0
target distance 1.0
model initialize at round 2674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.15976581,  8.56206083,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5843265302380095}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7886010122445586
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.15976581,  8.56206083,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5843265302380095}
episode index:2675
target Thresh 19.0
target distance 8.0
model initialize at round 2675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 7.02619855, 10.13224966,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 8.72534567379791}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7886106928266794
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.2334207 , 2.93245302, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.24299756793445892}
episode index:2676
target Thresh 19.0
target distance 8.0
model initialize at round 2676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.60432687,  9.00001895,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 6.013051195051319}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.788636379904443
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.31837782,  3.00484109,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.318414626065716}
episode index:2677
target Thresh 19.0
target distance 13.0
model initialize at round 2677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 3.99541445, 11.86734416,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 13.527601681109532}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.788602661053132
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.61575219,  3.99242264,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.6157988091299779}
episode index:2678
target Thresh 19.0
target distance 1.0
model initialize at round 2678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.12259102, 10.54502216,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9883579086374893}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7886815701008912
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.12259102, 10.54502216,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9883579086374893}
episode index:2679
target Thresh 19.0
target distance 11.0
model initialize at round 2679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.055385138137432}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7886912061754804
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.94551476, 9.20343097, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.2363334469137925}
episode index:2680
target Thresh 19.0
target distance 10.0
model initialize at round 2680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([10.0261867 , 11.86774721,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 10.563409709119096}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7887008350616513
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.86606731, 5.96495853, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.974208875016497}
episode index:2681
target Thresh 19.0
target distance 7.0
model initialize at round 2681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.00003624, 8.00000012, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.099012523436042}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7887264406414195
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.06453872, 2.17457909, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8279401721158816}
episode index:2682
target Thresh 19.0
target distance 5.0
model initialize at round 2682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.99990153,  9.00003934,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.00003934068154}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7887688459933981
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.95466137,  5.13638902,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8648002782192808}
episode index:2683
target Thresh 19.0
target distance 3.0
model initialize at round 2683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.35194159,  5.99990189,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.1917113478805854}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7888289172132217
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.77549765,  7.94737935,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9736164241625148}
episode index:2684
target Thresh 19.0
target distance 14.0
model initialize at round 2684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 13.416407864998765}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7887952145610356
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.97630746, 11.76740957,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.2418106558996906}
episode index:2685
target Thresh 19.0
target distance 7.0
model initialize at round 2685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.13259767, 8.00493836, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 5.079546710325626}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7888207468713256
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.93593883, 2.22558095, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7770641510114177}
episode index:2686
target Thresh 19.0
target distance 7.0
model initialize at round 2686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.85100446,  5.99988661,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.072015629003407}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.788846260177291
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.31149842, 11.67213978,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.7408124913424645}
episode index:2687
target Thresh 19.0
target distance 4.0
model initialize at round 2687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.90328145, 9.00253534, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.004869641275336}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7889062132054987
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.0353065 , 7.00597572, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.03580863161887767}
episode index:2688
target Thresh 19.0
target distance 1.0
model initialize at round 2688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.54587412, 8.09965563, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.0529000617226711}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7889847159153517
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.54587412, 8.09965563, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.0529000617226711}
episode index:2689
target Thresh 19.0
target distance 3.0
model initialize at round 2689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.71784484,  9.99986303,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.2310869918779668}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7890445728982827
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.46193819, 11.78363705,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9505774756794217}
episode index:2690
target Thresh 19.0
target distance 2.0
model initialize at round 2690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.58546804,  8.99099421,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5855373020559139}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7891229658477816
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.58546804,  8.99099421,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5855373020559139}
episode index:2691
target Thresh 19.0
target distance 12.0
model initialize at round 2691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4., 4., 0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 11.661903789690625}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7890762709983913
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.13131154, 10.38355131,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.4054063699393948}
episode index:2692
target Thresh 19.0
target distance 8.0
model initialize at round 2692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([10.02600999, 11.86770228,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 7.74643930009305}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7891016325761863
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.8658795 , 7.96471773, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.973996208966346}
episode index:2693
target Thresh 19.0
target distance 3.0
model initialize at round 2693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 9.99998951, 11.35304201,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0604997120666266}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7891613572856976
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.74390936, 11.24985665,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7847480362122925}
episode index:2694
target Thresh 19.0
target distance 2.0
model initialize at round 2694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.01311135, 10.61638242,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6165218530905736}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7892395905483003
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.01311135, 10.61638242,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6165218530905736}
episode index:2695
target Thresh 19.0
target distance 2.0
model initialize at round 2695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.63095737,  3.99519837,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.369073870764645}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7893177657743581
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.63095737,  3.99519837,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.369073870764645}
episode index:2696
target Thresh 19.0
target distance 12.0
model initialize at round 2696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.891740256644566}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7892587860353704
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.81262917, 1.89751429, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.21356766867686322}
episode index:2697
target Thresh 19.0
target distance 8.0
model initialize at round 2697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.99986994,  8.00000286,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 6.000002862432441}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7892840329641935
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.38826156,  2.00449049,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.3882875234875377}
episode index:2698
target Thresh 19.0
target distance 11.0
model initialize at round 2698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 6.        , 10.99604881,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.038969911976775}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7892639543638454
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.04820675,  3.98298324,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.984164591237914}
episode index:2699
target Thresh 19.0
target distance 8.0
model initialize at round 2699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 7.9814179 , 11.86466272,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 7.73875144386282}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.789289180677044
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.14430376,  7.95638835,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9672136557677812}
episode index:2700
target Thresh 19.0
target distance 2.0
model initialize at round 2700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.00246942, 9.00819492, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.00855890219715925}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7893671928278485
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.00246942, 9.00819492, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.00855890219715925}
episode index:2701
target Thresh 19.0
target distance 3.0
model initialize at round 2701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 3.99507541, 11.8673862 ,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.327490960242963}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.789426642423397
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.652852  , 10.06371706,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.1414208132894892}
episode index:2702
target Thresh 19.0
target distance 4.0
model initialize at round 2702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 4.99506008, 11.86740239,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.184529921625225}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7894860480310835
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.98346934, 10.56056676,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.4397440546734351}
episode index:2703
target Thresh 19.0
target distance 8.0
model initialize at round 2703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([10.97380153, 10.13224968,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 8.725345644108025}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7894953010643558
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.82086429,  2.92134131,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8246243835711576}
episode index:2704
target Thresh 19.0
target distance 1.0
model initialize at round 2704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.32825947, 9.42745185, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8826362373357131}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7895731216554597
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.32825947, 9.42745185, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8826362373357131}
episode index:2705
target Thresh 19.0
target distance 13.0
model initialize at round 2705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.704699910719649}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7895394055336703
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.80092308, 7.90770716, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8062231387034358}
episode index:2706
target Thresh 19.0
target distance 10.0
model initialize at round 2706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.13228730466267}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7894805617967626
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.15635364,  4.30884158,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.3461640988105887}
episode index:2707
target Thresh 19.0
target distance 5.0
model initialize at round 2707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.99673724, 6.        , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 3.161247400802656}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7895222971875319
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.32979214, 9.86937392, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9298246423042912}
episode index:2708
target Thresh 19.0
target distance 8.0
model initialize at round 2708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4., 9., 0.]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.0827625302982415}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7895473443277359
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.17169917, 3.00914919, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.17194275632878192}
episode index:2709
target Thresh 19.0
target distance 11.0
model initialize at round 2709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.972155793073501}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.789513687483369
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.8347522 , 7.93882061, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.17620940161052456}
episode index:2710
target Thresh 19.0
target distance 9.0
model initialize at round 2710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.058442237262242}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.789522906429336
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.41112658, 11.01284478,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.5890134930632936}
episode index:2711
target Thresh 19.0
target distance 1.0
model initialize at round 2711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.74358588,  3.82996976,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8686760082677617}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7896005159771128
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.74358588,  3.82996976,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8686760082677617}
episode index:2712
target Thresh 19.0
target distance 3.0
model initialize at round 2712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.85455288, 3.98710467, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.325223593174775}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7896596385292775
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.00410527, 5.97635959, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.976368223031786}
episode index:2713
target Thresh 19.0
target distance 11.0
model initialize at round 2713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.972155793073503}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7896395325057313
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.96064276, 10.86296323,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8638602444177159}
episode index:2714
target Thresh 19.0
target distance 6.0
model initialize at round 2714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.99999368, 7.        , 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 4.123107157984711}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7896811017386942
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.67986124, 11.14762597,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.69570448836928}
episode index:2715
target Thresh 19.0
target distance 14.0
model initialize at round 2715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 3.97380116, 11.86775028,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.170371793518292}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7896610026182547
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.83619891, 10.06130703,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.17489810859820168}
episode index:2716
target Thresh 19.0
target distance 12.0
model initialize at round 2716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 5.99505771, 11.86740302,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.126117129419107}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7896551579126535
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.04677842,  6.01934828,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9817667799875919}
episode index:2717
target Thresh 19.0
target distance 2.0
model initialize at round 2717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.39280856,  9.01594377,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.3931319953488671}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7897325474792787
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.39280856,  9.01594377,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.3931319953488671}
episode index:2718
target Thresh 19.0
target distance 1.0
model initialize at round 2718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.6192286 ,  8.06251991,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0118575892569261}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7898098801208825
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.6192286 ,  8.06251991,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.0118575892569261}
episode index:2719
target Thresh 19.0
target distance 1.0
model initialize at round 2719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.27098393, 10.03395001,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.27310235403255784}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7898871559002498
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.27098393, 10.03395001,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.27310235403255784}
episode index:2720
target Thresh 19.0
target distance 3.0
model initialize at round 2720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.1327522, 5.0043073, 0.       ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.3269332662515665}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7899459992828664
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.44177158, 3.04597102, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.051348386906413}
episode index:2721
target Thresh 19.0
target distance 4.0
model initialize at round 2721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.64189176, 4.9999994 , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.5876265081658816}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7900047994300805
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.09006982, 6.99981558, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.0900700096039594}
episode index:2722
target Thresh 19.0
target distance 14.0
model initialize at round 2722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 3.97380116, 11.86775028,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.170371793518292}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7899846331029394
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.82822081, 10.49320442,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5222630493129207}
episode index:2723
target Thresh 19.0
target distance 5.0
model initialize at round 2723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.0000205 , 6.00000238, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 3.0000023842558377}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7900259383037094
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.89228512, 2.07114673, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9350780189128399}
episode index:2724
target Thresh 19.0
target distance 4.0
model initialize at round 2724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([4.00000012, 9.00073755, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.236398022840077}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.790084644381396
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.01563454, 7.1077764 , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8923605689522188}
episode index:2725
target Thresh 19.0
target distance 13.0
model initialize at round 2725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.531018778398135}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7900509879807035
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.6012071 , 10.31007632,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.676459388105011}
episode index:2726
target Thresh 19.0
target distance 6.0
model initialize at round 2726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.99995112,  9.00001252,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.000012517273961}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7900922234086534
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.00771165,  5.00585127,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.00968023117423002}
episode index:2727
target Thresh 19.0
target distance 12.0
model initialize at round 2727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([12.02619884, 11.86775028,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 10.063679927751899}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7900862441982762
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.12727448, 11.24864658,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.2793276081895619}
episode index:2728
target Thresh 19.0
target distance 7.0
model initialize at round 2728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 8.02587317, 10.13233302,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 7.183330936675945}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7900951925331248
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.37589834, 4.03846831, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0323966127860644}
episode index:2729
target Thresh 19.0
target distance 6.0
model initialize at round 2729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.60459328,  7.00001585,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 4.236423817513082}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7901363664552739
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.54097181,  3.00317979,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5409811570880211}
episode index:2730
target Thresh 19.0
target distance 2.0
model initialize at round 2730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.039336 , 11.8608423,  0.       ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8617405572244666}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.790213211432771
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.039336 , 11.8608423,  0.       ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8617405572244666}
episode index:2731
target Thresh 19.0
target distance 11.0
model initialize at round 2731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([11.02619884, 11.86775028,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 9.819967294377982}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7902221034673856
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.89852435, 7.63332832, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9704607771870828}
episode index:2732
target Thresh 19.0
target distance 13.0
model initialize at round 2732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.176978770806391}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7901757069535992
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.6060689 ,  4.91803793,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9989871675213386}
episode index:2733
target Thresh 19.0
target distance 6.0
model initialize at round 2733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 3.99505769, 11.867403  ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.0977982930423895}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7902167911866081
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.96849703, 10.49769567,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.5032912422643433}
episode index:2734
target Thresh 19.0
target distance 2.0
model initialize at round 2734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.77523336,  8.00755212,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7752701462808035}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7902934943708178
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.77523336,  8.00755212,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7752701462808035}
episode index:2735
target Thresh 19.0
target distance 2.0
model initialize at round 2735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.54865741,  8.00266516,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5486638842138245}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7903701414854484
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.54865741,  8.00266516,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5486638842138245}
episode index:2736
target Thresh 19.0
target distance 4.0
model initialize at round 2736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.22958411,  3.99999457,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.1432597487205167}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.790428464415121
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.69489821,  5.99445867,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6949203062443718}
episode index:2737
target Thresh 19.0
target distance 7.0
model initialize at round 2737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 7.02202844, 11.86651439,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 5.096235556768359}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.79045291530467
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.2586107 , 10.78928036,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7707534344232267}
episode index:2738
target Thresh 19.0
target distance 1.0
model initialize at round 2738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.04020869,  8.0256083 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9752209605026295}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7905294202643981
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.04020869,  8.0256083 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9752209605026295}
episode index:2739
target Thresh 19.0
target distance 10.0
model initialize at round 2739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 5.94477733, 10.04567154,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.014103977312276}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7905233076794476
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.38744733,  6.00511725,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.612574047044875}
episode index:2740
target Thresh 19.0
target distance 4.0
model initialize at round 2740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.        , 10.45088011,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.0740136579357307}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7905814896175433
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.95792985, 11.29358017,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.2965791907406789}
episode index:2741
target Thresh 19.0
target distance 10.0
model initialize at round 2741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([12.       , 10.9999944,  0.       ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.99999663829902}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7905612527105438
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.46758768, 4.08113995, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.030990798079441}
episode index:2742
target Thresh 19.0
target distance 6.0
model initialize at round 2742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 8.01831999, 10.1367127 ,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.176004225954196}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7906020615866975
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.88549176, 8.26600246, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7428758450603178}
episode index:2743
target Thresh 19.0
target distance 4.0
model initialize at round 2743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 9.00001943, 11.27747104,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.0191750544460936}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7906601512143991
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.00495136, 11.21707855,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.21713501221817474}
episode index:2744
target Thresh 19.0
target distance 6.0
model initialize at round 2744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.07676589,  8.00003219,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 4.1051941130645515}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7907008943287108
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.35496244,  4.00503063,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.35499808435289115}
episode index:2745
target Thresh 19.0
target distance 4.0
model initialize at round 2745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.47643346, 6.99999738, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.055966762466527}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7907589056563405
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.40598414, 8.9982574 , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5940184209213927}
episode index:2746
target Thresh 19.0
target distance 11.0
model initialize at round 2746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.99983213, 9.00007785, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.055560578267828}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7907527251073211
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.5075426 ,  7.06306573,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.065572764192682}
episode index:2747
target Thresh 19.0
target distance 12.0
model initialize at round 2747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.86740305, 5.99505742, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 10.891740209019883}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7906943177873127
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.36491351,  2.17117551,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.4030669005937101}
episode index:2748
target Thresh 19.0
target distance 7.0
model initialize at round 2748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.00042737,  6.        ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.831171783995005}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.790734989188627
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.918267  , 10.18274849,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.2292738935713239}
episode index:2749
target Thresh 19.0
target distance 3.0
model initialize at round 2749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.01033392,  9.13717411,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.3129765695597762}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7908110855561947
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.01033392,  9.13717411,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.3129765695597762}
episode index:2750
target Thresh 19.0
target distance 2.0
model initialize at round 2750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.0300765 , 10.94913381,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.05909285082969768}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7908871266010671
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.0300765 , 10.94913381,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.05909285082969768}
episode index:2751
target Thresh 19.0
target distance 7.0
model initialize at round 2751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.95431936, 9.05521705, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.144506278993724}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7909112864387847
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.86403522, 3.11916428, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.2338672624135563}
episode index:2752
target Thresh 19.0
target distance 3.0
model initialize at round 2752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.02073442,  8.00060654,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.000821341517915}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7909690738392791
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.94877632,  6.02912629,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9722240651170811}
episode index:2753
target Thresh 19.0
target distance 14.0
model initialize at round 2753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4., 4., 0.]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.369316876853004}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.790910715210334
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.68967186,  7.76185107,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8226302998898122}
episode index:2754
target Thresh 19.0
target distance 7.0
model initialize at round 2754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.86579776,  5.97971307,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.094397576115103}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7909348401775897
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.2998504 , 11.64125579,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.7078977707686852}
episode index:2755
target Thresh 19.0
target distance 2.0
model initialize at round 2755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.99565876,  9.00725222,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.008452280600750968}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7910106983633018
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.99565876,  9.00725222,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.008452280600750968}
episode index:2756
target Thresh 19.0
target distance 2.0
model initialize at round 2756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.84240007,  4.01881051,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.15871853068860864}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7910865015194994
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.84240007,  4.01881051,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.15871853068860864}
episode index:2757
target Thresh 19.0
target distance 10.0
model initialize at round 2757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([12.00461562, 11.8673492 ,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.705843205133904}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.791052872365973
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.62015158, 1.9744943 , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.6206758594138241}
episode index:2758
target Thresh 19.0
target distance 1.0
model initialize at round 2758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.56848192, 6.37812841, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.842553244471302}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7911286052864638
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.56848192, 6.37812841, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.842553244471302}
episode index:2759
target Thresh 19.0
target distance 2.0
model initialize at round 2759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.04275358,  3.96679127,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9677361377449339}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7912042833280266
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.04275358,  3.96679127,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9677361377449339}
episode index:2760
target Thresh 19.0
target distance 11.0
model initialize at round 2760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.574514830248852}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7911706480555766
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.14558684,  7.95498524,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.2814127533361406}
episode index:2761
target Thresh 19.0
target distance 5.0
model initialize at round 2761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([12.14606557,  9.82341879,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.386708995021567}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.791228153251791
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.20403718,  7.89978617,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8022466137653674}
episode index:2762
target Thresh 19.0
target distance 5.0
model initialize at round 2762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.51312447,  8.0001055 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.0393553260719437}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7912684253642588
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.19027086,  4.1504544 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8705921689397207}
episode index:2763
target Thresh 19.0
target distance 8.0
model initialize at round 2763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.44890213,  9.00001132,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 6.1972607255660535}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7912923423594237
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.67280945,  3.00361053,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.6728191404622867}
episode index:2764
target Thresh 19.0
target distance 5.0
model initialize at round 2764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.00814986,  7.00016296,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 3.0001740285196976}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7913325621271057
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.18273742,  3.11663473,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.902068274096779}
episode index:2765
target Thresh 19.0
target distance 3.0
model initialize at round 2765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.95595032,  9.99972427,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0012451808631242}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.791389925625975
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.97365396, 11.48888266,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.4895920443263082}
episode index:2766
target Thresh 19.0
target distance 13.0
model initialize at round 2766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 13.15309597366658}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7913436771639812
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.92231865, 11.6181283 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6229903577318493}
episode index:2767
target Thresh 19.0
target distance 13.0
model initialize at round 2767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4., 7., 0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.18033988749897}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7913233549867633
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.1739247 ,  8.04975013,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9660355144301821}
episode index:2768
target Thresh 19.0
target distance 13.0
model initialize at round 2768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7912897738892938
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.73885963, 4.87382621, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.2900243413827498}
episode index:2769
target Thresh 19.0
target distance 13.0
model initialize at round 2769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4., 8., 0.]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7912694858447941
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.3604631 ,  6.01442714,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.36075169792280193}
episode index:2770
target Thresh 19.0
target distance 11.0
model initialize at round 2770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.322711635547794}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.7912000045937632
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.10056695, 1.95800876, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.10898153669531642}
episode index:2771
target Thresh 19.0
target distance 3.0
model initialize at round 2771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.99944627, 4.99999774, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0000024182830403}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7912572917493931
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.86032801, 6.84675336, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.207126976823799}
episode index:2772
target Thresh 19.0
target distance 11.0
model initialize at round 2772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.18772204862523}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7912237829157631
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.01847685,  6.91702294,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9172090604026814}
episode index:2773
target Thresh 19.0
target distance 6.0
model initialize at round 2773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.00001407, 6.00000048, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 4.0000004768618815}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7912638969089443
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.7662192 , 2.00737556, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.2338971217329698}
episode index:2774
target Thresh 19.0
target distance 4.0
model initialize at round 2774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.9772837 , 9.10113789, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.172004589440074}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7913210991082562
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.12040491, 10.93934592,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9470312019235311}
episode index:2775
target Thresh 19.0
target distance 1.0
model initialize at round 2775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.61225207,  5.02309108,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.1529109416144028}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7913962716229868
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.61225207,  5.02309108,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.1529109416144028}
episode index:2776
target Thresh 19.0
target distance 3.0
model initialize at round 2776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.7762731 ,  9.99961969,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.266238793229594}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7914533849569358
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.52008269, 11.8184787 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.948803457878709}
episode index:2777
target Thresh 19.0
target distance 11.0
model initialize at round 2777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.295630140987008}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7914470233847771
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.14545317, 5.96793043, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8551483768710132}
episode index:2778
target Thresh 19.0
target distance 4.0
model initialize at round 2778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([11.02529455, 11.86749545,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 2.203262663495741}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7915040773526126
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.02829965, 10.57931762,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.42163317143512746}
episode index:2779
target Thresh 19.0
target distance 2.0
model initialize at round 2779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.81272721,  3.02302289,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.18868267078380482}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.791579075885939
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.81272721,  3.02302289,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.18868267078380482}
episode index:2780
target Thresh 19.0
target distance 1.0
model initialize at round 2780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.13713265,  5.89772796,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.2451729013282973}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7916540204828877
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.13713265,  5.89772796,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.2451729013282973}
episode index:2781
target Thresh 19.0
target distance 10.0
model initialize at round 2781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86775045, 7.97379765, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.95936994014297}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7916204774475213
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.37429737,  4.11286578,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.39094399328616775}
episode index:2782
target Thresh 19.0
target distance 7.0
model initialize at round 2782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([10.97646614, 10.13298748,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 5.149713882557577}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7916603191731959
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.16697669,  9.1304656 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8431779829767891}
episode index:2783
target Thresh 19.0
target distance 12.0
model initialize at round 2783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.        , 6.99999976, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.770329525722575}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7916142559950766
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.96046002,  2.92335108,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.08624654360957347}
episode index:2784
target Thresh 19.0
target distance 4.0
model initialize at round 2784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13485944, 7.01924385, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 2.1967735236263564}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7916711269983099
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.99033009, 5.0205847 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.022742843889299057}
episode index:2785
target Thresh 19.0
target distance 14.0
model initialize at round 2785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 3.9738015 , 11.86775019,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.97399098922688}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7916508185861156
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.01842073,  6.06317257,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9370085139604728}
episode index:2786
target Thresh 19.0
target distance 11.0
model initialize at round 2786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.00494239, 11.86740304,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.324761009831922}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7916305247475935
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.25430674, 4.06372052, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9702016169672713}
episode index:2787
target Thresh 19.0
target distance 12.0
model initialize at round 2787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.53624130055866}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.791584538343914
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.1012902 ,  6.01645639,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9887455349125088}
episode index:2788
target Thresh 19.0
target distance 4.0
model initialize at round 2788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.13481845, 5.01929816, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 2.196839588508574}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7916413384377311
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.25550149, 3.02310974, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.25654448398548513}
episode index:2789
target Thresh 19.0
target distance 7.0
model initialize at round 2789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.        , 10.99999917,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.000000000000089}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.791664898889904
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.06258599, 11.76573929,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.210413848142625}
episode index:2790
target Thresh 19.0
target distance 2.0
model initialize at round 2790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.19279861, 10.99499691,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8072168901269745}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7917395442145583
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.19279861, 10.99499691,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8072168901269745}
episode index:2791
target Thresh 19.0
target distance 13.0
model initialize at round 2791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4., 9., 0.]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7917331120488295
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.13832107,  7.96222781,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.2916551192335897}
episode index:2792
target Thresh 19.0
target distance 12.0
model initialize at round 2792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.        , 5.99999988, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.770329569995797}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7916752947547642
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.55807877,  2.14188022,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.4641383131670967}
episode index:2793
target Thresh 19.0
target distance 13.0
model initialize at round 2793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.18033988749897}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.791655042999528
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.20338759, 9.32126669, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0465515947804342}
episode index:2794
target Thresh 19.0
target distance 11.0
model initialize at round 2794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.132283595738201}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7915972950090897
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.32766477, 3.95860707, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.3302689409763756}
episode index:2795
target Thresh 19.0
target distance 12.0
model initialize at round 2795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 5.94408547, 10.04456391,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.877749241312936}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7915639402169168
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.13412096,  2.42355198,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.44427999205787827}
episode index:2796
target Thresh 19.0
target distance 12.0
model initialize at round 2796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.180339887498974}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7915437499953966
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.03173818, 11.86425759,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8648401543110542}
episode index:2797
target Thresh 19.0
target distance 2.0
model initialize at round 2797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.49644804, 5.99904776, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5035528603627409}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7916182518717385
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.49644804, 5.99904776, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5035528603627409}
episode index:2798
target Thresh 19.0
target distance 2.0
model initialize at round 2798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.30864203,  5.00689149,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.3087189582076724}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7916927005134421
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.30864203,  5.00689149,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.3087189582076724}
episode index:2799
target Thresh 19.0
target distance 6.0
model initialize at round 2799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.72901297, 3.99999988, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 4.065889922083212}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.791732274548973
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.35680508, 7.99857854, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6431964865678578}
episode index:2800
target Thresh 19.0
target distance 13.0
model initialize at round 2800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.02619884, 11.86775028,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.393026484038703}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7917258656460636
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.96629826, 8.26907692, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.2116026090681034}
episode index:2801
target Thresh 19.0
target distance 8.0
model initialize at round 2801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([12.00494245, 11.86740305,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 6.067266420292909}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7917492950302014
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.05898381, 11.21787183,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.22571491347229777}
episode index:2802
target Thresh 19.0
target distance 13.0
model initialize at round 2802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.831072604542193}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7917159693081405
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.18392082,  8.89222176,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8231654618461017}
episode index:2803
target Thresh 19.0
target distance 5.0
model initialize at round 2803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([3.99999976, 8.        , 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 4.242640855706696}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7917554785915543
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.86361092, 11.61797667,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0619411431172345}
episode index:2804
target Thresh 19.0
target distance 10.0
model initialize at round 2804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.06225774829857}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7917635893835001
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.94506522, 11.86641189,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8681517085623371}
episode index:2805
target Thresh 19.0
target distance 11.0
model initialize at round 2805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.611175621019552}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7917060290913907
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.02789411,  2.98227632,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.9826723009758035}
episode index:2806
target Thresh 19.0
target distance 14.0
model initialize at round 2806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 3.97380116, 11.86775028,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.057464453276573}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7916858601785063
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.823204  , 11.73902645,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.759879538129351}
episode index:2807
target Thresh 19.0
target distance 14.0
model initialize at round 2807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.64911064067353}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7916283685650968
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.60296117, 1.86704248, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.41870937238344924}
episode index:2808
target Thresh 19.0
target distance 1.0
model initialize at round 2808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.19707203, 11.17134985,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.26114776827905106}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7917025485691676
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.19707203, 11.17134985,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.26114776827905106}
episode index:2809
target Thresh 19.0
target distance 2.0
model initialize at round 2809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.40875769, 2.02112627, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.4093032685715812}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7917766757760825
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.40875769, 2.02112627, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.4093032685715812}
episode index:2810
target Thresh 19.0
target distance 5.0
model initialize at round 2810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([5.00000012, 9.88601077, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 3.2001519852435956}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7918160650767669
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.24775967, 10.97862563,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7525439413491576}
episode index:2811
target Thresh 19.0
target distance 12.0
model initialize at round 2811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 10.198039027185583}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7917958928952407
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.20816444,  9.64932188,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.6818734029199587}
episode index:2812
target Thresh 19.0
target distance 6.0
model initialize at round 2812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.00494166, 11.86740282,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.097797621059326}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7918352473591955
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.04035031, 10.52895042,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.4727746376358956}
episode index:2813
target Thresh 19.0
target distance 6.0
model initialize at round 2813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.99914372, 6.        , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.12289803082279}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7918745738526713
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.10867733, 9.99568105, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.1087631136582843}
episode index:2814
target Thresh 19.0
target distance 3.0
model initialize at round 2814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.96481663,  3.00275278,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.003369826715924}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7919307462953523
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.86723496,  1.26969994,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7422699852250556}
episode index:2815
target Thresh 19.0
target distance 2.0
model initialize at round 2815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.97197124, 2.01627433, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.03241088416040741}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7920046345246509
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.97197124, 2.01627433, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.03241088416040741}
episode index:2816
target Thresh 19.0
target distance 13.0
model initialize at round 2816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.176978770806391}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7919589887301051
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.13170895, 4.90795898, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.2563116080815044}
episode index:2817
target Thresh 19.0
target distance 3.0
model initialize at round 2817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.18249464,  9.00076842,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.2922277081423792}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.792015071416858
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.66032887,  7.04245758,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.163151621687666}
episode index:2818
target Thresh 19.0
target distance 7.0
model initialize at round 2818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([4.60411784, 4.        , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 5.637502082262614}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7920382569183064
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.05243904, 9.77777506, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7795408211780743}
episode index:2819
target Thresh 19.0
target distance 10.0
model initialize at round 2819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 6., 11.,  0.]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.000000000000018}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7920462242917395
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.96135589, 10.1479942 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8528817320784793}
episode index:2820
target Thresh 19.0
target distance 2.0
model initialize at round 2820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.924317  ,  6.91659665,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9197159001058434}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7921199406248514
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.924317  ,  6.91659665,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9197159001058434}
episode index:2821
target Thresh 19.0
target distance 4.0
model initialize at round 2821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.99982988, 9.00007784, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.2360744403807167}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7921758867833825
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.91417652, 11.04064034,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9150794262295884}
episode index:2822
target Thresh 19.0
target distance 1.0
model initialize at round 2822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.78431904,  7.0424453 ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9815443315444563}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7922495049602215
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.78431904,  7.0424453 ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9815443315444563}
episode index:2823
target Thresh 19.0
target distance 12.0
model initialize at round 2823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.        ,  9.99999166,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.806243262005653}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7922162499287532
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.93247476, 2.975978  , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9783111562190544}
episode index:2824
target Thresh 19.0
target distance 6.0
model initialize at round 2824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.00001478, 9.00000167, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.0000016689573545}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7922552884243537
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.08989835, 5.01455724, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.09106934859265166}
episode index:2825
target Thresh 19.0
target distance 5.0
model initialize at round 2825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.35106418, 7.        , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.290193673017008}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.792294299291861
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.37623569, 10.77516031,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8616419253497253}
episode index:2826
target Thresh 19.0
target distance 11.0
model initialize at round 2826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.350086702751616}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7922877505257514
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.20610363,  8.91834547,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7980845244712006}
episode index:2827
target Thresh 19.0
target distance 4.0
model initialize at round 2827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.98378694, 3.99999964, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.228864772694125}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7923435186479133
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.03715771, 5.99839544, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.03719234249491479}
episode index:2828
target Thresh 19.0
target distance 2.0
model initialize at round 2828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.97798109, 4.54970074, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.5498571568471513}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7923992473440434
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.84559667, 2.56836971, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9493884558957836}
episode index:2829
target Thresh 19.0
target distance 9.0
model initialize at round 2829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.000000000000019}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7924070590057593
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.433652  , 11.90324361,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.00194963989827}
episode index:2830
target Thresh 19.0
target distance 12.0
model initialize at round 2830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([12.05521794, 10.04567912,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.877902021799366}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7923738305483546
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.85747334, 2.02409145, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.14454842053712616}
episode index:2831
target Thresh 19.0
target distance 11.0
model initialize at round 2831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 5.97380124, 11.86775025,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 9.470810712129103}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7923816456682174
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.05910099,  8.0831635 ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.3137275629171214}
episode index:2832
target Thresh 19.0
target distance 5.0
model initialize at round 2832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([13.04567188,  9.05523382,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 4.250000980286976}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7924205155426728
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.38440182,  5.08737219,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9902799006982764}
episode index:2833
target Thresh 19.0
target distance 10.0
model initialize at round 2833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.544003745317555}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7924283086740973
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.07818092, 11.72669857,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.7308919690521744}
episode index:2834
target Thresh 19.0
target distance 13.0
model initialize at round 2834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.205906925752268}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.792395119604404
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.99340556, 11.33844241,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.338506652401862}
episode index:2835
target Thresh 19.0
target distance 10.0
model initialize at round 2835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86775028, 6.97380104, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.604242688027984}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7923619539402605
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.16412057,  4.91053417,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.2360286789281596}
episode index:2836
target Thresh 19.0
target distance 12.0
model initialize at round 2836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4., 6., 0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.770329614269032}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7923288116569166
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.9980205 , 10.37799721,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.3780023924416005}
episode index:2837
target Thresh 19.0
target distance 5.0
model initialize at round 2837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.02607093, 11.8677168 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.1480212422443814}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7923676316669037
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.05888029, 10.98434664,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9412498794225285}
episode index:2838
target Thresh 19.0
target distance 11.0
model initialize at round 2838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.00016787,  9.00007773,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.816836619995275}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7923345107315133
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.91645693, 3.03188226, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.08941992470866494}
episode index:2839
target Thresh 19.0
target distance 12.0
model initialize at round 2839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.181338705408916}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7923279777831923
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.72922385, 7.83023401, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.105013997353578}
episode index:2840
target Thresh 19.0
target distance 9.0
model initialize at round 2840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([ 8.02621818, 10.13224956,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.074354842600227}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7923214494339197
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.45609235, 1.94681452, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.4591828898429656}
episode index:2841
target Thresh 19.0
target distance 2.0
model initialize at round 2841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.00158584, 9.01014292, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.010266146652206385}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7923945242229999
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.00158584, 9.01014292, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.010266146652206385}
episode index:2842
target Thresh 19.0
target distance 3.0
model initialize at round 2842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([10.99998415, 11.36945018,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0660793341479622}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7924499605493373
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.82403052, 10.96056783,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8249734488729814}
episode index:2843
target Thresh 19.0
target distance 2.0
model initialize at round 2843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.93692458, 9.34814632, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6548982592301621}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7925229387629275
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.93692458, 9.34814632, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6548982592301621}
episode index:2844
target Thresh 19.0
target distance 12.0
model initialize at round 2844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.891740190748873}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7924898330888784
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.19491709, 3.84422238, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.24951821169928753}
episode index:2845
target Thresh 19.0
target distance 14.0
model initialize at round 2845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.64911064067353}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7924328266154546
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.0952144 , 1.87608765, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.15626917135162918}
episode index:2846
target Thresh 19.0
target distance 12.0
model initialize at round 2846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 11.180339887498958}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7923997758495529
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.8582008 , 3.98207634, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.14292750180351435}
episode index:2847
target Thresh 19.0
target distance 4.0
model initialize at round 2847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.86734368, 7.99541839, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.1841777638020483}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7924551130069091
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.05869245, 9.99380589, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.05901839443888795}
episode index:2848
target Thresh 19.0
target distance 9.0
model initialize at round 2848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 3.99505755, 11.86740305,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.058442237262244}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7924628529637335
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.27997264, 11.88161211,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9249997794746974}
episode index:2849
target Thresh 19.0
target distance 10.0
model initialize at round 2849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([12.05522201, 10.0456726 ,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.701780415163872}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.792429826452551
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.8429507 , 2.08709791, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.242560304012734}
episode index:2850
target Thresh 19.0
target distance 1.0
model initialize at round 2850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.15276074, 9.27819085, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1130241704569634}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7925026325463944
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.15276074, 9.27819085, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1130241704569634}
episode index:2851
target Thresh 19.0
target distance 8.0
model initialize at round 2851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.86775027, 4.97380117, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.306884437188084}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7925103476997791
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.01100995, 11.59159247,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5916949134521914}
episode index:2852
target Thresh 19.0
target distance 12.0
model initialize at round 2852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.568784232893615}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.792477339269493
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.82827969, 7.9289989 , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.244622913190826}
episode index:2853
target Thresh 19.0
target distance 4.0
model initialize at round 2853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.14204419, 7.99999952, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.3030993967863447}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7925325329137574
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.39455783, 9.99726748, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6054483325672909}
episode index:2854
target Thresh 19.0
target distance 5.0
model initialize at round 2854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([4.00000119, 9.00000238, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.1622802989785708}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7925710504153638
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.48818207, 5.07612538, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0449239391606366}
episode index:2855
target Thresh 19.0
target distance 6.0
model initialize at round 2855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.01841021,  6.0000788 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 4.00012116324136}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7926095409439299
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.25225258,  2.01249886,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.252562043873485}
episode index:2856
target Thresh 19.0
target distance 3.0
model initialize at round 2856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13280388, 7.00391411, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.3266019190987461}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7926646303590702
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.33700891, 5.02340361, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.0331096344792927}
episode index:2857
target Thresh 19.0
target distance 9.0
model initialize at round 2857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 8.97389557, 10.13227365,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 8.700941118659232}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7926722726332622
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.15885844,  4.96715991,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.16221737619469562}
episode index:2858
target Thresh 19.0
target distance 7.0
model initialize at round 2858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.922302179221124}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7926949038775318
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.03065245, 11.86627863,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8668207645025504}
episode index:2859
target Thresh 19.0
target distance 5.0
model initialize at round 2859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([11.02598213, 11.8676921 ,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.147929066424049}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7927332972677845
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.12232796, 11.56607538,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0443895527574532}
episode index:2860
target Thresh 19.0
target distance 9.0
model initialize at round 2860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([4., 9., 0.]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 7.280109889280543}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7927409075273902
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.41619623, 1.44305335, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.806855883191786}
episode index:2861
target Thresh 19.0
target distance 13.0
model initialize at round 2861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.311697890307714}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.79270792233821
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.80696266, 6.91825715, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8110922419620893}
episode index:2862
target Thresh 19.0
target distance 6.0
model initialize at round 2862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([1.13450016, 6.00053423, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 4.920910835758405}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.792746270950736
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.36396691, 2.01116486, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.3641381131445775}
episode index:2863
target Thresh 19.0
target distance 2.0
model initialize at round 2863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.01978209, 11.85421672,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8544457481983068}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7928186360795939
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.01978209, 11.85421672,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8544457481983068}
episode index:2864
target Thresh 19.0
target distance 4.0
model initialize at round 2864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4.00000107, 9.23307562, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 2.1420031041295573}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7928734986848017
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.03236449, 10.81041253,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8110585198186185}
episode index:2865
target Thresh 19.0
target distance 3.0
model initialize at round 2865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.85511089, 6.98371246, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.3281773213405623}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7929283230048698
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.22327039, 8.89788983, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9252328479123091}
episode index:2866
target Thresh 19.0
target distance 8.0
model initialize at round 2866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4., 9., 0.]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.0827625302982415}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7929508017900093
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.42997825, 3.01022828, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.4300998888180777}
episode index:2867
target Thresh 19.0
target distance 14.0
model initialize at round 2867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4., 9., 0.]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.165525060596453}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7929306278321415
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.05694011,  6.01624038,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9854060902679324}
episode index:2868
target Thresh 19.0
target distance 1.0
model initialize at round 2868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.75037988, 6.06397808, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9687348702808576}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7930028025871668
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.75037988, 6.06397808, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9687348702808576}
episode index:2869
target Thresh 19.0
target distance 3.0
model initialize at round 2869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 6.98069857, 11.86500172,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.3368632666824376}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7930575054434083
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.81860393, 10.26679321,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.09895614138904}
episode index:2870
target Thresh 19.0
target distance 13.0
model initialize at round 2870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.00016787,  9.00007785,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.180518975641132}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7930507912086665
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.91675777, 7.02219845, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9170264857278847}
episode index:2871
target Thresh 19.0
target distance 11.0
model initialize at round 2871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.611175621019552}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7930056552894744
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.74849401, 3.89135196, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.926155267566588}
episode index:2872
target Thresh 19.0
target distance 2.0
model initialize at round 2872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.79313195,  3.02086937,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.20791806540216745}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7930777034428718
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.79313195,  3.02086937,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.20791806540216745}
episode index:2873
target Thresh 19.0
target distance 9.0
model initialize at round 2873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.074346404723803}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7930709891888902
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.99233085, 10.93484304,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.06560675190110674}
episode index:2874
target Thresh 19.0
target distance 2.0
model initialize at round 2874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.93126893,  6.86604297,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8687660148741968}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7931429644969984
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.93126893,  6.86604297,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8687660148741968}
episode index:2875
target Thresh 19.0
target distance 2.0
model initialize at round 2875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.21574251,  4.99820707,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.7842595432460966}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7932148897527366
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.21574251,  4.99820707,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.7842595432460966}
episode index:2876
target Thresh 19.0
target distance 2.0
model initialize at round 2876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.07299745, 6.99834049, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.0730163119396673}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.793286765008297
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.07299745, 6.99834049, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.0730163119396673}
episode index:2877
target Thresh 19.0
target distance 2.0
model initialize at round 2877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.23086214,  7.99593031,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.769148626092894}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7933585903157993
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.23086214,  7.99593031,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.769148626092894}
episode index:2878
target Thresh 19.0
target distance 12.0
model initialize at round 2878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.778321394833641}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7933255853508037
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.04317349, 11.57087854,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.5725087386969165}
episode index:2879
target Thresh 19.0
target distance 13.0
model initialize at round 2879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.132598050286157}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7932689616787112
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.93430849, 3.939349  , 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.08940870860363016}
episode index:2880
target Thresh 19.0
target distance 1.0
model initialize at round 2880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.13370672, 10.08921826,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.2569755006545449}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7933407183737203
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.13370672, 10.08921826,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.2569755006545449}
episode index:2881
target Thresh 19.0
target distance 12.0
model initialize at round 2881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.440306508910563}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.793284128745459
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.27690786, 2.90170555, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.155826607498649}
episode index:2882
target Thresh 19.0
target distance 4.0
model initialize at round 2882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.93724442,  4.99999344,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.00099087691568}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7933384873549818
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.1097008 ,  6.99230039,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.10997067461104802}
episode index:2883
target Thresh 19.0
target distance 1.0
model initialize at round 2883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.40794122,  3.46673131,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.796811829876859}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.793410145299727
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.40794122,  3.46673131,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.796811829876859}
episode index:2884
target Thresh 19.0
target distance 5.0
model initialize at round 2884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([1.13285832, 9.00348283, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.536541692441933}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7934479580743199
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.72590973, 5.04047538, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.997904291477596}
episode index:2885
target Thresh 19.0
target distance 5.0
model initialize at round 2885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.88494349,  7.99999976,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 3.127798970940417}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7934857446446337
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.54776299, 11.118083  ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.5603462216733768}
episode index:2886
target Thresh 19.0
target distance 8.0
model initialize at round 2886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.56799185,  8.00001526,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 6.01554770116422}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7935078746257059
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.13980225,  2.00744009,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.14000008317163898}
episode index:2887
target Thresh 19.0
target distance 14.0
model initialize at round 2887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4., 8., 0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.000000000000018}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7934749208242751
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.74342205,  8.33490344,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.42189164558250986}
episode index:2888
target Thresh 19.0
target distance 8.0
model initialize at round 2888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.70183122,  5.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 6.040907801281641}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7934970392317433
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.82125437, 10.98084555,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.17976900052188355}
episode index:2889
target Thresh 19.0
target distance 3.0
model initialize at round 2889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.43340838,  4.99984086,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.1494974460557663}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7935511925053655
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.67043218,  6.79348862,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8592084384221542}
episode index:2890
target Thresh 19.0
target distance 5.0
model initialize at round 2890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.1326928 , 5.00437144, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 3.5373837703358584}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.793588878014703
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.0212047 , 1.15932099, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8409463896306182}
episode index:2891
target Thresh 19.0
target distance 12.0
model initialize at round 2891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.182309676547058}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7935686508406402
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.15718423,  7.00149604,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0108001982935835}
episode index:2892
target Thresh 19.0
target distance 9.0
model initialize at round 2892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.13259865,  3.99506802,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.67347680829629}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7935758881718393
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.98940518, 11.86622587,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8662906639543665}
episode index:2893
target Thresh 19.0
target distance 7.0
model initialize at round 2893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.86769155, 4.97403191, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 5.100318014860799}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7935979334765485
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.86272192, 10.78778803,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.7996594617827156}
episode index:2894
target Thresh 19.0
target distance 3.0
model initialize at round 2894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.05471146,  4.99993932,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0015561405686104}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7936519583699935
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.15041923,  6.91674006,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.2498799209721456}
episode index:2895
target Thresh 19.0
target distance 2.0
model initialize at round 2895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.99670935,  9.08583021,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9141757108729277}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7937232111467994
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.99670935,  9.08583021,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9141757108729277}
episode index:2896
target Thresh 19.0
target distance 9.0
model initialize at round 2896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.280109889280542}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7937303851332865
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.14859808, 11.86004029,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.2101878077336115}
episode index:2897
target Thresh 19.0
target distance 5.0
model initialize at round 2897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.4297753 ,  8.00012052,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.053830275122642}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7937679177816187
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.6185208 ,  4.08453071,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9917713447255702}
episode index:2898
target Thresh 19.0
target distance 14.0
model initialize at round 2898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.00000000000001}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7937229548680305
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.84752872, 2.04574013, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9663640017635721}
episode index:2899
target Thresh 19.0
target distance 1.0
model initialize at round 2899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.54681166, 1.09010832, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.061558225498393}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7937940848835932
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.54681166, 1.09010832, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.061558225498393}
episode index:2900
target Thresh 19.0
target distance 1.0
model initialize at round 2900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.11880517,  9.76538396,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.5168325871572683}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7938479304248259
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.53375876, 11.44566524,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6953530899780772}
episode index:2901
target Thresh 19.0
target distance 3.0
model initialize at round 2901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.79014357, 7.99993258, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.2745437252732092}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7939017388567953
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.89550214, 9.83824456, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8447329438165523}
episode index:2902
target Thresh 19.0
target distance 4.0
model initialize at round 2902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.47113621,  6.99999523,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.068747441650738}
done in step count: 15
reward sum = 0.46329123015975304
running average episode reward sum: 0.7937878530460144
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.06466254,  9.697887  ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.1670057490512589}
episode index:2903
target Thresh 19.0
target distance 12.0
model initialize at round 2903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.181338705408916}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7937809636122863
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.72939813, 7.83001851, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.104967133060077}
episode index:2904
target Thresh 19.0
target distance 5.0
model initialize at round 2904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([1.5185945 , 7.00003469, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 3.8932738638793407}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.793818388409666
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.86461548, 3.07892832, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.2633024043833678}
episode index:2905
target Thresh 19.0
target distance 7.0
model initialize at round 2905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.510231603874827}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7938402592326496
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.08238093, 11.43105122,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0138194688660371}
episode index:2906
target Thresh 19.0
target distance 12.0
model initialize at round 2906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 3.9738031 , 11.86774979,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 11.145385244343139}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7938200499555228
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.47359661,  6.04970143,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.0863553277455378}
episode index:2907
target Thresh 19.0
target distance 1.0
model initialize at round 2907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.46124613,  2.10832274,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0417984767126696}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7938909509012052
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.46124613,  2.10832274,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0417984767126696}
episode index:2908
target Thresh 19.0
target distance 13.0
model initialize at round 2908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.0049423 , 11.86740302,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.971891890800423}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7938581033058777
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.24803265, 4.07414709, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9585008151723932}
episode index:2909
target Thresh 19.0
target distance 4.0
model initialize at round 2909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 4.99999976, 10.9999975 ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.000000238420167}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7939117603150508
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.99257779, 11.39607498,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.39614451827487995}
episode index:2910
target Thresh 19.0
target distance 13.0
model initialize at round 2910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.704699910719636}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7939048448829604
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.1083944 ,  7.03038035,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8921230373439869}
episode index:2911
target Thresh 19.0
target distance 13.0
model initialize at round 2911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4., 7., 0.]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.04536101718728}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7938720263565905
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.3932007 ,  8.70152089,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9275435077855403}
episode index:2912
target Thresh 19.0
target distance 12.0
model initialize at round 2912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.181338705408916}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7938651293126987
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.27000479,  7.83070273,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.1058752333839406}
episode index:2913
target Thresh 19.0
target distance 3.0
model initialize at round 2913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.23056466,  7.00037992,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.2620581341088848}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7939187102566545
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.16539445,  5.08644327,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.2373973966449272}
episode index:2914
target Thresh 19.0
target distance 14.0
model initialize at round 2914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4., 4., 0.]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.369316876853004}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7938625629837447
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.47538689,  7.74992645,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9152095861071645}
episode index:2915
target Thresh 19.0
target distance 2.0
model initialize at round 2915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.59090674,  9.98617649,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.40932674651242307}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7939332548345732
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.59090674,  9.98617649,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.40932674651242307}
episode index:2916
target Thresh 19.0
target distance 12.0
model initialize at round 2916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.049875621120908}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7938885126941737
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.0753507 , 4.89725982, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9004181880025065}
episode index:2917
target Thresh 19.0
target distance 12.0
model initialize at round 2917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 5.94478322, 10.045681  ,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 10.055320539963583}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7938816218185074
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.8991547 , 10.63187721,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6398738771993885}
episode index:2918
target Thresh 19.0
target distance 5.0
model initialize at round 2918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.14689075, 6.00552635, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.5308925919173557}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.793918832636658
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.09692016, 2.1191675 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8861486425062916}
episode index:2919
target Thresh 19.0
target distance 10.0
model initialize at round 2919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 5.94478322, 10.045681  ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 8.055346302818595}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7939119360972275
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.40818206, 10.60339487,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7284901975469642}
episode index:2920
target Thresh 19.0
target distance 9.0
model initialize at round 2920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([5.85387008, 9.82335934, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 8.32763626557908}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7939189865299228
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.25129049, 1.91286591, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.2659685331242829}
episode index:2921
target Thresh 19.0
target distance 1.0
model initialize at round 2921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.42417383, 9.81114674, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9947536395417583}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7939895139130405
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.42417383, 9.81114674, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9947536395417583}
episode index:2922
target Thresh 19.0
target distance 13.0
model initialize at round 2922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4., 6., 0.]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.704699910719636}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7939334960874542
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.41874765,  2.12237915,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.4362639694522699}
episode index:2923
target Thresh 19.0
target distance 3.0
model initialize at round 2923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 6.97739754, 11.86114209,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.336892478878371}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7939868704047978
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.85878549, 10.28339781,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.1184950724325664}
episode index:2924
target Thresh 19.0
target distance 11.0
model initialize at round 2924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 5.99505755, 11.86740305,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 9.196531015704496}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7939799623935482
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.62442286, 10.27867582,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6837866068439062}
episode index:2925
target Thresh 19.0
target distance 6.0
model initialize at round 2925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([12.14612912,  9.82336008,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 6.173906639965678}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7940170505813836
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.96656363,  5.89674985,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8973729888368562}
episode index:2926
target Thresh 19.0
target distance 3.0
model initialize at round 2926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.25554468,  8.00095545,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.2474476079843795}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7940703416471228
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.69186925,  6.03458918,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.013391635579173}
episode index:2927
target Thresh 19.0
target distance 13.0
model initialize at round 2927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.00000000000002}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7940257207760989
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.66606475, 4.89508933, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.35002713623216325}
episode index:2928
target Thresh 19.0
target distance 4.0
model initialize at round 2928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.66204119,  9.00033641,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.107046340566609}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7940789724931435
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.1889472 ,  7.00279999,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8110576323924477}
episode index:2929
target Thresh 19.0
target distance 13.0
model initialize at round 2929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.083045973594581}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7940720448361492
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.86567813, 6.96624399, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.2973149480276}
episode index:2930
target Thresh 19.0
target distance 9.0
model initialize at round 2930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.219544457292915}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7940651219063176
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.94358804, 11.48171303,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.4850049001009134}
episode index:2931
target Thresh 19.0
target distance 9.0
model initialize at round 2931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.00000083, 9.        , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 7.071067693854367}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7940720936416837
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.12427213, 1.20386751, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.1835228165297238}
episode index:2932
target Thresh 19.0
target distance 12.0
model initialize at round 2932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 11.179448465417412}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7940275482402679
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.95059256,  5.94134748,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9426431830827132}
episode index:2933
target Thresh 19.0
target distance 8.0
model initialize at round 2933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 8.02185905, 10.13354716,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 6.788365357944604}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7940491390554554
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.84560591, 6.01084864, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.0011283421974868}
episode index:2934
target Thresh 19.0
target distance 5.0
model initialize at round 2934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.14047554, 6.0291168 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.5543466092222475}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7940860899450447
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.21527429, 2.13248657, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.893824681914267}
episode index:2935
target Thresh 19.0
target distance 2.0
model initialize at round 2935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.92374849,  5.84110248,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8445517601938969}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7941562241105947
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.92374849,  5.84110248,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8445517601938969}
episode index:2936
target Thresh 19.0
target distance 7.0
model initialize at round 2936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.13225076,  5.97380525,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.506775695938079}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7941777490598251
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.96933676, 11.86670706,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.3003056893156908}
episode index:2937
target Thresh 19.0
target distance 12.0
model initialize at round 2937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 11.180339887498974}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7941576381481726
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.85512068, 9.15353537, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.2032180836755746}
episode index:2938
target Thresh 19.0
target distance 12.0
model initialize at round 2938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.00016787,  9.00007785,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.198218904566868}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7941507049393777
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.93565188, 7.01034867, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.06517495907521974}
episode index:2939
target Thresh 19.0
target distance 5.0
model initialize at round 2939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([14.93975055,  8.00007951,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 3.1819185994471275}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.794187558441099
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.05349362,  4.04163587,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9598559116613591}
episode index:2940
target Thresh 19.0
target distance 12.0
model initialize at round 2940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.049875621120913}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.794180619773659
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.10004172, 11.8669269 ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8726801251486752}
episode index:2941
target Thresh 19.0
target distance 12.0
model initialize at round 2941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.04987562112091}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7941736858231919
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.93687191, 10.19513332,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8073385411210995}
episode index:2942
target Thresh 19.0
target distance 13.0
model initialize at round 2942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.99983213, 9.00007785, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.045535245749349}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7941667565848898
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.07576913,  7.21697509,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.2113342731076386}
episode index:2943
target Thresh 19.0
target distance 7.0
model initialize at round 2943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4.4770087, 5.       , 0.       ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 5.579925815072541}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7941882267762672
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.07438145, 10.78177477,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.7853052811111868}
episode index:2944
target Thresh 19.0
target distance 3.0
model initialize at round 2944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.10988307,  3.99985015,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0061679798516137}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7942411339997727
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.51542562,  5.72323513,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8705638294115557}
episode index:2945
target Thresh 19.0
target distance 11.0
model initialize at round 2945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.186646103472004}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.794221056184642
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.1088265 , 6.88013224, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8868348044579442}
episode index:2946
target Thresh 19.0
target distance 1.0
model initialize at round 2946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.55885005, 6.35183185, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8558243532979203}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7942908827689025
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.55885005, 6.35183185, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8558243532979203}
episode index:2947
target Thresh 19.0
target distance 6.0
model initialize at round 2947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.00000346, 8.00000024, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 4.000000238420112}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7943275887109754
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.02080756, 4.02388264, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.031675463570690655}
episode index:2948
target Thresh 19.0
target distance 2.0
model initialize at round 2948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.44650334,  4.99666572,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5535067016339547}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7943973318141593
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.44650334,  4.99666572,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5535067016339547}
episode index:2949
target Thresh 19.0
target distance 2.0
model initialize at round 2949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.97790456, 10.91799289,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9182587622350323}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7944670276338832
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.97790456, 10.91799289,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9182587622350323}
episode index:2950
target Thresh 19.0
target distance 13.0
model initialize at round 2950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 13.15309597366658}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7944226201122483
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.94884308, 11.63429376,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6363533643859496}
episode index:2951
target Thresh 19.0
target distance 10.0
model initialize at round 2951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 7.97422644, 11.8676414 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 9.386531527816247}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7944294235099066
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.07129548,  6.00386135,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9986867656195729}
episode index:2952
target Thresh 19.0
target distance 2.0
model initialize at round 2952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.04046047,  6.93805385,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9389260184309868}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7944990376570418
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.04046047,  6.93805385,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9389260184309868}
episode index:2953
target Thresh 19.0
target distance 8.0
model initialize at round 2953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4., 6., 0.]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.810249675906681}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7945058105792974
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.06882106, 10.32654495,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6769623616428292}
episode index:2954
target Thresh 19.0
target distance 13.0
model initialize at round 2954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 3.99505761, 11.86740304,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 12.03330251202226}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7944857043458102
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.6509245 ,  6.03549317,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.1636047942738859}
episode index:2955
target Thresh 19.0
target distance 4.0
model initialize at round 2955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([1.30376468, 9.00023941, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.622626921030131}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7945383140534064
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.12887772, 7.01101924, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.12934794306384292}
episode index:2956
target Thresh 19.0
target distance 13.0
model initialize at round 2956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7945312943115891
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.869122  , 7.24925279, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.1484748298632734}
episode index:2957
target Thresh 19.0
target distance 10.0
model initialize at round 2957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([11.02619882, 11.86775027,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 8.909509505240658}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.794538047170172
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.00716439, 7.05780007, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9422271731271398}
episode index:2958
target Thresh 19.0
target distance 11.0
model initialize at round 2958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.96818994525352}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7945055359329037
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.06516245, 3.8396686 , 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.17306733801090027}
episode index:2959
target Thresh 19.0
target distance 4.0
model initialize at round 2959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 6.97398339, 11.86770357,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 2.204008340412354}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7945580678464399
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.96904364, 10.61993884,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.3813197917754436}
episode index:2960
target Thresh 19.0
target distance 2.0
model initialize at round 2960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.99886807, 11.74627742,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.7462782766738272}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7946274504645262
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.99886807, 11.74627742,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.7462782766738272}
episode index:2961
target Thresh 19.0
target distance 7.0
model initialize at round 2961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.80039941,  9.00003852,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.0636967145760385}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7946486346473538
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.32983135,  3.04581264,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0095851829633555}
episode index:2962
target Thresh 19.0
target distance 5.0
model initialize at round 2962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.04567666,  9.05522488,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.626813862144236}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7946850340281681
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.85024873,  5.10900046,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9034963313446931}
episode index:2963
target Thresh 19.0
target distance 2.0
model initialize at round 2963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.49602938,  5.99156547,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.5040411989396153}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7947543035848388
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.49602938,  5.99156547,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.5040411989396153}
episode index:2964
target Thresh 19.0
target distance 14.0
model initialize at round 2964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.        , 8.99999988, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.9999999541503}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7947217852011993
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.26982102,  3.97914347,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.27062590003988135}
episode index:2965
target Thresh 19.0
target distance 2.0
model initialize at round 2965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.48424017,  2.01212192,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.48439187363431974}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7947909956579756
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.48424017,  2.01212192,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.48439187363431974}
episode index:2966
target Thresh 19.0
target distance 2.0
model initialize at round 2966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.96502197, 8.31568718, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6852061725799015}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7948601594612591
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.96502197, 8.31568718, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6852061725799015}
episode index:2967
target Thresh 19.0
target distance 10.0
model initialize at round 2967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86775042, 7.97379887, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.959370451247889}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7948276382808792
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.81384715,  4.12333298,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.2233022813120262}
episode index:2968
target Thresh 19.0
target distance 2.0
model initialize at round 2968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.10254705, 4.00517303, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.10267744470763072}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7948967431517849
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.10254705, 4.00517303, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.10267744470763072}
episode index:2969
target Thresh 19.0
target distance 5.0
model initialize at round 2969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.89454007,  5.00013745,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 3.0019904239153195}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7949329732045958
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.13273792,  1.25936158,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7524391132443332}
episode index:2970
target Thresh 19.0
target distance 2.0
model initialize at round 2970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.12904416,  9.99089789,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.12936476965997518}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7950019961015313
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.12904416,  9.99089789,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.12936476965997518}
episode index:2971
target Thresh 19.0
target distance 9.0
model initialize at round 2971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([12.00494245, 11.86740305,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.058442237262242}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7950085587710798
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.48455569, 11.43712465,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.675840806906251}
episode index:2972
target Thresh 19.0
target distance 1.0
model initialize at round 2972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.55494284,  9.10451674,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0534950501947997}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7950775098108473
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.55494284,  9.10451674,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0534950501947997}
episode index:2973
target Thresh 19.0
target distance 5.0
model initialize at round 2973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.162277660168403}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7951136303522695
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.89068791, 10.37931285,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0856230911951015}
episode index:2974
target Thresh 19.0
target distance 7.0
model initialize at round 2974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.904457013955612}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7951345585437477
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.05279903, 11.86625414,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8678617237210478}
episode index:2975
target Thresh 19.0
target distance 12.0
model initialize at round 2975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.02619874, 11.86775025,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 11.145387100880779}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7951143829160868
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.20323837, 6.10451188, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9182618434833006}
episode index:2976
target Thresh 19.0
target distance 8.0
model initialize at round 2976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.13259958, 9.00492662, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.2885871277778955}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7951352967948521
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0167119 , 3.03407738, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.037954647482410876}
episode index:2977
target Thresh 19.0
target distance 2.0
model initialize at round 2977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.02294641, 11.86716619,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8674697286637272}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7952040895091587
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.02294641, 11.86716619,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8674697286637272}
episode index:2978
target Thresh 19.0
target distance 12.0
model initialize at round 2978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([5.17664051, 9.14612965, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.984480007996332}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7951839108589793
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.91745729,  4.85934169,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.863296846940915}
episode index:2979
target Thresh 19.0
target distance 5.0
model initialize at round 2979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([12.00493643, 11.86740169,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 3.127623483827275}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7952199229694294
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.03890391, 11.34928401,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0225971858606673}
episode index:2980
target Thresh 19.0
target distance 5.0
model initialize at round 2980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.43161726, 3.99999857, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 3.030891295012564}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7952559109187856
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.31194359, 7.86851966, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9228408368553341}
episode index:2981
target Thresh 19.0
target distance 6.0
model initialize at round 2981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 7.02308445, 10.13312816,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.553618808876721}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7952918747313548
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.00421007, 7.13488317, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8651270747728128}
episode index:2982
target Thresh 19.0
target distance 9.0
model initialize at round 2982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.058442237262245}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7952983160237679
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.92574555, 10.28223742,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.1714042671070235}
episode index:2983
target Thresh 19.0
target distance 11.0
model initialize at round 2983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 5.94477972, 10.04567537,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.366091565576673}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7952781396077495
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.24782895,  4.08910616,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9440057043276106}
episode index:2984
target Thresh 19.0
target distance 5.0
model initialize at round 2984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.00003886, 9.00000453, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.0000045302047695}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7953140598289865
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.60735822, 5.13688052, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0553953031788823}
episode index:2985
target Thresh 19.0
target distance 12.0
model initialize at round 2985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.440306508910576}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7953068484685281
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.83860677, 9.22416055, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.142448323173573}
episode index:2986
target Thresh 19.0
target distance 2.0
model initialize at round 2986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.5608567 ,  4.99512565,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.560877880778204}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7953753764737277
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.5608567 ,  4.99512565,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.560877880778204}
episode index:2987
target Thresh 19.0
target distance 9.0
model initialize at round 2987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 4.97380117, 11.86775027,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.079580537345779}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7953817790418422
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.91760727, 10.33535685,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.133028517756814}
episode index:2988
target Thresh 19.0
target distance 12.0
model initialize at round 2988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 4.        , 10.99536681,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.803354656279375}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7953265323475239
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.93791197,  2.09973107,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9024073719684447}
episode index:2989
target Thresh 19.0
target distance 3.0
model initialize at round 2989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.46196163,  4.00199628,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.103360814729784}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7953782626042638
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.19136113,  2.03650999,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.982309564158304}
episode index:2990
target Thresh 19.0
target distance 12.0
model initialize at round 2990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.315143201383691}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7953581066791621
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.23408524,  9.58249932,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9622530167713255}
episode index:2991
target Thresh 19.0
target distance 2.0
model initialize at round 2991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.97724885,  5.99527538,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.02323654431445242}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7954265030338816
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.97724885,  5.99527538,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.02323654431445242}
episode index:2992
target Thresh 19.0
target distance 4.0
model initialize at round 2992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.81976801,  5.00041366,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 2.0085164589079203}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7954781480378795
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.25371084,  3.0075829 ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.2538241296797134}
episode index:2993
target Thresh 19.0
target distance 10.0
model initialize at round 2993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 10.109325747416417}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7954579789472273
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.01587713, 10.95559653,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.04715666826581455}
episode index:2994
target Thresh 19.0
target distance 3.0
model initialize at round 2994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.96455133,  4.99992728,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0007007788811535}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7955095789542566
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.57708304,  6.71581995,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9194688929290011}
episode index:2995
target Thresh 19.0
target distance 12.0
model initialize at round 2995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.049875621120906}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7955023264037043
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.01639551, 10.94838338,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9485250897528715}
episode index:2996
target Thresh 19.0
target distance 5.0
model initialize at round 2996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([1.13337807, 9.00137632, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.5344783549128422}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.795538027996496
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.91699979, 5.09036805, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.913410816544482}
episode index:2997
target Thresh 19.0
target distance 11.0
model initialize at round 2997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.05521678, 10.045681  ,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.366091240270828}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7955178658426029
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.33537098, 4.12878744, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9335336165069147}
episode index:2998
target Thresh 19.0
target distance 14.0
model initialize at round 2998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.649110640673541}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7954738186820315
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.854694  , 8.73300204, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7472655685485771}
episode index:2999
target Thresh 19.0
target distance 2.0
model initialize at round 2999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.02994267, 11.86725787,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8677746090459917}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7955419940758042
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.02994267, 11.86725787,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8677746090459917}
episode index:3000
target Thresh 19.0
target distance 11.0
model initialize at round 3000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([12.05521678, 10.045681  ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 9.105365210222166}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7955347428073682
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.28014961, 10.55174774,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8480062939312705}
episode index:3001
target Thresh 19.0
target distance 8.0
model initialize at round 3001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.28138085,  4.99999987,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.0065944333494325}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7955553424933085
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.11098188, 11.1152147 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.1599731440179228}
episode index:3002
target Thresh 19.0
target distance 8.0
model initialize at round 3002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.0827625302982415}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7955759284598443
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.02619845, 10.66324936,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.33776818915777}
episode index:3003
target Thresh 19.0
target distance 1.0
model initialize at round 3003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.57007789, 5.81797405, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.5984331634255232}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7956439790828603
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.57007789, 5.81797405, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.5984331634255232}
episode index:3004
target Thresh 19.0
target distance 8.0
model initialize at round 3004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 9.01852315, 10.13540393,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 7.911699844174056}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7956502560448959
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.43358078, 4.95709472, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5680418984076462}
episode index:3005
target Thresh 19.0
target distance 9.0
model initialize at round 3005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.86775026, 3.97380122, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.3426124931273895}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7956565288306426
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.91245304, 11.79820507,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.2123126137982931}
episode index:3006
target Thresh 19.0
target distance 10.0
model initialize at round 3006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([12.05522552, 10.04566698,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.701779351320544}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7956241646029283
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.82281015, 2.05749156, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.2511349658269024}
episode index:3007
target Thresh 19.0
target distance 13.0
model initialize at round 3007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4., 7., 0.]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7955918218939824
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.89530704,  4.86439277,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.17131823324576156}
episode index:3008
target Thresh 19.0
target distance 5.0
model initialize at round 3008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.82879603,  7.99999905,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 3.1123798900332638}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7956273513649382
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.51515138, 11.34497929,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6199932691219224}
episode index:3009
target Thresh 19.0
target distance 3.0
model initialize at round 3009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.10590363, 8.99994838, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.005643483447605}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.795678637959169
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.94347768, 10.88987207,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8916653416494187}
episode index:3010
target Thresh 19.0
target distance 2.0
model initialize at round 3010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.01264751,  6.21510112,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7850007686876471}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7957464962660574
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.01264751,  6.21510112,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7850007686876471}
episode index:3011
target Thresh 19.0
target distance 7.0
model initialize at round 3011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.01167595,  6.        ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.101322204647438}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.795766957256673
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.51453352, 11.40176797,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.6301549051393786}
episode index:3012
target Thresh 19.0
target distance 5.0
model initialize at round 3012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.18178123,  4.99999905,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.0055033086796095}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7958023814328241
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.06702317,  8.71818614,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7213067568589808}
episode index:3013
target Thresh 19.0
target distance 12.0
model initialize at round 3013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4., 5., 0.]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 11.661903789690625}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.79578223860243
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.04048289, 11.57712013,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.1197056415756201}
episode index:3014
target Thresh 19.0
target distance 11.0
model initialize at round 3014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 10.902878591090003}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7957621091337808
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.01939658, 11.57275948,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.5730878231934875}
episode index:3015
target Thresh 19.0
target distance 13.0
model initialize at round 3015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 5.        , 10.99625158,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 14.210297060456497}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7957182292671214
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.19667761,  1.47563254,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.5600386733153717}
episode index:3016
target Thresh 19.0
target distance 1.0
model initialize at round 3016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.50170302,  4.05699563,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.1685630658110588}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.795769366744991
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.38711673,  2.13996089,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.943147200631199}
episode index:3017
target Thresh 19.0
target distance 9.0
model initialize at round 3017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.13259732,  3.9950598 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.673483770886763}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7957755751224778
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.98688271, 11.86595836,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8660577013936427}
episode index:3018
target Thresh 19.0
target distance 2.0
model initialize at round 3018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.00219297, 9.16559911, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8344037740421782}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7958432215036892
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.00219297, 9.16559911, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8344037740421782}
episode index:3019
target Thresh 19.0
target distance 11.0
model initialize at round 3019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.559639045500587}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7957993728976579
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.06728432, 5.77425105, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.777169140996714}
episode index:3020
target Thresh 19.0
target distance 11.0
model initialize at round 3020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.788874285776417}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7957671113694208
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.97879452, 10.02302557,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9772045394833485}
episode index:3021
target Thresh 19.0
target distance 8.0
model initialize at round 3021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.13224978,  4.97380138,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.915494781459838}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.795773312275652
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.98228219, 10.56619082,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0738103640259389}
episode index:3022
target Thresh 19.0
target distance 7.0
model initialize at round 3022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 7.00094926, 10.15135918,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 5.131775666461808}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7958086171673902
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.81215627, 9.7553739 , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.1091381979856965}
episode index:3023
target Thresh 19.0
target distance 3.0
model initialize at round 3023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.0352298 ,  8.99992204,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0006982915880496}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7958596063812897
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.97276528, 10.4715476 ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.4723334336527366}
episode index:3024
target Thresh 19.0
target distance 12.0
model initialize at round 3024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 3.97478562, 11.86749374,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 12.151847338806862}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7958273676010293
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.42297859,  4.11279231,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.982877595051263}
episode index:3025
target Thresh 19.0
target distance 11.0
model initialize at round 3025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.929943319783787}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7958072963925111
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.01097966, 11.60975323,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.161877895722304}
episode index:3026
target Thresh 19.0
target distance 9.0
model initialize at round 3026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([10.02619852, 11.8677502 ,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 8.02040879529601}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.79582763590477
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.85721469, 7.96099335, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8581017058591224}
episode index:3027
target Thresh 19.0
target distance 7.0
model initialize at round 3027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.        ,  9.00000226,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.385166910112419}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7958479619827407
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.51400835,  3.03535259,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0930458444852948}
episode index:3028
target Thresh 19.0
target distance 5.0
model initialize at round 3028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.86740318, 7.99505786, 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.2113007979429145}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7958988540388704
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.10357796, 10.20426608,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.802446796336925}
episode index:3029
target Thresh 19.0
target distance 4.0
model initialize at round 3029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.99468338, 7.99999976, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.2336955902408304}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7959497125028839
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.50814068, 9.99249756, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.4919165316432425}
episode index:3030
target Thresh 19.0
target distance 2.0
model initialize at round 3030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.92522955,  2.00698507,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.07509601494876791}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7960170336139024
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.92522955,  2.00698507,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.07509601494876791}
episode index:3031
target Thresh 19.0
target distance 11.0
model initialize at round 3031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.186646103472004}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.7959519676197152
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.60336634,  2.33646028,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.773048009022772}
episode index:3032
target Thresh 19.0
target distance 3.0
model initialize at round 3032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.00031233, 10.99935365,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.000312537159274}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7960027582667248
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.01441944, 10.89771719,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9908737666410975}
episode index:3033
target Thresh 19.0
target distance 12.0
model initialize at round 3033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.049875621120908}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7959826821732371
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.02516498, 6.05449983, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9458349994907057}
episode index:3034
target Thresh 19.0
target distance 9.0
model initialize at round 3034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.6157731058639335}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7959887854904781
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.16019604, 11.5393055 ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9980586715928623}
episode index:3035
target Thresh 19.0
target distance 2.0
model initialize at round 3035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.82599974, 5.9862479 , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8261142085106509}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7960559828602112
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.82599974, 5.9862479 , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8261142085106509}
episode index:3036
target Thresh 19.0
target distance 7.0
model initialize at round 3036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.86561979,  5.98006662,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.094018924314312}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7960761735145213
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.37034562, 11.57280973,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6821047304937967}
episode index:3037
target Thresh 19.0
target distance 12.0
model initialize at round 3037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4., 7., 0.]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.04987562112091}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7960688347271564
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.24075146,  8.8611755 ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1480773472407697}
episode index:3038
target Thresh 19.0
target distance 7.0
model initialize at round 3038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.13357891, 8.02178193, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.357408084685008}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7960890078647914
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.24803299, 2.0922667 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9410101558819979}
episode index:3039
target Thresh 19.0
target distance 11.0
model initialize at round 3039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.902878591090003}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7960568526964457
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.14428419,  7.95839005,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8567268692150881}
episode index:3040
target Thresh 19.0
target distance 6.0
model initialize at round 3040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.86169673, 3.97931174, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.430784218014961}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7960918553755985
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.8634594 , 7.97817881, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.13827328399334324}
episode index:3041
target Thresh 19.0
target distance 5.0
model initialize at round 3041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.88468301,  8.00008452,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.1278061252074467}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7961268350418129
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.09268415,  4.13554811,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8694063569353483}
episode index:3042
target Thresh 19.0
target distance 1.0
model initialize at round 3042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.05369013, 4.14095449, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8607216799919197}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7961938324670375
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.05369013, 4.14095449, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8607216799919197}
episode index:3043
target Thresh 19.0
target distance 3.0
model initialize at round 3043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.97952342, 3.99983275, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0003768387733125}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7962443601173439
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.43732214, 5.76762855, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8834615133070157}
episode index:3044
target Thresh 19.0
target distance 7.0
model initialize at round 3044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4., 6., 0.]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.403124237432875}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7962644358611477
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.07608621, 11.86655232,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8698862165378198}
episode index:3045
target Thresh 19.0
target distance 13.0
model initialize at round 3045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.18033988749897}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7962570545419221
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.98954514, 9.74900133, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.020881928586014}
episode index:3046
target Thresh 19.0
target distance 7.0
model initialize at round 3046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.17251354,  6.        ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.002975207119927}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7962771129421381
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.73148422, 11.63698535,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.6912677158059909}
episode index:3047
target Thresh 19.0
target distance 4.0
model initialize at round 3047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13264501, 8.00467914, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.184271762934443}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7963275469602016
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.85402414, 6.00588066, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.1460942606743686}
episode index:3048
target Thresh 19.0
target distance 4.0
model initialize at round 3048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.86737325, 5.99523961, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.184353581574119}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7963779478959313
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.29195921, 7.99508646, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.29200055029731004}
episode index:3049
target Thresh 19.0
target distance 8.0
model initialize at round 3049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([10.02619863, 11.86775022,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 6.309006328632826}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.796397946929408
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.96891463, 9.33224248, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6684806676525865}
episode index:3050
target Thresh 19.0
target distance 12.0
model initialize at round 3050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.00000000000002}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7963434898539558
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.96594293, 3.91557787, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.09103285440837534}
episode index:3051
target Thresh 19.0
target distance 5.0
model initialize at round 3051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.00167298, 9.0000205 , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.1617684698505193}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.796378272458853
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.39691103, 5.0836761 , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9985929360472691}
episode index:3052
target Thresh 19.0
target distance 13.0
model initialize at round 3052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.971892036384785}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7963581983082358
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13391108, 4.99353663, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8661130351321827}
episode index:3053
target Thresh 19.0
target distance 2.0
model initialize at round 3053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.02461946,  5.85504079,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.1470349984560363}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7964248786624244
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.02461946,  5.85504079,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.1470349984560363}
episode index:3054
target Thresh 19.0
target distance 1.0
model initialize at round 3054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.58059239,  8.05053735,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.1129091857651408}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7964915153633532
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.58059239,  8.05053735,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.1129091857651408}
episode index:3055
target Thresh 19.0
target distance 3.0
model initialize at round 3055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.23259945, 7.99990773, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0267848117441796}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7965417471973311
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.1203948 , 9.88549018, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8936373784432268}
episode index:3056
target Thresh 19.0
target distance 4.0
model initialize at round 3056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.44105488, 3.99998832, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.0480664388383274}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7965919461678259
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.06868955, 5.9937191 , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.06897611651359814}
episode index:3057
target Thresh 19.0
target distance 1.0
model initialize at round 3057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.23771593, 11.7511476 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0701867700646692}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7966584628629966
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.23771593, 11.7511476 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0701867700646692}
episode index:3058
target Thresh 19.0
target distance 6.0
model initialize at round 3058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.86739967, 6.99507835, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 4.097777395237226}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7966930629078274
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.7703894 , 11.07556342,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.2417247611720341}
episode index:3059
target Thresh 19.0
target distance 5.0
model initialize at round 3059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.00493521, 11.86740153,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.1276222617561347}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7967276403382496
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.03275025, 11.22220274,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.992444531134753}
episode index:3060
target Thresh 19.0
target distance 7.0
model initialize at round 3060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.0251776,  6.       ,  0.       ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 5.104016957989094}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7967621951764273
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.2448863 , 10.07938396,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.1906849275949967}
episode index:3061
target Thresh 19.0
target distance 13.0
model initialize at round 3061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.00000000000002}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7967078147762144
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.14701704, 3.91381889, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.17041477367051217}
episode index:3062
target Thresh 19.0
target distance 3.0
model initialize at round 3062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.70130432, 3.00204718, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0456183168530728}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7967578611964637
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.1335831 , 1.23719681, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7744114848801709}
episode index:3063
target Thresh 19.0
target distance 12.0
model initialize at round 3063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.32713782374229}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7967035177070799
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.94148155, 2.92790279, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.0928569681252188}
episode index:3064
target Thresh 19.0
target distance 5.0
model initialize at round 3064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.13681135, 6.01840129, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.5471422750204544}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7967380353195735
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.31948543, 2.07799453, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9757894390992}
episode index:3065
target Thresh 19.0
target distance 4.0
model initialize at round 3065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.95432538, 9.05522869, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.8360805096165076}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.796788022914055
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.01761363, 7.06499259, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.06733704291127705}
episode index:3066
target Thresh 19.0
target distance 11.0
model initialize at round 3066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([12.82335934,  9.14612992,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 9.99676062351314}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7967805214189738
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.97643325, 11.61561027,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.6160611974602959}
episode index:3067
target Thresh 19.0
target distance 12.0
model initialize at round 3067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.568784232893615}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7967484343181507
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.08532204,  7.07119143,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9327192546531167}
episode index:3068
target Thresh 19.0
target distance 11.0
model initialize at round 3068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 10.332858569546817}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7967283442094204
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.12951026,  8.51102524,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5058351737190055}
episode index:3069
target Thresh 19.0
target distance 2.0
model initialize at round 3069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.20954135,  9.99325705,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.20964981527450843}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7967945564751502
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.20954135,  9.99325705,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.20964981527450843}
episode index:3070
target Thresh 19.0
target distance 10.0
model initialize at round 3070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.424042392439802}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.796774464431565
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.02409102, 10.09518497,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9051356918456358}
episode index:3071
target Thresh 19.0
target distance 3.0
model initialize at round 3071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.29955322, 8.00116777, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.2218684912906301}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7968243425355911
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.63880032, 6.06324589, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.1338316059048295}
episode index:3072
target Thresh 19.0
target distance 7.0
model initialize at round 3072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([10.97381034, 10.13225191,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 6.506772495622863}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7968587309695203
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.16463776,  6.92305051,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.244930646096098}
episode index:3073
target Thresh 19.0
target distance 12.0
model initialize at round 3073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.02619884, 11.86775028,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.746355444500661}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7968512235545986
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.03746631, 7.19577329, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8050989571244841}
episode index:3074
target Thresh 19.0
target distance 12.0
model initialize at round 3074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.00016787,  9.00007785,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.180524850122456}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7968191865050177
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.97540978, 3.05420806, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9461115556408012}
episode index:3075
target Thresh 19.0
target distance 3.0
model initialize at round 3075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.97923434, 9.99999666, 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.399609432688641}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7968689852090147
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.39064306, 11.77178645,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9833464337650399}
episode index:3076
target Thresh 19.0
target distance 1.0
model initialize at round 3076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.18008327, 1.13620231, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8823697777551518}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7969350011384235
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.18008327, 1.13620231, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8823697777551518}
episode index:3077
target Thresh 19.0
target distance 3.0
model initialize at round 3077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.00010276, 9.00021386, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0002138667438976}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7969847298580016
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.0621627 , 7.08913016, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9129885325363667}
episode index:3078
target Thresh 19.0
target distance 14.0
model initialize at round 3078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4., 9., 0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.041594578792314}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7969646282538337
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.93760467, 10.71813242,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.7208379492718463}
episode index:3079
target Thresh 19.0
target distance 13.0
model initialize at round 3079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.704699910719649}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7969326063927428
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.90708062, 8.45848495, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.5494293049629912}
episode index:3080
target Thresh 19.0
target distance 2.0
model initialize at round 3080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.47105539, 4.01156664, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.5290710625460924}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7969985159654812
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.47105539, 4.01156664, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.5290710625460924}
episode index:3081
target Thresh 19.0
target distance 10.0
model initialize at round 3081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([10.02583765, 11.86765799,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.960327260216127}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.796990982682397
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.89049128, 3.96666376, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9728468425114862}
episode index:3082
target Thresh 19.0
target distance 9.0
model initialize at round 3082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 9.97380125, 10.13224975,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 9.560133882037801}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7969834542862949
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.95508676,  1.94633313,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.06998093846708699}
episode index:3083
target Thresh 19.0
target distance 4.0
model initialize at round 3083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.74445689, 4.00064182, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 2.0168961266442973}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7970330705462539
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.91736069, 2.0047971 , 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.08277842233267367}
episode index:3084
target Thresh 19.0
target distance 11.0
model initialize at round 3084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 5.97557916, 11.86727423,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.340177550921748}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7970129923679975
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.37809072,  4.06667962,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0069952992871865}
episode index:3085
target Thresh 19.0
target distance 7.0
model initialize at round 3085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.34913635,  5.        ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 5.012174796698978}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7970325523186235
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.56474814, 10.47008467,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7347925244109732}
episode index:3086
target Thresh 19.0
target distance 9.0
model initialize at round 3086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 7.97380328, 11.86774974,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 7.270208348008917}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7970382127325144
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.4205521 , 10.46609708,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6277822535162894}
episode index:3087
target Thresh 19.0
target distance 2.0
model initialize at round 3087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.93797636,  8.08600754,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9160945132022639}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7971039386998938
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.93797636,  8.08600754,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9160945132022639}
episode index:3088
target Thresh 19.0
target distance 1.0
model initialize at round 3088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.52076232, 2.12813866, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.994892428130203}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7971696221124221
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.52076232, 2.12813866, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.994892428130203}
episode index:3089
target Thresh 19.0
target distance 11.0
model initialize at round 3089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.132283595738201}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7971495322316818
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.07304628,  6.90539106,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.11952660688541458}
episode index:3090
target Thresh 19.0
target distance 10.0
model initialize at round 3090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.06225774829857}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7971551474752173
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.93817211, 11.86744761,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8696482340994658}
episode index:3091
target Thresh 19.0
target distance 14.0
model initialize at round 3091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.041594578792314}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7971011676117792
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.46575634, 3.90729685, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0529025899078426}
episode index:3092
target Thresh 19.0
target distance 4.0
model initialize at round 3092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.87246275, 9.00312829, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.0071842708567877}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7971506014405499
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.17153516, 7.00581908, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.1716338377106214}
episode index:3093
target Thresh 19.0
target distance 7.0
model initialize at round 3093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 5.94478322, 10.045681  ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.144505943758207}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7971700663398904
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.59547728, 11.90395934,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0824674077388423}
episode index:3094
target Thresh 19.0
target distance 4.0
model initialize at round 3094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.58692062, 3.99999774, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.08434279160873}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.797219445963044
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.14251301, 5.99547315, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.14258488540560352}
episode index:3095
target Thresh 19.0
target distance 12.0
model initialize at round 3095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.305877894274122}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7971762292270381
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.42276474,  3.93558926,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.42764327121353196}
episode index:3096
target Thresh 19.0
target distance 3.0
model initialize at round 3096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.2523084 ,  6.99987817,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0314568369917965}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7972255749715562
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.04921997,  8.81501222,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8164971023841222}
episode index:3097
target Thresh 19.0
target distance 1.0
model initialize at round 3097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.30398155,  6.90432357,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1411585342103119}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7972910283043608
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.30398155,  6.90432357,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1411585342103119}
episode index:3098
target Thresh 19.0
target distance 11.0
model initialize at round 3098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.02619884, 11.86775028,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.06781429073956}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7972834419568924
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.45359966, 11.59826623,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8102319484531557}
episode index:3099
target Thresh 19.0
target distance 12.0
model initialize at round 3099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.        , 10.99999964,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.661903605692618}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7972633801661402
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.76709631, 4.99618903, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.23293486899595153}
episode index:3100
target Thresh 19.0
target distance 5.0
model initialize at round 3100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.78289176,  4.99998474,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.100485619080756}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7972973165156513
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.52077847,  8.86535784,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.009977425088927}
episode index:3101
target Thresh 19.0
target distance 2.0
model initialize at round 3101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.94774213, 3.9984951 , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.052279537306822}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7973626623194825
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.94774213, 3.9984951 , 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.052279537306822}
episode index:3102
target Thresh 19.0
target distance 13.0
model initialize at round 3102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.00492869, 11.86740081,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.132913803639417}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7973194969211485
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.40008659, 2.0573913 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0240021681599962}
episode index:3103
target Thresh 19.0
target distance 13.0
model initialize at round 3103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7973119136223659
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.13490864,  7.35972288,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.076261048749068}
episode index:3104
target Thresh 19.0
target distance 6.0
model initialize at round 3104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([4.        , 9.00000012, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 4.472136061623569}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7973457906228096
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.64968556, 5.01350307, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.3505745837000826}
episode index:3105
target Thresh 19.0
target distance 13.0
model initialize at round 3105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 5.       , 10.9993943,  0.       ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 14.21228685763381}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7973026723487163
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.21918423,  1.38788343,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.6501756853257259}
episode index:3106
target Thresh 19.0
target distance 4.0
model initialize at round 3106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.5852716 ,  4.00037873,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 2.0429181832411447}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7973518185758328
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.27054805,  2.00876212,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.2706898959416889}
episode index:3107
target Thresh 19.0
target distance 10.0
model initialize at round 3107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.184022742676934}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7973087261088808
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.51510782, 4.93904114, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0568437423687922}
episode index:3108
target Thresh 19.0
target distance 7.0
model initialize at round 3108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([1.13284775, 8.00351794, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 5.766780195263668}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7973280462355747
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.54558706, 2.04993366, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0955780643202482}
episode index:3109
target Thresh 19.0
target distance 7.0
model initialize at round 3109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.74324811,  8.00003707,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 5.155559723999643}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7973473539377498
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.39267622,  2.07516456,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0047463415141193}
episode index:3110
target Thresh 19.0
target distance 8.0
model initialize at round 3110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.00191772,  5.        ,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 6.325162018242205}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7973666492273873
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.14283293, 11.14032237,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8685768566325952}
episode index:3111
target Thresh 19.0
target distance 1.0
model initialize at round 3111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.19796714,  6.09125284,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.2120553263841776}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.79743176277198
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.19796714,  6.09125284,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.2120553263841776}
episode index:3112
target Thresh 19.0
target distance 3.0
model initialize at round 3112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.13068867,  9.00146151,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0099528085703757}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7974807728064253
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.57415861,  7.04993331,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0411376458447037}
episode index:3113
target Thresh 19.0
target distance 2.0
model initialize at round 3113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.14474833, 2.24633646, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7674378218628901}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7975458078825953
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.14474833, 2.24633646, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7674378218628901}
episode index:3114
target Thresh 19.0
target distance 2.0
model initialize at round 3114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.28493106,  9.00924742,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.2850810864501731}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7976108012026972
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.28493106,  9.00924742,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.2850810864501731}
episode index:3115
target Thresh 19.0
target distance 1.0
model initialize at round 3115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.93914157,  9.86461782,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.137012063540425}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7976597065938388
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.83951961, 11.71438787,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7321912240785572}
episode index:3116
target Thresh 19.0
target distance 9.0
model initialize at round 3116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.86740304, 3.99505756, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.673485664252652}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7976651113238374
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.01229186, 11.86529234,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.865379643201773}
episode index:3117
target Thresh 19.0
target distance 14.0
model initialize at round 3117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.        , 10.99999964,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 13.892443809251523}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7976332550649439
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.89849381, 4.02782351, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.10525043479420354}
episode index:3118
target Thresh 19.0
target distance 12.0
model initialize at round 3118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 4.        , 10.99999774,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.180338874570884}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.79761320332899
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.59675992,  6.0063246 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.4032896752901676}
episode index:3119
target Thresh 19.0
target distance 11.0
model initialize at round 3119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.611175540405096}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7975931644467131
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.15146462,  5.90752909,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.2424255857363262}
episode index:3120
target Thresh 19.0
target distance 5.0
model initialize at round 3120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 7.02705417, 10.14323562,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.0304411234311326}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7976419971399374
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.90884807, 9.18241173, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.2224791965631447}
episode index:3121
target Thresh 19.0
target distance 3.0
model initialize at round 3121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.28008942,  6.99989305,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.2322682927898923}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7976907985502064
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.15623412,  8.90395409,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.2365572592158807}
episode index:3122
target Thresh 19.0
target distance 9.0
model initialize at round 3122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 9.02619422, 11.86774911,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 7.270205768689784}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7976961829406802
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.15657042, 9.4568898 , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.003166057282907}
episode index:3123
target Thresh 19.0
target distance 14.0
model initialize at round 3123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 3.99505755, 11.86740305,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.14931427864758}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7976761431544076
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.85452825,  9.90531604,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.17357154532494343}
episode index:3124
target Thresh 19.0
target distance 7.0
model initialize at round 3124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.00494232, 11.86740303,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.0795507322179905}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7976952467885982
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.06983459, 11.55333269,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.082305298047006}
episode index:3125
target Thresh 19.0
target distance 11.0
model initialize at round 3125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 5.        , 10.99999988,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.848857753380667}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7976875966576676
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.11981236,  6.98924716,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.12029391457616626}
episode index:3126
target Thresh 19.0
target distance 10.0
model initialize at round 3126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 8.544003745317543}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.79769297518448
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.87244755, 7.97064004, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.1308878681644823}
episode index:3127
target Thresh 19.0
target distance 12.0
model initialize at round 3127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.177724088861572}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7976853306711537
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.06088307, 10.80637807,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8086732016497636}
episode index:3128
target Thresh 19.0
target distance 9.0
model initialize at round 3128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 6.97380173, 11.86775013,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 7.588903272694471}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.797704406947705
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.11355636,  9.00499871,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8864577386346879}
episode index:3129
target Thresh 19.0
target distance 12.0
model initialize at round 3129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.99983213, 9.00007785, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.440489668834756}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7976844029488798
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.06482492,  5.05205498,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9501589463338388}
episode index:3130
target Thresh 19.0
target distance 11.0
model initialize at round 3130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.559639095810898}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.797630926425972
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.18022102,  2.13665555,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8819542257128757}
episode index:3131
target Thresh 19.0
target distance 1.0
model initialize at round 3131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.75664472, 3.05674765, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.209229598490556}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7976955397955677
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.75664472, 3.05674765, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.209229598490556}
episode index:3132
target Thresh 19.0
target distance 2.0
model initialize at round 3132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.4286983 ,  7.00311708,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.42870963339583923}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7977601119181993
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.4286983 ,  7.00311708,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.42870963339583923}
episode index:3133
target Thresh 19.0
target distance 4.0
model initialize at round 3133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([5.00000405, 9.97752309, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.246213535513053}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7978086887810204
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.04636174, 11.86134297,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.862589776521111}
episode index:3134
target Thresh 19.0
target distance 6.0
model initialize at round 3134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.00000024,  8.99999988,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.47213622155962}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7978420831386661
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.00093281, 10.82911767,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.1708848753453652}
episode index:3135
target Thresh 19.0
target distance 13.0
model initialize at round 3135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4., 4., 0.]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.045361017187277}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.7977785929779836
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.03056413,  2.17455435,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8260113132996891}
episode index:3136
target Thresh 19.0
target distance 6.0
model initialize at round 3136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 9.97509091, 10.13258923,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 4.181226082593238}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7978119756388131
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.13559643,  8.0358659 ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.973622594828019}
episode index:3137
target Thresh 19.0
target distance 8.0
model initialize at round 3137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([10.0261479 , 11.86773738,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 7.746568640839524}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7978309568447918
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.8659071 , 7.96463524, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9739107002932755}
episode index:3138
target Thresh 19.0
target distance 4.0
model initialize at round 3138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.82423233,  9.14518263,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.0297059856165607}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7978794337620122
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.17898963, 11.17536143,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8395294314591419}
episode index:3139
target Thresh 19.0
target distance 13.0
model initialize at round 3139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4., 8., 0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.04536101718728}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.797871759081674
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.05570925,  8.08472892,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.3150688794357785}
episode index:3140
target Thresh 19.0
target distance 10.0
model initialize at round 3140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.3988789191938}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7978400703000795
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.76734949, 3.85058089, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.881824305203667}
episode index:3141
target Thresh 19.0
target distance 12.0
model initialize at round 3141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.522607225942417}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.7977867314520287
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.35349616,  2.94014807,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.0044092471955957}
episode index:3142
target Thresh 19.0
target distance 11.0
model initialize at round 3142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.86775037, 7.97380131, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.178937641609224}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7977790935920376
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.24529248,  7.85542089,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.1407577948197263}
episode index:3143
target Thresh 19.0
target distance 12.0
model initialize at round 3143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 11.305877894274122}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7977474645215864
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.23602307,  4.90465621,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.2545531949673268}
episode index:3144
target Thresh 19.0
target distance 13.0
model initialize at round 3144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4., 6., 0.]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.04536101718728}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7977275422405382
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.24494043,  7.85575438,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.141240777232257}
episode index:3145
target Thresh 19.0
target distance 9.0
model initialize at round 3145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([12.14612991,  9.82335935,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 8.04001155322093}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7977328755869334
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.7501997 ,  1.94200314,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.2564445894216899}
episode index:3146
target Thresh 19.0
target distance 4.0
model initialize at round 3146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.99961114,  9.00010395,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.000103988301665}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7977812604373983
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.70620921,  7.00371611,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.29381428875700477}
episode index:3147
target Thresh 19.0
target distance 3.0
model initialize at round 3147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.19682968, 3.99984491, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0193390644070675}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7978296145478057
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.79066914, 5.69497252, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.7258141665478997}
episode index:3148
target Thresh 19.0
target distance 14.0
model initialize at round 3148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 13.00000000000001}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7978096914852706
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.8898421 , 5.98963157, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9957436464183077}
episode index:3149
target Thresh 19.0
target distance 11.0
model initialize at round 3149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.99983197, 9.00003048, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.401905600430288}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7977670282280654
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.69368197,  1.31337949,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7518500310045141}
episode index:3150
target Thresh 19.0
target distance 3.0
model initialize at round 3150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.08949184,  4.00174034,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.005729830412439}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7978153408182818
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.81465763,  2.04124141,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9765090033148215}
episode index:3151
target Thresh 19.0
target distance 13.0
model initialize at round 3151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.401754250991404}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.79779544124652
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.37879988, 10.58167728,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7489215503617093}
episode index:3152
target Thresh 19.0
target distance 5.0
model initialize at round 3152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.        ,  9.00001228,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.1622893086324892}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7978286491623949
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.80985007,  5.07123148,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9480337296759882}
episode index:3153
target Thresh 19.0
target distance 6.0
model initialize at round 3153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 8.02389447, 10.13287906,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.554217806804328}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7978618360206187
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.86025683, 7.95947958, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.1454993416169992}
episode index:3154
target Thresh 19.0
target distance 12.0
model initialize at round 3154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([12.02619884, 11.86775028,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.617020118792258}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7978542034061905
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.93526111, 6.0240116 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.06904839919827213}
episode index:3155
target Thresh 19.0
target distance 2.0
model initialize at round 3155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.6378243 ,  2.00790274,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.36226191243481876}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7979182546725384
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.6378243 ,  2.00790274,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.36226191243481876}
episode index:3156
target Thresh 19.0
target distance 2.0
model initialize at round 3156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.12746239, 2.28966963, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7216757516566659}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7979822653615873
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.12746239, 2.28966963, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7216757516566659}
episode index:3157
target Thresh 19.0
target distance 2.0
model initialize at round 3157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.69488078, 3.01099038, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.30531708881434433}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7980462355118845
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.69488078, 3.01099038, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.30531708881434433}
episode index:3158
target Thresh 19.0
target distance 12.0
model initialize at round 3158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.633815097396205}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.79800361892302
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.47086911, 1.86129493, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.547008769740189}
episode index:3159
target Thresh 19.0
target distance 12.0
model initialize at round 3159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.        , 7.99999654, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.180338341450966}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7979610293066801
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.92378032,  2.52410516,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.48195989107884085}
episode index:3160
target Thresh 19.0
target distance 7.0
model initialize at round 3160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.506778200582048}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7979798252480573
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.02555704, 11.86722315,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.3044596894505112}
episode index:3161
target Thresh 19.0
target distance 13.0
model initialize at round 3161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.00000000000002}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7979372700950026
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.73690134, 4.91649915, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.27603133172372535}
episode index:3162
target Thresh 19.0
target distance 12.0
model initialize at round 3162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 6., 11.,  0.]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 10.770329614269018}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7979174011795838
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.92094079,  7.7494111 ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7535697460241244}
episode index:3163
target Thresh 19.0
target distance 5.0
model initialize at round 3163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.10641265,  8.00004017,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.1302938194981436}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7979504550983008
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.40674782,  4.06535113,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0193195280638174}
episode index:3164
target Thresh 19.0
target distance 9.0
model initialize at round 3164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.99996197, 4.        , 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 7.000000000103312}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7979556859971637
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.50883268, 11.59018714,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7678321386378673}
episode index:3165
target Thresh 19.0
target distance 12.0
model initialize at round 3165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 6., 11.,  0.]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 10.198039027185583}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7979480502585354
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.07553466,  9.37295722,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.3805293353359385}
episode index:3166
target Thresh 19.0
target distance 11.0
model initialize at round 3166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([12.05521678, 10.045681  ,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 9.917887154191176}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7979404193419712
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.50374886, 6.01950891, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.49663446880103895}
episode index:3167
target Thresh 19.0
target distance 11.0
model initialize at round 3167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.99983213, 9.00007785, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.055560578267828}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7979327932429049
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.24970168,  7.09358397,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9401813417676518}
episode index:3168
target Thresh 19.0
target distance 14.0
model initialize at round 3168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.369316876853006}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7979013651907909
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.86004517, 9.78942297, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8017331065122814}
episode index:3169
target Thresh 19.0
target distance 6.0
model initialize at round 3169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.86775027, 6.97380118, 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 4.5559593798074625}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7979343616055573
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.06062232, 11.16104479,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.17207699232063725}
episode index:3170
target Thresh 19.0
target distance 7.0
model initialize at round 3170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([4.03553363, 4.        , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 5.398462480766791}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7979531066823137
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.42973634, 9.78444231, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8944400770512151}
episode index:3171
target Thresh 19.0
target distance 11.0
model initialize at round 3171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([12.        , 10.99997973,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041581115083826}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7979332891488782
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.80955916, 3.96353756, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9821773535914469}
episode index:3172
target Thresh 19.0
target distance 3.0
model initialize at round 3172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 5.97127904, 11.85680726,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.3388000171574008}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7979812143650303
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.87575993, 10.36393238,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0823758492265354}
episode index:3173
target Thresh 19.0
target distance 1.0
model initialize at round 3173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.84714527, 7.08693469, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9257714811654917}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7980448623756274
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.84714527, 7.08693469, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9257714811654917}
episode index:3174
target Thresh 19.0
target distance 10.0
model initialize at round 3174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.02619884, 11.86775028,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.072970849349375}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7980500470646429
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.11559766, 11.34728759,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.3660211626465609}
episode index:3175
target Thresh 19.0
target distance 8.0
model initialize at round 3175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.13259841, 9.00493383, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.288594356492715}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7980687262059953
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.96630287, 3.00949303, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.035008775156598435}
episode index:3176
target Thresh 19.0
target distance 10.0
model initialize at round 3176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.184022742676934}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7980263439916683
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.91124113,  4.93925536,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9434398621846484}
episode index:3177
target Thresh 19.0
target distance 3.0
model initialize at round 3177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([1.65057492, 9.00121176, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.6802895681531764}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7980741645253399
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.43160665, 7.05347097, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.040289145754782}
episode index:3178
target Thresh 19.0
target distance 12.0
model initialize at round 3178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14., 11.,  0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.440306508910563}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7980665227426957
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.9147944 , 8.01736885, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.0869578740864282}
episode index:3179
target Thresh 19.0
target distance 8.0
model initialize at round 3179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.0873906,  4.       ,  0.       ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 6.069007819568578}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7980851732072421
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.53891087,  9.98966682,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5390099218728094}
episode index:3180
target Thresh 19.0
target distance 11.0
model initialize at round 3180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.332858443941447}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7980775327684784
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.82652405, 9.29688993, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.085129381110843}
episode index:3181
target Thresh 19.0
target distance 11.0
model initialize at round 3181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 4.97380122, 11.86775026,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.819967232977854}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7980698971320018
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.27212487,  7.1379611 ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9039706924404453}
episode index:3182
target Thresh 19.0
target distance 11.0
model initialize at round 3182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.184084079649052}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7980501113932311
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.01310643,  7.09679483,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9033002627121886}
episode index:3183
target Thresh 19.0
target distance 8.0
model initialize at round 3183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 8.        , 10.70848423,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 7.626914432914653}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7980552797784719
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.56962234,  5.97977338,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.4308526971522569}
episode index:3184
target Thresh 19.0
target distance 6.0
model initialize at round 3184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.00000036, 9.00000012, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 4.0000001192092505}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7980880724692793
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.2078548 , 5.02569533, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.20943702857997723}
episode index:3185
target Thresh 19.0
target distance 9.0
model initialize at round 3185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 9.97380122, 10.13224974,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.074346391633426}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7980804431111596
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.83053828,  1.95969637,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.17418856659514914}
episode index:3186
target Thresh 19.0
target distance 3.0
model initialize at round 3186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.05577636, 9.00468981, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0062368616464366}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7981281116260289
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.25487632, 7.0693301 , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.1922062635377952}
episode index:3187
target Thresh 19.0
target distance 13.0
model initialize at round 3187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.704699910719638}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7980858570211554
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.93398124, 2.86291301, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.15215557749234784}
episode index:3188
target Thresh 19.0
target distance 7.0
model initialize at round 3188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([1.13274507, 7.00407116, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.767311244570699}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7981044487875332
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.92939317, 1.13257262, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8702962596551354}
episode index:3189
target Thresh 19.0
target distance 2.0
model initialize at round 3189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.31658483,  2.00996804,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.3167417143598321}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7981677389289791
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.31658483,  2.00996804,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.3167417143598321}
episode index:3190
target Thresh 19.0
target distance 14.0
model initialize at round 3190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.00494207, 11.86740298,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 14.353210925173883}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7981364539265237
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.72696991, 4.05006478, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.27758226051708934}
episode index:3191
target Thresh 19.0
target distance 3.0
model initialize at round 3191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.86229318, 5.99991214, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.3205018958887373}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7981840302254187
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.68856555, 7.90151119, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9537892037218557}
episode index:3192
target Thresh 19.0
target distance 10.0
model initialize at round 3192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 5.99505852, 11.86740289,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 8.890213337688767}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7981891421013269
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.11804636,  7.98932834,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.11852775131970014}
episode index:3193
target Thresh 19.0
target distance 5.0
model initialize at round 3193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.82109956,  5.99989171,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.1104427730974966}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7982218004788781
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.12645505,  9.21180941,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.24668625065251254}
episode index:3194
target Thresh 19.0
target distance 4.0
model initialize at round 3194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.77934754, 8.00153065, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.0136564852428234}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7982693053926562
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.25662836, 6.00364196, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.2566542056356074}
episode index:3195
target Thresh 19.0
target distance 2.0
model initialize at round 3195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.00171089, 11.08564112,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.08565821128116696}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7983324251343982
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.00171089, 11.08564112,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.08565821128116696}
episode index:3196
target Thresh 19.0
target distance 8.0
model initialize at round 3196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.85958148, 4.97118371, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.6726180841554195}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7983508932529048
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.58012004, 10.96700391,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.5810576588180887}
episode index:3197
target Thresh 19.0
target distance 2.0
model initialize at round 3197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.00610137, 9.15530169, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8447203451198061}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7984139480079852
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.00610137, 9.15530169, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8447203451198061}
episode index:3198
target Thresh 19.0
target distance 8.0
model initialize at round 3198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.00571179,  8.00001454,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 6.0000172622419505}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7984323790964478
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.81345558,  2.04586974,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.19210115465637845}
episode index:3199
target Thresh 19.0
target distance 5.0
model initialize at round 3199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.86679057, 7.9981152 , 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 3.5349992091565814}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7984649002279802
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.36852383, 11.82907107,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0421712894525699}
episode index:3200
target Thresh 19.0
target distance 5.0
model initialize at round 3200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.13269953, 6.00433568, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.537349845387207}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7984974010401552
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.28907918, 2.12004683, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9262204685900559}
episode index:3201
target Thresh 19.0
target distance 1.0
model initialize at round 3201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.0732283 , 8.08802164, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.3002347176632545}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.798560331270936
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.0732283 , 8.08802164, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.3002347176632545}
episode index:3202
target Thresh 19.0
target distance 13.0
model initialize at round 3202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4., 6., 0.]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7985181396068767
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.04165852,  3.95463248,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.061592564911313125}
episode index:3203
target Thresh 19.0
target distance 11.0
model initialize at round 3203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.574514830248853}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7984868721775654
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.9689177 , 7.08025685, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9202682026769855}
episode index:3204
target Thresh 19.0
target distance 13.0
model initialize at round 3204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.401754250991392}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7984791636176035
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.10828082,  7.57509992,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9877768798875911}
episode index:3205
target Thresh 19.0
target distance 2.0
model initialize at round 3205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.87390053,  4.99002629,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.12649328268909302}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.798542021021341
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.87390053,  4.99002629,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.12649328268909302}
episode index:3206
target Thresh 19.0
target distance 13.0
model initialize at round 3206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 3.97380715, 11.86774876,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 12.490292399552414}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7985222361350309
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.83576332,  6.03506956,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.16793915581847776}
episode index:3207
target Thresh 19.0
target distance 10.0
model initialize at round 3207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 9.980393886139279}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.798502463583438
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.80804366, 8.91386322, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8126217507100342}
episode index:3208
target Thresh 19.0
target distance 4.0
model initialize at round 3208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.17241108,  3.99999774,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 2.1644658633268463}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7985496737848766
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.11478382,  5.99635148,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.11484179478605762}
episode index:3209
target Thresh 19.0
target distance 11.0
model initialize at round 3209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([12.00486842, 11.86739101,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 13.358632436636086}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.798518454975627
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.88397282, 1.98148624, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.11749495570267585}
episode index:3210
target Thresh 19.0
target distance 1.0
model initialize at round 3210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.87202001, 3.81383228, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8238336383662489}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7985812022646411
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.87202001, 3.81383228, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8238336383662489}
episode index:3211
target Thresh 19.0
target distance 4.0
model initialize at round 3211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.55464596,  6.99999726,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.048987845781148}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7986283438579584
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.92813358,  8.9959141 ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.07198247915270481}
episode index:3212
target Thresh 19.0
target distance 12.0
model initialize at round 3212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 4.97394927, 11.86771268,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.15266105384258}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7986085690514745
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.70845196,  5.01335815,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.2918539056368076}
episode index:3213
target Thresh 19.0
target distance 12.0
model initialize at round 3213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.049875621120913}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7986008442127839
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.74266663, 8.8433907 , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1237711484351616}
episode index:3214
target Thresh 19.0
target distance 7.0
model initialize at round 3214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 8.97382312, 10.13225528,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 7.183487888853167}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7986057914618623
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.9643139 ,  4.96753792,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.04824193686732972}
episode index:3215
target Thresh 19.0
target distance 7.0
model initialize at round 3215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 8.02579194, 10.13235392,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 6.506530066630289}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7986240654694923
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.35806994, 5.9420918 , 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.6445367079016789}
episode index:3216
target Thresh 19.0
target distance 13.0
model initialize at round 3216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 3.9950576 , 11.86740304,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 12.033302519659383}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7986163430175278
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.10417909,  7.99652394,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.3399832315999305}
episode index:3217
target Thresh 19.0
target distance 10.0
model initialize at round 3217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 5.99510697, 11.86739502,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.924950214994873}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7986086253650985
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.4248412 ,  5.98264643,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.575420539167615}
episode index:3218
target Thresh 19.0
target distance 3.0
model initialize at round 3218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.67697152, 3.99984276, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0510289694009116}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7986556559257181
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.97727868, 5.68952727, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6899015283768362}
episode index:3219
target Thresh 19.0
target distance 2.0
model initialize at round 3219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.44530427,  7.00276232,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.5547026034023446}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7987181852251201
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.44530427,  7.00276232,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.5547026034023446}
episode index:3220
target Thresh 19.0
target distance 5.0
model initialize at round 3220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.99819601, 6.        , 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 3.1617076503113903}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.798750405596053
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.48074222, 9.86238825, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9873330631700126}
episode index:3221
target Thresh 19.0
target distance 14.0
model initialize at round 3221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 13.416407864998765}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7987084037418298
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.93719766, 10.8230021 ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.825394811335131}
episode index:3222
target Thresh 19.0
target distance 11.0
model initialize at round 3222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([12.05521678, 10.045681  ,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 9.115393552020814}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.798713305338559
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.89966735, 8.05638934, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.3037647086524513}
episode index:3223
target Thresh 19.0
target distance 7.0
model initialize at round 3223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.50505853,  4.        ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.025443673516628}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7987315006532802
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.60272458,  9.14593768,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.6201408940624843}
episode index:3224
target Thresh 19.0
target distance 2.0
model initialize at round 3224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.59450257, 10.98721707,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.4056988667636072}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7987939094902871
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.59450257, 10.98721707,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.4056988667636072}
episode index:3225
target Thresh 19.0
target distance 1.0
model initialize at round 3225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.04173472, 7.02808678, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9728088667725713}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7988562796361363
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.04173472, 7.02808678, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9728088667725713}
episode index:3226
target Thresh 19.0
target distance 9.0
model initialize at round 3226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4., 8., 0.]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.615773105863933}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7988611293325614
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.62414073, 11.89940876,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0947546635884167}
episode index:3227
target Thresh 19.0
target distance 8.0
model initialize at round 3227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 9.97392109, 10.13228014,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 6.791524564014878}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7988659760242178
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.7010741 ,  7.64723904,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.71293412062464}
episode index:3228
target Thresh 19.0
target distance 12.0
model initialize at round 3228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.32905180961646}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7988462256106535
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.66111099, 10.42717859,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7871145301471725}
episode index:3229
target Thresh 19.0
target distance 11.0
model initialize at round 3229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.184084079649052}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7988264874264475
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.07066255,  7.06851596,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9341604276920112}
episode index:3230
target Thresh 19.0
target distance 4.0
model initialize at round 3230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.02302006, 11.86684264,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.2009148869628925}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7988732758859254
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.02806783, 10.27644353,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7241006619981162}
episode index:3231
target Thresh 19.0
target distance 9.0
model initialize at round 3231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.13261027,  3.99513942,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.095835245218559}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7988913766669014
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.97645552, 10.13044083,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8698778640283474}
episode index:3232
target Thresh 19.0
target distance 7.0
model initialize at round 3232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.53145599,  9.00003767,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 5.211237684212145}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7989094662503635
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.48723698,  3.0631386 ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0559872851492589}
episode index:3233
target Thresh 19.0
target distance 12.0
model initialize at round 3233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([5.17664043, 9.14612952, 0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.984480019440753}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7988897329245671
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.56337669,  4.85944744,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9639967978452435}
episode index:3234
target Thresh 19.0
target distance 8.0
model initialize at round 3234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 9.       , 10.6069794,  0.       ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 6.21147186999714}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.798907811832473
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.01807358,  8.97072748,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.034402545089358146}
episode index:3235
target Thresh 19.0
target distance 14.0
model initialize at round 3235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.000000000000018}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7988880912140529
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.89858773, 10.15857605,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.1882307424940313}
episode index:3236
target Thresh 19.0
target distance 1.0
model initialize at round 3236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.0177302 , 7.02110648, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9790540742690303}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7989502203177866
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.0177302 , 7.02110648, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9790540742690303}
episode index:3237
target Thresh 19.0
target distance 2.0
model initialize at round 3237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.47486687,  3.99866605,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.525133715956805}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7989968694158971
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.31536675,  3.51426125,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.5791358408262091}
episode index:3238
target Thresh 19.0
target distance 9.0
model initialize at round 3238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 6.97420291, 11.8676475 ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 8.547269564220786}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.799001657739634
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.18746487,  6.99269804,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.18760702751839087}
episode index:3239
target Thresh 19.0
target distance 6.0
model initialize at round 3239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.66800863,  6.99999982,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 4.333849815045724}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7990336016724305
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.43197293, 10.98608846,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.4321968803484699}
episode index:3240
target Thresh 19.0
target distance 10.0
model initialize at round 3240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([12.05521678, 10.045681  ,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 8.12280530707726}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7990383757077059
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.97120011, 8.30723239, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6933659941763037}
episode index:3241
target Thresh 19.0
target distance 5.0
model initialize at round 3241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.00004554, 9.00000548, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.0000054839729007}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7990702886084746
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.02837431, 5.08729494, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9131460098116132}
episode index:3242
target Thresh 19.0
target distance 6.0
model initialize at round 3242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 9.97394732, 10.13228682,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.555847596560155}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7991021818281452
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.15985578,  7.94782146,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.16815608713635022}
episode index:3243
target Thresh 19.0
target distance 12.0
model initialize at round 3243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.99983211, 9.00007261, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.662085111169356}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7990711199028263
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.13271882,  3.03091387,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.13627160974669397}
episode index:3244
target Thresh 19.0
target distance 13.0
model initialize at round 3244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.831072604542193}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7990400771219914
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.94945331, 9.83587194, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.17173522551938636}
episode index:3245
target Thresh 19.0
target distance 14.0
model initialize at round 3245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.00000000000001}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7989982965779886
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.98569273, 2.0622349 , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9378742314580819}
episode index:3246
target Thresh 19.0
target distance 10.0
model initialize at round 3246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.00000000000002}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.79900307266466
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.06537397, 11.54829663,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5521801832657994}
episode index:3247
target Thresh 19.0
target distance 13.0
model initialize at round 3247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.02619882, 11.86775027,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.490298379729788}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7989833955765936
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.30728847, 5.31500901, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9741980891088389}
episode index:3248
target Thresh 19.0
target distance 11.0
model initialize at round 3248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.132283595738201}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7989637306012314
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.06363731,  6.91247905,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.10821101312161978}
episode index:3249
target Thresh 19.0
target distance 2.0
model initialize at round 3249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.00236058, 9.01146412, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.011704630387696422}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7990255879148925
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.00236058, 9.01146412, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.011704630387696422}
episode index:3250
target Thresh 19.0
target distance 14.0
model initialize at round 3250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4., 6., 0.]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.000000000000018}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7989946164317115
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.0149    ,  5.86177274,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.13902800740924298}
episode index:3251
target Thresh 19.0
target distance 7.0
model initialize at round 3251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.99999142, 6.        , 0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.000000000007386}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7990125685791803
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.46697607, 11.50003141,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.730852870821744}
episode index:3252
target Thresh 19.0
target distance 6.0
model initialize at round 3252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.15309169,  6.99991302,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 4.003015477436238}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7990443814999983
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.76803979, 11.07123086,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.24265072188774925}
episode index:3253
target Thresh 19.0
target distance 4.0
model initialize at round 3253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.72017407,  4.00128913,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.0207574638358112}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7990907722862614
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.12674707,  2.00411451,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.12681383748120226}
episode index:3254
target Thresh 19.0
target distance 2.0
model initialize at round 3254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.0149374,  5.9027456,  0.       ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9028691779070251}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.799152495551304
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.0149374,  5.9027456,  0.       ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9028691779070251}
episode index:3255
target Thresh 19.0
target distance 2.0
model initialize at round 3255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.56260085, 3.99658787, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.43741245765112474}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7992141809027932
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.56260085, 3.99658787, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.43741245765112474}
episode index:3256
target Thresh 19.0
target distance 4.0
model initialize at round 3256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.11815164, 7.000332  , 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.186088888298312}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7992604768251441
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.15411935, 5.00943983, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.15440817453917136}
episode index:3257
target Thresh 19.0
target distance 12.0
model initialize at round 3257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.837958482002342}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7992407811264945
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.9883785 , 11.81718303,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.282450843302653}
episode index:3258
target Thresh 19.0
target distance 4.0
model initialize at round 3258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.80168867, 3.9999913 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.1547017275200284}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7992870404756426
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.60759223, 5.99624228, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.39242576654121686}
episode index:3259
target Thresh 19.0
target distance 12.0
model initialize at round 3259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.04987562112091}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7992792165176745
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.86744995, 7.97158583, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.1355613549292361}
episode index:3260
target Thresh 19.0
target distance 7.0
model initialize at round 3260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.04568015,  9.05521837,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 5.419833839879038}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7992970318453293
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.42725917,  3.10673407,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9901890876781685}
episode index:3261
target Thresh 19.0
target distance 11.0
model initialize at round 3261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.132598136344583}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7992892096214343
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.26775051,  7.84166309,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.31106416266109543}
episode index:3262
target Thresh 19.0
target distance 7.0
model initialize at round 3262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 8.01828394, 10.13576499,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.145205129376901}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.79932084026513
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.84323564, 8.05222225, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.268593314091433}
episode index:3263
target Thresh 19.0
target distance 14.0
model initialize at round 3263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.165525060596464}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.7992792041104191
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.83482773, 5.91740151, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.18467373476601245}
episode index:3264
target Thresh 19.0
target distance 2.0
model initialize at round 3264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.00411415,  9.91405487,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.914064129432992}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7993406806175828
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.00411415,  9.91405487,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.914064129432992}
episode index:3265
target Thresh 19.0
target distance 8.0
model initialize at round 3265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([11.02619879, 11.86775026,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 6.088354655898978}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7993584498519314
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.04790705, 10.81451773,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.1915692040071102}
episode index:3266
target Thresh 19.0
target distance 14.0
model initialize at round 3266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 4.        , 10.99999905,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 13.416407438502693}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7993275281642184
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.18516765,  4.03398277,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.983603754695349}
episode index:3267
target Thresh 19.0
target distance 6.0
model initialize at round 3267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.90649652, 4.        , 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 4.431109228975949}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.799359098688036
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.63871199, 7.99472415, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.3613265302906466}
episode index:3268
target Thresh 19.0
target distance 8.0
model initialize at round 3268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.63822144,  4.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 6.010897081511433}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7993768459811874
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.57628149,  9.98619843,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5764467337133026}
episode index:3269
target Thresh 19.0
target distance 5.0
model initialize at round 3269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([2.07550848, 5.00001156, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 3.5642302126220904}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.799408382113915
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.38709178, 1.10673247, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9735332209996818}
episode index:3270
target Thresh 19.0
target distance 7.0
model initialize at round 3270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.02619744, 11.86774992,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.100553955756457}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7994261034889948
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.09674808, 11.49540863,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0301911203737641}
episode index:3271
target Thresh 19.0
target distance 4.0
model initialize at round 3271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.75480628,  7.00053275,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.015502723724091}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7994721224060213
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.08249563,  5.00473535,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.08263142549553387}
episode index:3272
target Thresh 19.0
target distance 13.0
model initialize at round 3272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.401754250991392}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7994642729758636
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.13670413,  7.00437812,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.863306969760448}
episode index:3273
target Thresh 19.0
target distance 4.0
model initialize at round 3273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.12515712,  6.00014806,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 2.1831038276113426}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.799510252122786
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.1563921 ,  4.01408195,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.15702480570729208}
episode index:3274
target Thresh 19.0
target distance 14.0
model initialize at round 3274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.041594578792314}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7994905823940844
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.97973338, 6.19611225, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8041431738977237}
episode index:3275
target Thresh 19.0
target distance 2.0
model initialize at round 3275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.14524007, 9.0126034 , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.854852844852998}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7995517879550141
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.14524007, 9.0126034 , 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.854852844852998}
episode index:3276
target Thresh 19.0
target distance 2.0
model initialize at round 3276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.03019941,  2.71895945,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.2826584416115315}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7996129561613141
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.03019941,  2.71895945,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.2826584416115315}
episode index:3277
target Thresh 19.0
target distance 5.0
model initialize at round 3277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.37795675, 4.99999928, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.0237155281044936}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7996443433009843
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.26272815, 8.81447744, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8558034760029595}
episode index:3278
target Thresh 19.0
target distance 5.0
model initialize at round 3278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.00491881, 11.86739854,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.127605679974918}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.799675711296318
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.03604016, 11.35808194,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.02831962680921}
episode index:3279
target Thresh 19.0
target distance 12.0
model initialize at round 3279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.        ,  8.99999976,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.661903667025253}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.7996448154380245
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.7739838 , 3.96859152, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9946119130169317}
episode index:3280
target Thresh 19.0
target distance 13.0
model initialize at round 3280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7996369325127156
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.89891597, 9.10560518, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9050979919459726}
episode index:3281
target Thresh 19.0
target distance 2.0
model initialize at round 3281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.7100625 ,  2.00545347,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.28998877909423393}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7996979815887325
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.7100625 ,  2.00545347,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.28998877909423393}
episode index:3282
target Thresh 19.0
target distance 2.0
model initialize at round 3282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.25699234, 2.00762987, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.2571055771486635}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7997589934737191
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.25699234, 2.00762987, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.2571055771486635}
episode index:3283
target Thresh 19.0
target distance 13.0
model initialize at round 3283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4., 4., 0.]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.401754250991402}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.799728109887428
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.19518518,  7.90555777,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.2115121829998667}
episode index:3284
target Thresh 19.0
target distance 9.0
model initialize at round 3284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([12.02619884, 11.86775028,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.079580542486185}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7997326085602172
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.53215059, 10.90679199,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.4770438178703846}
episode index:3285
target Thresh 19.0
target distance 9.0
model initialize at round 3285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.86775027, 3.9738012 , 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.342612510877773}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7997371044949219
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.94195973, 11.64907027,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.1439319712648621}
episode index:3286
target Thresh 19.0
target distance 12.0
model initialize at round 3286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.536241166319517}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7997174375603706
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.96179161, 10.88675031,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.11952143323639622}
episode index:3287
target Thresh 19.0
target distance 5.0
model initialize at round 3287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.00618088, 8.00000823, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.602133260472942}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.7997486974637891
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.5359565 , 4.08069026, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.064133337088673}
episode index:3288
target Thresh 19.0
target distance 13.0
model initialize at round 3288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.045361017187282}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.799729038963686
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.93147846, 7.0078983 , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.99446517661896}
episode index:3289
target Thresh 19.0
target distance 12.0
model initialize at round 3289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.407731439784726}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7997211520027547
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.00835869, 8.22493071, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.7751143656239079}
episode index:3290
target Thresh 19.0
target distance 2.0
model initialize at round 3290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.99291503, 11.57727167,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5773151491980578}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7997820085351149
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.99291503, 11.57727167,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5773151491980578}
episode index:3291
target Thresh 19.0
target distance 9.0
model initialize at round 3291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([10.02619577, 11.8677495 ,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 8.547655366809181}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7997995033684883
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.86330887, 7.96388518, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.2939771433263825}
episode index:3292
target Thresh 19.0
target distance 9.0
model initialize at round 3292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([11.02619883, 11.86775027,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 7.270210523591523}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7998169875763933
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.9055092 , 9.13051096, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.255371696469744}
episode index:3293
target Thresh 19.0
target distance 1.0
model initialize at round 3293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.13614542,  4.97426873,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.3020922739621075}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7998777595898795
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.13614542,  4.97426873,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.3020922739621075}
episode index:3294
target Thresh 19.0
target distance 7.0
model initialize at round 3294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.954319  , 9.05521678, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.41983204601529}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.7998952094352241
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.99598635, 3.08219956, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9178092157008273}
episode index:3295
target Thresh 19.0
target distance 9.0
model initialize at round 3295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([10.97380136, 10.13224964,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 8.67705954833595}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7998996423965604
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.84688486,  2.92100679,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.9336475523525998}
episode index:3296
target Thresh 19.0
target distance 2.0
model initialize at round 3296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.78346801, 8.99846053, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.21653746443893762}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7999603340427853
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.78346801, 8.99846053, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.21653746443893762}
episode index:3297
target Thresh 19.0
target distance 11.0
model initialize at round 3297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 5.97380117, 11.86775027,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 9.21741593215608}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.7999523960814321
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.64196431, 10.46812297,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7945170195686861}
episode index:3298
target Thresh 19.0
target distance 12.0
model initialize at round 3298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 5.94478292, 10.04568051,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 11.250168103165393}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.7999327354250342
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.01598625,  4.02503599,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9750950602464936}
episode index:3299
target Thresh 19.0
target distance 4.0
model initialize at round 3299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([14.        ,  9.00006711,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.8284745824958035}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.7999782103536932
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.88748753,  7.00793159,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.11279169236839107}
episode index:3300
target Thresh 19.0
target distance 9.0
model initialize at round 3300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 6.9783465 , 11.86134034,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.146525486933417}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.7999826114562822
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.10890327,  6.98184313,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9878642927331158}
episode index:3301
target Thresh 19.0
target distance 1.0
model initialize at round 3301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.11082917,  4.39580649,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.65496239093157}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8000280437362772
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.33212075,  2.43385738,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.6563700637033353}
episode index:3302
target Thresh 19.0
target distance 8.0
model initialize at round 3302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.13259698, 9.00494241, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 6.0672663796643915}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8000454058181009
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.89213595, 3.01796916, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.10935055319328091}
episode index:3303
target Thresh 19.0
target distance 2.0
model initialize at round 3303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.8061583 , 4.99143422, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.19403086267908137}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8001059247630713
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.8061583 , 4.99143422, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.19403086267908137}
episode index:3304
target Thresh 19.0
target distance 3.0
model initialize at round 3304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.10691929,  6.00050914,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.3411232985782664}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.800151278492341
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.04866809,  4.11537707,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8859606726866478}
episode index:3305
target Thresh 19.0
target distance 4.0
model initialize at round 3305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.99373031,  5.99999845,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.000011376952737}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8001966047843881
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.89774731,  7.99339366,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.10246588091443912}
episode index:3306
target Thresh 19.0
target distance 9.0
model initialize at round 3306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 9.02611473, 10.13227104,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 10.747098229219912}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8001886169805524
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.02297607, 2.97495664, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9752273321508431}
episode index:3307
target Thresh 19.0
target distance 3.0
model initialize at round 3307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.13695121, 10.01027373,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.313168424549408}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8002490194542584
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.13695121, 10.01027373,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.313168424549408}
episode index:3308
target Thresh 19.0
target distance 1.0
model initialize at round 3308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.328825  ,  8.05189228,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0035108999131381}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8003093854199719
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.328825  ,  8.05189228,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0035108999131381}
episode index:3309
target Thresh 19.0
target distance 3.0
model initialize at round 3309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.98660588,  5.99995315,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0001365422161954}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8003546091706002
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.70353979,  7.65597224,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7198529291537403}
episode index:3310
target Thresh 19.0
target distance 10.0
model initialize at round 3310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.13228730466267}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8003237975387436
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.65075954,  5.8910323 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.3658453968950667}
episode index:3311
target Thresh 19.0
target distance 13.0
model initialize at round 3311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        , 10.99994612,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.601438816585565}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8002930045129449
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.06738459, 3.98026338, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9825767034587488}
episode index:3312
target Thresh 19.0
target distance 1.0
model initialize at round 3312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.15542333, 1.15661677, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8575848017043531}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8003532843184044
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.15542333, 1.15661677, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8575848017043531}
episode index:3313
target Thresh 19.0
target distance 3.0
model initialize at round 3313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.99990499, 8.99999893, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0000010773970538}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8003984402374392
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.5112173 , 10.54303467,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.7458081386324568}
episode index:3314
target Thresh 19.0
target distance 12.0
model initialize at round 3314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.049875621120908}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8003571195710898
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.59410275, 4.89787842, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9853619838824433}
episode index:3315
target Thresh 19.0
target distance 11.0
model initialize at round 3315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([12.00494245, 11.86740305,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 9.450449133057383}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8003613864982396
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.87524506, 8.04493653, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.2954536460199466}
episode index:3316
target Thresh 19.0
target distance 12.0
model initialize at round 3316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.770329614269034}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8003533730978784
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.92976006, 10.86610743,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9393513653212879}
episode index:3317
target Thresh 19.0
target distance 7.0
model initialize at round 3317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.86769117, 3.97403339, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.10031648905538}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8003705586394402
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.24191875, 9.74816676, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7863066720585149}
episode index:3318
target Thresh 19.0
target distance 2.0
model initialize at round 3318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.47281462, 5.98259163, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.4731349869030537}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8004307061059543
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.47281462, 5.98259163, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.4731349869030537}
episode index:3319
target Thresh 19.0
target distance 11.0
model initialize at round 3319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.186646103472004}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8003999550788421
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.14238492,  5.89506656,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.2396159794896424}
episode index:3320
target Thresh 19.0
target distance 6.0
model initialize at round 3320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.13226661, 7.02613407, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 4.11858189490397}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8004306988442506
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.80670917, 3.03144162, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.19583135309646268}
episode index:3321
target Thresh 19.0
target distance 5.0
model initialize at round 3321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([12.        , 10.86064571,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 3.003234858905509}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8004614241004685
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.57157099, 11.18686527,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6013418566977157}
episode index:3322
target Thresh 19.0
target distance 2.0
model initialize at round 3322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.99911368, 10.93662612,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.06338007615658733}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8005214718211725
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.99911368, 10.93662612,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.06338007615658733}
episode index:3323
target Thresh 19.0
target distance 11.0
model initialize at round 3323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.929943319783787}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8004802260207717
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.06030792, 10.47560365,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.47941201458750987}
episode index:3324
target Thresh 19.0
target distance 13.0
model initialize at round 3324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4., 8., 0.]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.800460560355991
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.10005554,  6.01409566,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.1010435526073725}
episode index:3325
target Thresh 19.0
target distance 1.0
model initialize at round 3325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.86048192, 10.09902477,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9117135819727823}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8005205541742845
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.86048192, 10.09902477,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9117135819727823}
episode index:3326
target Thresh 19.0
target distance 4.0
model initialize at round 3326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.14707994, 8.99999559, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 2.005405234136186}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8005654833735106
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.08898963, 10.99600124,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.08907943098249331}
episode index:3327
target Thresh 19.0
target distance 3.0
model initialize at round 3327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 5.99999034, 10.9999305 ,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0000096583674576}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8006103855720161
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.93827748, 11.01000903,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9383308668701613}
episode index:3328
target Thresh 19.0
target distance 4.0
model initialize at round 3328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.82358113,  9.14588937,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.0287957244585417}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8006552607941333
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.12024007, 11.18232471,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8984541364161029}
episode index:3329
target Thresh 19.0
target distance 1.0
model initialize at round 3329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.49409616,  5.02911778,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5067410980899781}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8007151240791801
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.49409616,  5.02911778,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5067410980899781}
episode index:3330
target Thresh 19.0
target distance 4.0
model initialize at round 3330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.63519106, 7.99999952, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.583379902532343}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8007599409137405
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.92230119, 9.99281752, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.07803007434636358}
episode index:3331
target Thresh 19.0
target distance 1.0
model initialize at round 3331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.24029639,  8.64058495,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.684172075886727}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8008197368498408
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.24029639,  8.64058495,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.684172075886727}
episode index:3332
target Thresh 19.0
target distance 4.0
model initialize at round 3332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.09534669,  8.00022626,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 2.1952910287286884}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8008644954046412
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.98773623,  6.002599  ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.012536146933750627}
episode index:3333
target Thresh 19.0
target distance 6.0
model initialize at round 3333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.        ,  9.00000703,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.1231124489679205}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.800894979959109
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.00709118,  5.00450397,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.008400626803067388}
episode index:3334
target Thresh 19.0
target distance 7.0
model initialize at round 3334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.52766007,  5.        ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.027765423269546}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8009119154973522
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.56241608, 10.18062425,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5907088637769347}
episode index:3335
target Thresh 19.0
target distance 2.0
model initialize at round 3335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.01763988, 10.99031496,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.0201237470432564}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.800971594179757
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.01763988, 10.99031496,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.0201237470432564}
episode index:3336
target Thresh 19.0
target distance 12.0
model initialize at round 3336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.214321868139345}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8009303741728974
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.48007817, 10.39633311,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6225391440696834}
episode index:3337
target Thresh 19.0
target distance 3.0
model initialize at round 3337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.14936352,  8.99989879,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.3129298734252635}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.800975032538933
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.46456575, 10.37897146,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5995337399315603}
episode index:3338
target Thresh 19.0
target distance 10.0
model initialize at round 3338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.959371473456923}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8009442934744091
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.53835464, 3.86192751, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.5557784956019987}
episode index:3339
target Thresh 19.0
target distance 12.0
model initialize at round 3339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([12.00494245, 11.86740305,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 10.407731439784726}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8009361607330994
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.94062592, 9.93351598, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9354022502128956}
episode index:3340
target Thresh 19.0
target distance 9.0
model initialize at round 3340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 7.97380118, 11.86775027,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 7.079580521924544}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8009402224180041
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.61928724, 11.48370282,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7858022020607289}
episode index:3341
target Thresh 19.0
target distance 2.0
model initialize at round 3341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.00552344, 8.13585341, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8641642423961053}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8009997854872986
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.00552344, 8.13585341, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8641642423961053}
episode index:3342
target Thresh 19.0
target distance 6.0
model initialize at round 3342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.101104333878249}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8010301475018102
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.06099201, 11.16139214,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9527766949137603}
episode index:3343
target Thresh 19.0
target distance 2.0
model initialize at round 3343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.56220381, 7.99857259, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.562205623943939}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.801089648055787
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.56220381, 7.99857259, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.562205623943939}
episode index:3344
target Thresh 19.0
target distance 5.0
model initialize at round 3344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13252043, 8.025196  , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.1471148116551912}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.801119965051884
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.93364844, 4.12354714, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8789608317493894}
episode index:3345
target Thresh 19.0
target distance 5.0
model initialize at round 3345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.13606951,  8.00046767,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.122368032025555}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8011502639266443
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.1349925 ,  4.12947573,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8809287571863207}
episode index:3346
target Thresh 19.0
target distance 11.0
model initialize at round 3346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([5.        , 9.99999332, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.81665012337006}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8011305273346809
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.04693859,  4.97485   ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9759793845667871}
episode index:3347
target Thresh 19.0
target distance 5.0
model initialize at round 3347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.8004154 ,  5.00035429,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 3.0069851811159825}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8011608049549513
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.53682274,  1.11845508,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.0321337583106605}
episode index:3348
target Thresh 19.0
target distance 7.0
model initialize at round 3348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 7.02385182, 11.86710848,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 5.0981334071450135}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8011775903222387
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.13875932, 10.79805753,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8845994990316041}
episode index:3349
target Thresh 19.0
target distance 2.0
model initialize at round 3349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.94430792,  3.01781034,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.05847064035451345}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8012369402952768
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.94430792,  3.01781034,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.05847064035451345}
episode index:3350
target Thresh 19.0
target distance 9.0
model initialize at round 3350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.86740277, 3.99505924, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.006195623018952}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8012536929242546
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.93912366, 10.08821079,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.3089357540616302}
episode index:3351
target Thresh 19.0
target distance 13.0
model initialize at round 3351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 4.97344002, 11.86005684,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 14.792083895528375}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8012125732161296
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.22293366,  1.13668707,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8916325674550343}
episode index:3352
target Thresh 19.0
target distance 13.0
model initialize at round 3352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.        , 9.99999928, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.083045677619642}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8011928533585122
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.79596492,  5.98208906,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.003059937743221}
episode index:3353
target Thresh 19.0
target distance 11.0
model initialize at round 3353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.414114271867186}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8011731452599035
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.09167491,  9.30798104,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.1419039853844661}
episode index:3354
target Thresh 19.0
target distance 3.0
model initialize at round 3354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.99999881,  9.95311642,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.4477457030453758}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8012175049781568
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.67113638, 11.31480551,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7413005781698979}
episode index:3355
target Thresh 19.0
target distance 12.0
model initialize at round 3355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 3.99505755, 11.86740305,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.726401057702596}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8012093296004815
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.11468086,  7.99264613,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.11491639722046873}
episode index:3356
target Thresh 19.0
target distance 12.0
model initialize at round 3356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4., 5., 0.]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.440306508910572}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.801189634206089
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.20267722,  8.90020226,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.2025338803496086}
episode index:3357
target Thresh 19.0
target distance 3.0
model initialize at round 3357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.99876094,  9.99997747,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0000232981745913}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8012339493835142
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.31546718, 11.62378094,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6990151634270961}
episode index:3358
target Thresh 19.0
target distance 12.0
model initialize at round 3358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.02528953, 11.86751391,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.384290535311681}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8012033162625586
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.94992126, 3.02751519, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.05713987458550104}
episode index:3359
target Thresh 19.0
target distance 11.0
model initialize at round 3359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.        , 10.99913907,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 12.041022625565274}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8011727013756035
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.89455091, 2.99869551, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.10545715642585679}
episode index:3360
target Thresh 19.0
target distance 12.0
model initialize at round 3360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.049875621120913}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8011645514904873
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.09850839, 11.51323598,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5226041314988716}
episode index:3361
target Thresh 19.0
target distance 12.0
model initialize at round 3361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([14.00490254, 11.86739657,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 12.727686499714267}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8011339663461098
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.12284724, 3.99237552, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.12308361906647344}
episode index:3362
target Thresh 19.0
target distance 12.0
model initialize at round 3362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.568784232893615}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8011033993909351
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.08704643,  7.09865379,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9055396523665181}
episode index:3363
target Thresh 19.0
target distance 4.0
model initialize at round 3363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.69822109,  6.00009501,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 2.022733437776313}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8011476611628164
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.20279247,  4.00754988,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.20293295655595925}
episode index:3364
target Thresh 19.0
target distance 14.0
model initialize at round 3364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.649110640673541}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8011067318225865
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.10828142, 8.32392845, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.1190329570402804}
episode index:3365
target Thresh 19.0
target distance 6.0
model initialize at round 3365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.9582417, 4.       , 0.       ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 4.453617693519443}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8011368545998229
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.54223633, 7.99194574, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.457834522873998}
episode index:3366
target Thresh 19.0
target distance 13.0
model initialize at round 3366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8011287298843195
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.88548243, 7.98823824, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8855605430857044}
episode index:3367
target Thresh 19.0
target distance 13.0
model initialize at round 3367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.02619883, 11.86775027,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.684885660367536}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.801120609993469
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.95446654, 8.05785097, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9562181274151118}
episode index:3368
target Thresh 19.0
target distance 14.0
model initialize at round 3368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.02619883, 11.86775027,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.973991333071407}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8011010110859687
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.99666961, 6.23314407, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7668631601819859}
episode index:3369
target Thresh 19.0
target distance 6.0
model initialize at round 3369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 5.97381058, 11.86774787,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.118639047581451}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8011310998067147
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.97391069, 10.66713187,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.33388897233425624}
episode index:3370
target Thresh 19.0
target distance 12.0
model initialize at round 3370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.034082698457306}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.801122986439077
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.23819345,  6.90274385,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7679895731927429}
episode index:3371
target Thresh 19.0
target distance 13.0
model initialize at round 3371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.648875396109553}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8010821493823895
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.92708798, 9.98660283, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.0741326313484458}
episode index:3372
target Thresh 19.0
target distance 2.0
model initialize at round 3372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.11791003, 8.99768126, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.11793282469552135}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8011411229520953
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.11791003, 8.99768126, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.11793282469552135}
episode index:3373
target Thresh 19.0
target distance 2.0
model initialize at round 3373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.92220235,  3.41799524,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.42517349255827885}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8012000615641427
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.92220235,  3.41799524,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.42517349255827885}
episode index:3374
target Thresh 19.0
target distance 2.0
model initialize at round 3374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.34574541, 8.99785352, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.3457520774075216}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8012589652496052
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.34574541, 8.99785352, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.3457520774075216}
episode index:3375
target Thresh 19.0
target distance 12.0
model initialize at round 3375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.049875621120913}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8012393659976429
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.75818124, 7.85599135, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8894928394692259}
episode index:3376
target Thresh 19.0
target distance 8.0
model initialize at round 3376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.211102550928006}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.801255988927463
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.89613876, 10.74773157,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9309694022683475}
episode index:3377
target Thresh 19.0
target distance 11.0
model initialize at round 3377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.000000000000018}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8012478554012855
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.90523097, 10.2782235 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.1577583607912392}
episode index:3378
target Thresh 19.0
target distance 3.0
model initialize at round 3378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 5.99998355, 10.99988806,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0000164571468844}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8012918779359403
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.92123139, 10.97001155,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9217193604086626}
episode index:3379
target Thresh 19.0
target distance 12.0
model initialize at round 3379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.00000000000002}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8012510875671098
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.35090465, 4.90338697, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.3639617409809148}
episode index:3380
target Thresh 19.0
target distance 10.0
model initialize at round 3380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.620595846702553}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8012206487053903
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.85503178, 7.954801  , 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9657436141169085}
episode index:3381
target Thresh 19.0
target distance 12.0
model initialize at round 3381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 6.        , 10.92840743,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.405836760443773}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8011902278441805
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.26465304,  1.99432212,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.2647139350196434}
episode index:3382
target Thresh 19.0
target distance 8.0
model initialize at round 3382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.597648464992652}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8011821257778653
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.98632192, 11.22016303,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.22058751379189379}
episode index:3383
target Thresh 19.0
target distance 10.0
model initialize at round 3383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([12.05521744, 10.04567994,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.071582491234645}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8011625955665317
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.17042315, 4.02565126, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.17234278992048313}
episode index:3384
target Thresh 19.0
target distance 5.0
model initialize at round 3384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.       ,  9.0000267,  0.       ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.6055734936343247}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8011925327613422
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.10582832,  5.03878236,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9670258462037103}
episode index:3385
target Thresh 19.0
target distance 7.0
model initialize at round 3385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.86740304, 5.99505761, 0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.0066985418937096}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8012224522732261
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.98336678, 10.13275785,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.311151847456616}
episode index:3386
target Thresh 19.0
target distance 2.0
model initialize at round 3386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.94865191,  9.00823736,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.05200461994371281}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8012811406546039
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.94865191,  9.00823736,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.05200461994371281}
episode index:3387
target Thresh 19.0
target distance 1.0
model initialize at round 3387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.34715852,  3.24521082,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9979522601016902}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8013397943911286
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.34715852,  3.24521082,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9979522601016902}
episode index:3388
target Thresh 19.0
target distance 5.0
model initialize at round 3388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.38156807, 7.99999976, 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 3.0241685842195665}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8013696439649288
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.6340295 , 11.89135339,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9635586540662312}
episode index:3389
target Thresh 19.0
target distance 3.0
model initialize at round 3389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([13.99999952,  9.1331383 ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.3234236518800235}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8014134877277709
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.67226851, 10.51753652,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8484037953009848}
episode index:3390
target Thresh 19.0
target distance 2.0
model initialize at round 3390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.99883592, 10.78904562,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.21095759477376283}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8014720505447194
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.99883592, 10.78904562,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.21095759477376283}
episode index:3391
target Thresh 19.0
target distance 11.0
model initialize at round 3391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.350086702751616}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8014524809221015
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.21777365,  7.8922176 ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.2429864349431611}
episode index:3392
target Thresh 19.0
target distance 9.0
model initialize at round 3392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.9000259,  4.       ,  0.       ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 7.057623297880201}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8014563281867869
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.4198663, 11.6657709,  0.       ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8830662530065186}
episode index:3393
target Thresh 19.0
target distance 12.0
model initialize at round 3393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.895369891272122}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.80143677472846
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.04573305, 10.00233628,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9987113740926556}
episode index:3394
target Thresh 19.0
target distance 3.0
model initialize at round 3394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.99683499,  8.99998462,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0000203865423156}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8014805341468021
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.45881408, 10.31965053,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5591840716733657}
episode index:3395
target Thresh 19.0
target distance 12.0
model initialize at round 3395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 12.31822794829574}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8014398804062668
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.68905586, 11.44403931,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8197370784229407}
episode index:3396
target Thresh 19.0
target distance 11.0
model initialize at round 3396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 5.97382552, 11.86774412,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.255084524177857}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8014437268500683
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.11758792,  7.98309914,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.3210355786648946}
episode index:3397
target Thresh 19.0
target distance 10.0
model initialize at round 3397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([12.82335957,  9.14612952,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.752980150591515}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8014133835802753
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.65539927, 2.89588485, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.35998559634524846}
episode index:3398
target Thresh 19.0
target distance 2.0
model initialize at round 3398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.39518261, 10.99114645,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6048821878780517}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8014718085924611
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.39518261, 10.99114645,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6048821878780517}
episode index:3399
target Thresh 19.0
target distance 14.0
model initialize at round 3399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 12.165525060596464}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8014522850871766
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.05098655, 10.94269131,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.07670668147806012}
episode index:3400
target Thresh 19.0
target distance 4.0
model initialize at round 3400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.99600971,  9.00037968,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.0003836614363872}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8014959627451926
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.83240581,  7.00347173,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.16763014921174169}
episode index:3401
target Thresh 19.0
target distance 12.0
model initialize at round 3401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.305877894274122}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.8014456257219649
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.21471109,  2.94533731,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.22156006782068463}
episode index:3402
target Thresh 19.0
target distance 2.0
model initialize at round 3402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.9903301 ,  5.99851775,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.009782843395406138}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8015039725848148
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.9903301 ,  5.99851775,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.009782843395406138}
episode index:3403
target Thresh 19.0
target distance 9.0
model initialize at round 3403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.98667979,  4.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 7.072963865532059}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8015077922902833
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.28034673, 11.75501592,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0430483476461836}
episode index:3404
target Thresh 19.0
target distance 2.0
model initialize at round 3404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.33043718,  4.99807453,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6695655851585437}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8015660866244124
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.33043718,  4.99807453,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6695655851585437}
episode index:3405
target Thresh 19.0
target distance 12.0
model initialize at round 3405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.02619884, 11.86775028,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.428262309182267}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8015579289176819
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.99187832, 8.13511617, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8649219627207378}
episode index:3406
target Thresh 19.0
target distance 6.0
model initialize at round 3406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 8.03253383, 10.14150022,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 4.035015664130402}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8015875567636115
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.02379499, 9.86660856, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.13549715431028295}
episode index:3407
target Thresh 19.0
target distance 11.0
model initialize at round 3407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.972155793073503}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8015680451244863
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.17228486,  8.95277073,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8290614913106136}
episode index:3408
target Thresh 19.0
target distance 6.0
model initialize at round 3408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.        ,  9.99999893,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.123105885830311}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8015976526207831
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.0061113 , 10.81412849,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.1859719497709791}
episode index:3409
target Thresh 19.0
target distance 3.0
model initialize at round 3409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.00501668, 8.00013947, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.4107695656095691}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8016411723707476
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.45968485, 6.0531038 , 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.052579006577599}
episode index:3410
target Thresh 19.0
target distance 4.0
model initialize at round 3410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.62577939,  4.00104177,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.035733093975773}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8016846666034152
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.09481055,  2.00571942,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.09498290019945721}
episode index:3411
target Thresh 19.0
target distance 3.0
model initialize at round 3411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.24971962, 3.0109545 , 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.0413399483509802}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8017281353412219
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.58067481, 1.22916751, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8775057515978495}
episode index:3412
target Thresh 19.0
target distance 13.0
model initialize at round 3412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.        , 8.99998868, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.5299586632015}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8016978420979615
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.85004674,  3.98494287,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9962923469640222}
episode index:3413
target Thresh 19.0
target distance 14.0
model initialize at round 3413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.02619884, 11.86775028,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.170371793518289}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8016783324460948
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.06783291, 10.39618598,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.4019510318453075}
episode index:3414
target Thresh 19.0
target distance 5.0
model initialize at round 3414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.3411629 ,  7.00004388,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 3.428119571167707}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8017078556283946
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.11608121,  3.08058572,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9267132626746272}
episode index:3415
target Thresh 19.0
target distance 6.0
model initialize at round 3415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.65987706, 9.00007796, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.014512090172523}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8017373615254589
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.04404831, 5.00767922, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.044712683166585256}
episode index:3416
target Thresh 19.0
target distance 2.0
model initialize at round 3416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.60599762,  4.99783206,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.39400834232759097}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8017953839540438
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.60599762,  4.99783206,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.39400834232759097}
episode index:3417
target Thresh 19.0
target distance 4.0
model initialize at round 3417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.11548126, 7.00019956, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.748128300551178}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8018387439938466
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.0101217 , 5.01169002, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.015463035126892831}
episode index:3418
target Thresh 19.0
target distance 10.0
model initialize at round 3418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 6.97380122, 11.86775026,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 8.240652762119847}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.801842449026314
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.96565869,  9.63190749,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.36969098389671845}
episode index:3419
target Thresh 19.0
target distance 7.0
model initialize at round 3419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 8.02614628, 10.13226303,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 7.183472015473603}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8018586866143179
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.03474649, 5.93932889, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.3468678874937299}
episode index:3420
target Thresh 19.0
target distance 9.0
model initialize at round 3420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.13259711,  3.99505849,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.322375023073766}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8018749147094322
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.98192171, 10.11855554,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.3195130067249488}
episode index:3421
target Thresh 19.0
target distance 5.0
model initialize at round 3421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.00004363, 6.0000025 , 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 3.000002503712324}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8019043200528836
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.03361574, 2.05550849, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9450895312326035}
episode index:3422
target Thresh 19.0
target distance 12.0
model initialize at round 3422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.407731439784726}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8018848013764512
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.98862366, 9.75275806, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.7528440197990304}
episode index:3423
target Thresh 19.0
target distance 2.0
model initialize at round 3423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.32249212,  8.00640619,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6775381635863323}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8019426621237128
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.32249212,  8.00640619,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6775381635863323}
episode index:3424
target Thresh 19.0
target distance 10.0
model initialize at round 3424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.4240423924398}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8019124123818062
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.89044937, 8.09905049, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9075854529062751}
episode index:3425
target Thresh 19.0
target distance 1.0
model initialize at round 3425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.57791471,  8.06431377,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0997700352264732}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8019702312923778
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.57791471,  8.06431377,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0997700352264732}
episode index:3426
target Thresh 19.0
target distance 13.0
model initialize at round 3426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.132598050286157}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.8019201230864927
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.82390619, 3.93708354, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.1869960165217593}
episode index:3427
target Thresh 19.0
target distance 4.0
model initialize at round 3427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 3.995061  , 11.86740247,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.1845291138995506}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8019633202501197
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.97216619, 10.36074724,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.63985843416697}
episode index:3428
target Thresh 19.0
target distance 7.0
model initialize at round 3428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.        , 10.99999928,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.000000000000071}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8019794799700818
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.05084532, 11.43580991,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0444256288746812}
episode index:3429
target Thresh 19.0
target distance 9.0
model initialize at round 3429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([12.14612986,  9.82335939,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 8.721081764418155}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.801983132089624
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.83735077,  1.9008355 ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.19049505902869365}
episode index:3430
target Thresh 19.0
target distance 3.0
model initialize at round 3430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.28679729,  5.99986827,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.0404403647547382}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8020262731178694
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.36765051,  7.71948469,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9578747796725252}
episode index:3431
target Thresh 19.0
target distance 5.0
model initialize at round 3431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.13568401,  3.9999994 ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 3.0030673864837314}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8020555486793154
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.22381568,  7.82644415,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8562145699867016}
episode index:3432
target Thresh 19.0
target distance 2.0
model initialize at round 3432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.95107555, 9.44432902, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.5578206124781113}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8021132080009934
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.95107555, 9.44432902, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.5578206124781113}
episode index:3433
target Thresh 19.0
target distance 1.0
model initialize at round 3433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.3130883 ,  5.42716849,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8944180382368306}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8021708337412377
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.3130883 ,  5.42716849,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8944180382368306}
episode index:3434
target Thresh 19.0
target distance 4.0
model initialize at round 3434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.17739499,  7.0000962 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.162652035267835}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8022138698886201
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.23661602,  5.00483835,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.23666547776537422}
episode index:3435
target Thresh 19.0
target distance 9.0
model initialize at round 3435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.86401801,  3.9817937 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 7.0711913242814495}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8022174474148457
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.42310016, 11.7044241 ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8217220124342082}
episode index:3436
target Thresh 19.0
target distance 6.0
model initialize at round 3436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.25748944,  9.00005543,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.3631166043921334}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8022466247650305
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.75574897,  5.0070895 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.24435389146564734}
episode index:3437
target Thresh 19.0
target distance 6.0
model initialize at round 3437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.1325968 ,  7.99505777,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.109602151288248}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8022757851417714
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.90125105, 11.86781505,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.2511420468731904}
episode index:3438
target Thresh 19.0
target distance 5.0
model initialize at round 3438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.00016823,  9.00007773,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.605648134006405}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.802304928559875
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.07303245, 10.95114015,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9282543381069279}
episode index:3439
target Thresh 19.0
target distance 3.0
model initialize at round 3439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.76722591,  5.00098193,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0276909052513874}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8023478631736657
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.44972776,  3.04720294,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.053602052715595}
episode index:3440
target Thresh 19.0
target distance 2.0
model initialize at round 3440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.00112534, 9.00931323, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.009380967659182797}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8024053034924179
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.00112534, 9.00931323, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.009380967659182797}
episode index:3441
target Thresh 19.0
target distance 4.0
model initialize at round 3441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 7.9760478 , 11.86713771,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 2.201887897537354}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8024481839969233
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.97571942, 10.37563361,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6248383296403656}
episode index:3442
target Thresh 19.0
target distance 2.0
model initialize at round 3442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.54505074,  3.99818325,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.454950394016625}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8024910395926256
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.23346114,  3.25917107,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7767442372678315}
episode index:3443
target Thresh 19.0
target distance 2.0
model initialize at round 3443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.31064185, 4.01853339, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6896072381286915}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8025483883035452
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.31064185, 4.01853339, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6896072381286915}
episode index:3444
target Thresh 19.0
target distance 12.0
model initialize at round 3444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 10.440306508910563}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8025400378098432
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.92723397, 7.47734508, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.5276959935333061}
episode index:3445
target Thresh 19.0
target distance 11.0
model initialize at round 3445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.        , 10.99999964,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.295629967307592}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8025316921626261
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.8929357 , 6.98557581, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9913740143184768}
episode index:3446
target Thresh 19.0
target distance 13.0
model initialize at round 3446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4., 4., 0.]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.18033988749897}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8024913349648095
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.04162973,  5.89860053,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.10961243603642304}
episode index:3447
target Thresh 19.0
target distance 13.0
model initialize at round 3447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.827730043456995}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8024611278769699
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.55271586, 3.83737665, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.47593008968880857}
episode index:3448
target Thresh 19.0
target distance 5.0
model initialize at round 3448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([12.00000036, 10.99997151,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 3.0000003577631227}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8024901330587974
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.04870763, 10.89964232,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9565713960379127}
episode index:3449
target Thresh 19.0
target distance 13.0
model initialize at round 3449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.52844675707443}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.8024402082114541
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.20345551,  2.92057388,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9427886361479628}
episode index:3450
target Thresh 19.0
target distance 13.0
model initialize at round 3450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.00493905, 11.8674025 ,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 14.132922927274727}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8024100421980904
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.10223831, 3.01870204, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.10393477648656943}
episode index:3451
target Thresh 19.0
target distance 12.0
model initialize at round 3451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.182309676547058}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8023905409954332
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.8512446 , 7.08638845, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9256426050194932}
episode index:3452
target Thresh 19.0
target distance 4.0
model initialize at round 3452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.99998069, 8.        , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.000000000093256}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8024332891735404
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.55061388, 9.99238241, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.5506665712954428}
episode index:3453
target Thresh 19.0
target distance 14.0
model initialize at round 3453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.        , 7.99999988, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.999999954150285}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8023930422546393
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.03687278,  2.40108209,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.6000518896817738}
episode index:3454
target Thresh 19.0
target distance 7.0
model initialize at round 3454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.86683092, 5.9979587 , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 5.765339252580739}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8024089559906004
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.08939224, 11.86757793,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8721711022777808}
episode index:3455
target Thresh 19.0
target distance 6.0
model initialize at round 3455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.95775415,  5.        ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 4.000223082719865}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8024379175195383
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.05272426,  8.99714077,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.947280054276726}
episode index:3456
target Thresh 19.0
target distance 4.0
model initialize at round 3456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.13701046,  8.9999969 ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.178247771105915}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8024806025303801
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.68998492, 10.98502409,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6901474221604108}
episode index:3457
target Thresh 19.0
target distance 8.0
model initialize at round 3457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.00483096,  9.00000727,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 6.000009216609045}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8024964771392493
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.01719585,  3.03392896,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.03803776475868531}
episode index:3458
target Thresh 19.0
target distance 2.0
model initialize at round 3458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.9960196 , 10.99779809,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.004548845830209836}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8025535755847135
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.9960196 , 10.99779809,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.004548845830209836}
episode index:3459
target Thresh 19.0
target distance 7.0
model initialize at round 3459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.13224973,  5.97380117,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.506778194434803}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8025694199270301
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.96888222, 11.86666684,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.2999400644457506}
episode index:3460
target Thresh 19.0
target distance 6.0
model initialize at round 3460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.4721359549996045}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8025982932526796
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.00055099, 10.62216321,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.377837195401607}
episode index:3461
target Thresh 19.0
target distance 7.0
model initialize at round 3461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([10.97520927, 10.13262174,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 5.150859528472709}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8026271498981873
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.14661188,  9.53529192,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0073771520702395}
episode index:3462
target Thresh 19.0
target distance 4.0
model initialize at round 3462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.13615443,  3.9818187 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.1952869814697875}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8026697063088433
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.75475793,  5.97941338,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7550386403259248}
episode index:3463
target Thresh 19.0
target distance 11.0
model initialize at round 3463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([5.01813026, 8.77612986, 0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.064562135553016}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8026295073264472
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.89474784,  1.30390295,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7040093170754191}
episode index:3464
target Thresh 19.0
target distance 2.0
model initialize at round 3464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.0171442 ,  8.26417804,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7360216599999916}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8026864685075941
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.0171442 ,  8.26417804,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.7360216599999916}
episode index:3465
target Thresh 19.0
target distance 4.0
model initialize at round 3465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.75340431, 5.99999895, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.6597802331804488}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8027289709690748
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.01785465, 7.99927975, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.017869173763875246}
episode index:3466
target Thresh 19.0
target distance 5.0
model initialize at round 3466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.86725278,  7.99594219,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 3.1267380267050107}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8027577483065513
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.47826012, 11.58595193,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.7563546828519706}
episode index:3467
target Thresh 19.0
target distance 12.0
model initialize at round 3467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([12.05521678, 10.045681  ,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 10.100401435738299}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8027493928247731
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.07136093, 10.46592199,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.5388243746993011}
episode index:3468
target Thresh 19.0
target distance 2.0
model initialize at round 3468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.27184402,  7.99359489,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7281841474057872}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8028062537665935
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.27184402,  7.99359489,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7281841474057872}
episode index:3469
target Thresh 19.0
target distance 1.0
model initialize at round 3469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.56825447, 5.13323677, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9683401809367416}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8028630819355369
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.56825447, 5.13323677, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9683401809367416}
episode index:3470
target Thresh 19.0
target distance 11.0
model initialize at round 3470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.186646103472004}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8028329679090771
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.14508017,  5.96076169,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.2860602380369754}
episode index:3471
target Thresh 19.0
target distance 1.0
model initialize at round 3471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.36470199,  2.71993044,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.4598331188095804}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8028897556487347
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.36470199,  2.71993044,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.4598331188095804}
episode index:3472
target Thresh 19.0
target distance 8.0
model initialize at round 3472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 6.0000000000000195}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8029054438849429
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.03189557, 10.94259915,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.0656672273091737}
episode index:3473
target Thresh 19.0
target distance 11.0
model initialize at round 3473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.99983213, 9.00007785, 0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.219725217502885}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8028970603194895
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.20066536,  7.02330298,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.20201389744190432}
episode index:3474
target Thresh 19.0
target distance 12.0
model initialize at round 3474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.049875621120911}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8028886815791098
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.99094839, 9.70820017, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.29194018137942934}
episode index:3475
target Thresh 19.0
target distance 6.0
model initialize at round 3475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.167361955705681}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8029173384601285
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.10377754, 11.67984986,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.124904677150776}
episode index:3476
target Thresh 19.0
target distance 8.0
model initialize at round 3476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.13276249,  4.99602738,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.0054402945261005}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8029330007153888
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.26345427, 11.09432829,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.2798320547495428}
episode index:3477
target Thresh 19.0
target distance 6.0
model initialize at round 3477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.86739542, 3.99510452, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.09775092481822}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8029616283747576
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.65904271, 7.96709245, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.6598637709093527}
episode index:3478
target Thresh 19.0
target distance 2.0
model initialize at round 3478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.00018167, 8.00172782, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.001737344472707436}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8030182648713443
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.00018167, 8.00172782, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.001737344472707436}
episode index:3479
target Thresh 19.0
target distance 13.0
model initialize at round 3479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.02619884, 11.86775028,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.060291649189555}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.802998745798285
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.17078229, 10.46085461,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.989080260706047}
episode index:3480
target Thresh 19.0
target distance 1.0
model initialize at round 3480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.03157324,  2.03851974,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9619985238396561}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8030553390916495
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.03157324,  2.03851974,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9619985238396561}
episode index:3481
target Thresh 19.0
target distance 4.0
model initialize at round 3481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.71736288,  4.00082922,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.0206932741216246}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8030975403153451
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.80937876,  2.01220274,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.19101142565398388}
episode index:3482
target Thresh 19.0
target distance 5.0
model initialize at round 3482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 6.97387476, 11.86773161,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.1480775304568076}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8031260796376778
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.44536647, 11.90324715,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0070783048237706}
episode index:3483
target Thresh 19.0
target distance 3.0
model initialize at round 3483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.13261759, 8.00496819, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.327521487382758}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8031682363312376
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.13578103, 6.02602473, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9833942824595273}
episode index:3484
target Thresh 19.0
target distance 11.0
model initialize at round 3484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 5.        , 10.99999845,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.29562938837614}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8031598038208124
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.95371938,  6.98472818,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9858151341985096}
episode index:3485
target Thresh 19.0
target distance 4.0
model initialize at round 3485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.15147963,  8.00035836,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.1728829663333493}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8032019266539102
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.84041972,  6.00213788,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.1595945973479607}
episode index:3486
target Thresh 19.0
target distance 7.0
model initialize at round 3486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.079550858512994}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8032174623789881
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.482745  , 11.90470577,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0254439416116985}
episode index:3487
target Thresh 19.0
target distance 6.0
model initialize at round 3487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.00001097, 7.00000024, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 4.000000238433653}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8032459264092693
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.01582589, 3.02152839, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.026719475904350587}
episode index:3488
target Thresh 19.0
target distance 4.0
model initialize at round 3488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.        , 10.28679317,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 2.12336148099966}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8032879883392179
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.95800996, 11.47436327,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.47621809403225746}
episode index:3489
target Thresh 19.0
target distance 13.0
model initialize at round 3489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.704699910719649}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8032579165076289
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.80354906, 7.90936055, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8086449161972746}
episode index:3490
target Thresh 19.0
target distance 6.0
model initialize at round 3490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.000000000000019}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8032863444891506
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.96939636, 10.53804481,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.46296779735446003}
episode index:3491
target Thresh 19.0
target distance 6.0
model initialize at round 3491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 8.0256544 , 10.13238982,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 5.100760662225027}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.803314756188896
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.84175303, 7.93976807, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9529984899686407}
episode index:3492
target Thresh 19.0
target distance 12.0
model initialize at round 3492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 6., 11.,  0.]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 10.198039027185583}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8033063010446965
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.0465901 ,  9.48020467,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.48245949729255344}
episode index:3493
target Thresh 19.0
target distance 9.0
model initialize at round 3493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.39879537,  4.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 7.0113506362306905}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8033095065252217
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.61578386, 11.74221083,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9643996459114061}
episode index:3494
target Thresh 19.0
target distance 2.0
model initialize at round 3494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.88901854, 4.99612963, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8890269604538442}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8033657842057581
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.88901854, 4.99612963, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8890269604538442}
episode index:3495
target Thresh 19.0
target distance 12.0
model initialize at round 3495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4., 7., 0.]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.440306508910561}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8033357417320418
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.94108528,  4.85944318,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8614601126907842}
episode index:3496
target Thresh 19.0
target distance 12.0
model initialize at round 3496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.82024025163806}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.8032862460694717
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.93090376,  2.21451234,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.22536600165064843}
episode index:3497
target Thresh 19.0
target distance 3.0
model initialize at round 3497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 9.99999726, 11.17179094,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0146514729585505}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8033281882518417
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.73158765, 11.34022126,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8068277327444857}
episode index:3498
target Thresh 19.0
target distance 5.0
model initialize at round 3498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.86739469, 4.99510897, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.1275779240057524}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8033565311531702
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.07312701, 8.87807759, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8811173715663221}
episode index:3499
target Thresh 19.0
target distance 12.0
model initialize at round 3499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([12.02619884, 11.86775028,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 10.746355444500663}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.803348080983555
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.98251406, 8.02966709, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.034436818294470234}
episode index:3500
target Thresh 19.0
target distance 11.0
model initialize at round 3500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([5.        , 9.99528229, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 12.038460820828494}
done in step count: 11
reward sum = 0.5688000922764597
running average episode reward sum: 0.8032810864138015
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.64201464,  2.0386084 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.36006128280470023}
episode index:3501
target Thresh 19.0
target distance 9.0
model initialize at round 3501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14., 11.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.0000000000000195}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8032842917717643
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.16100775, 11.56827147,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0133313685439325}
episode index:3502
target Thresh 19.0
target distance 7.0
model initialize at round 3502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([10.97405537, 10.13231433,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 5.459568098306522}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8033126148400568
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.17658129,  7.39326439,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0228130198007344}
episode index:3503
target Thresh 19.0
target distance 11.0
model initialize at round 3503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4.8674032 , 7.99505776, 0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 9.350086506959181}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8032931454552922
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.95586952, 10.65148967,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6529826122126794}
episode index:3504
target Thresh 19.0
target distance 14.0
model initialize at round 3504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.       , 10.9999491,  0.       ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 14.999969458635261}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.803253238832135
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.88667633, 2.04815013, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.12312874563343941}
episode index:3505
target Thresh 19.0
target distance 13.0
model initialize at round 3505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.083045973594581}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.803233797489235
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.74763749, 5.98790681, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0196306738895635}
episode index:3506
target Thresh 19.0
target distance 1.0
model initialize at round 3506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.63009081,  2.55955338,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.5722280014295225}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8032756469909489
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.80055136,  4.55578588,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.5904893783975935}
episode index:3507
target Thresh 19.0
target distance 4.0
model initialize at round 3507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.17846607, 5.00014722, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.16229205222115}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8033174726331977
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.08711605, 3.00575709, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.08730607022762744}
episode index:3508
target Thresh 19.0
target distance 12.0
model initialize at round 3508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 3.97380117, 11.86775027,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 10.42826230400414}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8033090552678135
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.04444695,  8.11065154,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8904584317369517}
episode index:3509
target Thresh 19.0
target distance 14.0
model initialize at round 3509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4., 6., 0.]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.041594578792312}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8032791487837182
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.98512219,  5.84020246,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8403341751081964}
episode index:3510
target Thresh 19.0
target distance 2.0
model initialize at round 3510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.26178563, 10.98923147,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7382929111608613}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8033351786473515
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.26178563, 10.98923147,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7382929111608613}
episode index:3511
target Thresh 19.0
target distance 10.0
model initialize at round 3511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.05180052677325}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8033383594763243
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.06432601, 11.19266496,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.20311972179939985}
episode index:3512
target Thresh 19.0
target distance 11.0
model initialize at round 3512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([12.82335949,  9.14612965,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.147656592966651}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8033084701898503
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.93532779, 2.86706473, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.869473254678666}
episode index:3513
target Thresh 19.0
target distance 3.0
model initialize at round 3513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 9.00011945, 10.77886496,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0242751649132809}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8033502150759658
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.04446125, 11.60999776,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.1336452569575375}
episode index:3514
target Thresh 19.0
target distance 10.0
model initialize at round 3514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([11.02618605, 11.86774704,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.563409101399241}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8033418027637109
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.51930446, 4.99153762, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.4807700212199797}
episode index:3515
target Thresh 19.0
target distance 1.0
model initialize at round 3515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.87405956,  6.0194546 ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9886001615915994}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8033977351292503
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.87405956,  6.0194546 ,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9886001615915994}
episode index:3516
target Thresh 19.0
target distance 9.0
model initialize at round 3516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([ 8.02619876, 10.13224974,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.074346388360844}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8033893140892646
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.20028978, 1.94432486, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.20788389959593254}
episode index:3517
target Thresh 19.0
target distance 2.0
model initialize at round 3517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.0204256 , 11.86704375,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8672843102836453}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8034452011517748
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.0204256 , 11.86704375,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8672843102836453}
episode index:3518
target Thresh 19.0
target distance 9.0
model initialize at round 3518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.0000000000000195}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8034483443881624
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.5552285 , 10.86127552,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.46590360914158874}
episode index:3519
target Thresh 19.0
target distance 12.0
model initialize at round 3519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.568784232893615}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8034289249410707
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.06368417,  9.56911072,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.4355700257951346}
episode index:3520
target Thresh 19.0
target distance 9.0
model initialize at round 3520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([12.00494245, 11.86740305,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.058442237262242}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.803432071014646
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.51678626, 10.60970261,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.6211501987516943}
episode index:3521
target Thresh 19.0
target distance 6.0
model initialize at round 3521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.523056363717578}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8034473870081114
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.06093962, 11.24665957,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9709146873753366}
episode index:3522
target Thresh 19.0
target distance 10.0
model initialize at round 3522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([12.02619884, 11.86775028,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.072970849349378}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8034505260552279
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.11715485, 11.49790642,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5115037263860643}
episode index:3523
target Thresh 19.0
target distance 13.0
model initialize at round 3523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.132598050286157}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8034107899329901
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13379433, 4.93679544, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8685085387177163}
episode index:3524
target Thresh 19.0
target distance 12.0
model initialize at round 3524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 5.97380485, 11.86774934,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.145383479261769}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8034023843010942
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.01111463,  6.04371976,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9563448311293768}
episode index:3525
target Thresh 19.0
target distance 5.0
model initialize at round 3525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.86681575,  5.99801734,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.1246230905455046}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8034304891268739
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.1620546 ,  9.49522883,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5210693691193151}
episode index:3526
target Thresh 19.0
target distance 4.0
model initialize at round 3526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.63917117, 5.99999867, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.585901669627794}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8034720455518449
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.0194434 , 7.99913346, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.019462704123943405}
episode index:3527
target Thresh 19.0
target distance 3.0
model initialize at round 3527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.86435184, 6.98163994, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.3357250178430542}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8035135784187519
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.29754305, 8.87985939, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9288080621488328}
episode index:3528
target Thresh 19.0
target distance 3.0
model initialize at round 3528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.59561491,  9.00169027,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0802364069746082}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8035550877476216
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.26632059,  7.02615976,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.009599653913295}
episode index:3529
target Thresh 19.0
target distance 13.0
model initialize at round 3529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.99983213, 9.00007749, 0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.53014856676563}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8035252810077763
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.31114664,  3.01984383,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.3117787812985057}
episode index:3530
target Thresh 19.0
target distance 2.0
model initialize at round 3530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.01129603, 10.1284771 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8715961057769875}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.803580923805565
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.01129603, 10.1284771 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8715961057769875}
episode index:3531
target Thresh 19.0
target distance 3.0
model initialize at round 3531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.00003362, 9.00802851, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0000658438811953}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8036223788101501
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.22351397, 8.83400156, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7940315087369486}
episode index:3532
target Thresh 19.0
target distance 11.0
model initialize at round 3532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.132283595738201}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8035925783338645
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.67853995, 5.91254523, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.3331439618733322}
episode index:3533
target Thresh 19.0
target distance 11.0
model initialize at round 3533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.614260567258981}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8035841426686597
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.80637288, 9.0438318 , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.2507976833613934}
episode index:3534
target Thresh 19.0
target distance 12.0
model initialize at round 3534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.214321868139345}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8035444923966995
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.9338558 , 9.71855314, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7215910688253584}
episode index:3535
target Thresh 19.0
target distance 1.0
model initialize at round 3535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.8545721 ,  3.19326234,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8197407685675386}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8036000510809764
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.8545721 ,  3.19326234,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8197407685675386}
episode index:3536
target Thresh 19.0
target distance 13.0
model initialize at round 3536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.17786313458049}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8035604187315866
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.80661067,  7.72563532,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7509634158847}
episode index:3537
target Thresh 19.0
target distance 13.0
model initialize at round 3537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 12.529964086141694}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8035306778829042
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.02594433, 11.71540184,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7158721248496681}
episode index:3538
target Thresh 19.0
target distance 11.0
model initialize at round 3538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([12.05521678, 10.045681  ,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 9.55369685540216}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8035222716267915
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.62691238, 6.09242265, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9812701073946534}
episode index:3539
target Thresh 19.0
target distance 10.0
model initialize at round 3539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 10.109325747416417}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8035029410106892
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.01415704, 10.66185224,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.3384439798262574}
episode index:3540
target Thresh 19.0
target distance 5.0
model initialize at round 3540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.94903076, 6.        , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.5775299986916576}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8035308983840271
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.09744034, 9.90260077, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9078451199972072}
episode index:3541
target Thresh 19.0
target distance 4.0
model initialize at round 3541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.14219492,  5.96810235,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.2055470077064347}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8035722504736984
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.68783345,  7.96582884,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.6886817299190428}
episode index:3542
target Thresh 19.0
target distance 10.0
model initialize at round 3542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.43398113205663}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8035638419744114
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.93923953, 11.25766476,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.2647318704574988}
episode index:3543
target Thresh 19.0
target distance 5.0
model initialize at round 3543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13260965, 5.00487182, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 3.1275582572702696}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8035917584975564
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.43089866, 1.21755359, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9675219532907156}
episode index:3544
target Thresh 19.0
target distance 6.0
model initialize at round 3544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.86666633, 5.97753764, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.434483810037768}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8036196592708998
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.93328768, 9.9744381 , 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.0714418933785948}
episode index:3545
target Thresh 19.0
target distance 11.0
model initialize at round 3545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([5.        , 9.99998581, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.816645957473925}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8036003338990313
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.99369788,  4.9882334 ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9882534929932286}
episode index:3546
target Thresh 19.0
target distance 3.0
model initialize at round 3546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.28052574, 3.99991202, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0386869854177974}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8036416081212193
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.11567011, 5.80208051, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8103781359666685}
episode index:3547
target Thresh 19.0
target distance 2.0
model initialize at round 3547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.88449311, 3.02895522, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.11908083757890477}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8036969515236654
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.88449311, 3.02895522, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.11908083757890477}
episode index:3548
target Thresh 19.0
target distance 3.0
model initialize at round 3548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.82268184,  5.00096226,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0165466880935143}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8037381752623175
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.2716248 ,  3.05725217,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.981098113037006}
episode index:3549
target Thresh 19.0
target distance 5.0
model initialize at round 3549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.13902316,  6.02284657,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 3.1430689654465467}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.803765995494638
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.68023374,  2.15086143,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9073515103763612}
episode index:3550
target Thresh 19.0
target distance 13.0
model initialize at round 3550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.401754250991392}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8037575513780526
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.11211531,  7.33309562,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.1104506605937814}
episode index:3551
target Thresh 19.0
target distance 14.0
model initialize at round 3551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.02619884, 11.86775028,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.97399133820345}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8037382198294172
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.01577564, 6.24020746, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7599562966039847}
episode index:3552
target Thresh 19.0
target distance 5.0
model initialize at round 3552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 4.97380368, 11.86774964,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.1481508250379067}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8037660165589895
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.88430701, 11.06505398,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8866966259815978}
episode index:3553
target Thresh 19.0
target distance 12.0
model initialize at round 3553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.32905200177065}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8037466935072355
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.77841539, 7.88598639, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.786720803219574}
episode index:3554
target Thresh 19.0
target distance 11.0
model initialize at round 3554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.800295646565443}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.803738264321298
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.96864743,  7.11282324,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8877305803740758}
episode index:3555
target Thresh 19.0
target distance 7.0
model initialize at round 3555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.8309518948453265}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8037533477677768
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.08913411, 11.09356319,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9156586407578758}
episode index:3556
target Thresh 19.0
target distance 12.0
model initialize at round 3556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 5.94478322, 10.045681  ,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 10.83858473947639}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8037449214505804
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.9408219 ,  6.01877671,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.06208551970599749}
episode index:3557
target Thresh 19.0
target distance 7.0
model initialize at round 3557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.506778200582048}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8037599945474183
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.02586307, 11.86719695,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.3042136759344571}
episode index:3558
target Thresh 19.0
target distance 9.0
model initialize at round 3558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.93390071,  9.00000405,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 7.062027137238979}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8037630140066632
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.08283848,  1.155558  ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8484954319741443}
episode index:3559
target Thresh 19.0
target distance 3.0
model initialize at round 3559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([12.9234055,  9.0224456,  0.       ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.4541899160672378}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8038040918117174
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.51563289, 10.80664146,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.9573649909547649}
episode index:3560
target Thresh 19.0
target distance 1.0
model initialize at round 3560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.79280634, 1.17259449, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8529531557520712}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.803859187545553
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.79280634, 1.17259449, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8529531557520712}
episode index:3561
target Thresh 19.0
target distance 4.0
model initialize at round 3561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.13260704, 6.00489105, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 2.1844813298186767}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8039002152862756
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.12777927, 4.00592942, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.12791676532562604}
episode index:3562
target Thresh 19.0
target distance 11.0
model initialize at round 3562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.347972064798812}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.8038426336763829
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.7052051 ,  2.27870255,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.4056835499466027}
episode index:3563
target Thresh 19.0
target distance 3.0
model initialize at round 3563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.14149668,  4.96744479,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.3428321614138723}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8038836430384265
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.72404583,  6.83012655,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1015227888340977}
episode index:3564
target Thresh 19.0
target distance 4.0
model initialize at round 3564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 7.00002491, 10.70578972,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.0215487506751106}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8039246293938154
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.04309773, 11.11181444,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.11983273154713105}
episode index:3565
target Thresh 19.0
target distance 13.0
model initialize at round 3565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.045361017187277}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8039161763113998
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.09742065,  9.6108433 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0898527495004398}
episode index:3566
target Thresh 19.0
target distance 2.0
model initialize at round 3566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.95258451,  4.00417018,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.04759852099666993}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8039711479468606
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.95258451,  4.00417018,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.04759852099666993}
episode index:3567
target Thresh 19.0
target distance 1.0
model initialize at round 3567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.26967866,  2.16346802,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.1104751262720198}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8040260887686244
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.26967866,  2.16346802,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.1104751262720198}
episode index:3568
target Thresh 19.0
target distance 1.0
model initialize at round 3568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.82916281, 1.12832946, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.888253836630557}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8040809988025922
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.82916281, 1.12832946, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.888253836630557}
episode index:3569
target Thresh 19.0
target distance 12.0
model initialize at round 3569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.049875621120913}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8040725113904627
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.07078501, 11.5887207 ,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.592960859310257}
episode index:3570
target Thresh 19.0
target distance 6.0
model initialize at round 3570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([12.14421603,  9.8251211 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.9430818940683596}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8041000743948339
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.15092791,  9.36998249,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.3995826326933941}
episode index:3571
target Thresh 19.0
target distance 11.0
model initialize at round 3571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([12.02619882, 11.86775027,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.255108868461624}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8040915863945833
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.46802784, 6.09298088, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0515122749793613}
episode index:3572
target Thresh 19.0
target distance 9.0
model initialize at round 3572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.00069857,  4.        ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.071166637894263}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8041064991887634
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.0445304 , 10.13116055,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.2914349209422913}
episode index:3573
target Thresh 19.0
target distance 12.0
model initialize at round 3573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 11.180339887498974}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8040769051196265
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.88009663, 8.24114771, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7682666295994338}
episode index:3574
target Thresh 19.0
target distance 2.0
model initialize at round 3574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.14122409,  5.96685065,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8594154667041832}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8041317087825302
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.14122409,  5.96685065,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8594154667041832}
episode index:3575
target Thresh 19.0
target distance 1.0
model initialize at round 3575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.19684994,  2.10353625,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.22241774758278063}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8041864817946155
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.19684994,  2.10353625,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.22241774758278063}
episode index:3576
target Thresh 19.0
target distance 7.0
model initialize at round 3576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 9.02400365, 11.86715346,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 5.359745773311127}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8042139667032556
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.89535913, 9.13597309, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.244271055456494}
episode index:3577
target Thresh 19.0
target distance 2.0
model initialize at round 3577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.33393151, 3.01172245, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.33413719669318764}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8042686861088724
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.33393151, 3.01172245, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.33413719669318764}
episode index:3578
target Thresh 19.0
target distance 2.0
model initialize at round 3578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.99989152, 9.9999218 , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.00013372902144226408}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8043233749364475
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.99989152, 9.9999218 , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.00013372902144226408}
episode index:3579
target Thresh 19.0
target distance 13.0
model initialize at round 3579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.704699910719636}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8043148435293422
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.87138176, 6.99510427, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8713955150763063}
episode index:3580
target Thresh 19.0
target distance 6.0
model initialize at round 3580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 3.97380166, 11.86775015,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.11864824970311}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8043422618919422
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.94318288, 10.66759159,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.33722920573868514}
episode index:3581
target Thresh 19.0
target distance 2.0
model initialize at round 3581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.10556898,  8.99195445,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.10587512003202093}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8043968843760595
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.10556898,  8.99195445,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.10587512003202093}
episode index:3582
target Thresh 19.0
target distance 11.0
model initialize at round 3582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.132283595738201}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8043672835978618
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.82348037,  5.90596726,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.20000334158681535}
episode index:3583
target Thresh 19.0
target distance 13.0
model initialize at round 3583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8043587494611157
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.07091617,  9.62693536,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.1208232260232742}
episode index:3584
target Thresh 19.0
target distance 6.0
model initialize at round 3584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.98964107,  7.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 4.120605471063096}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8043861249842786
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.80010939, 10.99092495,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8001608505346499}
episode index:3585
target Thresh 19.0
target distance 3.0
model initialize at round 3585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.04522778,  9.0560645 ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.423679113323944}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8044267311959394
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.41606645,  7.20385715,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8983065926754447}
episode index:3586
target Thresh 19.0
target distance 1.0
model initialize at round 3586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.48284937, 1.36024099, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8015204944087002}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8044812539918145
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.48284937, 1.36024099, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8015204944087002}
episode index:3587
target Thresh 19.0
target distance 3.0
model initialize at round 3587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.00331891,  9.99995494,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0000505684160963}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8045218110559194
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.20401807, 11.63803365,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0201343907269134}
episode index:3588
target Thresh 19.0
target distance 4.0
model initialize at round 3588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.01943183,  8.00036228,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.000456656707161}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.804562345519264
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.3283445 ,  6.00120945,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6716565866949611}
episode index:3589
target Thresh 19.0
target distance 13.0
model initialize at round 3589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.180339887498965}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.804553771310902
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.8645172 , 7.96508012, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.2956734250480884}
episode index:3590
target Thresh 19.0
target distance 12.0
model initialize at round 3590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.00016787,  9.00007785,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.770514389973467}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8045344279857318
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.4867631 , 5.02115989, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5136729044479423}
episode index:3591
target Thresh 19.0
target distance 8.0
model initialize at round 3591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 8.583011083224646}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8045258663235698
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.03763156, 11.75242401,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7533644712379629}
episode index:3592
target Thresh 19.0
target distance 13.0
model initialize at round 3592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.00494245, 11.86740305,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.03907362243233}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8045173094271536
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.96669139, 11.12478015,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9747114062723568}
episode index:3593
target Thresh 19.0
target distance 8.0
model initialize at round 3593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.000000000000018}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8045320166309857
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.03238849, 11.26615741,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.2681208315398891}
episode index:3594
target Thresh 19.0
target distance 7.0
model initialize at round 3594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([2.0040611 , 7.00000048, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.383658314702006}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8045467156527852
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.79625952, 1.12401294, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8993683961736608}
episode index:3595
target Thresh 19.0
target distance 11.0
model initialize at round 3595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 6.        , 10.99997032,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.816637361191509}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8045381600971253
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.73294474,  5.97411921,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.010062740666656}
episode index:3596
target Thresh 19.0
target distance 3.0
model initialize at round 3596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.74433496,  7.99985352,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.2467267207861228}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8045785998635704
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.95825683,  9.79011881,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7912207211182597}
episode index:3597
target Thresh 19.0
target distance 14.0
model initialize at round 3597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.000000000000027}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8045393674654118
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.88440439, 9.76250355, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7712159326501675}
episode index:3598
target Thresh 19.0
target distance 12.0
model initialize at round 3598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4., 4., 0.]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.440306508910572}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8045098586931495
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.20049621,  7.90068355,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.922729425857314}
episode index:3599
target Thresh 19.0
target distance 3.0
model initialize at round 3599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.86728095, 4.99577908, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.326889553339141}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8045502726212903
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.78925391, 6.97443205, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.2539695155196269}
episode index:3600
target Thresh 19.0
target distance 5.0
model initialize at round 3600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.29073812,  5.00007028,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 3.082770522843192}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.804577473323145
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.92574561,  1.19088443,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8125156773526426}
episode index:3601
target Thresh 19.0
target distance 4.0
model initialize at round 3601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.82344941,  9.14603217,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.0286117515114763}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8046178460401568
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.1579365 , 11.18075187,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8612445486743378}
episode index:3602
target Thresh 19.0
target distance 9.0
model initialize at round 3602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.99997795,  9.00000083,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 7.000000834499771}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.804620590531958
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.96754936,  1.16333452,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8372945522184426}
episode index:3603
target Thresh 19.0
target distance 10.0
model initialize at round 3603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.051800526773253}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8046233335007338
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.93972822, 10.00294177,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9988782685316069}
episode index:3604
target Thresh 19.0
target distance 14.0
model initialize at round 3604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 13.000000000000027}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8045938505499967
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.812881  , 10.79695723,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.2761157137089497}
episode index:3605
target Thresh 19.0
target distance 9.0
model initialize at round 3605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 9.02619819, 10.13224989,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.121687254664064}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8045853056489845
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.85085172, 2.97501431, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.986355981418206}
episode index:3606
target Thresh 19.0
target distance 13.0
model initialize at round 3606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.401754250991393}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8045767654859267
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.88909072, 7.98973465, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.330434886743676}
episode index:3607
target Thresh 19.0
target distance 14.0
model initialize at round 3607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4., 9., 0.]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.16552506059646}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8045575069285927
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.89247298, 11.50575096,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.5170552129180729}
episode index:3608
target Thresh 19.0
target distance 13.0
model initialize at round 3608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.18033988749897}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.804528074894557
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.8903738 , 6.88523298, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.15871159944222488}
episode index:3609
target Thresh 19.0
target distance 9.0
model initialize at round 3609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.323801432949267}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8045195576819824
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.96078161, 10.94883557,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9621429741569119}
episode index:3610
target Thresh 19.0
target distance 5.0
model initialize at round 3610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 7.01940279, 10.13768318,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.022540300497085}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.804559845813336
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.89619392, 9.16063924, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.227880295320377}
episode index:3611
target Thresh 19.0
target distance 10.0
model initialize at round 3611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.13224972,  3.97380116,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.522389298803166}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8045304375769794
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.07264546, 11.41202804,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.41838315859815056}
episode index:3612
target Thresh 19.0
target distance 6.0
model initialize at round 3612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.00000012, 9.00000024, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 4.123105885830197}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8045575534259756
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.0032619, 5.0058925, 0.       ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.00673510085121739}
episode index:3613
target Thresh 19.0
target distance 9.0
model initialize at round 3613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([11.02619884, 11.86775028,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 7.079580542486185}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8045603062473851
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.46019813, 10.55264333,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.701080625827417}
episode index:3614
target Thresh 19.0
target distance 8.0
model initialize at round 3614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 9.9739102 , 10.13227737,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 6.791532949507576}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.804563057545795
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.8238387 ,  7.76702067,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.786990162674943}
episode index:3615
target Thresh 19.0
target distance 1.0
model initialize at round 3615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.20434243,  7.88084879,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.1869985512317351}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8046171053728013
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.20434243,  7.88084879,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.1869985512317351}
episode index:3616
target Thresh 19.0
target distance 14.0
model initialize at round 3616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.041594578792314}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8045978835827133
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.77655942, 6.87500621, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.169906149324697}
episode index:3617
target Thresh 19.0
target distance 11.0
model initialize at round 3617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([11.01818084, 11.86384986,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.36499606390666}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8045786724182695
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.94307451, 2.99186208, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9934942860366006}
episode index:3618
target Thresh 19.0
target distance 2.0
model initialize at round 3618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.138656  ,  4.97805035,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8616236252188956}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8046326711271896
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.138656  ,  4.97805035,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8616236252188956}
episode index:3619
target Thresh 19.0
target distance 3.0
model initialize at round 3619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.99999928, 10.17557877,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.2960215237428048}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8046728278478726
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.69428205, 11.50884527,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8607851533531582}
episode index:3620
target Thresh 19.0
target distance 5.0
model initialize at round 3620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 4.        , 10.99999988,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.000000000000021}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8046998444654236
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.88026541, 10.36296219,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.086593008827134}
episode index:3621
target Thresh 19.0
target distance 7.0
model initialize at round 3621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.1373283,  7.0188852,  0.       ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.092485746070382}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8047143875784923
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.75418321,  1.57768791,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.4886444465896824}
episode index:3622
target Thresh 19.0
target distance 13.0
model initialize at round 3622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.000000000000018}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8046951707700591
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.65688019, 11.44049079,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7909005730778185}
episode index:3623
target Thresh 19.0
target distance 4.0
model initialize at round 3623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([12.91786073,  9.0297143 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.2478992733884113}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8047352659216126
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.59902704, 11.02460427,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5995321255358796}
episode index:3624
target Thresh 19.0
target distance 2.0
model initialize at round 3624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.00864875,  2.26558197,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.2657227526986371}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8047891320551515
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.00864875,  2.26558197,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.2657227526986371}
episode index:3625
target Thresh 19.0
target distance 2.0
model initialize at round 3625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.26706886,  2.02696868,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7334271396820448}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8048429684776404
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.26706886,  2.02696868,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7334271396820448}
episode index:3626
target Thresh 19.0
target distance 9.0
model initialize at round 3626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 6.97729158, 11.86674919,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 8.54421917263644}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8048456327405359
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.40906382,  6.98978072,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.4091914520043302}
episode index:3627
target Thresh 19.0
target distance 9.0
model initialize at round 3627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.19373542,  4.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 7.101056559037072}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8048482955347088
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.80475301, 11.40674186,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9017019122383855}
episode index:3628
target Thresh 19.0
target distance 2.0
model initialize at round 3628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.90531456, 5.99145234, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.905354915832105}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8049020711490559
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.90531456, 5.99145234, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.905354915832105}
episode index:3629
target Thresh 19.0
target distance 9.0
model initialize at round 3629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.15125498959279}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.804893497834001
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.95939252, 10.49997943,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0818754888385}
episode index:3630
target Thresh 19.0
target distance 13.0
model initialize at round 3630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 12.529964086141694}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8048641515928165
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.03680241, 11.73478866,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7357097215781202}
episode index:3631
target Thresh 19.0
target distance 1.0
model initialize at round 3631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.95067787, 10.97931502,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9509028784758041}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8049178784233252
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.95067787, 10.97931502,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9509028784758041}
episode index:3632
target Thresh 19.0
target distance 3.0
model initialize at round 3632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.61886573, 8.99986148, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.1761257804070806}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8049578129461924
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.28678854, 10.82247097,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0886363443341511}
episode index:3633
target Thresh 19.0
target distance 12.0
model initialize at round 3633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.214321868139345}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8049284732332445
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.86145978, 10.84889846,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.209438526686869}
episode index:3634
target Thresh 19.0
target distance 6.0
model initialize at round 3634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4., 7., 0.]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.472135954999605}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8049429014386824
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.47365596, 10.75427317,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.5808784071734882}
episode index:3635
target Thresh 19.0
target distance 4.0
model initialize at round 3635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13385299, 4.02114477, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 2.1989171930881577}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8049827961302558
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.05040489, 2.02434171, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.05597473990624319}
episode index:3636
target Thresh 19.0
target distance 10.0
model initialize at round 3636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.980393886139277}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8049534737491625
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.14362139,  7.96508966,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.14780336749376455}
episode index:3637
target Thresh 19.0
target distance 5.0
model initialize at round 3637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.00000012, 10.99998021,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.0000001192745622}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8049802869779286
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.03239467, 10.68277021,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0182803230165594}
episode index:3638
target Thresh 19.0
target distance 13.0
model initialize at round 3638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.52844675707443}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8049509814019778
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.1309635 , 4.88762395, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.2422160486618403}
episode index:3639
target Thresh 19.0
target distance 1.0
model initialize at round 3639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.23199201, 3.05812275, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9700273384486762}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8050045662971971
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.23199201, 3.05812275, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9700273384486762}
episode index:3640
target Thresh 19.0
target distance 6.0
model initialize at round 3640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.00091076, 9.00000322, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.471731604514121}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8050313434006585
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.47759753, 5.00615096, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.4776371420129187}
episode index:3641
target Thresh 19.0
target distance 4.0
model initialize at round 3641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.4822253 ,  5.00022745,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.0661559711806308}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8050711480839642
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.99088794,  3.01301301,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.015886091439494026}
episode index:3642
target Thresh 19.0
target distance 7.0
model initialize at round 3642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([10.97380175, 10.13224974,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 6.83830115480308}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8050855054410644
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.94048142,  4.91881019,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.920735911993405}
episode index:3643
target Thresh 19.0
target distance 3.0
model initialize at round 3643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([5.11386323, 9.0719275 , 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.4498309795437068}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8051252734143242
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.78854414, 10.03016406,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.21359646400037152}
episode index:3644
target Thresh 19.0
target distance 10.0
model initialize at round 3644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.620595846702555}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8051060598662338
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.8494084 , 8.95022413, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.2745275707451653}
episode index:3645
target Thresh 19.0
target distance 13.0
model initialize at round 3645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.18033988749897}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8050974682254312
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.97169207, 9.44134426, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1208397376651746}
episode index:3646
target Thresh 19.0
target distance 6.0
model initialize at round 3646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.000000000000019}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8051241758020078
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.97463199, 10.21567461,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7847355288814081}
episode index:3647
target Thresh 19.0
target distance 4.0
model initialize at round 3647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.70919806, 4.00030077, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 2.0213285038022417}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8051638895696058
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.84084903, 2.00385308, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.1591976081504707}
episode index:3648
target Thresh 19.0
target distance 3.0
model initialize at round 3648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.77879643, 7.99993002, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.024241658025847}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8052035815702718
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.24581093, 9.81379545, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8501094291022696}
episode index:3649
target Thresh 19.0
target distance 6.0
model initialize at round 3649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.13259717, 6.00494137, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 4.097797336803193}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8052302381232663
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.5383643 , 2.00947883, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.46173300713453863}
episode index:3650
target Thresh 19.0
target distance 3.0
model initialize at round 3650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.00010347, 9.12743783, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.3272421401532306}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8052698902081408
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.26187064, 10.90817362,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.1703052085399117}
episode index:3651
target Thresh 19.0
target distance 14.0
model initialize at round 3651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4., 8., 0.]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.165525060596453}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8052506738884301
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.96080242,  6.01894549,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.04353598480833593}
episode index:3652
target Thresh 19.0
target distance 5.0
model initialize at round 3652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 4.242640687119312}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8052772956585127
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.07468871, 11.49685111,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.050267588108302}
episode index:3653
target Thresh 19.0
target distance 14.0
model initialize at round 3653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.369316876852997}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8052480291014342
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.12997862, 4.86833404, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.229203482387108}
episode index:3654
target Thresh 19.0
target distance 7.0
model initialize at round 3654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([12.82335934,  9.14612992,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.111329500017495}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8052622909265774
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.90368136, 11.20116032,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.2230308365472323}
episode index:3655
target Thresh 19.0
target distance 3.0
model initialize at round 3655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.13265333, 7.00479546, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.327367377790738}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8053018800154924
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.95220128, 5.01577901, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.985380975969404}
episode index:3656
target Thresh 19.0
target distance 14.0
model initialize at round 3656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 4., 11.,  0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.369316876852995}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8052726307445266
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.18898141,  8.23965777,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.30520455663887297}
episode index:3657
target Thresh 19.0
target distance 13.0
model initialize at round 3657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.99983213, 9.00007785, 0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 11.704884278105096}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8052534451950134
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.82505711,  5.01990219,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.17607132809877807}
episode index:3658
target Thresh 19.0
target distance 9.0
model initialize at round 3658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([12., 11.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.0000000000000195}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8052559739746813
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.6724328 , 10.84727321,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.3614218324119247}
episode index:3659
target Thresh 19.0
target distance 13.0
model initialize at round 3659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4., 5., 0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.704699910719649}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8052267612211618
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.10458014,  9.5828263 ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5921346991251675}
episode index:3660
target Thresh 19.0
target distance 13.0
model initialize at round 3660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.00494245, 11.86740305,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.03907362243233}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8052076039224466
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.25913549, 10.73442775,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7870253139320635}
episode index:3661
target Thresh 19.0
target distance 1.0
model initialize at round 3661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.05074692,  9.05993652,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9414322012898039}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8052607968214301
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.05074692,  9.05993652,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9414322012898039}
episode index:3662
target Thresh 19.0
target distance 12.0
model initialize at round 3662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 5.94456924, 10.04533795,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.877855044145955}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8052316066765413
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.11382943,  2.44792934,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.4621664508807973}
episode index:3663
target Thresh 19.0
target distance 2.0
model initialize at round 3663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.08854771,  2.0054996 ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.0887183293349621}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8052847639891295
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.08854771,  2.0054996 ,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.0887183293349621}
episode index:3664
target Thresh 19.0
target distance 9.0
model initialize at round 3664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([4., 9., 0.]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 7.280109889280541}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.805287280083539
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.81793643, 11.87334813,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.1965604665748637}
episode index:3665
target Thresh 19.0
target distance 8.0
model initialize at round 3665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.27325723,  5.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 6.133611005185257}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8053014884086662
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.59638819, 10.99057925,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.40372173951201296}
episode index:3666
target Thresh 19.0
target distance 13.0
model initialize at round 3666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.18033988749897}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8052928926762123
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.98164559, 9.84849131, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9932688187539574}
episode index:3667
target Thresh 19.0
target distance 5.0
model initialize at round 3667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.00000024, 10.99996829,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 3.0000002385861397}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8053193940686125
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.03282219, 10.86258901,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9768903230629886}
episode index:3668
target Thresh 19.0
target distance 3.0
model initialize at round 3668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.41549386, 9.9999572 , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.1583320012109797}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8053588273217962
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.60491379, 11.87959678,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9642528767684199}
episode index:3669
target Thresh 19.0
target distance 11.0
model initialize at round 3669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.184084079649052}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8053396810175192
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.92558671, 7.0333009 , 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9695589126220064}
episode index:3670
target Thresh 19.0
target distance 7.0
model initialize at round 3670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.000000000000019}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8053538557162341
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.6908546 , 11.88746481,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.1246661175520372}
episode index:3671
target Thresh 19.0
target distance 14.0
model initialize at round 3671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.041594578792317}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8053247117729817
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.88416117, 6.162312  , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8456594013106847}
episode index:3672
target Thresh 19.0
target distance 3.0
model initialize at round 3672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.14566586,  6.0116717 ,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.3241474452240993}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8053641006344647
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.36054972,  4.06919645,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9981940414892603}
episode index:3673
target Thresh 19.0
target distance 12.0
model initialize at round 3673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.04987562112091}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8053555042373132
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.97787481, 11.60201587,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6024223010262961}
episode index:3674
target Thresh 19.0
target distance 12.0
model initialize at round 3674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.440306508910576}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8053263836364577
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.80711133, 7.90641008, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9267066826713278}
episode index:3675
target Thresh 19.0
target distance 1.0
model initialize at round 3675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.05940855, 11.42616251,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0326309941918113}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8053793416387329
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.05940855, 11.42616251,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0326309941918113}
episode index:3676
target Thresh 19.0
target distance 7.0
model initialize at round 3676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.04309845,  7.00000536,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 5.090747903909855}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8053934824215344
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.1680464 ,  1.13368157,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8824665556659834}
episode index:3677
target Thresh 19.0
target distance 12.0
model initialize at round 3677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.        , 10.99999964,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.661903605692618}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8053743683400237
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.48876702, 4.99212983, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.511293550555182}
episode index:3678
target Thresh 19.0
target distance 12.0
model initialize at round 3678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4., 6., 0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.770329614269032}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8053452742730907
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.93784984, 10.35268561,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.3581197902828241}
episode index:3679
target Thresh 19.0
target distance 11.0
model initialize at round 3679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.219544457292901}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8053477636686687
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.84054863, 8.94159547, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.2621901712129246}
episode index:3680
target Thresh 19.0
target distance 5.0
model initialize at round 3680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.99918735,  6.        ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.000000110066613}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8053741565609075
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.53391576,  9.32872784,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.6269992268569513}
episode index:3681
target Thresh 19.0
target distance 2.0
model initialize at round 3681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.00702798,  4.39615208,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.3962144145963524}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8054270152907932
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.00702798,  4.39615208,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.3962144145963524}
episode index:3682
target Thresh 19.0
target distance 2.0
model initialize at round 3682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.10367966,  3.00343788,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.10373663898640019}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8054798453165085
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.10367966,  3.00343788,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.10373663898640019}
episode index:3683
target Thresh 19.0
target distance 12.0
model initialize at round 3683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.440306508910561}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8054712408355593
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.96401562, 6.95604408, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.056806675376436695}
episode index:3684
target Thresh 19.0
target distance 12.0
model initialize at round 3684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.        ,  9.99999976,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.661903667025252}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8054326924476227
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.5998063 , 4.88093252, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0657437296098518}
episode index:3685
target Thresh 19.0
target distance 11.0
model initialize at round 3685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([5.       , 9.9999994, 0.       ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.295629851521293}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8054136092132703
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.96667638,  4.96157119,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.05086489081650929}
episode index:3686
target Thresh 19.0
target distance 6.0
model initialize at round 3686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 7.02619324, 10.132251  ,  0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 5.769378681616765}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8054399412964781
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.92569446, 6.91922032, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9222186865170653}
episode index:3687
target Thresh 19.0
target distance 2.0
model initialize at round 3687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.01335514,  2.88296968,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.883070671548122}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8054926961930897
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.01335514,  2.88296968,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.883070671548122}
episode index:3688
target Thresh 19.0
target distance 6.0
model initialize at round 3688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([4.95431904, 9.05521685, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 5.017248712002226}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.805518992561701
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.1311204 , 5.06907913, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8716212940776557}
episode index:3689
target Thresh 19.0
target distance 8.0
model initialize at round 3689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([3., 9., 0.]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 6.0827625302982415}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.805533045680248
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.95419169, 3.01418345, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.04795384972541371}
episode index:3690
target Thresh 19.0
target distance 11.0
model initialize at round 3690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.848857801796115}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8055244431042033
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.02154987,  6.99382423,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.022417331878814117}
episode index:3691
target Thresh 19.0
target distance 8.0
model initialize at round 3691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.00000799, 9.00000012, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.324552907714901}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8055384871336985
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.18589307, 3.02856899, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.18807557557867732}
episode index:3692
target Thresh 19.0
target distance 1.0
model initialize at round 3692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.09677655, 4.80084062, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8066668418318755}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8055911439202856
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.09677655, 4.80084062, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8066668418318755}
episode index:3693
target Thresh 19.0
target distance 13.0
model initialize at round 3693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.,  9.,  0.]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.401754250991393}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8055720591197184
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.84140364, 5.97074023, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.16127287247060246}
episode index:3694
target Thresh 19.0
target distance 3.0
model initialize at round 3694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.79138007,  6.99982006,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.2753988866388903}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8056111465191447
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.00613283,  8.95181649,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9518362464472239}
episode index:3695
target Thresh 19.0
target distance 8.0
model initialize at round 3695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.8934456 ,  9.00001335,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 6.1011984778611845}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8056251518907575
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.99595796,  3.00371122,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.005487376659445196}
episode index:3696
target Thresh 19.0
target distance 13.0
model initialize at round 3696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.00494245, 11.86740305,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.03907362243233}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8056060733781079
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.27915648, 10.56935647,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8396840083463805}
episode index:3697
target Thresh 19.0
target distance 8.0
model initialize at round 3697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.1333575 ,  8.00138817,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 6.063639931498967}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8056200725470158
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.87251119,  2.01781659,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.12872772936294272}
episode index:3698
target Thresh 19.0
target distance 7.0
model initialize at round 3698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4., 6., 0.]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 5.385164807134529}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8056462633897985
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.0821196 , 10.18276723,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8213482980515524}
episode index:3699
target Thresh 19.0
target distance 6.0
model initialize at round 3699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.0022459, 9.0000037, 0.       ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.4711353139821}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8056724400753689
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.77534631, 5.00489848, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.7753617818033044}
episode index:3700
target Thresh 19.0
target distance 3.0
model initialize at round 3700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.96885204, 8.99995661, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0005283511532983}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.805711436984292
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.00720575, 10.89143491,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8914640306509917}
episode index:3701
target Thresh 19.0
target distance 4.0
model initialize at round 3701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.86738414, 7.99517471, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.184417469786634}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.805750412825193
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.16490594, 9.98960137, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.165233472565782}
episode index:3702
target Thresh 19.0
target distance 13.0
model initialize at round 3702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4., 5., 0.]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.000000000000018}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8057119764272627
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.77505501,  4.90109063,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.24573015868608505}
episode index:3703
target Thresh 19.0
target distance 7.0
model initialize at round 3703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([12.76308553,  9.19399037,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 5.339240899348344}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8057259243277952
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.11728948,  3.27453873,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.73488153644963}
episode index:3704
target Thresh 19.0
target distance 1.0
model initialize at round 3704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.68305206, 8.91249692, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.139829266887444}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.80577835997575
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.68305206, 8.91249692, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.139829266887444}
episode index:3705
target Thresh 19.0
target distance 2.0
model initialize at round 3705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.01125145,  2.80239892,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.19792115074326325}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8058307673259993
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.01125145,  2.80239892,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.19792115074326325}
episode index:3706
target Thresh 19.0
target distance 7.0
model initialize at round 3706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.        ,  9.00000358,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 5.385168127626054}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8058446718937561
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.514129  ,  3.02399266,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.1031404980820934}
episode index:3707
target Thresh 19.0
target distance 11.0
model initialize at round 3707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.347971959324063}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8058255907769091
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.67672005, 5.83389587, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.6968074391950358}
episode index:3708
target Thresh 19.0
target distance 1.0
model initialize at round 3708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.89650285, 3.03529084, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.10934853914205385}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.805877943003715
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.89650285, 3.03529084, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.10934853914205385}
episode index:3709
target Thresh 19.0
target distance 2.0
model initialize at round 3709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.85603138, 4.97142357, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8565082271739082}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8059302670082962
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.85603138, 4.97142357, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8565082271739082}
episode index:3710
target Thresh 19.0
target distance 13.0
model initialize at round 3710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.704699910719636}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.805921603755936
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.90287059, 7.06201686, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9049980114203591}
episode index:3711
target Thresh 19.0
target distance 6.0
model initialize at round 3711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.769381671780255}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8059354651234587
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.94099362, 10.88383484,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.13029235166396178}
episode index:3712
target Thresh 19.0
target distance 7.0
model initialize at round 3712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.06664883, 6.        , 0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 5.410271469161241}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8059493190245836
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.4951415 , 11.90962363,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.040335166179713}
episode index:3713
target Thresh 19.0
target distance 2.0
model initialize at round 3713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.05337942, 3.99800968, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.05341650933142608}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8060015674578025
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.05337942, 3.99800968, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.05341650933142608}
episode index:3714
target Thresh 19.0
target distance 14.0
model initialize at round 3714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 13.000000000000027}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8059725864964663
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.84290362, 10.69040963,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.34716778286549366}
episode index:3715
target Thresh 19.0
target distance 8.0
model initialize at round 3715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.289560442313221}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8059748829613488
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.96219658, 10.77102796,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.2320717436756361}
episode index:3716
target Thresh 19.0
target distance 4.0
model initialize at round 3716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.03434134,  8.00046706,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.0007618032746586}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8060136306387872
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.55321521,  6.00114703,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.44678626671707816}
episode index:3717
target Thresh 19.0
target distance 11.0
model initialize at round 3717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([12.01973423, 11.86551174,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 13.367270755978147}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8059846698172312
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.06279986, 2.01032839, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.06364351965653185}
episode index:3718
target Thresh 19.0
target distance 9.0
model initialize at round 3718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.040011542952131}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8059869611805499
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.08234771, 11.64929255,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.6544936727096838}
episode index:3719
target Thresh 19.0
target distance 2.0
model initialize at round 3719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.98061771, 3.99355376, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.02042614430767302}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8060391152232433
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.98061771, 3.99355376, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.02042614430767302}
episode index:3720
target Thresh 19.0
target distance 1.0
model initialize at round 3720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.41456294,  4.90235472,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9930289358667116}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8060912412336644
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.41456294,  4.90235472,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9930289358667116}
episode index:3721
target Thresh 19.0
target distance 2.0
model initialize at round 3721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.28445542,  8.01158204,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7156383098597356}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8061433392344076
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.28445542,  8.01158204,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7156383098597356}
episode index:3722
target Thresh 19.0
target distance 3.0
model initialize at round 3722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.86102637, 3.97729636, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.336895337636041}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8061819792184972
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.1107988 , 5.93740588, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9439312238876832}
episode index:3723
target Thresh 19.0
target distance 2.0
model initialize at round 3723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.73839661,  5.00473225,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.2616461835273486}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8062340248739165
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.73839661,  5.00473225,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.2616461835273486}
episode index:3724
target Thresh 19.0
target distance 5.0
model initialize at round 3724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 4.99505774, 11.86740302,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.12762944840445}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8062598680887154
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.90892153, 11.01405141,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9090301416716889}
episode index:3725
target Thresh 19.0
target distance 5.0
model initialize at round 3725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.13264226,  7.99533431,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.0075920425764044}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8062856974316869
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.91915863, 11.23175445,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9479254778228237}
episode index:3726
target Thresh 19.0
target distance 12.0
model initialize at round 3726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14., 11.,  0.]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.198039027185585}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8062769760042836
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.93358325, 8.14979021, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8528000201411382}
episode index:3727
target Thresh 19.0
target distance 11.0
model initialize at round 3727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.00472769, 11.86736787,  0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 12.637853205511524}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8062480222274835
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.92178516, 3.00608953, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.07845154125410732}
episode index:3728
target Thresh 19.0
target distance 3.0
model initialize at round 3728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.5871278 ,  5.00113487,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0829286598912857}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8062865719667628
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.50287646,  3.0363816 ,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0869430302402607}
episode index:3729
target Thresh 19.0
target distance 13.0
model initialize at round 3729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 4.97380116, 11.86775028,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.183271074044923}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8062674849208267
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.60954093,  9.46261058,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8126053969383701}
episode index:3730
target Thresh 19.0
target distance 10.0
model initialize at round 3730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.343119119802346}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8062484081064885
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.72297027, 5.85796607, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7367900960102423}
episode index:3731
target Thresh 19.0
target distance 4.0
model initialize at round 3731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.86649631, 5.97802158, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.1998210400437967}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.806286926753834
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.15971594, 7.97737194, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.1613109145975326}
episode index:3732
target Thresh 19.0
target distance 11.0
model initialize at round 3732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.287213209236013}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8062580090922588
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0304248 , 3.85464321, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8551845933999294}
episode index:3733
target Thresh 19.0
target distance 3.0
model initialize at round 3733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.89006102,  7.00169837,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.00771335860431}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8062965045370653
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.46928918,  5.03085923,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0767851067683782}
episode index:3734
target Thresh 19.0
target distance 3.0
model initialize at round 3734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.99899185,  7.99998951,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0000109985982573}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8063349793685144
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.24697036,  9.5714339 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.6225199278837183}
episode index:3735
target Thresh 19.0
target distance 4.0
model initialize at round 3735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([5.17664064, 9.1461299 , 0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.6002833846352926}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8063734336031588
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.10379934, 11.20412146,  0.        ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.22899754095728983}
episode index:3736
target Thresh 19.0
target distance 9.0
model initialize at round 3736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.6927536328546795}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8063756098986891
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.99306631, 11.86582226,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8658500231189212}
episode index:3737
target Thresh 19.0
target distance 5.0
model initialize at round 3737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13226761, 8.02613731, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.1480893502235823}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8064013253588553
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.11553503, 4.03683671, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9700679664889985}
episode index:3738
target Thresh 19.0
target distance 5.0
model initialize at round 3738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([4.95432489, 9.05522779, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 3.5988882808164546}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8064397309952931
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.95838306, 7.09820563, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9634014913865696}
episode index:3739
target Thresh 19.0
target distance 3.0
model initialize at round 3739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.00105321, 7.00019765, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.000198203522231}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8064781160939575
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.63359264, 5.06933844, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.0001926286640175}
episode index:3740
target Thresh 19.0
target distance 7.0
model initialize at round 3740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.13555799,  7.0184105 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.092318134553509}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8064917212487038
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.73011303,  1.1572123 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8849463716103743}
episode index:3741
target Thresh 19.0
target distance 10.0
model initialize at round 3741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 5.94478322, 10.045681  ,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 8.055346302818595}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8064829797244524
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.24783518, 10.38835597,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.4606979869241263}
episode index:3742
target Thresh 19.0
target distance 12.0
model initialize at round 3742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 4.99505755, 11.86740305,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 10.042472879490532}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8064639064973352
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.26237287, 11.80647213,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8480783045261259}
episode index:3743
target Thresh 19.0
target distance 13.0
model initialize at round 3743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.083045973594581}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8064257004409228
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.24232013, 2.07810354, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9532114776316926}
episode index:3744
target Thresh 19.0
target distance 12.0
model initialize at round 3744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.99983213, 9.00007785, 0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.050050404146477}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8064169835482816
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.97928703,  7.20154972,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7987188945934203}
episode index:3745
target Thresh 19.0
target distance 5.0
model initialize at round 3745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([12.14494111,  9.8244552 ,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.971714600618463}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8064426330454657
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.52333913,  9.41675167,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.6690035862056607}
episode index:3746
target Thresh 19.0
target distance 14.0
model initialize at round 3746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.00000000000002}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8064235909471417
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.99474119, 10.8860324 ,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8860480013094015}
episode index:3747
target Thresh 19.0
target distance 4.0
model initialize at round 3747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.00164652,  6.9999994 ,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.0000012738020785}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.806449224994381
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.37595765,  8.84365936,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.40716894927820735}
episode index:3748
target Thresh 19.0
target distance 13.0
model initialize at round 3748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 3.97380116, 11.86775028,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.06029164918956}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8064301912962296
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.64230444, 11.51348806,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.822328995146233}
episode index:3749
target Thresh 19.0
target distance 4.0
model initialize at round 3749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.13282943,  4.9963893 ,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 2.183217955593334}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8064684765785506
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.74638956,  6.99386877,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7464147414408744}
episode index:3750
target Thresh 19.0
target distance 2.0
model initialize at round 3750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.06475699,  9.01248407,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.06594937286287654}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8065200712262236
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.06475699,  9.01248407,  0.        ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.06594937286287654}
episode index:3751
target Thresh 19.0
target distance 2.0
model initialize at round 3751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13317588, 5.00604729, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.866845217179659}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8065716383714191
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13317588, 5.00604729, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.866845217179659}
episode index:3752
target Thresh 19.0
target distance 12.0
model initialize at round 3752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 4.        , 10.99999905,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.180339461002909}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8065525923421768
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.70204774,  6.00734516,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.29804278540994555}
episode index:3753
target Thresh 19.0
target distance 6.0
model initialize at round 3753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.31616592,  4.        ,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 4.058032656768569}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8065781510549254
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.06602728,  7.99567449,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9339827326458481}
episode index:3754
target Thresh 19.0
target distance 4.0
model initialize at round 3754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.82532158,  9.14399694,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.8642049025906764}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8066163459547775
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.06009871, 11.14350297,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.15557942023012813}
episode index:3755
target Thresh 19.0
target distance 11.0
model initialize at round 3755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 5.94478322, 10.045681  ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.105365210222164}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8066076038332507
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.64278437, 11.61608492,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8903551936797361}
episode index:3756
target Thresh 19.0
target distance 8.0
model initialize at round 3756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.13226448,  4.97385944,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 6.131589750443811}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8066211165817646
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.06287905, 11.11026751,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9435860287243357}
episode index:3757
target Thresh 19.0
target distance 1.0
model initialize at round 3757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.7050569 ,  7.67242523,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9743002239710007}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8066725745071021
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.7050569 ,  7.67242523,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9743002239710007}
episode index:3758
target Thresh 19.0
target distance 6.0
model initialize at round 3758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 7.02617356, 10.13225598,  0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 5.1010882235441475}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8066980673045198
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.91558643, 6.92979495, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.1097925305452996}
episode index:3759
target Thresh 19.0
target distance 12.0
model initialize at round 3759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 4.99505757, 11.86740304,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.126117265489178}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8066893127487206
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.98633499,  7.00225714,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.013850171031472605}
episode index:3760
target Thresh 19.0
target distance 1.0
model initialize at round 3760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.90231381,  4.04740298,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9575926469281695}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8067407114956633
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.90231381,  4.04740298,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9575926469281695}
episode index:3761
target Thresh 19.0
target distance 1.0
model initialize at round 3761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.80150557,  9.02833259,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.5428954801886328}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.806778792114617
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.56137156, 10.74867558,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9357634121389198}
episode index:3762
target Thresh 19.0
target distance 2.0
model initialize at round 3762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.69908136, 9.99423528, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6991051292457041}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8068301397648656
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.69908136, 9.99423528, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6991051292457041}
episode index:3763
target Thresh 19.0
target distance 11.0
model initialize at round 3763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([12.82335941,  9.14612979,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.147656609725379}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8068013159487999
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.95958915, 2.86768214, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8686226681188606}
episode index:3764
target Thresh 19.0
target distance 12.0
model initialize at round 3764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.536241300558661}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8067725074441903
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.1581788 ,  7.25759345,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.12242167635212}
episode index:3765
target Thresh 19.0
target distance 14.0
model initialize at round 3765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 3.99513889, 11.86738981,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.924720845548043}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8067344426337402
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.95471781,  2.15866319,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8425545146166072}
episode index:3766
target Thresh 19.0
target distance 3.0
model initialize at round 3766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.05998085, 9.99997079, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0018263902984623}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8067724743718251
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.21245947, 11.86728515,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.1714963197062653}
episode index:3767
target Thresh 19.0
target distance 9.0
model initialize at round 3767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([1.13259772, 9.00493795, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 7.569091858472256}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8067745268600491
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.87554421, 1.16210934, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8470832304428205}
episode index:3768
target Thresh 19.0
target distance 3.0
model initialize at round 3768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.91350758, 7.00031197, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0040443104433743}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8068125277815508
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.2895577 , 5.07319295, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9709865907192301}
episode index:3769
target Thresh 19.0
target distance 6.0
model initialize at round 3769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.1881094,  4.       ,  0.       ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 4.081564203806881}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8067744927426933
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.06899691,  8.52065802,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0667012376265288}
episode index:3770
target Thresh 19.0
target distance 14.0
model initialize at round 3770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.000000000000027}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8067364778762246
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.88921024, 9.73235884, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7406914645287381}
episode index:3771
target Thresh 19.0
target distance 13.0
model initialize at round 3771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.045361017187282}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.80671748408321
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.84064091, 7.05693325, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9564362051197152}
episode index:3772
target Thresh 19.0
target distance 10.0
model initialize at round 3772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([10.02619883, 11.86775027,  0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 8.909509520468086}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.806719548426151
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.96492258, 7.97693695, 0.        ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.04198010623267111}
episode index:3773
target Thresh 19.0
target distance 9.0
model initialize at round 3773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([11.02619883, 11.86775027,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 7.5889038472430705}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8067329706443741
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.89065926, 8.99595936, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8906684210399426}
episode index:3774
target Thresh 19.0
target distance 11.0
model initialize at round 3774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.13259829055323}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8067042565584004
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.7168962 , 5.88355273, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7262920366171374}
episode index:3775
target Thresh 19.0
target distance 12.0
model initialize at round 3775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.770329614269018}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8066955374590734
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.97347609,  6.96366473,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9640296857700541}
episode index:3776
target Thresh 19.0
target distance 2.0
model initialize at round 3776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.62766695, 5.99403095, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.627695332095426}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8067467168243212
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.62766695, 5.99403095, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.627695332095426}
episode index:3777
target Thresh 19.0
target distance 8.0
model initialize at round 3777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.13224974,  4.97380121,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.791616913935635}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.806760117640408
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.98208513, 11.1182062 ,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.989173348279832}
episode index:3778
target Thresh 19.0
target distance 6.0
model initialize at round 3778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 8.0248296 , 10.13261116,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.554918628165279}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8067854523539194
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.84145691, 7.9491791 , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.1664892648629277}
episode index:3779
target Thresh 19.0
target distance 1.0
model initialize at round 3779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.01288307, 10.72103548,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.025778261758809}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8068365673136141
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.01288307, 10.72103548,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.025778261758809}
episode index:3780
target Thresh 19.0
target distance 10.0
model initialize at round 3780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 5.94478322, 10.045681  ,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 8.310916198291961}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8068385957935629
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.08482718,  8.01984432,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.08711743892418157}
episode index:3781
target Thresh 19.0
target distance 1.0
model initialize at round 3781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.43300557, 7.03875327, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.1160098368375384}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.806889669670931
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.43300557, 7.03875327, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.1160098368375384}
episode index:3782
target Thresh 19.0
target distance 13.0
model initialize at round 3782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.205906925752268}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8068517449449512
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.09317326,  9.25743328,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.2737757336231843}
episode index:3783
target Thresh 19.0
target distance 14.0
model initialize at round 3783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.369316876852997}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8068230677650221
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.76756556, 4.07343829, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.955270836015433}
episode index:3784
target Thresh 19.0
target distance 6.0
model initialize at round 3784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.25387645, 6.        , 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.008048558923101}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8068483456863524
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.25621462, 9.99708867, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.25623115865349405}
episode index:3785
target Thresh 19.0
target distance 13.0
model initialize at round 3785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 4.       , 11.6405571,  0.       ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 14.626699605541505}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8068104619266067
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.89587493,  2.86015554,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.866434988586283}
episode index:3786
target Thresh 19.0
target distance 4.0
model initialize at round 3786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.86659519, 6.97774348, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 2.2001155984970073}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.806848272736766
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.47785698, 8.97643332, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.4784377530028856}
episode index:3787
target Thresh 19.0
target distance 13.0
model initialize at round 3787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.529964086141694}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8068196267556036
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.87148487, 9.65078774, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9388477422465904}
episode index:3788
target Thresh 19.0
target distance 12.0
model initialize at round 3788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.13259695,  5.99505755,  0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.568784232893615}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8067909958950436
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.69947097, 9.67232278, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.736434399690179}
episode index:3789
target Thresh 19.0
target distance 3.0
model initialize at round 3789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([14.91952038,  6.0002178 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.4723694684054147}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8068287819119577
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.3947835 ,  4.02550519,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0514248207096129}
episode index:3790
target Thresh 19.0
target distance 11.0
model initialize at round 3790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 9.055385138137432}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8068308070947823
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.00154868,  9.42354155,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.1529134331277602}
episode index:3791
target Thresh 19.0
target distance 6.0
model initialize at round 3791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.99437499,  5.        ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 4.000003955089599}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8068560363123206
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.35357419,  8.99558234,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.6464409083418053}
episode index:3792
target Thresh 19.0
target distance 11.0
model initialize at round 3792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 9.13259829055323}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8068182204396543
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.1525062 , 4.92191037, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.17133630776581588}
episode index:3793
target Thresh 19.0
target distance 2.0
model initialize at round 3793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.61844534, 5.00726652, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.3816238505772888}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.806869138146444
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.61844534, 5.00726652, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.3816238505772888}
episode index:3794
target Thresh 19.0
target distance 5.0
model initialize at round 3794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.00000203, 8.00000083, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.0000008344657147}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8068943373195281
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.28220149, 4.03425709, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0061297423922417}
episode index:3795
target Thresh 19.0
target distance 13.0
model initialize at round 3795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.176978636124282}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8068754220279858
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.88039145, 6.01927286, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9879938871027693}
episode index:3796
target Thresh 19.0
target distance 13.0
model initialize at round 3796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4., 6., 0.]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.04536101718728}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8068565166997258
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.2447029 ,  7.85599251,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.1415764966693185}
episode index:3797
target Thresh 19.0
target distance 12.0
model initialize at round 3797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.        , 7.99999988, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.770329569995802}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8068279439718148
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.95240679,  4.32543292,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.3288946606259472}
episode index:3798
target Thresh 19.0
target distance 11.0
model initialize at round 3798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.05521678, 10.045681  ,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.366091240270828}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8068090610938609
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.52611013, 4.13308754, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.014065617726277}
episode index:3799
target Thresh 19.0
target distance 2.0
model initialize at round 3799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.01789309, 11.86686129,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8670459420499482}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8068599008146257
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.01789309, 11.86686129,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8670459420499482}
episode index:3800
target Thresh 19.0
target distance 12.0
model initialize at round 3800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.32713782374229}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8068313497478745
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.7111682 ,  4.88072592,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.31249018706238224}
episode index:3801
target Thresh 19.0
target distance 7.0
model initialize at round 3801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 8.02347404, 10.13300601,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 5.45756411605343}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8068565124649318
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.83652757, 7.94161897, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8385622937172731}
episode index:3802
target Thresh 19.0
target distance 1.0
model initialize at round 3802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.83424139, 11.4965741 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9708473297933389}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8069072996033845
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.83424139, 11.4965741 ,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9708473297933389}
episode index:3803
target Thresh 19.0
target distance 6.0
model initialize at round 3803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([1.18774041, 7.00009271, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 4.889738814485294}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.806932429125045
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.45701947, 3.00887094, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.45710555609748577}
episode index:3804
target Thresh 19.0
target distance 1.0
model initialize at round 3804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.52691549, 6.02923667, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.1045457740294573}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8069831696167336
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.52691549, 6.02923667, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.1045457740294573}
episode index:3805
target Thresh 19.0
target distance 4.0
model initialize at round 3805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([13.01564637,  9.11924485,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 2.1710340768452268}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8070207462931348
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.96018056, 10.83823995,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8391852015092915}
episode index:3806
target Thresh 19.0
target distance 2.0
model initialize at round 3806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.13775832,  3.97967526,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8624811941898141}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8070714369297796
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.13775832,  3.97967526,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8624811941898141}
episode index:3807
target Thresh 19.0
target distance 13.0
model initialize at round 3807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.704699910719635}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8070525347379979
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.14956501,  5.98308044,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.1505189848547178}
episode index:3808
target Thresh 19.0
target distance 1.0
model initialize at round 3808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.59631085, 7.09648693, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0825536981114463}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8071031904127844
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.59631085, 7.09648693, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0825536981114463}
episode index:3809
target Thresh 19.0
target distance 12.0
model initialize at round 3809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.440306508910576}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8070944444146446
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.7993856, 9.0507616, 0.       ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.2409959267860762}
episode index:3810
target Thresh 19.0
target distance 3.0
model initialize at round 3810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.13286661,  5.99656133,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.3262011480697748}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8071319425924418
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.54160249,  7.84694798,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.0053129535482765}
episode index:3811
target Thresh 19.0
target distance 3.0
model initialize at round 3811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.39684242,  8.99992049,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.1678861664152362}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8071694210964836
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.58637208, 10.7041316 ,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.916315190266605}
episode index:3812
target Thresh 19.0
target distance 4.0
model initialize at round 3812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.9999851, 8.       , 0.       ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.00000000005553}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8072068799422489
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.77657735, 9.99442863, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.7765973384232671}
episode index:3813
target Thresh 19.0
target distance 6.0
model initialize at round 3813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 8.01887341, 10.13506335,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.176088153342338}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8072318650287875
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.89360752, 8.20071904, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8063308344219483}
episode index:3814
target Thresh 19.0
target distance 1.0
model initialize at round 3814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.23469656, 11.87027891,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9013699917261289}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8072823940287799
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.23469656, 11.87027891,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9013699917261289}
episode index:3815
target Thresh 19.0
target distance 12.0
model initialize at round 3815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4., 6., 0.]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.440306508910561}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.8072360017372956
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.50032519,  2.3023981 ,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8580928447756941}
episode index:3816
target Thresh 19.0
target distance 10.0
model initialize at round 3816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4., 9., 0.]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.246211251235342}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8072379064394865
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.95324912, 11.86645786,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8677181911550983}
episode index:3817
target Thresh 19.0
target distance 13.0
model initialize at round 3817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4., 4., 0.]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.180339887498963}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.8071832964428386
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.85198511,  2.21616782,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.26198651470312134}
episode index:3818
target Thresh 19.0
target distance 5.0
model initialize at round 3818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([4.99983213, 9.00007785, 0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.605647771579942}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8072082549931285
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.87202582, 11.20865491,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8966414533772097}
episode index:3819
target Thresh 19.0
target distance 2.0
model initialize at round 3819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.29970766, 8.99894876, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.700293127206222}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8072587240363241
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.29970766, 8.99894876, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.700293127206222}
episode index:3820
target Thresh 19.0
target distance 12.0
model initialize at round 3820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 5.        , 10.99201059,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 13.448280726995355}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8072210798874764
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.24689214,  1.43636733,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.6153352870632659}
episode index:3821
target Thresh 19.0
target distance 3.0
model initialize at round 3821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.0000304 , 9.01040971, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.000084576314482}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8072584370094315
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.28810739, 8.86214913, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7251165047791573}
episode index:3822
target Thresh 19.0
target distance 2.0
model initialize at round 3822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.41754688, 9.99831319, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5824555637624976}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8073088533220107
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.41754688, 9.99831319, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5824555637624976}
episode index:3823
target Thresh 19.0
target distance 12.0
model initialize at round 3823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([12.82335934,  9.14612992,  0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.969667697804468}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8072803565758736
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.27682642, 2.86809903, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9111689167209577}
episode index:3824
target Thresh 19.0
target distance 8.0
model initialize at round 3824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86700583,  3.97648747,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 6.0855897286102625}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8072934532146773
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.93150502,  9.92848381,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.09902589017484793}
episode index:3825
target Thresh 19.0
target distance 6.0
model initialize at round 3825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.44895542, 8.00002074, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 4.037798419011109}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8073183373094983
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.92952442, 4.01248825, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.07157348269521628}
episode index:3826
target Thresh 19.0
target distance 14.0
model initialize at round 3826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 13.416407864998748}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8072898604238918
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.06876858, 3.98754611, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.06988717054124545}
episode index:3827
target Thresh 19.0
target distance 2.0
model initialize at round 3827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.90539956,  6.00585449,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.09478142273402719}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8073402026756098
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.90539956,  6.00585449,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.09478142273402719}
episode index:3828
target Thresh 19.0
target distance 12.0
model initialize at round 3828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.568784232893615}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8073117349538594
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.89233023, 7.12168703, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8848878209851392}
episode index:3829
target Thresh 19.0
target distance 6.0
model initialize at round 3829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.15748503,  6.99993351,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.087831140370245}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8073365882867697
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.54021959, 10.99590979,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.540235076710968}
episode index:3830
target Thresh 19.0
target distance 5.0
model initialize at round 3830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.86041031, 3.99975894, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.5302369488169627}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.807361428644826
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.59174861, 7.88085399, 0.        ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.061164532064792}
episode index:3831
target Thresh 19.0
target distance 13.0
model initialize at round 3831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.176978770806391}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.8073152094332079
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.14844487, 3.94757002, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.2739839404505406}
episode index:3832
target Thresh 19.0
target distance 4.0
model initialize at round 3832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.99997377, 4.        , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.00000000017197}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8073524347894736
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.63846977, 5.99193226, 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.6385207416726163}
episode index:3833
target Thresh 19.0
target distance 11.0
model initialize at round 3833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 5., 11.,  0.]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 9.055385138137432}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8073436785303997
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.57895924, 10.81559377,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0001934794016707}
episode index:3834
target Thresh 19.0
target distance 13.0
model initialize at round 3834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 5., 10.,  0.]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.401754250991392}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8073349268378233
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.13831002,  6.995084  ,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8617040006322326}
episode index:3835
target Thresh 19.0
target distance 10.0
model initialize at round 3835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([12.        , 10.99998236,  0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.63013419476275}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.80731609392953
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.10548744, 3.96157392, 0.        ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.11226826608046592}
episode index:3836
target Thresh 19.0
target distance 4.0
model initialize at round 3836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.99985659,  8.00011992,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 2.000119929686446}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8073532802485476
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.34765917,  6.00142884,  0.        ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.6523423941897387}
episode index:3837
target Thresh 19.0
target distance 3.0
model initialize at round 3837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.7707838 ,  6.99986267,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0260676122084031}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8073904471895978
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.4887922,  8.608518 ,  0.       ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7805203216372505}
episode index:3838
target Thresh 19.0
target distance 11.0
model initialize at round 3838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4., 9., 0.]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.219544457292908}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8073816924332317
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.94855432, 10.09724419,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.3094744539483223}
episode index:3839
target Thresh 19.0
target distance 8.0
model initialize at round 3839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.747700512325594}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8073835477867647
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.93427248, 11.19303942,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.20392234618523247}
episode index:3840
target Thresh 19.0
target distance 10.0
model initialize at round 3840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 5.94478307, 10.04568075,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.014102979728802}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.807374799385232
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.26755925,  6.00572864,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7324631553361014}
episode index:3841
target Thresh 19.0
target distance 4.0
model initialize at round 3841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.7779659 ,  8.00030661,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 2.344055002067806}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.807411922029848
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.10853684,  6.0015527 ,  0.        ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.10854794519392931}
episode index:3842
target Thresh 19.0
target distance 12.0
model initialize at round 3842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 10.04987562112091}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.80740317079786
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.10576881, 11.4311553 ,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.4439391124545605}
episode index:3843
target Thresh 19.0
target distance 10.0
model initialize at round 3843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 5.97380552, 11.86774917,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 8.909505125273483}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8074050186332403
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.11576496,  7.99086405,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.1161249004912564}
episode index:3844
target Thresh 19.0
target distance 5.0
model initialize at round 3844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.03601575, 5.99999988, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.0002163004407447}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8074297507480301
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.24077976, 9.79311323, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8288567366282774}
episode index:3845
target Thresh 19.0
target distance 10.0
model initialize at round 3845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.13224963,  7.97380131,  0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.13228720679134}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8074210017066239
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.76706734, 7.86903609, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.2672249411856729}
episode index:3846
target Thresh 19.0
target distance 6.0
model initialize at round 3846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.14978841,  7.00002182,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 4.089380670901771}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8074457168088578
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.24476752,  3.0050776 ,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.24482017754292756}
episode index:3847
target Thresh 19.0
target distance 14.0
model initialize at round 3847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.00494231, 11.86740302,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 14.353211141456839}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8074173622296698
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.88176888, 4.03771662, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.1241013389331455}
episode index:3848
target Thresh 19.0
target distance 12.0
model initialize at round 3848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 4.9738013 , 11.86775024,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.145387060298983}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8074086232261026
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.00048961,  7.03843622,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.03843934310129615}
episode index:3849
target Thresh 19.0
target distance 10.0
model initialize at round 3849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.980393886139279}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8073898396591933
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.96483003, 9.72220125, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.2800162016365638}
episode index:3850
target Thresh 19.0
target distance 1.0
model initialize at round 3850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.14472011,  6.79758707,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1694651874826052}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8074398552811982
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.14472011,  6.79758707,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1694651874826052}
episode index:3851
target Thresh 19.0
target distance 3.0
model initialize at round 3851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.0014565 ,  4.99997306,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.000028001965092}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8074768646645623
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.85692531,  6.86324346,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8750197938637092}
episode index:3852
target Thresh 19.0
target distance 5.0
model initialize at round 3852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.84685671,  5.99999988,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.117237079227161}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8075015267811819
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.66832399,  9.32275009,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7421755736546143}
episode index:3853
target Thresh 19.0
target distance 12.0
model initialize at round 3853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.        , 10.99978352,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 13.453479227828367}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8074641419613864
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.30079978, 1.1473254 , 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.1026943894335262}
episode index:3854
target Thresh 19.0
target distance 1.0
model initialize at round 3854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.11072707, 3.02607024, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9802038907120775}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8075140864122394
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.11072707, 3.02607024, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9802038907120775}
episode index:3855
target Thresh 19.0
target distance 10.0
model initialize at round 3855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.13224972,  6.97380116,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.620595846702555}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8075053381889739
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.8683438 , 9.15180402, 0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.2138605217946563}
episode index:3856
target Thresh 19.0
target distance 7.0
model initialize at round 3856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.86736181, 5.99530881, 0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 5.341720143716261}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8075182678394304
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.81776814, 11.75659941,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.778235903100681}
episode index:3857
target Thresh 19.0
target distance 3.0
model initialize at round 3857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.43180217, 8.99992359, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.1502180689751116}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.807555199340768
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.21274934, 10.84133399,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.867816314371697}
episode index:3858
target Thresh 19.0
target distance 11.0
model initialize at round 3858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.86740305, 6.99505755, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.968190001795818}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8075178490510422
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.01240856,  2.89377992,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.10694240085476776}
episode index:3859
target Thresh 19.0
target distance 11.0
model initialize at round 3859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.848857801796113}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.807509108918516
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.91241212, 6.9638249 , 0.        ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9677965050854611}
episode index:3860
target Thresh 19.0
target distance 1.0
model initialize at round 3860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.25603707, 10.89336034,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.1625719538084676}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8075589641091613
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.25603707, 10.89336034,  0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.1625719538084676}
episode index:3861
target Thresh 19.0
target distance 2.0
model initialize at round 3861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.02120602, 9.99762094, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.021339054928565113}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.807608793481479
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.02120602, 9.99762094, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.021339054928565113}
episode index:3862
target Thresh 19.0
target distance 14.0
model initialize at round 3862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.,  4.,  0.]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.165525060596456}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.8075547236253456
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.00628151, 1.92365776, 0.        ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.07660023163136495}
episode index:3863
target Thresh 19.0
target distance 9.0
model initialize at round 3863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.13224972,  4.97380116,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 10.121687508845325}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8075359703041758
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.03698215, 11.64110575,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.6421715192227262}
episode index:3864
target Thresh 19.0
target distance 12.0
model initialize at round 3864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 3.97636478, 11.8670433 ,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 12.742198923026255}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8075077170896322
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.2274359 ,  4.45153162,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.5055767942731961}
episode index:3865
target Thresh 19.0
target distance 3.0
model initialize at round 3865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.86080194,  6.99997151,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.3194835938689138}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8075445748969033
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.13874221,  8.81624448,  0.        ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.1866002020355673}
episode index:3866
target Thresh 19.0
target distance 8.0
model initialize at round 3866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 6., 11.,  0.]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 6.000000000000019}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8075574609649414
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.95593816, 11.86642091,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8675405667955708}
episode index:3867
target Thresh 19.0
target distance 9.0
model initialize at round 3867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.98738563,  4.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 7.000011365870447}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8075592574460776
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.19613276, 11.80442574,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8279908367618662}
episode index:3868
target Thresh 19.0
target distance 14.0
model initialize at round 3868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.02619883, 11.86775027,  0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.381328466292324}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8075405271884345
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.00608245, 6.07419912, 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.07444801055781698}
episode index:3869
target Thresh 19.0
target distance 4.0
model initialize at round 3869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.00003839,  8.99999952,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 2.2360855707250673}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8075773384217191
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.24158239, 10.99503279,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7584338724167696}
episode index:3870
target Thresh 19.0
target distance 1.0
model initialize at round 3870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.79032187, 4.11724079, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.907319539324397}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8076270471950537
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.79032187, 4.11724079, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.907319539324397}
episode index:3871
target Thresh 19.0
target distance 14.0
model initialize at round 3871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.165525060596464}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8075988215361949
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.78992631, 6.93105497, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7929293719087287}
episode index:3872
target Thresh 19.0
target distance 8.0
model initialize at round 3872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 4.97380117, 11.86775027,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 6.088354691965317}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8076116736349462
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.91652712, 11.80228761,  0.        ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8066183324345749}
episode index:3873
target Thresh 19.0
target distance 8.0
model initialize at round 3873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 9.97380792, 10.13225143,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 7.91549087541067}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.807624519098644
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.93529229,  5.93997277,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9421973737560398}
episode index:3874
target Thresh 19.0
target distance 12.0
model initialize at round 3874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 10.04987562112091}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8076157852711345
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.09633242, 11.70541118,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7119584700723898}
episode index:3875
target Thresh 19.0
target distance 1.0
model initialize at round 3875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.41059543,  6.05182064,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.1164416025561554}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8076654200014568
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.41059543,  6.05182064,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.1164416025561554}
episode index:3876
target Thresh 19.0
target distance 13.0
model initialize at round 3876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.831072604542193}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8076467010101294
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.94680054, 11.41305086,  0.        ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.0329773811404035}
episode index:3877
target Thresh 19.0
target distance 2.0
model initialize at round 3877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.98320055, 4.13464427, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8655187814598096}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8076963021702608
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.98320055, 4.13464427, 0.        ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8655187814598096}
episode index:3878
target Thresh 19.0
target distance 2.0
model initialize at round 3878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.0022434 , 10.99951684,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.002294837870434771}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8077458777561927
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.0022434 , 10.99951684,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.002294837870434771}
episode index:3879
target Thresh 19.0
target distance 12.0
model initialize at round 3879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.13259815856355}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8077086804761754
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.76360968,  4.92214268,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.24888178770265015}
episode index:3880
target Thresh 19.0
target distance 5.0
model initialize at round 3880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([12.14612769,  9.8233614 ,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 4.249109894773662}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8077331049336668
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.33547104,  5.89768421,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6723594767336156}
episode index:3881
target Thresh 19.0
target distance 12.0
model initialize at round 3881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.198039027185585}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.8076873852800838
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.19605776, 2.90368884, 0.        ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.21843645815864668}
episode index:3882
target Thresh 19.0
target distance 2.0
model initialize at round 3882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.02359664, 5.00914824, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.025307941910548234}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8077369120930428
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.02359664, 5.00914824, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.025307941910548234}
episode index:3883
target Thresh 19.0
target distance 8.0
model initialize at round 3883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.,  6.,  0.]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.8102496759066815}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8077386549709796
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.93130972, 10.96119293,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.07889450863117664}
episode index:3884
target Thresh 19.0
target distance 12.0
model initialize at round 3884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 5.94469441, 10.04553857,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.877882643124163}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8077104950330447
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.18782323,  2.43803995,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.4766094391928261}
episode index:3885
target Thresh 19.0
target distance 2.0
model initialize at round 3885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.9998374 , 7.99983871, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.0002290278517856886}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8077599776642765
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.9998374 , 7.99983871, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.0002290278517856886}
episode index:3886
target Thresh 19.0
target distance 11.0
model initialize at round 3886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 6., 11.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 9.219544457292901}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.807761713263025
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.09539254,  8.05792272,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.306072067477228}
episode index:3887
target Thresh 19.0
target distance 6.0
model initialize at round 3887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([12.02619246, 11.86774866,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 4.11864218803049}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8077860801063216
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.04903288, 10.83058268,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.17637021034734537}
episode index:3888
target Thresh 19.0
target distance 9.0
model initialize at round 3888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 7.9744563 , 10.13242024,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.011477551858835}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8077773361766208
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.7025472 ,  3.98225764,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0263080650286662}
episode index:3889
target Thresh 19.0
target distance 8.0
model initialize at round 3889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([10.02619865, 11.86775023,  0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 6.673759173398402}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8077900862187348
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.89843701, 8.18731796, 0.        ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8190037518369443}
episode index:3890
target Thresh 19.0
target distance 11.0
model initialize at round 3890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.178937723926285}
done in step count: 10
reward sum = 0.5987369392383787
running average episode reward sum: 0.8077363588615052
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.32205248,  2.31046166,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7610393653973306}
episode index:3891
target Thresh 19.0
target distance 3.0
model initialize at round 3891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.21388531,  8.00124037,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.2729723460860591}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8077729116983855
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.30926418,  6.01439512,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.032986601466809}
episode index:3892
target Thresh 19.0
target distance 1.0
model initialize at round 3892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.07427502, 1.44557852, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.5593746066296411}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8078222893218897
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.07427502, 1.44557852, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.5593746066296411}
episode index:3893
target Thresh 19.0
target distance 10.0
model initialize at round 3893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([5.17664066, 9.14612992, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.016013769250424}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8078240057986945
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.10403458, 11.63450576,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.09788504846455}
episode index:3894
target Thresh 19.0
target distance 11.0
model initialize at round 3894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([11.02619882, 11.86775027,  0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 10.255108868461624}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8078257213941248
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.85717274, 6.98413686, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8573195140572919}
episode index:3895
target Thresh 19.0
target distance 9.0
model initialize at round 3895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.86775028, 3.97380116, 0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.701003238415892}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8078169829999015
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.98080971, 10.13518092,  0.        ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8650319744888487}
episode index:3896
target Thresh 19.0
target distance 3.0
model initialize at round 3896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.06884351,  9.00090527,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.367063923280979}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.807853468249324
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.35279708,  7.19361332,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8801847897691546}
episode index:3897
target Thresh 19.0
target distance 9.0
model initialize at round 3897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 6.97380117, 11.86775027,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 7.079580532205362}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8078551749660379
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.35921709, 11.74010277,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8226718795438605}
episode index:3898
target Thresh 19.0
target distance 1.0
model initialize at round 3898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.2486044 ,  1.12211314,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9124086163295643}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8079044555059286
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.2486044 ,  1.12211314,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9124086163295643}
episode index:3899
target Thresh 19.0
target distance 14.0
model initialize at round 3899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4., 5., 0.]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 12.369316876852995}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.8078589029300872
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.32376887,  2.89755994,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9541698652730134}
episode index:3900
target Thresh 19.0
target distance 1.0
model initialize at round 3900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.15601374, 1.14357639, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8705180522499764}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8079081572487413
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.15601374, 1.14357639, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8705180522499764}
episode index:3901
target Thresh 19.0
target distance 5.0
model initialize at round 3901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.13259945, 7.00493021, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 3.127617188498105}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8079323991356586
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.06162628, 3.04184065, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9601391266586142}
episode index:3902
target Thresh 19.0
target distance 3.0
model initialize at round 3902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.14483388,  6.01674233,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.3285608975837504}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.807968798725939
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.4355942 ,  4.06193979,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0342626680536149}
episode index:3903
target Thresh 19.0
target distance 10.0
model initialize at round 3903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 3.99505755, 11.86740305,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.051800526773253}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8079704732780071
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.89516726, 11.86876431,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8750664689449669}
episode index:3904
target Thresh 19.0
target distance 9.0
model initialize at round 3904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.13264613,  3.99535649,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.005899349932356}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8079721469724301
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.92045552, 11.22937531,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9486049782848224}
episode index:3905
target Thresh 19.0
target distance 13.0
model initialize at round 3905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 4.        , 10.99970734,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 13.601298377168161}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8079351393647283
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.51054317,  2.66414625,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.6111072487726543}
episode index:3906
target Thresh 19.0
target distance 3.0
model initialize at round 3906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.22366083,  8.00232673,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.02697762043648}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8079715009876193
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.52154666,  6.02439344,  0.        ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0866120550031828}
episode index:3907
target Thresh 19.0
target distance 10.0
model initialize at round 3907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13., 11.,  0.]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.000000000000018}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8079731731342447
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.07824822, 11.77150899,  0.        ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7754668992754599}
episode index:3908
target Thresh 19.0
target distance 7.0
model initialize at round 3908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.        ,  9.00000381,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 5.099023254211262}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8079858111047911
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.44041535,  3.05296528,  0.        ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0444330697789146}
episode index:3909
target Thresh 19.0
target distance 1.0
model initialize at round 3909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.61883581, 10.84956169,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0510531970995107}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8080349195929996
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.61883581, 10.84956169,  0.        ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0510531970995107}
episode index:3910
target Thresh 19.0
target distance 14.0
model initialize at round 3910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4., 8., 0.]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.000000000000018}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.808006871108341
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.10895401,  8.26470163,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.28624802266267224}
episode index:3911
target Thresh 19.0
target distance 1.0
model initialize at round 3911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.67947121,  3.72241276,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.4470350003290324}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8080431679204299
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.9961721 ,  5.71005737,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.7100676909208337}
episode index:3912
target Thresh 19.0
target distance 9.0
model initialize at round 3912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([5.85387008, 9.82335934, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 8.32763626557908}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8080448196153134
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.76138749, 1.90735592, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.2559665103712562}
episode index:3913
target Thresh 19.0
target distance 11.0
model initialize at round 3913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 5.94316028, 10.04308927,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.11270534016168}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8080167900998505
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.20302464,  2.46593355,  0.        ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5082450982995037}
episode index:3914
target Thresh 19.0
target distance 6.0
model initialize at round 3914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([10.97380398, 10.1322503 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 6.5230550788250365}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8080293975608723
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.61428994,  4.92155539,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.3936061612344926}
episode index:3915
target Thresh 19.0
target distance 10.0
model initialize at round 3915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 5.97380118, 11.86775027,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 8.240652803557364}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8080206517845544
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.27408977, 10.61028625,  0.        ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6690100954806257}
episode index:3916
target Thresh 19.0
target distance 12.0
model initialize at round 3916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 4.        , 10.99982738,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 12.206456628320831}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8079926499066654
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.93671747,  4.22948562,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.2380511055843052}
episode index:3917
target Thresh 19.0
target distance 6.0
model initialize at round 3917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.99966335,  9.00002313,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.000023140768564}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8080167712313447
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.0100596 ,  5.00374401,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.0107337335463173}
episode index:3918
target Thresh 19.0
target distance 11.0
model initialize at round 3918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 4.97741604, 11.86670756,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 11.970468207131542}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8079887846339633
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.83672701,  3.16265869,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8531110946420882}
episode index:3919
target Thresh 19.0
target distance 14.0
model initialize at round 3919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14., 10.,  0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.369316876852997}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8079701884875324
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.96039608, 6.15388218, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8470441784997534}
episode index:3920
target Thresh 19.0
target distance 9.0
model initialize at round 3920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 7.02619869, 10.13224963,  0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 9.074346249380351}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.807971855424924
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.9984972 , 2.92559973, 0.        ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9256009462424393}
episode index:3921
target Thresh 19.0
target distance 11.0
model initialize at round 3921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([12.82335934,  9.14612992,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.089681773474469}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8079532730779582
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.40246439, 4.85904376, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0464248553376598}
episode index:3922
target Thresh 19.0
target distance 8.0
model initialize at round 3922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.99981713,  9.00000346,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 6.000003459856144}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8079658710200744
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.30394998,  3.00479281,  0.        ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.30398776708379}
episode index:3923
target Thresh 19.0
target distance 3.0
model initialize at round 3923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.95442042, 9.05540519, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.4229541268896475}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8080020672812823
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.69199656, 7.1276367 , 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9251399034148757}
episode index:3924
target Thresh 19.0
target distance 7.0
model initialize at round 3924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.13259753, 8.00493934, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.341966847424985}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8080146463724208
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.0338361 , 2.09585535, 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9047775569522052}
episode index:3925
target Thresh 19.0
target distance 2.0
model initialize at round 3925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.99983394, 6.99968052, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.0003600603862459654}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8080635473794578
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.99983394, 6.99968052, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.0003600603862459654}
episode index:3926
target Thresh 19.0
target distance 11.0
model initialize at round 3926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.574514830248853}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8080449653431059
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.0154379 ,  9.26110793,  0.        ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7390533236328576}
episode index:3927
target Thresh 19.0
target distance 11.0
model initialize at round 3927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.        , 10.74858859,  0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.551406389278364}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8080081464698742
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.47572189, 1.29577099, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8779556054356896}
episode index:3928
target Thresh 19.0
target distance 7.0
model initialize at round 3928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.00991166,  7.0000118 ,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 5.381502544155304}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8080207112073469
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.15224569,  1.12789454,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8852946921116646}
episode index:3929
target Thresh 19.0
target distance 3.0
model initialize at round 3929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.88383877,  9.00197387,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0086848178177823}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8080568382528411
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.53296399,  7.02812624,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.1084174391772865}
episode index:3930
target Thresh 19.0
target distance 10.0
model initialize at round 3930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.86775037, 7.97380131, 0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.604242683135503}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8080289268964027
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.8999589 ,  4.07000275,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9353625548990805}
episode index:3931
target Thresh 19.0
target distance 10.0
model initialize at round 3931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86775028, 6.97380116, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.604242728305834}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8080010297369921
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.18453314,  4.89745518,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9162304691498576}
episode index:3932
target Thresh 19.0
target distance 13.0
model initialize at round 3932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4., 8., 0.]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.045361017187277}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8079824919441845
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.39942788,  6.02382624,  0.        ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.0547311753576127}
episode index:3933
target Thresh 19.0
target distance 13.0
model initialize at round 3933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.309950023624257}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.807937313224759
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.19119951, 2.93025912, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.203521602275786}
episode index:3934
target Thresh 19.0
target distance 12.0
model initialize at round 3934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.891740190748873}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.807909460615577
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.80332799, 3.83542611, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.2564457909116101}
episode index:3935
target Thresh 19.0
target distance 2.0
model initialize at round 3935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.17606282,  4.9955169 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.1761198899337493}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8079582641062744
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.17606282,  4.9955169 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.1761198899337493}
episode index:3936
target Thresh 19.0
target distance 9.0
model initialize at round 3936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.86775027, 3.9738012 , 0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.11684333219618}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.807970815982295
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.99635611, 10.09680416,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9032031924651707}
episode index:3937
target Thresh 19.0
target distance 14.0
model initialize at round 3937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.00494231, 11.86740302,  0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 14.353211141456839}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8079429760838978
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.84348503, 4.03879066, 0.        ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.16125027684808785}
episode index:3938
target Thresh 19.0
target distance 14.0
model initialize at round 3938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.041594578792317}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8079151503210162
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.77938895, 6.8781918 , 0.        ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.1741669274630815}
episode index:3939
target Thresh 19.0
target distance 2.0
model initialize at round 3939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.31193164, 9.99572861, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.311960883337474}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8079639028209347
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.31193164, 9.99572861, 0.        ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.311960883337474}
episode index:3940
target Thresh 19.0
target distance 1.0
model initialize at round 3940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.68898752, 7.04379582, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0055124000999973}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8080126305796709
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.68898752, 7.04379582, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0055124000999973}
episode index:3941
target Thresh 19.0
target distance 11.0
model initialize at round 3941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.86775028, 4.97380116, 0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.788874285776417}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.807975950671175
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.02051916, 11.83337231,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8336248802529836}
episode index:3942
target Thresh 19.0
target distance 4.0
model initialize at round 3942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.00000072, 9.00002396, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.2360897288246546}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8080119699583495
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.13317425, 7.00675363, 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8668520631389023}
episode index:3943
target Thresh 19.0
target distance 2.0
model initialize at round 3943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.01060879,  3.1993773 ,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.1994242161486126}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8080479709801652
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.55400515,  1.26643695,  0.        ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8585022725860497}
episode index:3944
target Thresh 19.0
target distance 6.0
model initialize at round 3944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.62734428,  7.        ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.017321531230747}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8080719131928445
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.72620776, 10.99641174,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7262166288995676}
episode index:3945
target Thresh 19.0
target distance 2.0
model initialize at round 3945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.133096  ,  4.99622943,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8669122047485256}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.808120551836232
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.133096  ,  4.99622943,  0.        ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8669122047485256}
episode index:3946
target Thresh 19.0
target distance 2.0
model initialize at round 3946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.87868936,  7.00274467,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.12134168925327019}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.80816916583374
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.87868936,  7.00274467,  0.        ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.12134168925327019}
episode index:3947
target Thresh 19.0
target distance 11.0
model initialize at round 3947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13., 10.,  0.]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.219544457292901}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8081707709715733
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.8415989 , 8.94222118, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.263356429005497}
episode index:3948
target Thresh 19.0
target distance 1.0
model initialize at round 3948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.70842147, 9.81266141, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0780906936137924}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8082193476312411
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.70842147, 9.81266141, 0.        ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0780906936137924}
episode index:3949
target Thresh 19.0
target distance 2.0
model initialize at round 3949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.52549028, 4.99791527, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.4745142955890116}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.808267899695132
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.52549028, 4.99791527, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.4745142955890116}
episode index:3950
target Thresh 19.0
target distance 3.0
model initialize at round 3950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.3593837 , 3.00168717, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.0642057273632197}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8083037721578769
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.95331903, 1.13313371, 0.        ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8681222693414239}
episode index:3951
target Thresh 19.0
target distance 11.0
model initialize at round 3951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.86740305, 4.99505755, 0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.18772204862523}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8082759466325569
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.21533217,  6.91748541,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9424157393060558}
episode index:3952
target Thresh 19.0
target distance 7.0
model initialize at round 3952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.506778200582048}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8082883673392018
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.02533971, 11.86724512,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.3046366421123337}
episode index:3953
target Thresh 19.0
target distance 12.0
model initialize at round 3953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.1325968 ,  7.99505776,  0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.891740190748873}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8082605597845115
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.99309558, 3.83541263, 0.        ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.16473212297609985}
episode index:3954
target Thresh 19.0
target distance 4.0
model initialize at round 3954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.02264154,  8.00051367,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.000641795646855}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8082963978224926
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.2486488,  6.0013287,  0.       ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7513523729691077}
episode index:3955
target Thresh 19.0
target distance 6.0
model initialize at round 3955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([12.14612787,  9.82336124,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 6.173908328567357}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8083202106642968
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.96263675,  5.89694652,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.897724383307095}
episode index:3956
target Thresh 19.0
target distance 4.0
model initialize at round 3956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.00000155, 10.99976969,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.0000015629816983}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8083560155137625
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.01034999, 10.52735692,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.4727563863768281}
episode index:3957
target Thresh 19.0
target distance 14.0
model initialize at round 3957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.165525060596464}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8083282189702
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.78251688, 6.910717  , 0.        ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7875938799087933}
episode index:3958
target Thresh 19.0
target distance 1.0
model initialize at round 3958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.66357592,  8.08346927,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9763246075406966}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8083766331609122
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.66357592,  8.08346927,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9763246075406966}
episode index:3959
target Thresh 19.0
target distance 13.0
model initialize at round 3959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.,  7.,  0.]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.045361017187282}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8083581269127972
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.8154655 , 7.00932208, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0077180811516901}
episode index:3960
target Thresh 19.0
target distance 10.0
model initialize at round 3960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.86775037, 7.97380131, 0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.620595705988672}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8083493975037053
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.06050646, 10.0182544 ,  0.        ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.983608381499188}
episode index:3961
target Thresh 19.0
target distance 11.0
model initialize at round 3961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.82335934,  9.14612992,  0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.662494173405971}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8083309074716813
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.75830201, 4.86528429, 0.        ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.2767060522372802}
episode index:3962
target Thresh 19.0
target distance 11.0
model initialize at round 3962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.13259695,  4.99505755,  0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.350086702751616}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8083031523338116
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.9910099 , 6.94618576, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.054560008305284007}
episode index:3963
target Thresh 19.0
target distance 3.0
model initialize at round 3963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.97364044,  5.00069916,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0010462727057892}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8083388982590552
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.44251525,  3.04723954,  0.        ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0505104665614657}
episode index:3964
target Thresh 19.0
target distance 3.0
model initialize at round 3964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([12.86784197,  9.02936779,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.5301568080661436}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8083746261535674
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.39256917,  7.18325741,  0.        ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9061892817394559}
episode index:3965
target Thresh 19.0
target distance 8.0
model initialize at round 3965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.13262162,  4.9752083 ,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.026251196204082}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8083869812654803
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.11685698, 11.09095294,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.1480810284748578}
episode index:3966
target Thresh 19.0
target distance 11.0
model initialize at round 3966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.13259695,  3.99505755,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.509715282311054}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8083592399785703
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.03525069, 11.58387499,  0.        ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5849381262169572}
episode index:3967
target Thresh 19.0
target distance 6.0
model initialize at round 3967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.99976718,  7.        ,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.000000006775416}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.808382964968495
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.19458305, 10.98890781,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.19489894891921256}
episode index:3968
target Thresh 19.0
target distance 2.0
model initialize at round 3968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.98006511, 7.03237736, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.038022276779487586}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8084312433849807
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.98006511, 7.03237736, 0.        ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.038022276779487586}
episode index:3969
target Thresh 19.0
target distance 13.0
model initialize at round 3969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.045361017187282}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8084035119121113
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.89731831, 6.89147172, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8973657840071589}
episode index:3970
target Thresh 19.0
target distance 9.0
model initialize at round 3970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([10.02619884, 11.86775028,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 7.079580542486185}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8084050487386255
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.23960328, 10.56025525,  0.        ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8783954799358874}
episode index:3971
target Thresh 19.0
target distance 7.0
model initialize at round 3971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.85141623, 4.        , 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 5.331767254621541}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8084173775279662
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.13877603, 9.78539383, 0.        ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7975601922373446}
episode index:3972
target Thresh 19.0
target distance 2.0
model initialize at round 3972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.43920028, 8.00525773, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5608243640213532}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8084655986763357
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.43920028, 8.00525773, 0.        ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5608243640213532}
episode index:3973
target Thresh 19.0
target distance 13.0
model initialize at round 3973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 3.97382077, 11.86774532,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.990094457797154}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.808447135236967
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.91001115,  5.9997051 ,  0.        ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0037471185368516}
episode index:3974
target Thresh 19.0
target distance 10.0
model initialize at round 3974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 6.        , 10.99999225,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.43397702530706}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8084384141809324
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.31656373,  5.95711861,  0.        ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.684780214484251}
episode index:3975
target Thresh 19.0
target distance 7.0
model initialize at round 3975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 7.02619849, 10.13224968,  0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 7.1834990106723335}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8084507221753537
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.13682904, 4.93016459, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8659913966347051}
episode index:3976
target Thresh 19.0
target distance 2.0
model initialize at round 3976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.23965473,  7.99589133,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.23968995186795583}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8084988864393278
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.23965473,  7.99589133,  0.        ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.23968995186795583}
episode index:3977
target Thresh 19.0
target distance 8.0
model initialize at round 3977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.86775028, 5.97380116, 0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.72534589000672}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8084901589509066
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.01230266, 11.86602579,  0.        ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8661131682759773}
episode index:3978
target Thresh 19.0
target distance 13.0
model initialize at round 3978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4., 6., 0.]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.401754250991392}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.8084453635879445
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.10444132,  2.14751138,  0.        ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8588625200849986}
episode index:3979
target Thresh 19.0
target distance 11.0
model initialize at round 3979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.13224972,  5.97380116,  0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.902878591090003}
done in step count: 6
reward sum = 0.7350918906249998
running average episode reward sum: 0.8084269330670995
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.86745435, 9.58122473, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9632495923248255}
episode index:3980
target Thresh 19.0
target distance 13.0
model initialize at round 3980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.205906925752268}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8083905084246031
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.05721539,  9.3832071 ,  0.        ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.3874548763074542}
episode index:3981
target Thresh 19.0
target distance 1.0
model initialize at round 3981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.71888208, 8.93481982, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1792707665167055}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8084386273325829
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.71888208, 8.93481982, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1792707665167055}
episode index:3982
target Thresh 19.0
target distance 6.0
model initialize at round 3982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.67526686,  7.        ,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 4.056597753635598}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.808462243042517
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.71509111, 10.99307108,  0.        ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7151246775967373}
episode index:3983
target Thresh 19.0
target distance 3.0
model initialize at round 3983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.13924493, 3.9999299 , 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.009717458142539}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.808497769587938
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.30207404, 5.56469238, 0.        ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6404109699632761}
episode index:3984
target Thresh 19.0
target distance 6.0
model initialize at round 3984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.86281513,  5.98129125,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.110288307017918}
done in step count: 2
reward sum = 0.9025
running average episode reward sum: 0.8085213586043526
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.13282444,  9.97653296,  0.        ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.1348815504938352}
episode index:3985
target Thresh 19.0
target distance 8.0
model initialize at round 3985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([12.05521678, 10.045681  ,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 6.1299571741492365}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8085336149117776
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.07153535, 11.08809926,  0.        ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.11348473827230603}
episode index:3986
target Thresh 19.0
target distance 6.0
model initialize at round 3986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.13259695,  6.99505755,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.754817254247167}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8085458650710673
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.91883591, 10.79211604,  0.        ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.2231666414551047}
episode index:3987
target Thresh 19.0
target distance 8.0
model initialize at round 3987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.,  5.,  0.]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.485281374238598}
done in step count: 4
reward sum = 0.8145062499999999
running average episode reward sum: 0.8085473596510394
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.93910791, 10.61521161,  0.        ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0148821442957339}
episode index:3988
target Thresh 19.0
target distance 10.0
model initialize at round 3988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4.86740305, 3.99505755, 0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.73351535954344}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8085197311567908
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.98810426, 10.06372655,  0.        ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.36123400363153}
episode index:3989
target Thresh 19.0
target distance 1.0
model initialize at round 3989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.39729059, 7.05139256, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.0284434331147465}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8085677211991074
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.39729059, 7.05139256, 0.        ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.0284434331147465}
episode index:3990
target Thresh 19.0
target distance 13.0
model initialize at round 3990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 4., 10.,  0.]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.045361017187277}
done in step count: 5
reward sum = 0.7737809374999998
running average episode reward sum: 0.8085590048914905
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.08907148,  8.06333187,  0.        ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.3065748952698668}
episode index:3991
target Thresh 19.0
target distance 2.0
model initialize at round 3991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.6989398 , 5.99510789, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.698956920803589}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8086069610525898
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.6989398 , 5.99510789, 0.        ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.698956920803589}
episode index:3992
target Thresh 19.0
target distance 12.0
model initialize at round 3992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.        , 9.99998295, 0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 11.661895019127476}
done in step count: 7
reward sum = 0.6983372960937497
running average episode reward sum: 0.8085793453087984
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.11877844,  4.03348741,  0.        ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.12340876874400186}
episode index:3993
target Thresh 19.0
target distance 2.0
model initialize at round 3993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.14154875,  2.00333452,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.1415880236939741}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8086272723630525
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.14154875,  2.00333452,  0.        ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.1415880236939741}
episode index:3994
target Thresh 19.0
target distance 3.0
model initialize at round 3994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.83958082,  9.99986109,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.3058230350999132}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.808662659779232
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.58470691, 11.67171678,  0.        ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.7897289290949059}
episode index:3995
target Thresh 19.0
target distance 3.0
model initialize at round 3995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.72501385,  6.00058174,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0376806843221258}
done in step count: 1
reward sum = 0.95
running average episode reward sum: 0.8086980294839919
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.31699491,  4.0320574 ,  0.        ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0185276816282924}
episode index:3996
target Thresh 19.0
target distance 13.0
model initialize at round 3996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.02499488, 11.86743426,  0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.148565386635095}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8086616828244485
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.10052476, 2.1166498 , 0.        ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8890516310068115}
episode index:3997
target Thresh 19.0
target distance 14.0
model initialize at round 3997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.,  8.,  0.]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.00000000000001}
done in step count: 8
reward sum = 0.6634204312890623
running average episode reward sum: 0.8086253543473261
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.16126391, 2.07898152, 0.        ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9350299952161171}
episode index:3998
target Thresh 19.0
target distance 11.0
model initialize at round 3998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.86740305, 5.99505755, 0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.611175621019552}
done in step count: 9
reward sum = 0.6302494097246091
running average episode reward sum: 0.808580749209886
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.38079891,  2.34578435,  0.        ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9007819372966777}
episode index:3999
target Thresh 19.0
target distance 8.0
model initialize at round 3999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.86732107, 3.99555079, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.288107692037973}
done in step count: 3
reward sum = 0.8573749999999999
running average episode reward sum: 0.8085929477725835
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.05304712, 9.99039894, 0.        ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.05390896706807638}

Process finished with exit code 0
