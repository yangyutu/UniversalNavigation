: array([3.86569106, 8.18604873, 1.58463674]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.188249833732686}
episode index:2370
target Thresh 19.0
target distance 10.0
model initialize at round 2370
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.        , 8.        , 4.36787987]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.180339889524829}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8113073895644096
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.43164089,  3.63662061,  5.5360268 ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7691551563861053}
episode index:2371
target Thresh 19.0
target distance 3.0
model initialize at round 2371
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.        ,  2.        ,  4.84188533]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.1622776602059597}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8113622684682192
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.82860125,  5.06046262,  4.27551471]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.1817505439871199}
episode index:2372
target Thresh 19.0
target distance 13.0
model initialize at round 2372
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([3.40075535, 3.93288399, 1.59753149]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.997903030907835}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8113022640392691
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.51251666,  6.84666097,  6.19930781]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.5349637227287826}
episode index:2373
target Thresh 19.0
target distance 5.0
model initialize at round 2373
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.00684485, 10.00002606,  1.01380699]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 5.000030743582462}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8113453200558842
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.33788318,  4.78822006,  0.16425107]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.398767830949696}
episode index:2374
target Thresh 19.0
target distance 12.0
model initialize at round 2374
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 2.9928781 , 10.63609418,  2.58514929]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 14.229578724441927}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8112969275918817
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.63264191,  3.02229126,  5.47011091]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.36803378889948285}
episode index:2375
target Thresh 19.0
target distance 6.0
model initialize at round 2375
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.66377902, 7.51227547, 2.13132267]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.374909939495874}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8113174500780135
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.1512498 , 10.40038341,  0.71539613]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6183983827870757}
episode index:2376
target Thresh 19.0
target distance 9.0
model initialize at round 2376
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 5.88194409, 11.86810876,  2.02218603]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.633614137860997}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8112968464870455
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.69629583,  4.86857317,  5.75670357]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.7085907782852621}
episode index:2377
target Thresh 19.0
target distance 13.0
model initialize at round 2377
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.32519491,  5.83444004,  4.25012565]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.385015142679332}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8112698806535066
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.43392147, 6.96933211, 5.70145788]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.43500386671987423}
episode index:2378
target Thresh 19.0
target distance 10.0
model initialize at round 2378
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([2.        , 4.        , 5.72561932]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 12.206555615733702}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8112525512147305
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.90311775, 11.31808463,  1.17695155]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.33251165524293264}
episode index:2379
target Thresh 19.0
target distance 12.0
model initialize at round 2379
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([15.        , 10.        ,  1.90505713]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.999999999999998}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8112320008631054
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.88589486, 4.72076162, 5.63957467]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.30165220852149865}
episode index:2380
target Thresh 19.0
target distance 13.0
model initialize at round 2380
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([15.        ,  5.        ,  5.65183902]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8112019581804196
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.21912386, 11.39058913,  2.81998595]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.44785615559389436}
episode index:2381
target Thresh 19.0
target distance 3.0
model initialize at round 2381
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.61757383, 5.55921563, 2.19531763]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.567564045459201}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8112687495497813
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.7841734 , 6.79958195, 1.91213232]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8093795876490546}
episode index:2382
target Thresh 19.0
target distance 13.0
model initialize at round 2382
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([3.56606027, 2.61631431, 1.38492888]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 13.610796899495513}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8112264853776129
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.05427551,  9.5337348 ,  0.2698905 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0544183405706036}
episode index:2383
target Thresh 19.0
target distance 7.0
model initialize at round 2383
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.88341943, 4.58091664, 1.3613407 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 6.515472609715507}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8112542934890312
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.12618518, 10.72704711,  2.22859947]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9154537781251054}
episode index:2384
target Thresh 19.0
target distance 6.0
model initialize at round 2384
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 8.15471293, 10.12995286,  5.18427038]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 6.601367730571496}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8112711502930589
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.66348827, 4.61639619, 5.76834385]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.5102862178403806}
episode index:2385
target Thresh 19.0
target distance 3.0
model initialize at round 2385
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.87931741, 4.77231876, 0.21966475]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.9784622594996826}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.811337800691092
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.46758794, 3.20684891, 6.21966475]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.5112973179532682}
episode index:2386
target Thresh 19.0
target distance 14.0
model initialize at round 2386
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([16.        ,  4.        ,  1.36563414]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.8112617578886253
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.93233378, 3.77889804, 5.40103984]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.2148779528538896}
episode index:2387
target Thresh 19.0
target distance 1.0
model initialize at round 2387
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.99999994, 6.99999817, 5.68885756]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0000000614765734}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8113366063987222
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.55269226, 7.64855953, 1.40567225]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.8521139597555852}
episode index:2388
target Thresh 19.0
target distance 12.0
model initialize at round 2388
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 4.       , 11.       ,  5.3793931]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.649110640673536}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8113225774129543
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.14106248,  7.12876548,  5.11391064]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8685356692396882}
episode index:2389
target Thresh 19.0
target distance 2.0
model initialize at round 2389
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.44380411, 9.37660235, 5.98924828]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6717017171572949}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8114015219412335
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.44380411, 9.37660235, 5.98924828]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6717017171572949}
episode index:2390
target Thresh 19.0
target distance 1.0
model initialize at round 2390
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.00000001,  9.99999998,  6.18549013]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0000000167140062}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8114720775573183
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.42581186, 11.54497044,  3.90230482]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7916342642846048}
episode index:2391
target Thresh 19.0
target distance 1.0
model initialize at round 2391
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.2491953 , 5.83414769, 4.36900067]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.1222789657850347}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8115508935784064
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.2491953 , 5.83414769, 4.36900067]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.1222789657850347}
episode index:2392
target Thresh 19.0
target distance 14.0
model initialize at round 2392
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([15.46000147, 11.59398546,  2.90743417]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 13.473101289278372}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8115433981292067
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.5090693 , 10.42524399,  4.92513702]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7677864419827468}
episode index:2393
target Thresh 19.0
target distance 2.0
model initialize at round 2393
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4.       , 6.       , 1.3168537]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.0000000000021743}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8116138060664961
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.22318994, 5.68201113, 5.3168537 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.3884979693279571}
episode index:2394
target Thresh 19.0
target distance 2.0
model initialize at round 2394
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([4.00000001, 3.99999998, 0.11449068]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 2.2360679815802866}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8116800629324392
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.96146091, 2.50720271, 6.11449068]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.080396340841719}
episode index:2395
target Thresh 19.0
target distance 1.0
model initialize at round 2395
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([14.99999999,  8.        ,  4.3514812 ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.414213567659958}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8117503550597629
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.82528658,  9.46670243,  2.06829589]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.49833316169969977}
episode index:2396
target Thresh 19.0
target distance 11.0
model initialize at round 2396
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([3.        , 2.        , 5.67131329]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 13.601470509096686}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8117051724731712
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.97205148,  9.51094774,  0.2730896 ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.48985021082825664}
episode index:2397
target Thresh 19.0
target distance 14.0
model initialize at round 2397
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([15.63224   ,  1.40656525,  5.49556375]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.645150503252097}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.8115948519852303
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.87063374, 1.98847412, 6.11504291]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8707100262246101}
episode index:2398
target Thresh 19.0
target distance 7.0
model initialize at round 2398
at step 0:
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.07864644,  8.68113141,  2.53404856]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.448784262238663}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8116223326734394
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.83273394, 11.78256708,  3.40130733]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.142741019839783}
episode index:2399
target Thresh 19.0
target distance 4.0
model initialize at round 2399
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.27947332,  6.09331427,  1.7171027 ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.9946586795314554}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.811676440097076
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.80022322,  8.73702808,  1.15073209]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.3302498952362036}
episode index:2400
target Thresh 19.0
target distance 14.0
model initialize at round 2400
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([14.3258772 ,  8.17233963,  4.04901075]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 12.646063102728373}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.811662339688622
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.74998361, 11.33596695,  3.7835283 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.4187863286284586}
episode index:2401
target Thresh 19.0
target distance 3.0
model initialize at round 2401
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.        , 6.        , 2.24022633]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.811720344563814
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.20167312, 2.90325519, 5.95704102]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.2236774635282306}
episode index:2402
target Thresh 19.0
target distance 11.0
model initialize at round 2402
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.40446179,  3.53539321,  3.82784259]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.410408989502123}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8116842480142485
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.42722095, 8.70734755, 4.99598952]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8263524030461744}
episode index:2403
target Thresh 19.0
target distance 1.0
model initialize at round 2403
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.        , 11.        ,  3.38458085]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9999999999999999}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8117543044834606
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.67158582, 11.15318781,  1.10139554]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6888352598708007}
episode index:2404
target Thresh 19.0
target distance 13.0
model initialize at round 2404
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.23000746,  3.50350423,  5.24718475]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.322078053239384}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.8116658228626634
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.58887671, 2.33960082, 0.43303453]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.6797826823848175}
episode index:2405
target Thresh 19.0
target distance 11.0
model initialize at round 2405
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([3.98264906, 1.20047296, 0.22627657]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.401186059585172}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.8115724608340162
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.64008316,  4.63343356,  5.69531166]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9005245802215943}
episode index:2406
target Thresh 19.0
target distance 3.0
model initialize at round 2406
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.0001132 , 5.9999408 , 0.52813929]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.162298029565464}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8116343734011812
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.64517072, 9.26776781, 2.24495398]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.44452606346878687}
episode index:2407
target Thresh 19.0
target distance 1.0
model initialize at round 2407
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 3.00000001, 11.00000002,  2.28750944]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.414213583602118}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8117084455052505
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.33258165, 10.77196371,  4.28750944]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0204779383356206}
episode index:2408
target Thresh 19.0
target distance 8.0
model initialize at round 2408
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([10.67544313, 11.15899179,  1.10461205]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 9.74269228115103}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8117179122200471
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.63166531,  3.92461838,  5.68868551]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.995283671006034}
episode index:2409
target Thresh 19.0
target distance 2.0
model initialize at round 2409
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.99999954,  7.99999924,  5.17572594]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.2360679035486855}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8117837135012835
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.24208557,  8.46589269,  4.89254063]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.586409450936429}
episode index:2410
target Thresh 19.0
target distance 14.0
model initialize at round 2410
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([2.        , 8.        , 5.40434265]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 14.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8117569148205828
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.19594062,  7.86772644,  0.57248958]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.2364085859470104}
episode index:2411
target Thresh 19.0
target distance 14.0
model initialize at round 2411
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([16.        , 10.        ,  0.49155634]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 14.035668850014659}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8117461027252207
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.83169076, 11.02902015,  4.50925919]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8321969007121375}
episode index:2412
target Thresh 19.0
target distance 7.0
model initialize at round 2412
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([1.9984792 , 5.00091477, 3.61006808]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 9.2201039648949}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8117555381411863
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.35770115, 11.48970558,  1.91095624]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.807687665676949}
episode index:2413
target Thresh 19.0
target distance 7.0
model initialize at round 2413
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.88063683,  9.74325254,  4.99474549]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 5.810376147836405}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.811793908703269
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.95419229,  4.16528816,  6.14518957]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9684023482032731}
episode index:2414
target Thresh 19.0
target distance 9.0
model initialize at round 2414
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([10.59342049, 11.71762694,  3.58949137]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 8.065080967975987}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8118211248996646
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.74411946, 9.07450644, 4.45675014]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.7478402106915948}
episode index:2415
target Thresh 19.0
target distance 13.0
model initialize at round 2415
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([15.00007874,  2.99980272,  6.10215044]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 13.15305426032426}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8117537745285839
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.69009919, 5.88116335, 4.13755614]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9340703199943865}
episode index:2416
target Thresh 19.0
target distance 7.0
model initialize at round 2416
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.       , 11.       ,  4.0300467]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 7.071067811865519}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8117920981944837
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.2891849 ,  4.67055915,  5.18049078]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9771937719988842}
episode index:2417
target Thresh 19.0
target distance 13.0
model initialize at round 2417
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([3.61301049, 8.48019291, 1.29934519]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.397109951672318}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8117684948814189
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.55362356,  8.63171828,  0.75067743]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7735114216064136}
episode index:2418
target Thresh 19.0
target distance 9.0
model initialize at round 2418
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([2.00000611, 8.99998212, 6.05161524]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 9.219542373078236}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8117849021886588
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.91199965, 10.22153845,  0.3525034 ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7834197093015722}
episode index:2419
target Thresh 19.0
target distance 7.0
model initialize at round 2419
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.        , 11.        ,  2.33564511]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8118194283672001
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.19896334,  4.71429222,  5.48608919]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0732535119870763}
episode index:2420
target Thresh 19.0
target distance 12.0
model initialize at round 2420
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.        , 9.        , 4.69176078]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.649110642402865}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8117835592666592
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.90596575,  5.03016768,  5.85990771]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.0987548901208513}
episode index:2421
target Thresh 19.0
target distance 3.0
model initialize at round 2421
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.        ,  8.        ,  2.35499361]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8118410351091998
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.97512799,  4.73223746,  6.0718083 ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.26891522268416906}
episode index:2422
target Thresh 19.0
target distance 3.0
model initialize at round 2422
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.32899446,  8.20031569,  4.03228474]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.0991060799786525}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8118945386644172
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.70607432, 11.04424863,  3.46591412]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.2972376949193108}
episode index:2423
target Thresh 19.0
target distance 6.0
model initialize at round 2423
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([2.63188914, 5.98013269, 3.52993584]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.915611329912558}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8119038700269525
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.70251898, 11.4277619 ,  1.83082399]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5210328207351911}
episode index:2424
target Thresh 19.0
target distance 14.0
model initialize at round 2424
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.3207707 ,  1.88785264,  4.21827865]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.500502271518444}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.8118261881815357
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.99280654, 4.49058766, 3.97049904]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.49064040053279667}
episode index:2425
target Thresh 19.0
target distance 6.0
model initialize at round 2425
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.        ,  3.        ,  5.99096394]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8118719089178286
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.66232455,  8.033756  ,  3.14140801]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0235488178208039}
episode index:2426
target Thresh 19.0
target distance 1.0
model initialize at round 2426
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([16.        ,  8.        ,  2.01914326]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.414213562373958}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8119412241593127
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.85909761,  6.60816742,  6.01914326]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.4163967511700544}
episode index:2427
target Thresh 19.0
target distance 11.0
model initialize at round 2427
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([16.        ,  3.        ,  5.44563818]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 13.601470508735442}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8119176566399734
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.17312176, 10.63729428,  4.89697042]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.4019036956061424}
episode index:2428
target Thresh 19.0
target distance 4.0
model initialize at round 2428
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.        ,  2.        ,  5.32026291]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8119632832508371
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.73855542,  6.39457279,  2.47070699]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.47332964466873484}
episode index:2429
target Thresh 19.0
target distance 5.0
model initialize at round 2429
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([11.74856354, 10.07059489,  6.16254187]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.351839359133636}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8120127079688025
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.10646901, 11.13168908,  1.31298594]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.1693448121101012}
episode index:2430
target Thresh 19.0
target distance 13.0
model initialize at round 2430
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.38143175,  9.53888733,  4.42912936]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.484997292843927}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8119986432429409
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.41653198, 7.79376569, 4.1636469 ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.61884369431322}
episode index:2431
target Thresh 19.0
target distance 9.0
model initialize at round 2431
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.        , 11.        ,  2.47652128]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8120184046374304
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.53181768,  2.92885041,  1.06059475]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.0703238417034306}
episode index:2432
target Thresh 19.0
target distance 9.0
model initialize at round 2432
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 6.        , 11.        ,  2.87671655]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 9.219544457293297}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8120276505711799
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.53140314,  9.0070652 ,  1.17760471]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5314501027824765}
episode index:2433
target Thresh 19.0
target distance 10.0
model initialize at round 2433
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([3.1387885 , 4.44592629, 3.11798334]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 11.77733003144465}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8119979351738164
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.02501025,  9.67963379,  0.28613027]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6800938164964339}
episode index:2434
target Thresh 19.0
target distance 13.0
model initialize at round 2434
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 3.       , 11.       ,  3.1088388]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 13.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8119871262233578
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.39988323, 10.74424914,  0.84335634]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.47467368034746044}
episode index:2435
target Thresh 19.0
target distance 2.0
model initialize at round 2435
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.        ,  7.        ,  2.19631235]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.236067977500836}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8120561380763038
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.13810458,  5.4295859 ,  6.19631235]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.451239316226513}
episode index:2436
target Thresh 19.0
target distance 6.0
model initialize at round 2436
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 8.72805915, 11.8802395 ,  2.10565864]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 7.897504468795864}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8120688122885762
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.65589993,  6.27286689,  4.68973211]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.43915965396808154}
episode index:2437
target Thresh 19.0
target distance 1.0
model initialize at round 2437
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.        ,  9.        ,  2.65009964]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.414213562375331}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8121417947281625
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.52284197,  8.19588618,  4.65009964]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5158014960954014}
episode index:2438
target Thresh 19.0
target distance 12.0
model initialize at round 2438
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.00000006, 2.9999999 , 6.23910666]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 12.36931684449151}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.812072286654098
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.00937934,  6.33406921,  6.27451235]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.3342008547091565}
episode index:2439
target Thresh 19.0
target distance 9.0
model initialize at round 2439
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4.00513525, 4.68296218, 2.57774502]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.991476636415182}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8120519056818412
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.44870898, 10.41369731,  0.02907726]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8047811080087289}
episode index:2440
target Thresh 19.0
target distance 3.0
model initialize at round 2440
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([3.11766919, 3.56686343, 5.17053127]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.9246416367686843}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8121167344791858
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.87007486, 2.43737601, 4.88734597]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.4562656196804286}
episode index:2441
target Thresh 19.0
target distance 3.0
model initialize at round 2441
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([13.76419464,  9.85756355,  4.89775133]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.9067796538627033}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8121775368033138
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.73380983,  7.70684011,  0.33138072]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.39597970663985194}
episode index:2442
target Thresh 19.0
target distance 1.0
model initialize at round 2442
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.67395154, 3.10574957, 1.07308977]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.2949489485069008}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8122462729732675
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.65932847, 1.76914271, 5.07308977]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.4115242146807617}
episode index:2443
target Thresh 19.0
target distance 3.0
model initialize at round 2443
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([16.        , 10.        ,  2.52364564]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8123030421127628
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.80078203,  6.60654677,  6.24046033]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8922204295735456}
episode index:2444
target Thresh 19.0
target distance 11.0
model initialize at round 2444
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.03609393, 7.67376721, 0.10239476]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.103510325148573}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8122794904747632
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.07549118,  5.65165443,  5.8369123 ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.3564316956626269}
episode index:2445
target Thresh 19.0
target distance 6.0
model initialize at round 2445
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([15.10842536,  4.01508871,  1.14827412]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 6.086687929470606}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8123171448429292
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.19972204, 10.34877319,  2.2987182 ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.40190998305220466}
episode index:2446
target Thresh 19.0
target distance 2.0
model initialize at round 2446
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.        , 5.        , 2.72377592]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 2.8284271247506987}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8123738154212116
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.86193942, 2.6273843 , 0.15740531]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.39737033892637885}
episode index:2447
target Thresh 19.0
target distance 1.0
model initialize at round 2447
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.        ,  3.        ,  1.36396235]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.4142135632773225}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8124423310194872
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.19663501,  1.63794503,  5.36396235]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.4120062238364309}
episode index:2448
target Thresh 19.0
target distance 11.0
model initialize at round 2448
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.        ,  2.        ,  0.63054388]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 14.212670403551906}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8124036573574124
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.54817786, 11.25359713,  3.79869081]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.6039954210339706}
episode index:2449
target Thresh 19.0
target distance 2.0
model initialize at round 2449
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 2.00000008, 10.00000028,  2.30904061]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 2.2360680328144626}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8124602232319196
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.31770341, 8.43404737, 6.0258553 ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6490283822838548}
episode index:2450
target Thresh 19.0
target distance 12.0
model initialize at round 2450
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([16.        ,  7.        ,  0.05519217]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 12.649110640673518}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8124492962297063
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.38720417, 10.69821803,  4.07289502]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.4909169191750093}
episode index:2451
target Thresh 19.0
target distance 13.0
model initialize at round 2451
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4.66533798, 5.24296989, 1.15487593]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.35991451376498}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8123880534665555
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.48630258,  5.66549898,  1.47346694]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.5902382004891924}
episode index:2452
target Thresh 19.0
target distance 8.0
model initialize at round 2452
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.        , 11.        ,  1.60905474]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.062257748437762}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8124110285009226
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.17108199, 3.7763374 , 4.47631351]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1356957519232276}
episode index:2453
target Thresh 19.0
target distance 3.0
model initialize at round 2453
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.58726396, 3.63157502, 2.82856631]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.9668274059830615}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8124674991697893
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.19969954, 4.02322642, 0.2621957 ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9969787061085902}
episode index:2454
target Thresh 19.0
target distance 2.0
model initialize at round 2454
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.44488914,  5.86295043,  1.54839009]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.265318055895452}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8125317889868281
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.83065775,  6.24247246,  1.26520478]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.1242065047353804}
episode index:2455
target Thresh 19.0
target distance 7.0
model initialize at round 2455
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 7.83897039, 10.12697315,  4.98541212]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 6.623559185784185}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8125618594602521
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.27009595, 7.67990056, 3.85267089]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7315849859838195}
episode index:2456
target Thresh 19.0
target distance 2.0
model initialize at round 2456
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.96368641, 10.6825782 ,  2.60237509]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.31949221361138314}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8126381468597391
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.96368641, 10.6825782 ,  2.60237509]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.31949221361138314}
episode index:2457
target Thresh 19.0
target distance 12.0
model initialize at round 2457
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.83032278, 8.53617418, 6.23839378]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.46639136926417}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8126024846095756
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.55785761,  3.36898594,  1.1233554 ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.6688465755309284}
episode index:2458
target Thresh 19.0
target distance 8.0
model initialize at round 2458
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([1.99384364, 6.00501984, 3.46753752]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 9.43654420362432}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.812611395254895
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.431491  , 11.34973729,  1.76842568]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6674718422486403}
episode index:2459
target Thresh 19.0
target distance 12.0
model initialize at round 2459
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([15.95812984,  6.36665208,  1.96934717]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 13.76157846482372}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.812594091088452
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.26254634, 11.17233487,  3.70386472]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.31405395366432215}
episode index:2460
target Thresh 19.0
target distance 11.0
model initialize at round 2460
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.        ,  9.        ,  1.49070566]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8125929264306434
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.53425141, 7.71548296, 5.79159382]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8929392104580949}
episode index:2461
target Thresh 19.0
target distance 12.0
model initialize at round 2461
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([15.8917583 ,  4.8877344 ,  4.95523715]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 12.29227857399444}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8125663541998964
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.49817224, 7.90858474, 0.12338408]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.5064902039948616}
episode index:2462
target Thresh 19.0
target distance 11.0
model initialize at round 2462
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.        , 9.        , 4.92888784]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8125685225244589
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.28324118, 11.35051429,  0.94659069]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7978743439732827}
episode index:2463
target Thresh 19.0
target distance 6.0
model initialize at round 2463
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 9.60096719, 11.51893367,  1.32345169]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 6.306524493693409}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8125877959952855
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.07692682,  6.76352481,  6.19071047]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.248672979704055}
episode index:2464
target Thresh 19.0
target distance 5.0
model initialize at round 2464
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([ 3.99999343, 11.00000122,  3.96753931]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 5.000001223634289}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8126362655903815
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.81308867, 6.08871925, 5.4011687 ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.2068984054240582}
episode index:2465
target Thresh 19.0
target distance 10.0
model initialize at round 2465
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 7.44715317, 11.85830039,  1.54588145]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 9.382839163210914}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8126451372432038
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.49722286,  7.40378712,  6.12995491]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7799068263080194}
episode index:2466
target Thresh 19.0
target distance 7.0
model initialize at round 2466
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.33131721, 10.78117165,  4.28198695]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 6.98346555578269}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8126823228685647
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.19143187,  4.44018719,  5.43243103]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9206232576760133}
episode index:2467
target Thresh 19.0
target distance 14.0
model initialize at round 2467
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([16.        ,  7.        ,  5.70229769]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8126588370356372
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.92270978, 7.79436562, 5.15362992]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9453458843403287}
episode index:2468
target Thresh 19.0
target distance 4.0
model initialize at round 2468
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.        , 11.        ,  2.94778299]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 4.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8127148642583445
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.30401268,  7.41326298,  0.38141238]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.513039958650552}
episode index:2469
target Thresh 19.0
target distance 2.0
model initialize at round 2469
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.99999912, 7.00000006, 4.08843851]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.236068418818135}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8127866396169443
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.60238372, 5.42852913, 6.08843851]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.5845818301850346}
episode index:2470
target Thresh 19.0
target distance 14.0
model initialize at round 2470
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([1.82718361, 7.67407365, 2.68366301]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.923662047463763}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8127311731699364
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.48622863,  2.6703794 ,  5.28543932]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.6104185123813913}
episode index:2471
target Thresh 19.0
target distance 12.0
model initialize at round 2471
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([16.        ,  6.        ,  5.58740187]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.812707705578526
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.2227319 , 8.62822924, 5.0387341 ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.43338550087509936}
episode index:2472
target Thresh 19.0
target distance 9.0
model initialize at round 2472
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([2.        , 2.        , 5.13375735]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 10.816653826391969}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8127098079772396
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.30531317, 10.71407211,  1.1514602 ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7512286898460534}
episode index:2473
target Thresh 19.0
target distance 12.0
model initialize at round 2473
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4.      , 6.      , 4.781178]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.000000000003835}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.812674347398412
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.32540257,  6.22503197,  5.94932493]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.7111406841799988}
episode index:2474
target Thresh 19.0
target distance 12.0
model initialize at round 2474
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([15.        ,  9.        ,  5.86547852]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8126602653830588
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.69995816, 6.95741884, 5.59999606]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.3030482792100631}
episode index:2475
target Thresh 19.0
target distance 4.0
model initialize at round 2475
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.99999998,  8.99999995,  5.36216092]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.472135960748222}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8127122927998673
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.84467901, 10.23177297,  4.79579031]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.1417772989156478}
episode index:2476
target Thresh 19.0
target distance 5.0
model initialize at round 2476
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.19799888, 8.3287177 , 5.84031057]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.3346011876202426}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8127642782082651
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.32084421, 5.15042549, 5.27393996]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6956151388300927}
episode index:2477
target Thresh 19.0
target distance 5.0
model initialize at round 2477
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.        , 9.        , 2.24091153]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 5.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.812808660942817
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.73362604, 4.41105621, 5.67454091]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8409366061740265}
episode index:2478
target Thresh 19.0
target distance 12.0
model initialize at round 2478
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([15.99629033, 10.00673325,  3.0843668 ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 14.422855440300873}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8127617083639976
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.58630775, 2.69483617, 5.96932842]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8086646860545137}
episode index:2479
target Thresh 19.0
target distance 12.0
model initialize at round 2479
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([16.        ,  2.        ,  6.13138247]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 14.422205101855956}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8127292654854856
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.94361832, 9.94220878, 5.29952939]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.945386358633719}
episode index:2480
target Thresh 19.0
target distance 2.0
model initialize at round 2480
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.32598293, 6.04930025, 4.1221509 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 2.0750653905041214}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8128007168093528
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.98099325, 4.49902622, 6.1221509 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.1006247907592732}
episode index:2481
target Thresh 19.0
target distance 10.0
model initialize at round 2481
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.03887695,  9.38153196,  3.18862629]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.044855344808045}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8127803872354358
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.30535091, 4.72275943, 0.63995852]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.41243364980021124}
episode index:2482
target Thresh 19.0
target distance 3.0
model initialize at round 2482
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.        ,  7.        ,  5.73107481]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 3.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.812836049604612
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.53739383, 10.23863333,  3.1647042 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5205288941598105}
episode index:2483
target Thresh 19.0
target distance 1.0
model initialize at round 2483
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.48468698,  4.73210946,  3.70152318]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5807864037428343}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8129113974107293
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.48468698,  4.73210946,  3.70152318]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5807864037428343}
episode index:2484
target Thresh 19.0
target distance 9.0
model initialize at round 2484
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 6.        , 11.        ,  4.56422043]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 9.848857801796273}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8129134076884703
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.20063934,  6.41833702,  0.58192328]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6152950236497403}
episode index:2485
target Thresh 19.0
target distance 12.0
model initialize at round 2485
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([2.04875848, 3.73305507, 5.90305114]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.973702170479184}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.8128372258651407
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.36191989,  3.55398944,  5.65527153]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.6617328033552387}
episode index:2486
target Thresh 19.0
target distance 7.0
model initialize at round 2486
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 5.99999985, 11.00000014,  3.39503121]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 7.615773176668574}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8128632346295693
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.6698704 , 4.35724393, 4.26228999]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.48642448576024777}
episode index:2487
target Thresh 19.0
target distance 14.0
model initialize at round 2487
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([16.00082322,  5.00162235,  2.11122894]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 14.143180079160139}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.812808116387781
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.53114377, 3.93048376, 4.71300525]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0419338764865662}
episode index:2488
target Thresh 19.0
target distance 2.0
model initialize at round 2488
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.        , 11.        ,  3.39001715]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.0000000000007905}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8128793063771793
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.45087914,  9.4091621 ,  5.39001715]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.6847972958400979}
episode index:2489
target Thresh 19.0
target distance 12.0
model initialize at round 2489
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([2.        , 7.        , 5.21847582]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8128469465632344
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.0879154 ,  8.81598171,  4.38662275]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.2238155380012468}
episode index:2490
target Thresh 19.0
target distance 6.0
model initialize at round 2490
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4.81911951, 5.01076649, 1.64518898]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 5.730609555027479}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8128948062185309
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.81941835, 9.07940966, 3.07881837]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.2324500045636693}
episode index:2491
target Thresh 19.0
target distance 12.0
model initialize at round 2491
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        ,  5.        ,  0.96790522]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.165525060603226}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8128452694126123
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.50289137, 3.87160882, 5.85286684]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.006281104027041}
episode index:2492
target Thresh 19.0
target distance 14.0
model initialize at round 2492
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([16.00000015,  7.00000132,  2.47148168]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 14.317821482401119}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.812807116690264
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.98126837, 4.9776549 , 5.63962861]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.3851702860286288}
episode index:2493
target Thresh 19.0
target distance 1.0
model initialize at round 2493
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([14.99999759,  6.00000099,  3.7613219 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.4142145652312643}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8128741948311259
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.49826066,  6.7575586 ,  1.4781366 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.554113274734258}
episode index:2494
target Thresh 19.0
target distance 13.0
model initialize at round 2494
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([15.000568  ,  3.99907443,  6.27280521]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.153367132223616}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.8127908809522908
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.65787448, 2.94403543, 5.7418403 ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.004117900670991}
episode index:2495
target Thresh 19.0
target distance 1.0
model initialize at round 2495
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.99997874, 2.00001028, 3.70129943]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.0000212579386838}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8128579118493452
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.54175979, 2.66632962, 1.41811412]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8587775215126853}
episode index:2496
target Thresh 19.0
target distance 2.0
model initialize at round 2496
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([16.01079895,  8.94344703,  5.91107035]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.221126899830083}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8129209639471228
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.39597263,  8.10901465,  5.62788504]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.4107049038937827}
episode index:2497
target Thresh 19.0
target distance 3.0
model initialize at round 2497
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.76556095,  8.66656126,  2.72055161]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.8171127246999794}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8129839655628366
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.44951612, 10.04639741,  2.4373663 ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5524357187856976}
episode index:2498
target Thresh 19.0
target distance 1.0
model initialize at round 2498
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.83392771, 10.99463157,  4.18390727]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0084006957664615}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8130588019111508
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.83392771, 10.99463157,  4.18390727]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0084006957664615}
episode index:2499
target Thresh 19.0
target distance 3.0
model initialize at round 2499
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 1.13158755, 10.31483158,  4.56572413]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.4723643436990628}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8131178167943862
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.15469157, 7.58917805, 6.28253883]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.4389808154116991}
episode index:2500
target Thresh 19.0
target distance 12.0
model initialize at round 2500
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([15.42302634,  5.78234033,  1.51266831]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 13.604986233825148}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8130975148741754
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.69618406, 7.91862858, 5.24718585]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7009233607201523}
episode index:2501
target Thresh 19.0
target distance 9.0
model initialize at round 2501
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.78666596, 3.20628385, 1.80941361]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 9.376612569989117}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8130865558917345
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.08770232, 10.14721852,  5.82711646]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.2488087548595876}
episode index:2502
target Thresh 19.0
target distance 13.0
model initialize at round 2502
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([15.        ,  7.        ,  0.32421511]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.03840481040591}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.813057236601881
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.85395985, 6.93415645, 3.77554735]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9455030396822047}
episode index:2503
target Thresh 19.0
target distance 12.0
model initialize at round 2503
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([15.00000001,  2.        ,  1.27001446]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.00000001028033}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.8129466978593112
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.81324607, 2.82245819, 5.88949362]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8433946346226513}
episode index:2504
target Thresh 19.0
target distance 8.0
model initialize at round 2504
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([3.86782941, 9.558036  , 6.2641592 ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 6.818843218192484}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8129760144955813
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.26990679, 3.5352734 , 5.13141797]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.905292058695491}
episode index:2505
target Thresh 19.0
target distance 1.0
model initialize at round 2505
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([14.9971548 ,  7.00401128,  3.19773698]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.413397336305307}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8130387930213214
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.89529609,  8.45534144,  2.91455167]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.46722449946136224}
episode index:2506
target Thresh 19.0
target distance 10.0
model initialize at round 2506
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([3.32058651, 9.46509153, 3.01501298]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.483880844898502}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8130007761643517
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.07586482,  2.88080583,  6.18315991]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.14128948936889513}
episode index:2507
target Thresh 19.0
target distance 11.0
model initialize at round 2507
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([15.        ,  8.        ,  1.60994786]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8129715495284762
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.13211754, 4.45273745, 5.06128009]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9788723964777525}
episode index:2508
target Thresh 19.0
target distance 4.0
model initialize at round 2508
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 6.        , 11.        ,  1.23319834]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 5.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.81301530128013
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.54349241, 7.53539626, 4.66682773]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7150109399007203}
episode index:2509
target Thresh 19.0
target distance 12.0
model initialize at round 2509
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([14.42959364,  4.58335867,  2.92657399]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.648146664272137}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.812989068926764
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.50141144, 9.40037787, 4.37790623]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7816393848806729}
episode index:2510
target Thresh 19.0
target distance 3.0
model initialize at round 2510
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.        ,  6.        ,  3.69220483]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 3.1622776603178515}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8130440275014248
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.25425188,  2.97547603,  1.12583422]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.2554318766247573}
episode index:2511
target Thresh 19.0
target distance 8.0
model initialize at round 2511
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.        , 11.        ,  2.24217832]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8130662017789995
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.2467907 ,  3.30124806,  5.10943709]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8112180004394918}
episode index:2512
target Thresh 19.0
target distance 3.0
model initialize at round 2512
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.65339911,  2.31397634,  1.19766301]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.1541166306072816}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8131249084277146
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.1286251 ,  5.27690828,  2.9144777 ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.30532377720380705}
episode index:2513
target Thresh 19.0
target distance 13.0
model initialize at round 2513
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.00009196,  7.99995891,  0.58980912]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.928459362001892}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8130841082363242
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.98423389, 3.23639375, 5.75795604]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.23691892050731697}
episode index:2514
target Thresh 19.0
target distance 12.0
model initialize at round 2514
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([16.        ,  7.        ,  0.89484566]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.813043340490414
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.80635125, 4.90730491, 6.06299259]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9277402823532762}
episode index:2515
target Thresh 19.0
target distance 8.0
model initialize at round 2515
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.        , 3.        , 0.72399491]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 8.06225774829881}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8130689675502346
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.56779914, 10.53055036,  1.59125368]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7367352497758498}
episode index:2516
target Thresh 19.0
target distance 1.0
model initialize at round 2516
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.6936709 , 10.13316684,  6.14261198]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9193678531219365}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8131432349449306
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.6936709 , 10.13316684,  6.14261198]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9193678531219365}
episode index:2517
target Thresh 19.0
target distance 12.0
model initialize at round 2517
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.90323219,  5.72348582,  5.01258397]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.038608409622485}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8131082218794075
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.38061675, 4.79201925, 6.1807309 ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8787284021660959}
episode index:2518
target Thresh 19.0
target distance 6.0
model initialize at round 2518
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.44235999, 5.62379362, 2.31482679]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.398507090061197}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8131554458278105
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.34650913, 9.0538106 , 3.74845617]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1499237832162783}
episode index:2519
target Thresh 19.0
target distance 11.0
model initialize at round 2519
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([3.00000001, 2.99999999, 6.24346447]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.401754246710716}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8131036200933592
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.89226908,  6.78586202,  0.56205548]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1890009327078606}
episode index:2520
target Thresh 19.0
target distance 11.0
model initialize at round 2520
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.50402652,  3.23144896,  4.62617683]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 12.275052149653863}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8130896247499662
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.67801028, 10.55713399,  4.36069438]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8098322332056688}
episode index:2521
target Thresh 19.0
target distance 8.0
model initialize at round 2521
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([3.53030334, 6.29960027, 0.5807721 ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.996920187819892}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8131048381307532
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.43260741, 11.774572  ,  1.16484556]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9601542238749236}
episode index:2522
target Thresh 19.0
target distance 11.0
model initialize at round 2522
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.42501208, 6.6284091 , 2.32547   ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 10.837657172602597}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8130847183829197
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.7921453 ,  8.75109892,  6.05998754]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.3242766212369854}
episode index:2523
target Thresh 19.0
target distance 12.0
model initialize at round 2523
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([3.99999257, 7.9999872 , 5.19641256]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.416408786810928}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8130330028031367
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.25619565,  1.6443154 ,  5.79818888]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.43834660844634094}
episode index:2524
target Thresh 19.0
target distance 12.0
model initialize at round 2524
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([16.        ,  9.00000001,  2.43528375]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 12.369316880666133}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.813012927441372
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.1613749 , 6.37796638, 6.16980129]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.41097498666498267}
episode index:2525
target Thresh 19.0
target distance 1.0
model initialize at round 2525
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.39796243, 2.36475894, 5.96111441]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7496034272002372}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8130869524107143
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.39796243, 2.36475894, 5.96111441]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7496034272002372}
episode index:2526
target Thresh 19.0
target distance 8.0
model initialize at round 2526
at step 0:
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.09651219,  8.41989361,  3.14748502]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.550988955033404}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8131124506578803
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.90545518, 10.43088733,  4.01474379]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0694570213653642}
episode index:2527
target Thresh 19.0
target distance 5.0
model initialize at round 2527
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([2.        , 6.        , 0.00855797]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8131485541485255
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.13901046, 10.23993109,  1.15900205]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.7726762906368018}
episode index:2528
target Thresh 19.0
target distance 11.0
model initialize at round 2528
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([15.70273436,  5.52923177,  2.15004207]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.972927996079147}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8130836882229501
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.34228424, 3.44611373, 0.18544777]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.5622952616979612}
episode index:2529
target Thresh 19.0
target distance 11.0
model initialize at round 2529
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([15.00000022,  9.9999999 ,  0.58219307]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 11.045361223521311}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8130823618118822
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.29642295, 9.04546915, 4.88308123]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.29988999710379977}
episode index:2530
target Thresh 19.0
target distance 12.0
model initialize at round 2530
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 4.        , 11.        ,  4.47217655]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8130810364489464
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.57248811, 11.45643283,  2.4898794 ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6253777552441044}
episode index:2531
target Thresh 19.0
target distance 8.0
model initialize at round 2531
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.        , 10.        ,  1.23372382]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 8.06225774848943}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8130995898921503
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.55650345,  1.63156695,  6.10098259]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6674121699409414}
episode index:2532
target Thresh 19.0
target distance 3.0
model initialize at round 2532
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 3.39753751, 10.06230409,  0.41902893]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.4912264589081743}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8131616504567408
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.85426092, 8.32672698, 0.13584363]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9146104299444588}
episode index:2533
target Thresh 19.0
target distance 11.0
model initialize at round 2533
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([2.71505838, 9.44956777, 5.54063511]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.80039936861657}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8131297411115149
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.84373508,  6.04504778,  4.70878204]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.16262849851786104}
episode index:2534
target Thresh 19.0
target distance 10.0
model initialize at round 2534
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.22632256,  7.7977538 ,  5.00442338]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 9.766238207301571}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8131381766224967
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.92040116, 11.26773549,  3.30531154]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9585513018724147}
episode index:2535
target Thresh 19.0
target distance 10.0
model initialize at round 2535
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.96622598,  5.68263109,  2.60086578]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.296020197597416}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8131211833138148
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.80713296, 10.63355209,  4.33538332]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8864241031283006}
episode index:2536
target Thresh 19.0
target distance 12.0
model initialize at round 2536
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.        , 6.        , 4.48486423]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 12.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8130951884029034
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.27764783,  6.47328982,  5.93619647]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8635947606270546}
episode index:2537
target Thresh 19.0
target distance 14.0
model initialize at round 2537
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([2.02298516, 3.98339279, 0.38431996]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 14.013937140430373}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8130384021815349
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.65324082,  5.54940043,  4.98609627]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6496789649396192}
episode index:2538
target Thresh 19.0
target distance 8.0
model initialize at round 2538
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([2.        , 5.        , 5.01907444]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8130403196826833
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.50024667, 11.22250093,  1.03677729]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5470466686239415}
episode index:2539
target Thresh 19.0
target distance 13.0
model initialize at round 2539
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([14.8306124,  3.2103391,  3.3489877]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 11.856936964909073}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.8129706917954318
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.02970862, 4.29136142, 5.3843934 ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.29287212165662513}
episode index:2540
target Thresh 19.0
target distance 11.0
model initialize at round 2540
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 6.67900368, 10.88452388,  0.94133156]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 9.321711601038}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8129858382256951
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.0386243 , 10.60350277,  1.52540503]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0399294618457553}
episode index:2541
target Thresh 19.0
target distance 10.0
model initialize at round 2541
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.        ,  7.        ,  5.11805153]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.770329614266979}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8129845565695172
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.94973399, 11.62212305,  3.13575438]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.1353553414765958}
episode index:2542
target Thresh 19.0
target distance 13.0
model initialize at round 2542
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([16.12567767,  3.25109467,  1.84811705]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 14.390665380684387}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8129279255046332
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.76240372, 5.77782048, 0.16670806]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.7941178604289945}
episode index:2543
target Thresh 19.0
target distance 3.0
model initialize at round 2543
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.        , 8.        , 4.75658703]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8129821952076189
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.08625209, 10.40292978,  2.19021641]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0915255842609373}
episode index:2544
target Thresh 19.0
target distance 7.0
model initialize at round 2544
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([11.        , 11.        ,  3.70338392]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8130041062557766
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.6890971 ,  4.93740562,  0.28745739]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9876183042591478}
episode index:2545
target Thresh 19.0
target distance 10.0
model initialize at round 2545
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.00048963, 10.99942284,  0.1427328 ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.440609663577508}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.81300927692254
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.93938875, 8.57369732, 0.44362095]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1007178696446556}
episode index:2546
target Thresh 19.0
target distance 2.0
model initialize at round 2546
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.65843284,  7.95141565,  0.98071307]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.1517973060055477}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8130787668020364
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.99189398,  9.50159529,  2.98071307]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.49847061988161584}
episode index:2547
target Thresh 19.0
target distance 13.0
model initialize at round 2547
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([2.99996993, 6.99984084, 5.5356586 ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 13.60145244372953}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8130195844135677
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.24798943,  2.62196346,  5.8542496 ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.4521176640638747}
episode index:2548
target Thresh 19.0
target distance 7.0
model initialize at round 2548
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([3.00004804, 3.99998471, 0.70189923]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 9.899471775563336}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8130247429225603
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.64248274, 11.44541361,  1.00278738]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5711496097081775}
episode index:2549
target Thresh 19.0
target distance 3.0
model initialize at round 2549
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.87548322,  4.32164257,  5.64833498]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.5853106807342097}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8130864191018063
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.28692317,  2.74497347,  5.36514967]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.3838794550846283}
episode index:2550
target Thresh 19.0
target distance 13.0
model initialize at round 2550
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.62849397, 5.57527012, 0.75487202]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.920306447371635}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8130299257029305
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.18254544,  2.51024914,  5.35664833]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.9636317459205583}
episode index:2551
target Thresh 19.0
target distance 8.0
model initialize at round 2551
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.51433025,  8.39754798,  6.03296661]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 6.574330101753243}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8130586698040329
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.03957544,  2.5025038 ,  4.90022539]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.0839397645177173}
episode index:2552
target Thresh 19.0
target distance 10.0
model initialize at round 2552
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.12034587,  3.23068422,  3.35924959]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.531517123827213}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8130129802420453
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.94972519, 6.29689378, 6.24421121]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.30112035999398296}
episode index:2553
target Thresh 19.0
target distance 4.0
model initialize at round 2553
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 4.51298557, 10.2118527 ,  4.6389668 ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.347743843281825}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8130632806215122
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.18329017, 8.64701665, 4.07259618]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6724773857387844}
episode index:2554
target Thresh 19.0
target distance 6.0
model initialize at round 2554
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 9.99972225, 11.00154796,  2.75833529]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 7.211730249482141}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8130850741761687
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.74374257, 6.34940696, 5.62559406]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.699241861271816}
episode index:2555
target Thresh 19.0
target distance 11.0
model initialize at round 2555
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.        ,  8.        ,  1.43618935]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.529964086141668}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8130449599950639
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.18795539, 2.14073962, 0.32115097]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.23480815204925584}
episode index:2556
target Thresh 19.0
target distance 11.0
model initialize at round 2556
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([15.        ,  8.        ,  1.98634022]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 11.000000000004803}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8130436627358641
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.82338895e+00, 8.34931176e+00, 4.04307052e-03]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8944205202269674}
episode index:2557
target Thresh 19.0
target distance 3.0
model initialize at round 2557
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.        , 5.        , 3.19871473]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 3.0000000003254454}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8131013454361237
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.60084222, 2.2222495 , 4.91552943]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.4568607757232071}
episode index:2558
target Thresh 19.0
target distance 8.0
model initialize at round 2558
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.68253345,  3.03831365,  1.03276747]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 7.96801321091256}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8131265192061756
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.56781029, 10.74282215,  1.90002624]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.623336960879157}
episode index:2559
target Thresh 19.0
target distance 13.0
model initialize at round 2559
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([2.        , 7.        , 5.78535891]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 13.152946437965955}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8131007557589591
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.25390709,  9.45982687,  0.95350583]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8764105107083802}
episode index:2560
target Thresh 19.0
target distance 12.0
model initialize at round 2560
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([16.        ,  6.        ,  0.91835612]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8130809361410708
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.53715808, 8.22393616, 4.65287366]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.5141692738992176}
episode index:2561
target Thresh 19.0
target distance 3.0
model initialize at round 2561
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.89212377,  5.3204909 ,  5.65824628]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.3248899992635377}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8131423015055748
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.37663052,  3.74124738,  5.37506097]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6749388311377228}
episode index:2562
target Thresh 19.0
target distance 4.0
model initialize at round 2562
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.3192692 , 10.08666679,  4.10007334]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 4.099119210171698}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8131887014456456
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.68477206,  5.5757408 ,  5.53370272]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.5285494477837154}
episode index:2563
target Thresh 19.0
target distance 2.0
model initialize at round 2563
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([13.        , 11.        ,  3.58711505]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8132537994560023
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.39575645,  9.90588922,  1.30392974]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6115285014287372}
episode index:2564
target Thresh 19.0
target distance 2.0
model initialize at round 2564
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([4.28428396, 8.0592382 , 1.69966286]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.4704222242335527}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.813322706356799
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.77027398, 8.79419525, 3.69966286]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.797293920351105}
episode index:2565
target Thresh 19.0
target distance 10.0
model initialize at round 2565
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.87438369, 5.76136418, 0.8889262 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.870396831373267}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8132587078852526
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.29455627,  2.77372245,  5.20751721]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0470421627915123}
episode index:2566
target Thresh 19.0
target distance 2.0
model initialize at round 2566
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.59649312, 9.36611803, 5.48027158]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5448488025825036}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8133314547851803
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.59649312, 9.36611803, 5.48027158]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5448488025825036}
episode index:2567
target Thresh 19.0
target distance 7.0
model initialize at round 2567
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([15.        ,  6.        ,  1.11762112]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.602325267049999}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8133530335850182
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.72627361, 10.48635446,  3.98487989]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8895533107483966}
episode index:2568
target Thresh 19.0
target distance 1.0
model initialize at round 2568
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.        ,  7.        ,  2.77418506]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8134179409288933
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.2002095 ,  5.26119301,  0.49099975]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.7654538579587389}
episode index:2569
target Thresh 19.0
target distance 5.0
model initialize at round 2569
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.92282229, 4.40740439, 2.00043085]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.593424501257344}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8134641072351105
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.26983979, 7.57208934, 3.43406024]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8463104982504712}
episode index:2570
target Thresh 19.0
target distance 13.0
model initialize at round 2570
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([3.67132949, 2.80239987, 0.89231699]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.391796238377065}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8134079063993791
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.23510224,  3.50379351,  5.49409331]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9117507674401594}
episode index:2571
target Thresh 19.0
target distance 4.0
model initialize at round 2571
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.12539415,  2.78420011,  3.64670503]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.332612234370569}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8134577012061448
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.30776584,  6.13460125,  3.08033442]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.33591265173296764}
episode index:2572
target Thresh 19.0
target distance 7.0
model initialize at round 2572
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4.83582696, 2.88704582, 1.55595082]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 6.382669456296266}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8134930390894728
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.84016292, 8.78290524, 2.70639489]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.2695886274126525}
episode index:2573
target Thresh 19.0
target distance 3.0
model initialize at round 2573
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 7.        , 11.        ,  1.56483906]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 3.0}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.813550188650821
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.27146515, 11.37195038,  3.28165375]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.4604784616698661}
episode index:2574
target Thresh 19.0
target distance 8.0
model initialize at round 2574
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.66228524, 10.26140431,  1.16597861]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 8.287908238208411}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8135682500745066
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.72295062,  1.93363775,  6.03323738]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.2848864834936738}
episode index:2575
target Thresh 19.0
target distance 11.0
model initialize at round 2575
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([5.16013055, 9.15216974, 0.99036997]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.33243460012308}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8135454049802244
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.17097121,  5.53196735,  0.44170221]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.4982827664038258}
episode index:2576
target Thresh 19.0
target distance 9.0
model initialize at round 2576
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.        , 2.        , 5.64310265]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8135668253945777
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.17808743, 10.14991688,  2.22717611]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8685369599339896}
episode index:2577
target Thresh 19.0
target distance 12.0
model initialize at round 2577
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.63779499, 6.38731794, 1.24222105]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.342492162977708}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8135529598142848
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.39778466, 10.99835499,  0.9767386 ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6022175863936092}
episode index:2578
target Thresh 19.0
target distance 11.0
model initialize at round 2578
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.99999454,  2.99999738,  4.59819961]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 13.6014676284756}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8135301472231212
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.37629847, 10.79986156,  4.04953184]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.426211143244089}
episode index:2579
target Thresh 19.0
target distance 12.0
model initialize at round 2579
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.6641284 , 4.51174884, 2.13068454]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 11.346381545819256}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8134664156266659
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.06063797,  4.63458224,  0.16609024]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.37041476731252543}
episode index:2580
target Thresh 19.0
target distance 4.0
model initialize at round 2580
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([16.        ,  6.        ,  5.76619744]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8135087551380185
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.23045267, 10.04060525,  2.91664152]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7706178573615232}
episode index:2581
target Thresh 19.0
target distance 11.0
model initialize at round 2581
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.00225763, 4.98159625, 5.84445143]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.394746746958441}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.8134280027414265
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.5489915 ,  2.70538009,  5.31348651]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8372393549075422}
episode index:2582
target Thresh 19.0
target distance 9.0
model initialize at round 2582
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([ 3.9999987 , 11.00000023,  3.97613752]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.00000023075237}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8134494188506124
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.78947794, 2.61026273, 0.56021098]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9978456825142252}
episode index:2583
target Thresh 19.0
target distance 7.0
model initialize at round 2583
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.31906255,  7.9173116 ,  4.20074487]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.147795878105589}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.813477644645065
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.74111676, 11.67030577,  3.06800364]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9992816811711253}
episode index:2584
target Thresh 19.0
target distance 12.0
model initialize at round 2584
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.58083795, 6.57735558, 1.36017006]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.38029533650777}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.8133993620898959
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.82142511,  1.64589279,  5.11239045]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.39658657423942517}
episode index:2585
target Thresh 19.0
target distance 5.0
model initialize at round 2585
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 8.51068208, 11.90779047,  2.22179051]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 4.580181097244749}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8134488906232721
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.00831916, 11.26261363,  1.65541989]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.025863928952359}
episode index:2586
target Thresh 19.0
target distance 10.0
model initialize at round 2586
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.        , 9.        , 4.00303364]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8134232714519186
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.10270312,  3.34426783,  5.45436587]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.35926072509230145}
episode index:2587
target Thresh 19.0
target distance 10.0
model initialize at round 2587
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 3.52964373, 10.29816087,  0.5798313 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.499383129401503}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8134379679355519
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.03777868, 11.27802487,  1.16390476]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.28057985940986363}
episode index:2588
target Thresh 19.0
target distance 13.0
model initialize at round 2588
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([3.06210055, 3.68182389, 2.54388857]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 13.988289363198685}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8133821679319343
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.98804453,  9.39010106,  0.86247958]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.3902842160273759}
episode index:2589
target Thresh 19.0
target distance 6.0
model initialize at round 2589
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 2.        , 11.        ,  2.47987911]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 6.000000000113327}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8134138112085083
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.06497396, 4.7831307 , 5.63032319]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.22639326560700604}
episode index:2590
target Thresh 19.0
target distance 4.0
model initialize at round 2590
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.54564135,  5.59206269,  2.25061628]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.4504293265170567}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8134632385872008
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.18027529,  8.3258253 ,  1.68424567]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.372372541578062}
episode index:2591
target Thresh 19.0
target distance 13.0
model initialize at round 2591
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([2.99999605, 1.99998574, 5.45224357]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.000003947530688}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.8133542271305527
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.22261871,  1.49123494,  5.78853742]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.5553386137925485}
episode index:2592
target Thresh 19.0
target distance 14.0
model initialize at round 2592
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([3.63007499, 9.58137892, 0.75862139]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.383579699599286}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8133405237492448
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.41640699,  9.43681411,  0.49313893]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.728963214008639}
episode index:2593
target Thresh 19.0
target distance 13.0
model initialize at round 2593
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([15.        ,  8.        ,  0.21067875]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 13.152946437966136}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8133298597619888
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.26868503, 10.04375131,  4.2283816 ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.27222384453351994}
episode index:2594
target Thresh 19.0
target distance 12.0
model initialize at round 2594
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.93207621, 11.30074089,  3.26821876]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 12.149409210317454}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8133131789473619
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.37088099, 6.52983555, 5.0027363 ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8225061914621563}
episode index:2595
target Thresh 19.0
target distance 1.0
model initialize at round 2595
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.00000102, 10.00000005,  1.06100815]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.41421424362785}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8133812401265038
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.22252459, 11.49262072,  3.06100815]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5405482157795665}
episode index:2596
target Thresh 19.0
target distance 2.0
model initialize at round 2596
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 3.97130734, 11.37439033,  1.96557539]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0947039409894035}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8134416628295742
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.68759867, 10.81065074,  1.68239009]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7131935784459186}
episode index:2597
target Thresh 19.0
target distance 12.0
model initialize at round 2597
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([12.88719574, 11.26255881,  3.30323291]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.365532150558922}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8134433815650506
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.40733603, 7.52253748, 5.60412107]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6276090341918152}
episode index:2598
target Thresh 19.0
target distance 10.0
model initialize at round 2598
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.13189132,  6.88195134,  3.59299064]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.354945095770093}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.813382671999452
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.74902981, 1.85521994, 5.91158164]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.28973660684980307}
episode index:2599
target Thresh 19.0
target distance 6.0
model initialize at round 2599
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 6.65946022, 11.74485288,  3.52019954]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 5.407844933305892}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8134176717698403
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.78848724, 8.16776261, 4.67064362]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.858694779687037}
episode index:2600
target Thresh 19.0
target distance 12.0
model initialize at round 2600
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.64696738, 1.65405316, 0.80295676]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.228202807223166}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.8133470858257547
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.69584674,  6.01703859,  4.83836246]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.3046301300872064}
episode index:2601
target Thresh 19.0
target distance 13.0
model initialize at round 2601
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.36597089,  8.59708941,  4.39334512]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.617197316655332}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8133425829447836
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.69386996, 10.43736567,  4.41104797]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8933156827868595}
episode index:2602
target Thresh 19.0
target distance 7.0
model initialize at round 2602
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.        , 3.        , 5.17225623]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8133706437549146
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.07124675, 9.54368038, 4.039515  ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.46184812562251315}
episode index:2603
target Thresh 19.0
target distance 13.0
model initialize at round 2603
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([16.        ,  2.        ,  0.92684859]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 14.317821063397782}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8133339156016286
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.24576356, 8.90348798, 4.09499551]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9363173926702253}
episode index:2604
target Thresh 19.0
target distance 9.0
model initialize at round 2604
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 5.99999997, 10.99999988,  5.49174428]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.816653782943442}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8133388425529661
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.22108814,  5.77867135,  5.79263244]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.101377663630996}
episode index:2605
target Thresh 19.0
target distance 5.0
model initialize at round 2605
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.99999997,  6.        ,  4.26114106]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.000000002769716}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8133808248445528
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.59735452, 10.12400258,  1.41158514]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0602848216201501}
episode index:2606
target Thresh 19.0
target distance 14.0
model initialize at round 2606
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.93248996, 11.81127484,  3.26009429]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 13.058714067134733}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8133763176580143
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.95139961, 10.37360771,  3.27779713]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0221271613540357}
episode index:2607
target Thresh 19.0
target distance 13.0
model initialize at round 2607
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.55652539, 4.64001313, 1.40010756]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.560395939683549}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.813305937026705
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.10963891,  3.33140035,  5.43551326]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9500363487371568}
episode index:2608
target Thresh 19.0
target distance 6.0
model initialize at round 2608
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.82576955, 3.2056413 , 3.35299671]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.797523484722362}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8133443468812304
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.9576732 , 8.08470403, 2.50344079]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.09469071196026616}
episode index:2609
target Thresh 19.0
target distance 3.0
model initialize at round 2609
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.48948586, 4.38978521, 6.0175004 ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.037172321542188}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8134044827636514
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.99403868, 3.37878088, 5.73431509]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3788277892578649}
episode index:2610
target Thresh 19.0
target distance 10.0
model initialize at round 2610
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([2.4111799 , 5.32429577, 3.95024729]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 12.157777465027849}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8133762544567286
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.48645094,  9.41079705,  1.11839422]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6367014419011687}
episode index:2611
target Thresh 19.0
target distance 9.0
model initialize at round 2611
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 6.44727893, 10.15292352,  0.47741955]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 7.853577227569612}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8133843469172927
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.57470183,  7.80503637,  5.06149302]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.4678561199665761}
episode index:2612
target Thresh 19.0
target distance 1.0
model initialize at round 2612
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.        ,  6.        ,  5.16153097]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.4142135623730951}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8134481493103592
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.73287464,  7.44663042,  2.87834566]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.5204178070378}
episode index:2613
target Thresh 19.0
target distance 14.0
model initialize at round 2613
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.50390691,  7.22922536,  4.62731957]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.199774055477853}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8134143054643942
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.15290245, 3.91374962, 5.7954665 ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9264542753479609}
episode index:2614
target Thresh 19.0
target distance 9.0
model initialize at round 2614
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.       , 10.       ,  1.7067129]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.055385138137416}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.81343546474061
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.64487175, 10.91189079,  4.57397167]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.6508631255959105}
episode index:2615
target Thresh 19.0
target distance 12.0
model initialize at round 2615
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.37301384,  9.56953035,  4.41024685]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.33490290011494}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8134188774627295
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.53465274, 5.32985981, 6.14476439]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6282205416948726}
episode index:2616
target Thresh 19.0
target distance 2.0
model initialize at round 2616
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4.10881122, 6.32055122, 5.78708839]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.133034893394073}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8134788240131832
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.46537523, 5.64497232, 5.50390309]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5853364502906676}
episode index:2617
target Thresh 19.0
target distance 4.0
model initialize at round 2617
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.        , 11.        ,  3.69444108]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 4.472135954999579}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8135241206227684
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.70168344,  7.17503135,  5.12807047]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.3458738848223322}
episode index:2618
target Thresh 19.0
target distance 6.0
model initialize at round 2618
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.        , 9.        , 1.90729159]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8135553593144964
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.43632792, 2.79635363, 5.05773567]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.599331343258709}
episode index:2619
target Thresh 19.0
target distance 8.0
model initialize at round 2619
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 7.        , 11.        ,  4.05536151]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 8.06225774829855}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8135764243730668
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.16199916, 10.61471204,  0.63943497]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6357001034562388}
episode index:2620
target Thresh 19.0
target distance 13.0
model initialize at round 2620
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.78156201,  3.83905342,  4.91283321]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.811401867570533}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.8135111955206104
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.21266039, 2.79611377, 4.94823891]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8133100628689004}
episode index:2621
target Thresh 19.0
target distance 11.0
model initialize at round 2621
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.56366103, 4.62237631, 1.3888027 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.79394459447812}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.8134340048432696
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.2985647 ,  1.63669197,  5.1410231 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7899393667185951}
episode index:2622
target Thresh 19.0
target distance 4.0
model initialize at round 2622
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.        ,  4.        ,  4.92572212]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.9999999999999996}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8134864471021551
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.26580512,  7.32163718,  2.35935151]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7285797659532166}
episode index:2623
target Thresh 19.0
target distance 14.0
model initialize at round 2623
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([2.        , 2.        , 4.76129818]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 15.652475842498527}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8134262995310698
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.2412494 ,  8.83032176,  0.79670388]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.29494402371854606}
episode index:2624
target Thresh 19.0
target distance 2.0
model initialize at round 2624
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.9999774 ,  8.99998111,  4.84773779]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.236074764825098}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.81348606055982
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.4860414 , 10.00741322,  4.56455248]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.1177575569423308}
episode index:2625
target Thresh 19.0
target distance 2.0
model initialize at round 2625
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.        , 2.        , 5.19868326]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 2.000000000000033}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8135495083661567
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.44796155, 3.58950381, 2.91549795]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.6879342746373629}
episode index:2626
target Thresh 19.0
target distance 7.0
model initialize at round 2626
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([1.13223859, 3.44116044, 3.12370849]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.8642331113211394}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8135875623209026
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.95799405, 8.0769731 , 2.27415257]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9239822247169629}
episode index:2627
target Thresh 19.0
target distance 12.0
model initialize at round 2627
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.08139784,  8.65500777,  5.10237551]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.326798606725617}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8135860936397383
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.68062876, 10.98409589,  3.12007836]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.68081454696119}
episode index:2628
target Thresh 19.0
target distance 6.0
model initialize at round 2628
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([13.1236453 ,  5.19877372,  3.3854748 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.154850217005257}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8136241047290664
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.1204461 ,  9.45287439,  2.53591888]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5602264708241538}
episode index:2629
target Thresh 19.0
target distance 2.0
model initialize at round 2629
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([4.        , 4.        , 0.22655981]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8136874035485612
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.90120342, 5.45238466, 4.22655981]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.5564560416541702}
episode index:2630
target Thresh 19.0
target distance 7.0
model initialize at round 2630
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 8.43338571, 11.87925369,  1.5620262 ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 5.875269276100835}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8137253472368678
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.10719182, 10.37019976,  0.71247028]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.966516584642808}
episode index:2631
target Thresh 19.0
target distance 11.0
model initialize at round 2631
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([2.6036242 , 6.5709952 , 2.21395786]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 11.300474030474254}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8137207519641863
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.05974592, 11.86388494,  2.23166071]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.2768613530464352}
episode index:2632
target Thresh 19.0
target distance 14.0
model initialize at round 2632
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 2.       , 10.       ,  5.1705122]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8137041634316534
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.58803813,  8.38722368,  0.62184443]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5653801872686045}
episode index:2633
target Thresh 19.0
target distance 7.0
model initialize at round 2633
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([16.        , 11.        ,  1.88272398]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.071067811865476}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8137351558731215
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.32740884,  4.82343854,  5.03316806]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.063216770107181}
episode index:2634
target Thresh 19.0
target distance 11.0
model initialize at round 2634
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([15.63119195,  9.54801645,  2.19363333]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.721471001734828}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8137367390919922
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.78839721, 10.24303243,  4.49452149]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7859870465209489}
episode index:2635
target Thresh 19.0
target distance 11.0
model initialize at round 2635
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.32175245,  9.87401097,  4.22652388]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.094703051931392}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8137291053835521
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.1453177 , 5.81058559, 6.24422673]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.23873636218539845}
episode index:2636
target Thresh 19.0
target distance 8.0
model initialize at round 2636
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([16.03640123,  4.68257631,  2.55916548]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.496824460802471}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8137532964785903
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.76408093, 10.09656991,  3.42642425]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.9337257257916995}
episode index:2637
target Thresh 19.0
target distance 1.0
model initialize at round 2637
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.11950214, 6.69563063, 3.55753827]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9316207302824158}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8138238979583179
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.11950214, 6.69563063, 3.55753827]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9316207302824158}
episode index:2638
target Thresh 19.0
target distance 12.0
model initialize at round 2638
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([2.14202055, 6.24101102, 5.90736794]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 12.593563873592341}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.8137447703225388
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.37336624,  2.51873863,  5.37640303]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6391338814909326}
episode index:2639
target Thresh 19.0
target distance 9.0
model initialize at round 2639
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 5.        , 11.        ,  4.70246363]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.041594579074744}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8137282166768882
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.59154613,  3.82909566,  0.15379586]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0184922356060926}
episode index:2640
target Thresh 19.0
target distance 5.0
model initialize at round 2640
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([4.88352463, 8.2987028 , 0.5486986 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.923654739685238}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8137625422574759
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.91780039, 11.30527321,  1.69914268]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.31614634446726053}
episode index:2641
target Thresh 19.0
target distance 11.0
model initialize at round 2641
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.99999808,  1.99999836,  4.85758209]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.704698662744763}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8137128799004559
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.87686761, 5.9535762 , 5.45935841]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.13159314053896642}
episode index:2642
target Thresh 19.0
target distance 12.0
model initialize at round 2642
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([16.        ,  7.        ,  1.83448332]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 12.369316876888846}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8136738486281788
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.41591404, 3.99576   , 0.71944494]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.4159356507616965}
episode index:2643
target Thresh 19.0
target distance 9.0
model initialize at round 2643
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([3.99944974, 3.99996763, 4.2103436 ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 11.402208478647628}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8136458707631107
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.46395353, 10.85564219,  1.37849053]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.4858930471290552}
episode index:2644
target Thresh 19.0
target distance 8.0
model initialize at round 2644
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.65194482, 2.32166192, 1.20231074]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 7.705965719371586}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8136700201590412
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.22627951, 10.06485859,  2.06956951]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.2353912811150271}
episode index:2645
target Thresh 19.0
target distance 3.0
model initialize at round 2645
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.        ,  6.        ,  1.78746765]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 3.1622776602206164}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8137183233069027
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.38968941,  3.02993292,  1.22109704]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.3908373278469874}
episode index:2646
target Thresh 19.0
target distance 10.0
model initialize at round 2646
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 5.       , 11.       ,  2.6835162]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.180339887519345}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8137077301136653
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.9919245 ,  5.58601896,  0.41803375]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.41405979687772676}
episode index:2647
target Thresh 19.0
target distance 12.0
model initialize at round 2647
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([16.        ,  8.        ,  5.86224127]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 12.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8136941778588638
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.73354609, 8.27006056, 5.59675881]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.37938159528784526}
episode index:2648
target Thresh 19.0
target distance 7.0
model initialize at round 2648
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.        ,  2.        ,  0.03308647]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8137249985747564
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.27115291,  8.65550859,  3.18353055]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8061590453388966}
episode index:2649
target Thresh 19.0
target distance 3.0
model initialize at round 2649
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.71641884,  5.03746998,  4.79501605]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.4080831618530514}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8137804215979357
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.87074028,  2.87431621,  0.22864544]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8797642042918299}
episode index:2650
target Thresh 19.0
target distance 11.0
model initialize at round 2650
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.31735722,  7.03306814,  4.13194275]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 9.522707939956085}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8137758384851258
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.37846359, 9.74313239, 4.1496456 ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9687895846690897}
episode index:2651
target Thresh 19.0
target distance 12.0
model initialize at round 2651
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([1.99999999, 6.        , 3.86909783]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 12.165525070974825}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8137237846429597
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.36341795,  4.91940195,  0.18768884]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9886215423977861}
episode index:2652
target Thresh 19.0
target distance 3.0
model initialize at round 2652
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.3726279 ,  7.57113365,  4.40926719]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.646568343660689}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8137791454516128
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.01410768,  4.78983353,  6.12608188]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.21063943344479766}
episode index:2653
target Thresh 19.0
target distance 14.0
model initialize at round 2653
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([16.44689244,  6.62252964,  2.3119866 ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 14.537720029456036}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8137429553186613
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.94236754, 4.70682999, 5.48013352]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.2987811153384547}
episode index:2654
target Thresh 19.0
target distance 5.0
model initialize at round 2654
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.23889414, 7.50103496, 3.05007124]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 3.5071108836444336}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8137910672561687
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.09069309, 10.82998533,  2.48370063]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.19269204431404252}
episode index:2655
target Thresh 19.0
target distance 10.0
model initialize at round 2655
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.15304195,  5.18802658,  4.64690828]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.410931793130567}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.813749522311795
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.57330363, 3.53110664, 5.5318699 ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.6812811885333905}
episode index:2656
target Thresh 19.0
target distance 11.0
model initialize at round 2656
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.        , 9.        , 5.07489157]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8137510870145747
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.07054461,  9.85028755,  1.09259442]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.2597127605638079}
episode index:2657
target Thresh 19.0
target distance 13.0
model initialize at round 2657
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.09309277, 3.68039335, 2.52545351]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.91086374861065}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.8136748515565305
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.46654079,  3.73744434,  6.2776739 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.53534641344504}
episode index:2658
target Thresh 19.0
target distance 6.0
model initialize at round 2658
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([13.52190669,  4.80475354,  3.65301752]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 5.755999673161537}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8137124004079509
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.94209946,  9.50109978,  2.8034616 ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5022488415645664}
episode index:2659
target Thresh 19.0
target distance 11.0
model initialize at round 2659
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.       ,  9.       ,  2.1439258]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8136845763376428
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.86959999, 2.53835112, 5.59525804]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.5539188490573825}
episode index:2660
target Thresh 19.0
target distance 2.0
model initialize at round 2660
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 2.48139933, 10.60107466,  2.89404011]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.5701240153830547}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8137434318144042
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.98141329, 11.62925066,  2.6108548 ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6295251087883453}
episode index:2661
target Thresh 19.0
target distance 4.0
model initialize at round 2661
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.59221766, 11.54419755,  1.33933848]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 4.562457428125128}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8137843789453635
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.86320345,  7.36291977,  4.77296786]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.3878454011297895}
episode index:2662
target Thresh 19.0
target distance 11.0
model initialize at round 2662
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.50062687, 11.67192351,  3.68684578]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 13.55758147331075}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8137510315766111
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.52671138, 2.66537881, 4.85499271]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.816536023307494}
episode index:2663
target Thresh 19.0
target distance 7.0
model initialize at round 2663
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.        , 4.        , 0.35121601]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 7.000000000000001}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8137816574109512
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.30515837, 10.83870805,  3.50166009]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7133161920016887}
episode index:2664
target Thresh 19.0
target distance 2.0
model initialize at round 2664
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.60456742, 10.42268417,  3.85747242]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.4547434735161997}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8138477806164254
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.83121692,  8.755054  ,  5.85747242]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7736887394423863}
episode index:2665
target Thresh 19.0
target distance 11.0
model initialize at round 2665
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.05069225,  9.31779561,  5.75251293]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.729192457380572}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8138256018867132
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.62735676, 3.50870824, 5.20384517]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8076884143108322}
episode index:2666
target Thresh 19.0
target distance 5.0
model initialize at round 2666
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.70417269,  7.07388066,  3.45958197]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 4.134438472918257}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8138734663589718
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.15454846, 10.31774193,  2.89321136]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0863997342863914}
episode index:2667
target Thresh 19.0
target distance 5.0
model initialize at round 2667
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 5.30206715, 10.06629801,  1.69616383]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 3.813988298470154}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8139212949508167
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.48743384, 11.00013649,  1.12979321]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5125661733013634}
episode index:2668
target Thresh 19.0
target distance 2.0
model initialize at round 2668
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 4.        , 11.        ,  0.25067824]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 2.000000000165885}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8139835574854923
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.32528581, 10.83351737,  4.25067824]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.36541390837231097}
episode index:2669
target Thresh 19.0
target distance 1.0
model initialize at round 2669
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([14.99999994,  7.99999998,  4.56608367]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.000000056735644}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8140494812467336
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.77902615,  7.30689146,  0.28289836]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.727481197564178}
episode index:2670
target Thresh 19.0
target distance 3.0
model initialize at round 2670
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 7.        , 11.        ,  1.39255589]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 3.0}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8141043470381051
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.3919349 , 11.63602685,  3.10937058]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7470897627648416}
episode index:2671
target Thresh 19.0
target distance 6.0
model initialize at round 2671
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.67462059,  3.16743345,  1.10965174]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 5.8416354102928745}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8141381336129444
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.16059189,  9.42996368,  2.26009582]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9431196853593589}
episode index:2672
target Thresh 19.0
target distance 10.0
model initialize at round 2672
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.9013869 , 2.59071387, 0.72596234]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.107813977074654}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.8140553907445526
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.41750423,  3.44353037,  4.19499742]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7321342154409705}
episode index:2673
target Thresh 19.0
target distance 7.0
model initialize at round 2673
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 7.        , 11.        ,  4.49610734]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8140599207494483
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.05968649,  6.26059964,  4.7969955 ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.26734742956166585}
episode index:2674
target Thresh 19.0
target distance 8.0
model initialize at round 2674
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([15.0641944 ,  7.60119481,  5.1327703 ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.8393060468105595}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8140869581142957
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.8881535 , 10.45294697,  4.00002907]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.0431124862613042}
episode index:2675
target Thresh 19.0
target distance 4.0
model initialize at round 2675
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 8.58392327, 11.71353013,  3.59904122]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.680631401825976}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8141417073863007
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.56101862, 10.08854799,  5.31585592]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0702741046448954}
episode index:2676
target Thresh 19.0
target distance 7.0
model initialize at round 2676
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.65986867, 10.72210779,  0.8441202 ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 6.73070742887474}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.814168693999797
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.90046266,  4.0742537 ,  5.99456428]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.9035190176267079}
episode index:2677
target Thresh 19.0
target distance 6.0
model initialize at round 2677
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([15.        ,  4.        ,  4.62513471]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8142057920406797
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.10714641,  9.01247502,  3.77557878]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.9933206622120664}
episode index:2678
target Thresh 19.0
target distance 1.0
model initialize at round 2678
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.       , 7.       , 0.6185767]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.4142135656875578}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8142714113792235
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.93656109, 8.67835093, 2.6185767 ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.156419762005949}
episode index:2679
target Thresh 19.0
target distance 12.0
model initialize at round 2679
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([15.00589648,  2.00063004,  1.11644619]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.005896498816691}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.814186622338387
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.88714171, 2.84389145, 0.30229597]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.2244072837367883}
episode index:2680
target Thresh 19.0
target distance 2.0
model initialize at round 2680
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.01930578,  5.99130783,  0.58694952]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.2353087849332978}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8142521998757468
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.0089502 ,  7.67424599,  2.58694952]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0432139638449718}
episode index:2681
target Thresh 19.0
target distance 7.0
model initialize at round 2681
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 4.36809077, 10.01983893,  0.38832253]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 6.173340517864565}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8142790949808326
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.58386843, 3.48919307, 5.53876661]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.6588544601101111}
episode index:2682
target Thresh 19.0
target distance 2.0
model initialize at round 2682
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.36606417, 2.44105825, 5.33616734]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.772273923270431}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8143483163393936
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.36606417, 2.44105825, 5.33616734]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.772273923270431}
episode index:2683
target Thresh 19.0
target distance 4.0
model initialize at round 2683
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.56465505,  4.61984478,  1.3871867 ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.4269935293668965}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.814402805047911
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.88056859,  7.39062534,  3.10400139]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.6209680659665012}
episode index:2684
target Thresh 19.0
target distance 11.0
model initialize at round 2684
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([3.07606792, 8.87011153, 6.25218296]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.982210159496773}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8144071871033254
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.00768739,  9.69820351,  0.26988581]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.037191129142927}
episode index:2685
target Thresh 19.0
target distance 3.0
model initialize at round 2685
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 4.65313137, 10.68449284,  0.82141369]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.3833292730143558}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8144616133218274
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.17705114, 11.68261103,  2.53822838]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.7051985019120537}
episode index:2686
target Thresh 19.0
target distance 7.0
model initialize at round 2686
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([16.81599059,  6.03193217,  1.66334933]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 9.261285381577034}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.81447221309111
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.54319332, 10.84807656,  4.24742279]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.4814073894049601}
episode index:2687
target Thresh 19.0
target distance 3.0
model initialize at round 2687
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.        , 8.        , 4.67702579]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.6055512754663255}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8145230009768276
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.85233143, 10.18705515,  2.11065518]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8262477465417801}
episode index:2688
target Thresh 19.0
target distance 10.0
model initialize at round 2688
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.00000131, 3.99999543, 6.00127888]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.049874773895656}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.814476487806464
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.97949456,  5.41699378,  4.60305519]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.417497651703638}
episode index:2689
target Thresh 19.0
target distance 6.0
model initialize at round 2689
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([10.        , 11.        ,  0.52340716]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8145065479426915
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.7794153 , 10.63918108,  3.67385124]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6761730986400832}
episode index:2690
target Thresh 19.0
target distance 11.0
model initialize at round 2690
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 3.99997582, 10.99987842,  5.52607751]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.401745588935595}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.814501763119799
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.26042718,  8.20256182,  5.54378036]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.766811086746652}
episode index:2691
target Thresh 19.0
target distance 1.0
model initialize at round 2691
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.2732204 ,  6.66869723,  5.14866972]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.7582745257856263}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8145669556297842
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.93865911,  6.4263259 ,  0.86548441]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.5769442593360498}
episode index:2692
target Thresh 19.0
target distance 4.0
model initialize at round 2692
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.89350991,  8.66362868,  0.11502617]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.2680724503155916}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8146140826976531
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.40780871,  5.99340179,  5.83184086]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.59222804959477}
episode index:2693
target Thresh 19.0
target distance 8.0
model initialize at round 2693
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([10.        , 11.        ,  2.18674538]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8146153049897465
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.2230971 , 4.94065425, 4.48763353]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.23085543581183537}
episode index:2694
target Thresh 19.0
target distance 10.0
model initialize at round 2694
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([16.        ,  6.        ,  0.76919335]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8146165263747587
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.70318752, 11.66738058,  3.0700815 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9694686812614574}
episode index:2695
target Thresh 19.0
target distance 2.0
model initialize at round 2695
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.98019152, 10.33879296,  3.23176706]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.037090201588195}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8146852887907918
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.98019152, 10.33879296,  3.23176706]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.037090201588195}
episode index:2696
target Thresh 19.0
target distance 5.0
model initialize at round 2696
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.81792375,  5.32690815,  5.61398935]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 3.5306687945223105}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8147288112450433
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.44770974,  2.12293747,  0.76443343]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.4642818497378541}
episode index:2697
target Thresh 19.0
target distance 2.0
model initialize at round 2697
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([13.52913633,  3.72503168,  4.33640289]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 2.2669746912175404}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.814793774621157
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.49878428,  2.34946963,  0.05321759]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.6110206400303322}
episode index:2698
target Thresh 19.0
target distance 2.0
model initialize at round 2698
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.73172154,  7.3385628 ,  5.56229711]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.43197002662300876}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8148623949343763
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.73172154,  7.3385628 ,  5.56229711]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.43197002662300876}
episode index:2699
target Thresh 19.0
target distance 3.0
model initialize at round 2699
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([14.        , 11.        ,  2.58494967]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8149128125843635
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.19092939,  8.49500122,  0.01857906]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5305471104054157}
episode index:2700
target Thresh 19.0
target distance 10.0
model initialize at round 2700
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.99998477, 11.00008424,  2.75969815]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 12.806289205689394}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8148849664387893
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.11367946, 3.31980813, 6.21103039]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.33941163921254003}
episode index:2701
target Thresh 19.0
target distance 9.0
model initialize at round 2701
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 9.39161788, 10.50452543,  4.45042539]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 7.543183127816153}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8149081478068724
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.49814086, 8.85496703, 5.31768416]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5188245179865981}
episode index:2702
target Thresh 19.0
target distance 12.0
model initialize at round 2702
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 3.59302951, 10.4571878 ,  0.68159645]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.417007994970394}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.814912313724752
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.38680702, 10.81266054,  0.98248461]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.018048521050184}
episode index:2703
target Thresh 19.0
target distance 1.0
model initialize at round 2703
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.78312194,  5.83742222,  4.91417432]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.2710491227630404}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8149807633128715
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.78312194,  5.83742222,  4.91417432]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.2710491227630404}
episode index:2704
target Thresh 19.0
target distance 4.0
model initialize at round 2704
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([16.        ,  3.        ,  0.58509367]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8150240478173426
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.20647392,  6.78131958,  2.01872306]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.3007533946156743}
episode index:2705
target Thresh 19.0
target distance 7.0
model initialize at round 2705
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 9.9999965 , 11.00002666,  2.71124792]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.219562422822488}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8150220906186745
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.56782956, 4.04198908, 0.72895077]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.5693799145044728}
episode index:2706
target Thresh 19.0
target distance 8.0
model initialize at round 2706
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([1.48072479, 3.72401031, 3.70688057]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 11.981511856149309}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8150026301915332
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.06473339, 11.33672254,  1.1582128 ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9940350619410887}
episode index:2707
target Thresh 19.0
target distance 11.0
model initialize at round 2707
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.04210022, 6.96624143, 0.33412474]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.000417722007294}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8149803689865892
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.83173117,  6.64830287,  6.06864228]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.6697842999065751}
episode index:2708
target Thresh 19.0
target distance 12.0
model initialize at round 2708
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.84604243, 10.41981874,  2.93229157]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 14.53348096775419}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8149444701913184
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.07804966, 1.60918379, 6.1004385 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.39853363362738053}
episode index:2709
target Thresh 19.0
target distance 7.0
model initialize at round 2709
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.        ,  3.        ,  4.82390714]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8149708319630988
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.01898567,  9.58444478,  3.69116591]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0653991061630372}
episode index:2710
target Thresh 19.0
target distance 7.0
model initialize at round 2710
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.        ,  9.        ,  2.48002332]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8150004768993937
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.82768483,  2.33368755,  5.6304674 ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.37555278806856845}
episode index:2711
target Thresh 19.0
target distance 3.0
model initialize at round 2711
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([12.        , 11.        ,  4.01731277]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.815054162567941
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.3337824 ,  8.55193815,  5.73412746]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8651483181123412}
episode index:2712
target Thresh 19.0
target distance 4.0
model initialize at round 2712
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([2.91103696, 8.68061705, 2.63368171]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.121426573734321}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8151007626368069
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.23963   , 10.91850336,  2.0673111 ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.2531091497247682}
episode index:2713
target Thresh 19.0
target distance 5.0
model initialize at round 2713
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([4.00000029, 7.00000112, 2.32574138]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 5.099020664640504}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8151404251024631
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.33361539, 2.21361004, 5.75937077]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6997840360780406}
episode index:2714
target Thresh 19.0
target distance 12.0
model initialize at round 2714
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([15.        ,  9.        ,  1.65424317]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 12.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8151384315271846
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.92479791, 8.77472116, 5.95513132]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9518412342270679}
episode index:2715
target Thresh 19.0
target distance 10.0
model initialize at round 2715
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 3.99991861, 11.00043975,  2.76380432]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.4405108334343}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8151364394199291
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.55842914,  8.68930885,  0.78150717]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8871244493190722}
episode index:2716
target Thresh 19.0
target distance 12.0
model initialize at round 2716
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([16.00001028,  3.00000102,  1.10891884]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.041604911135835}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.8150659164002276
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.17565514, 2.9291233 , 0.86113923]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9455817399623784}
episode index:2717
target Thresh 19.0
target distance 6.0
model initialize at round 2717
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.8983782 , 7.58343569, 0.0115506 ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 4.9610203081381075}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8151021383763436
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.14529864, 2.90589529, 5.44517999]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.17311092448618837}
episode index:2718
target Thresh 19.0
target distance 11.0
model initialize at round 2718
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.36635261,  8.44087813,  5.3363626 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.204350762106865}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8150689931750128
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.57715224, 2.51731342, 0.22132422]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7750599257820279}
episode index:2719
target Thresh 19.0
target distance 9.0
model initialize at round 2719
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([2.        , 2.        , 0.59504431]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.8488578017961}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8150730739215791
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.12275796, 10.90265137,  0.89593247]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.15667250134835337}
episode index:2720
target Thresh 19.0
target distance 12.0
model initialize at round 2720
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([2.        , 4.        , 5.53627229]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 12.041594578792377}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8150169213920173
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.66149633,  5.48855129,  5.85486329]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8223501450414679}
episode index:2721
target Thresh 19.0
target distance 5.0
model initialize at round 2721
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.78532523,  6.16488338,  3.38711607]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.022878856630578}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8150633810643204
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.05086819,  9.25618053,  2.82074546]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.7455568276902247}
episode index:2722
target Thresh 19.0
target distance 12.0
model initialize at round 2722
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([2.87821736, 3.67855803, 2.65322137]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 14.16125442564292}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8150250005451165
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.01089408, 10.77287143,  1.53818299]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.22738968696358886}
episode index:2723
target Thresh 19.0
target distance 6.0
model initialize at round 2723
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 9.        , 11.        ,  3.52758753]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8150544841184327
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.24240174,  9.70513688,  0.3948463 ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.745638404126745}
episode index:2724
target Thresh 19.0
target distance 2.0
model initialize at round 2724
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([1.36846036, 5.45324268, 5.32181573]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.6933252833598138}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.815118684307747
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.05073811, 5.50142017, 1.03863042]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5039806934658219}
episode index:2725
target Thresh 19.0
target distance 10.0
model initialize at round 2725
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.        , 10.        ,  1.22596472]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.440306509271148}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8151167067523227
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.71925553, 6.8331613 , 5.52685288]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.32657710957355834}
episode index:2726
target Thresh 19.0
target distance 2.0
model initialize at round 2726
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 2.00113813, 11.00204198,  2.07231409]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.235964420428726}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8151736126171001
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.77558264, 10.60361732,  1.78912878]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.643985260786824}
episode index:2727
target Thresh 19.0
target distance 8.0
model initialize at round 2727
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.        ,  3.        ,  0.09631556]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8151807021144729
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.64007161, 10.87243217,  4.68038902]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.652660109357923}
episode index:2728
target Thresh 19.0
target distance 14.0
model initialize at round 2728
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([1.09838894, 4.38960468, 3.18993497]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 15.598517784378078}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8151423629884772
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.00335025,  9.77610542,  2.07489659]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.263190541953346}
episode index:2729
target Thresh 19.0
target distance 2.0
model initialize at round 2729
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.32368469,  4.85050652,  4.24053717]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9100181976563404}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8152100764086279
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.32368469,  4.85050652,  4.24053717]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9100181976563404}
episode index:2730
target Thresh 19.0
target distance 12.0
model initialize at round 2730
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([12.5128928 ,  9.50323929,  3.94483471]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.866193899404166}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8151935378035003
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.3781129 , 3.46238728, 5.67935225]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.6572646389423482}
episode index:2731
target Thresh 19.0
target distance 14.0
model initialize at round 2731
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([16.      ,  5.      ,  5.708323]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8151552360792941
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.61072203, 7.72353708, 4.59328462]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.47746108274821936}
episode index:2732
target Thresh 19.0
target distance 9.0
model initialize at round 2732
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 7.47427373, 11.81164718,  1.51330727]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.587997325420407}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8151444504608265
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.22599255,  3.86926658,  5.53101012]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8981631335867429}
episode index:2733
target Thresh 19.0
target distance 2.0
model initialize at round 2733
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.94463491, 4.31794091, 5.68948579]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.102216813259583}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8152084064043301
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.4971362 , 4.96756828, 1.40630048]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0878110047771905}
episode index:2734
target Thresh 19.0
target distance 4.0
model initialize at round 2734
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 6.00006452, 10.99983809,  6.10160542]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.1231289561425815}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8152545752317512
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.88462569, 9.1926408 , 5.5352348 ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.1976608433156017}
episode index:2735
target Thresh 19.0
target distance 2.0
model initialize at round 2735
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([4.54868287, 2.40898297, 6.05448008]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.601775793194179}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8153112435156576
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.07167698, 1.35136318, 5.77129477]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6525850971088055}
episode index:2736
target Thresh 19.0
target distance 1.0
model initialize at round 2736
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.12096207,  5.25529839,  1.85187071]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.2825050269174521}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8153787220529191
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.12096207,  5.25529839,  1.85187071]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.2825050269174521}
episode index:2737
target Thresh 19.0
target distance 12.0
model initialize at round 2737
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([15.        ,  6.        ,  0.70725029]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 12.369316876852983}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8153593517067884
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.07947437, 9.08770394, 4.44176784]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.11835605690778986}
episode index:2738
target Thresh 19.0
target distance 6.0
model initialize at round 2738
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 7.98978166, 11.02217881,  3.01253366]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 7.816633820599204}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8153820467309916
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.19818691, 6.61096182, 3.87979244]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.6423024194630775}
episode index:2739
target Thresh 19.0
target distance 2.0
model initialize at round 2739
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([15.47265373,  6.59821588,  2.89950722]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.5264794463599074}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8154385857650314
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.19626143,  7.15145837,  2.61632192]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.24790761580022885}
episode index:2740
target Thresh 19.0
target distance 14.0
model initialize at round 2740
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([16.        , 10.        ,  1.57342738]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 15.231546211727819}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.815402938901417
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.82935849, 3.71868198, 4.7415743 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.3290263736163543}
episode index:2741
target Thresh 19.0
target distance 5.0
model initialize at round 2741
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.43866043, 7.37520306, 5.98608088]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 3.6690243264074978}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.815448918919834
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.79428762, 4.1416923 , 5.41971027]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8068268272171202}
episode index:2742
target Thresh 19.0
target distance 13.0
model initialize at round 2742
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 3.        , 10.        ,  3.71535408]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 14.317821063276352}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8154055227474424
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.80534818,  4.47001731,  0.3171304 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.5087293948758601}
episode index:2743
target Thresh 19.0
target distance 7.0
model initialize at round 2743
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.        ,  2.        ,  5.65020013]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8154346527516374
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.41124894,  8.58096676,  2.5174589 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.587123963019053}
episode index:2744
target Thresh 19.0
target distance 5.0
model initialize at round 2744
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.75806747,  7.864222  ,  4.89237475]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 4.058892467038609}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8154771411651731
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.2086487 ,  3.86590923,  0.04281883]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.24802139567476242}
episode index:2745
target Thresh 19.0
target distance 3.0
model initialize at round 2745
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.88776669,  3.32077644,  5.65565181]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.303934350770824}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.815533522031464
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.26680362,  2.77078001,  5.3724665 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8156506550460808}
episode index:2746
target Thresh 19.0
target distance 3.0
model initialize at round 2746
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.70020471,  9.46961051,  6.15149021]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.4998774156355796}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8155898618487079
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.94481869,  7.80512144,  5.8683049 ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.2025404386590232}
episode index:2747
target Thresh 19.0
target distance 11.0
model initialize at round 2747
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([2.88738319, 6.67919789, 2.64776188]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 12.057576255373654}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.8155088489609905
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.01088182,  2.12064394,  6.11679697]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.12113370648006161}
episode index:2748
target Thresh 19.0
target distance 10.0
model initialize at round 2748
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.        ,  3.        ,  5.38359904]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8154867356245927
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.54034273, 10.72521154,  4.83493128]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6062004301240127}
episode index:2749
target Thresh 19.0
target distance 6.0
model initialize at round 2749
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.10677168,  9.42636995,  3.14027596]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.343727956290187}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8155190612025507
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.1085043 , 10.5682229 ,  4.29072003]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.445201804369959}
episode index:2750
target Thresh 19.0
target distance 11.0
model initialize at round 2750
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([3.       , 7.       , 4.1617825]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8155110987243396
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.42334411, 10.31500285,  6.17948535]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8954066701782964}
episode index:2751
target Thresh 19.0
target distance 12.0
model initialize at round 2751
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.85447728, 5.1553549 , 1.14910858]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.583563405978154}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.8154413364773071
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.35673758,  2.49978119,  0.90132898]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.614038224295667}
episode index:2752
target Thresh 19.0
target distance 11.0
model initialize at round 2752
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.08113451, 11.40999088,  3.15835297]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.642241812223663}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8154422320825088
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.81044318, 8.30059417, 5.45924113]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.3553711365012058}
episode index:2753
target Thresh 19.0
target distance 5.0
model initialize at round 2753
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 4.       , 10.       ,  3.3543396]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 5.385164807151581}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8154811944871367
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.65954545, 5.2680755 , 0.50478368]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.711944291910457}
episode index:2754
target Thresh 19.0
target distance 12.0
model initialize at round 2754
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 4.        , 11.        ,  3.35111427]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8154791061654431
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.43762252,  9.90314363,  1.36881712]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.0639252108029187}
episode index:2755
target Thresh 19.0
target distance 10.0
model initialize at round 2755
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.00004755, 6.99995108, 0.21038693]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.049823439761926}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8154489262900763
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.58977153,  5.42053705,  5.66171916]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7099751426116812}
episode index:2756
target Thresh 19.0
target distance 12.0
model initialize at round 2756
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([15.        ,  2.        ,  4.74412417]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 13.892443989449248}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8154083019769496
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.5108866 , 8.35011742, 5.6290858 ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8266513639803282}
episode index:2757
target Thresh 19.0
target distance 8.0
model initialize at round 2757
at step 0:
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([15.        ,  4.        ,  0.52138012]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8154062423562984
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.11164565, 10.71397411,  4.82226828]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.30704325498414403}
episode index:2758
target Thresh 19.0
target distance 12.0
model initialize at round 2758
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([2.        , 6.        , 5.18957996]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 12.041594578792317}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8153656629625483
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.28604592,  6.80778215,  6.07454158]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.3446301939749943}
episode index:2759
target Thresh 19.0
target distance 11.0
model initialize at round 2759
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.62735812, 4.64119655, 2.80406588]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.675311667699988}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.8153007426085995
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.50425251,  2.82962963,  4.83947158]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.9664630807695362}
episode index:2760
target Thresh 19.0
target distance 6.0
model initialize at round 2760
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([10.        , 11.        ,  3.59917462]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8153200997509973
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.07353858,  7.74578355,  0.18324808]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7494004469314609}
episode index:2761
target Thresh 19.0
target distance 8.0
model initialize at round 2761
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.70074502,  9.06973108,  3.46278   ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.973228948190932}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8153426199983718
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.06778107, 10.63751646,  4.33003877]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.3687663072352746}
episode index:2762
target Thresh 19.0
target distance 2.0
model initialize at round 2762
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 9.        , 11.        ,  4.45105243]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.0}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8154058329480646
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.12734933, 10.19798308,  0.16786712]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.1852216355465197}
episode index:2763
target Thresh 19.0
target distance 13.0
model initialize at round 2763
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([15.        ,  4.        ,  0.36475676]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.8153297114698407
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.11474478, 2.47512497, 6.11697715]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.5372710346561939}
episode index:2764
target Thresh 19.0
target distance 12.0
model initialize at round 2764
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.11209866,  6.42969203,  3.13654566]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.222504345916533}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8153050613370603
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.43963313, 8.40051173, 4.58787789]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5947158461213091}
episode index:2765
target Thresh 19.0
target distance 7.0
model initialize at round 2765
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.        , 10.        ,  1.37056416]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 7.000000000000001}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.815330759027002
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.02565635,  3.17941058,  0.23782293]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.18123577522168574}
episode index:2766
target Thresh 19.0
target distance 10.0
model initialize at round 2766
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([3.42853034, 3.58297521, 2.92724562]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 11.878557501163327}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8153034260361676
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.91424715,  8.53681928,  0.09539255]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.47105194380290916}
episode index:2767
target Thresh 19.0
target distance 12.0
model initialize at round 2767
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([15.01066017,  7.01910511,  2.07185102]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 12.010675365015748}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.815276112794604
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.31588732, 6.41184958, 5.52318325]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.66761194894327}
episode index:2768
target Thresh 19.0
target distance 5.0
model initialize at round 2768
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([13.14226866,  6.0747426 ,  4.16779137]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.370984587942461}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8153115917164852
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.77681847, 10.41716261,  3.31823545]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9711572307220763}
episode index:2769
target Thresh 19.0
target distance 6.0
model initialize at round 2769
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.        , 4.        , 6.18415737]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8153503762300993
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.80257638, 9.30703109, 3.33460145]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.720542849924334}
episode index:2770
target Thresh 19.0
target distance 4.0
model initialize at round 2770
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 6.50918228, 10.21907732,  4.63412213]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 2.7896496572433023}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8154062941744407
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.85880988, 9.88910824, 4.35093683]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.236150428389938}
episode index:2771
target Thresh 19.0
target distance 11.0
model initialize at round 2771
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 5.        , 11.        ,  4.29308987]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.401754250993429}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8154071962824576
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.5587627 ,  8.78738658,  0.31079272]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9025895932548983}
episode index:2772
target Thresh 19.0
target distance 11.0
model initialize at round 2772
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([16.       ,  6.       ,  1.2304942]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 12.083045973594574}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8153993373164863
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.02779057, 11.42329455,  3.24819705]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.42420583985684956}
episode index:2773
target Thresh 19.0
target distance 6.0
model initialize at round 2773
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 9.62976246, 10.1049146 ,  5.47349644]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 6.720054256437353}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8154032195394564
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.62884053,  4.79249186,  5.77438459]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6621933521289572}
episode index:2774
target Thresh 19.0
target distance 2.0
model initialize at round 2774
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.89957692,  3.62372188,  0.06627386]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.9993552558675967}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8154625697306134
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.13002504,  3.75336477,  4.06627386]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7645031025339757}
episode index:2775
target Thresh 19.0
target distance 5.0
model initialize at round 2775
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.92198852, 8.51729393, 0.65864294]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 4.909173487129711}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8154978920208702
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.09497664, 3.53968426, 6.09227233]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.4700118573406577}
episode index:2776
target Thresh 19.0
target distance 1.0
model initialize at round 2776
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.        , 6.        , 5.70413065]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.414213562373095}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8155571653762823
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.74663449, 7.12312072, 3.42094534]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.2816962792507094}
episode index:2777
target Thresh 19.0
target distance 12.0
model initialize at round 2777
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 3.        , 11.        ,  3.00966978]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 12.369316876852983}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8155464097878845
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.73673025,  8.12884059,  0.74418732]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.293105542999269}
episode index:2778
target Thresh 19.0
target distance 14.0
model initialize at round 2778
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([16.00000021,  9.99999997,  0.8508479 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 14.03566906341394}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8155356619401045
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.63066366, 10.58814397,  4.86855075]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7532343887689971}
episode index:2779
target Thresh 19.0
target distance 12.0
model initialize at round 2779
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.05812416,  8.70391678,  5.06676555]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.442538988929181}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8155057222666204
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.96766907, 3.6192129 , 4.23491248]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.6200563676725912}
episode index:2780
target Thresh 19.0
target distance 5.0
model initialize at round 2780
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([15.80479069, 10.67161042,  2.69704896]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 4.815999721227935}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8155476351129493
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.3101897 , 10.89389566,  4.13067835]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.3278349912188005}
episode index:2781
target Thresh 19.0
target distance 2.0
model initialize at round 2781
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([13.09856712,  6.38846903,  3.19100869]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.9651783938263008}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8155929020124058
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.23909391,  7.43915141,  2.62463807]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5000198605061716}
episode index:2782
target Thresh 19.0
target distance 8.0
model initialize at round 2782
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 5.        , 11.        ,  0.74818343]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 8.544003745317687}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8156088795376047
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.14739245, 2.95188123, 5.6154422 ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.155048224125999}
episode index:2783
target Thresh 19.0
target distance 3.0
model initialize at round 2783
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.34789992,  4.55150043,  2.97868174]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.5885167692548148}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.815664443517656
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.31498124,  6.171463  ,  2.69549644]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.7061517231962102}
episode index:2784
target Thresh 19.0
target distance 11.0
model initialize at round 2784
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.        ,  8.        ,  5.53355098]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8156623118927739
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.29679757, 11.7481552 ,  3.55125383]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8048757662030744}
episode index:2785
target Thresh 19.0
target distance 11.0
model initialize at round 2785
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.        , 11.        ,  0.71223753]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.000000000636321}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8156631175732135
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.35327321, 11.34525066,  3.01312569]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.49396354203396725}
episode index:2786
target Thresh 19.0
target distance 13.0
model initialize at round 2786
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([15.        ,  8.        ,  1.47730797]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 13.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8156523587010334
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.33919634, 7.98066476, 5.49501082]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.33974697584600283}
episode index:2787
target Thresh 19.0
target distance 3.0
model initialize at round 2787
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.23619485, 5.66631332, 2.43998742]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.2112731135774966}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8157078273672095
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.05963482, 7.05010697, 2.15680211]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.941699198619652}
episode index:2788
target Thresh 19.0
target distance 10.0
model initialize at round 2788
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 5.        , 11.        ,  5.49570346]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8157175926472442
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.60776748,  9.99691156,  1.79659162]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0712978101929593}
episode index:2789
target Thresh 19.0
target distance 6.0
model initialize at round 2789
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 7.        , 11.        ,  4.12214828]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8157559536156244
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.01040122, 11.57133704,  1.27259235]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.1426862005487874}
episode index:2790
target Thresh 19.0
target distance 7.0
model initialize at round 2790
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 7.        , 11.        ,  2.72507596]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 7.615773105864301}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8157749395916736
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.92451481, 3.7407037 , 5.59233473]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.27006033897203363}
episode index:2791
target Thresh 19.0
target distance 11.0
model initialize at round 2791
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.76740287, 6.27778371, 1.88105028]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.744761987050808}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8157223596557773
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.96638673,  3.66519374,  0.19964129]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.6660424644325494}
episode index:2792
target Thresh 19.0
target distance 12.0
model initialize at round 2792
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 3.        , 11.        ,  3.76397634]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8157116026851907
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.39832624,  9.81661717,  1.49849388]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9085853826863184}
episode index:2793
target Thresh 19.0
target distance 2.0
model initialize at round 2793
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.27416372, 6.34920843, 5.88696694]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.8444621255731024}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8157739822117887
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.66906075, 7.29082884, 1.60378164]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.4405703124369965}
episode index:2794
target Thresh 19.0
target distance 2.0
model initialize at round 2794
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.        ,  8.        ,  5.22395396]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.8284271247464337}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.815832775062518
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.08576407,  9.45870581,  2.94076866]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0624625824138159}
episode index:2795
target Thresh 19.0
target distance 11.0
model initialize at round 2795
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.55308548, 3.64831596, 1.40544766]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 11.970440770514752}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8158029004540028
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.04265214, 10.79726008,  0.57359459]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.9785797885703977}
episode index:2796
target Thresh 19.0
target distance 4.0
model initialize at round 2796
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([15.14465732,  5.55059426,  5.18925595]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.7956701310796266}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8158478333281347
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.33308273,  3.26945114,  4.62288534]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7192930988066458}
episode index:2797
target Thresh 19.0
target distance 8.0
model initialize at round 2797
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([3.67844691, 9.87670082, 0.93667143]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 8.053545789230387}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8158636340862881
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.28721175, 1.52931508, 5.8039302 ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5513935826761083}
episode index:2798
target Thresh 19.0
target distance 13.0
model initialize at round 2798
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.32868582,  4.19772387,  4.03383565]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.67012741805275}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8158234711927238
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.70539713, 6.93140546, 4.91879727]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7087244291854052}
episode index:2799
target Thresh 19.0
target distance 3.0
model initialize at round 2799
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.67949644, 7.04952502, 1.03947943]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 3.027708372857531}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8158751756708692
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.63779181, 10.21394536,  2.75629412]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.42067491981363603}
episode index:2800
target Thresh 19.0
target distance 2.0
model initialize at round 2800
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.        , 8.00000006, 2.54006976]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.236068001032567}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8159338064542785
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.22778207, 6.84903679, 0.25688445]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.7868357063278817}
episode index:2801
target Thresh 19.0
target distance 2.0
model initialize at round 2801
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.77358539, 1.84751441, 4.90592384]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7884708081453652}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.815999497458399
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.77358539, 1.84751441, 4.90592384]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7884708081453652}
episode index:2802
target Thresh 19.0
target distance 6.0
model initialize at round 2802
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([4.13554443, 3.67750284, 2.50017017]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 4.821258307826724}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8160342879507375
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.55715689, 7.28950325, 1.65061425]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9029005608947586}
episode index:2803
target Thresh 19.0
target distance 10.0
model initialize at round 2803
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([16.        ,  5.        ,  5.78114486]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 11.661903789690601}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8160234619353511
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.86403043, 11.7559434 ,  3.5156624 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.7680743125083981}
episode index:2804
target Thresh 19.0
target distance 14.0
model initialize at round 2804
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([16.00000166,  9.99999961,  0.78218478]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 14.035670532358832}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8160126436390488
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.90966916, 10.7358162 ,  4.79988763]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9472544803570738}
episode index:2805
target Thresh 19.0
target distance 12.0
model initialize at round 2805
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 4.        , 10.        ,  4.49347711]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.36931687716161}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.815996261066763
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.7814526 ,  6.96213464,  6.22799465]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.22180341173887202}
episode index:2806
target Thresh 19.0
target distance 2.0
model initialize at round 2806
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.0520149 ,  9.60965762,  5.12395811]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.1271016694876699}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8160618128084564
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.0520149 ,  9.60965762,  5.12395811]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.1271016694876699}
episode index:2807
target Thresh 19.0
target distance 2.0
model initialize at round 2807
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.57464278, 10.62833023,  2.83630955]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.5648603216775366}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.816127317860875
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.57464278, 10.62833023,  2.83630955]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.5648603216775366}
episode index:2808
target Thresh 19.0
target distance 5.0
model initialize at round 2808
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 7.        , 11.        ,  0.96153849]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.999999999999999}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8161652734951104
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.1287891 , 10.15351592,  4.39516788]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8562253926820783}
episode index:2809
target Thresh 19.0
target distance 1.0
model initialize at round 2809
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.        , 7.        , 6.01629162]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.0}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8162236132554325
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.70671068, 8.77460318, 3.73310631]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8282685027387074}
episode index:2810
target Thresh 19.0
target distance 3.0
model initialize at round 2810
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([14.        , 11.        ,  1.91879576]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.6055512754649173}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8162681726777538
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.89679226,  8.34387921,  1.35242515]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.3590330742491578}
episode index:2811
target Thresh 19.0
target distance 2.0
model initialize at round 2811
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([13.99778054,  6.98773163,  5.54341555]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.0022570460527525}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8163299549776549
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.62831478,  7.40447529,  1.26023024]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5493179096065676}
episode index:2812
target Thresh 19.0
target distance 6.0
model initialize at round 2812
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.52586053,  3.81197347,  3.64812636]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.209647541631204}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.81636450431733
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.27729532,  8.7401241 ,  2.79857044]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7680088182438138}
episode index:2813
target Thresh 19.0
target distance 12.0
model initialize at round 2813
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.        , 9.        , 5.62018418]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 12.369316876851622}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.816334631845879
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.86909017,  5.7651569 ,  4.78833111]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.26886552018972604}
episode index:2814
target Thresh 19.0
target distance 2.0
model initialize at round 2814
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.00033414,  7.9999654 ,  0.90682047]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 2.000034626703357}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8163963246942463
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.46131393,  9.59428196,  2.90682047]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6743810567091503}
episode index:2815
target Thresh 19.0
target distance 5.0
model initialize at round 2815
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([16.        ,  6.        ,  6.25062561]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 5.830951894845301}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8164243580499154
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.82007523, 11.27342449,  3.11788438]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.3273131161367527}
episode index:2816
target Thresh 19.0
target distance 14.0
model initialize at round 2816
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.56850201, 11.70670072,  3.61455309]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.85665087258988}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8164162607569065
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.88256953, 8.06048884, 5.63225594]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.2890346019121939}
episode index:2817
target Thresh 19.0
target distance 4.0
model initialize at round 2817
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 4.        , 11.        ,  3.21731925]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 4.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8164640158275749
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.89993259, 7.60668304, 0.65094864]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0853308130601087}
episode index:2818
target Thresh 19.0
target distance 13.0
model initialize at round 2818
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([15.71850293,  3.52188752,  2.13970613]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.940529544429841}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8164140921075439
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.2922382 , 5.9990861 , 4.74148245]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.29223962561282396}
episode index:2819
target Thresh 19.0
target distance 9.0
model initialize at round 2819
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.       ,  2.       ,  6.2769208]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8164326494552961
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.61182492, 10.53061169,  2.86099426]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6091020286355522}
episode index:2820
target Thresh 19.0
target distance 1.0
model initialize at round 2820
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([15.        ,  2.        ,  0.26221102]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.4142135623730951}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8164906669492858
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.84647091,  3.38738639,  4.26221102]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.4167006079421702}
episode index:2821
target Thresh 19.0
target distance 7.0
model initialize at round 2821
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 8.84074434, 10.13517424,  5.67488408]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 8.810502605543675}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8164940964166446
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.30304096,  5.13895185,  5.97577224]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.3333788224332888}
episode index:2822
target Thresh 19.0
target distance 10.0
model initialize at round 2822
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.        , 10.        ,  0.16061848]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8165004795781867
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.04815563, 9.49447225, 4.74469194]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.5078161801124533}
episode index:2823
target Thresh 19.0
target distance 6.0
model initialize at round 2823
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([2.        , 5.        , 4.59648371]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.48528137423857}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8165189800502797
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.3031991 , 10.73799932,  1.18055717]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7444298820241768}
episode index:2824
target Thresh 19.0
target distance 12.0
model initialize at round 2824
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 2.99999998, 10.00000004,  2.96314281]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 12.0000000174608}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8165165761168889
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.27267276, 10.76093571,  0.98084566]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0526291225925888}
episode index:2825
target Thresh 19.0
target distance 1.0
model initialize at round 2825
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.77660589,  2.52860335,  3.74374104]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.3110713007810548}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8165744612633444
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.2912865 ,  3.26131594,  1.46055573]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.3913231943271171}
episode index:2826
target Thresh 19.0
target distance 14.0
model initialize at round 2826
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([2.        , 7.        , 5.60779715]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 14.035668847618197}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8165499114342212
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.12170066,  8.65473806,  0.77594407]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0954869545335146}
episode index:2827
target Thresh 19.0
target distance 12.0
model initialize at round 2827
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 5.47557135, 10.19063156,  0.5083    ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 10.526154987168306}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8165533126762301
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.65489258, 10.76537245,  0.80918816]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8395797285455293}
episode index:2828
target Thresh 19.0
target distance 11.0
model initialize at round 2828
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 4.68294794, 10.99138023,  1.00487822]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 12.948064624121997}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8165158435756985
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.06560238,  2.13670648,  6.17302515]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.15163223903013207}
episode index:2829
target Thresh 19.0
target distance 9.0
model initialize at round 2829
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([12.49738506, 10.24302122,  4.61825514]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 8.95632235901363}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8165374001762015
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.00682048,  2.90509943,  5.48551391]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.3437300801260212}
episode index:2830
target Thresh 19.0
target distance 13.0
model initialize at round 2830
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([15.        ,  2.        ,  4.69668674]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 15.811388300836246}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8164999631670513
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.73619434, 11.71937492,  3.58164837]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.0293116083904996}
episode index:2831
target Thresh 19.0
target distance 13.0
model initialize at round 2831
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.43972986,  8.36917103,  4.53580689]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.682482217783155}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8164781479566122
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.03715902, 6.20223877, 3.98713913]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.20562420627555703}
episode index:2832
target Thresh 19.0
target distance 10.0
model initialize at round 2832
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.        , 3.        , 5.66930699]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.440306508916388}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8164237193978502
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.86178181,  6.51653782,  5.987898  ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.5347107516269514}
episode index:2833
target Thresh 19.0
target distance 9.0
model initialize at round 2833
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.        , 11.        ,  2.52346557]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8164421816749747
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.82173098,  2.78106073,  5.39072435]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.801146499751412}
episode index:2834
target Thresh 19.0
target distance 8.0
model initialize at round 2834
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.68239017,  2.0436596 ,  1.03594511]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 7.985550005494663}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8164606309275652
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.65023061, 10.15486367,  3.90320388]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.38251977744904464}
episode index:2835
target Thresh 19.0
target distance 10.0
model initialize at round 2835
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([6.00068614, 9.90614075, 0.15907639]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 9.45691831839932}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8164554017169203
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.75303077,  7.47169224,  0.17677924]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.5324353226640376}
episode index:2836
target Thresh 19.0
target distance 10.0
model initialize at round 2836
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.        , 6.        , 5.49616623]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8164231581265928
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.54707409,  6.91308893,  0.38112785]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0644348073479029}
episode index:2837
target Thresh 19.0
target distance 10.0
model initialize at round 2837
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 6.       , 11.       ,  4.8885982]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8164325027478957
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.07217079, 11.44611044,  1.18948636]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.4519105493769368}
episode index:2838
target Thresh 19.0
target distance 2.0
model initialize at round 2838
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.97408615, 7.99488235, 4.34657073]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.2292017892084255}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8164867001755999
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.12683218, 9.60303212, 4.06338542]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.4167372071019434}
episode index:2839
target Thresh 19.0
target distance 3.0
model initialize at round 2839
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([1.89140973, 9.73374801, 5.00328183]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.729841587323271}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8165374428903267
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.8769726 , 8.32159922, 0.43691122]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9340808342056853}
episode index:2840
target Thresh 19.0
target distance 1.0
model initialize at round 2840
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([1.99999958, 9.00000075, 3.09272182]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.000000422954753}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8165950150681197
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.64900974, 8.66493407, 0.80953651]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.730399081709331}
episode index:2841
target Thresh 19.0
target distance 3.0
model initialize at round 2841
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([4.        , 6.        , 0.72066086]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 3.6055512755840224}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8166423039614455
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.34232916, 3.73268665, 4.43747555]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8087144046242923}
episode index:2842
target Thresh 19.0
target distance 2.0
model initialize at round 2842
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.73029528,  3.5719734 ,  4.47673583]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.020705434912796}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.816703280991357
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.88281478,  2.3455612 ,  0.19355052]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.36489028327909645}
episode index:2843
target Thresh 19.0
target distance 2.0
model initialize at round 2843
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.00001355,  8.99999872,  0.91604822]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.236075181907604}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8167642151400942
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.44630455, 10.58927353,  2.91604822]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6065344011073106}
episode index:2844
target Thresh 19.0
target distance 5.0
model initialize at round 2844
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.        , 8.        , 3.69562805]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.830951894845377}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8167824863519144
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.7931328 , 10.69326825,  0.27970151]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.3699708128020873}
episode index:2845
target Thresh 19.0
target distance 1.0
model initialize at round 2845
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 4.        , 10.        ,  1.34132117]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.4142135623745837}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8168398712829223
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.76929868, 9.07339744, 5.34132117]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.24209560414590453}
episode index:2846
target Thresh 19.0
target distance 14.0
model initialize at round 2846
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([3.58051615, 7.57823593, 1.36072701]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.614708783137564}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8167951256610698
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.84078708,  2.99339935,  6.24568863]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.0060770462550863}
episode index:2847
target Thresh 19.0
target distance 13.0
model initialize at round 2847
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.3361736 ,  7.25312012,  4.00061893]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.405224324707257}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8167706793719794
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.69877103, 5.54049296, 5.45195117]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8363179212520372}
episode index:2848
target Thresh 19.0
target distance 8.0
model initialize at round 2848
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 8.40853889, 10.45260653,  4.48287344]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 9.094256521903198}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8167739780537849
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.76981411, 3.95961552, 4.7837616 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.23370162233267092}
episode index:2849
target Thresh 19.0
target distance 8.0
model initialize at round 2849
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([3.        , 3.        , 0.24616164]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 11.31370849898476}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.816752204478048
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.31242383, 10.14453437,  5.98067918]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0975347096300085}
episode index:2850
target Thresh 19.0
target distance 11.0
model initialize at round 2850
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.33319889,  6.76727981,  4.29031658]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.248147237183593}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8167469005092864
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.67201672, 10.69783815,  4.30801943]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.44595382503402753}
episode index:2851
target Thresh 19.0
target distance 9.0
model initialize at round 2851
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([5.88759521, 9.83650545, 0.08616179]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 8.114052127883095}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8167620868536525
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.10018063, 10.86026343,  4.95342056]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.244880740560957}
episode index:2852
target Thresh 19.0
target distance 2.0
model initialize at round 2852
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.01549343, 5.635033  , 5.09753156]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.1977035642245926}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8168159028063852
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.68898969, 4.59966159, 4.81434625]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.506949955385121}
episode index:2853
target Thresh 19.0
target distance 2.0
model initialize at round 2853
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([1.61633542, 5.63865439, 2.81078738]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.430069344444582}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8168696810464671
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.18592973, 6.54387282, 2.52760207]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.5747760471253626}
episode index:2854
target Thresh 19.0
target distance 12.0
model initialize at round 2854
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.        ,  9.        ,  0.67635458]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.041594578792658}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8168671795358453
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.83063407, 8.4244864 , 4.97724274]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9328138424892353}
episode index:2855
target Thresh 19.0
target distance 2.0
model initialize at round 2855
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.43363859,  6.62614441,  2.32019389]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.572547316814057}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8169313016718621
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.43363859,  6.62614441,  2.32019389]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.572547316814057}
episode index:2856
target Thresh 19.0
target distance 11.0
model initialize at round 2856
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([15.62153703,  3.56399482,  2.20253031]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 12.830053588899345}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.816904269495354
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.10602947, 8.33280236, 5.65386254]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6755700867514592}
episode index:2857
target Thresh 19.0
target distance 7.0
model initialize at round 2857
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.68288215,  3.01655758,  1.01983851]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 7.016751100713491}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8169348775448689
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.5916074 ,  9.41497143,  2.17028259]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8320202744663171}
episode index:2858
target Thresh 19.0
target distance 14.0
model initialize at round 2858
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([2.        , 6.        , 5.63114047]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8169104764314681
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.14217825,  8.35158213,  0.7992874 ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9270750512295152}
episode index:2859
target Thresh 19.0
target distance 6.0
model initialize at round 2859
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.        , 8.        , 5.62098122]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.70820393249937}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8169225559051265
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.42425672, 10.3124421 ,  6.20505468]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8967810142929361}
episode index:2860
target Thresh 19.0
target distance 12.0
model initialize at round 2860
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([12.89796141,  8.9728151 ,  3.6709224 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.08490151000303}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.816925787666025
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.9717571 , 10.26009804,  3.97181056]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.2213790436222975}
episode index:2861
target Thresh 19.0
target distance 13.0
model initialize at round 2861
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([2.       , 9.       , 3.2480222]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8168836798499466
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.97159499,  3.731211  ,  6.13298382]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.73176251021464}
episode index:2862
target Thresh 19.0
target distance 10.0
model initialize at round 2862
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([2.81215359, 3.19222858, 3.3643533 ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.189497712362185}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.8168118089312191
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.89003266,  3.94566655,  0.83338838]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.298631344697957}
episode index:2863
target Thresh 19.0
target distance 2.0
model initialize at round 2863
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.98003502, 6.63217382, 0.06091183]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0842622676649936}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.816865400827542
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.37482575, 6.50545641, 6.06091183]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6205382399282435}
episode index:2864
target Thresh 19.0
target distance 4.0
model initialize at round 2864
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([15.        ,  7.        ,  3.41569793]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.123105625617717}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8169155685794346
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.55573759,  3.99699992,  5.13251263]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0915026042805316}
episode index:2865
target Thresh 19.0
target distance 11.0
model initialize at round 2865
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([15.        ,  5.        ,  6.25525999]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.18033988749894}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8168710931842111
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.50050669, 3.78306723, 4.8570363 ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9288099107701051}
episode index:2866
target Thresh 19.0
target distance 11.0
model initialize at round 2866
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([2.03191441, 7.37585836, 3.1939497 ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.7429670382783}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.8168080706494639
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.30795297,  2.72534661,  5.2293554 ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7445559649616663}
episode index:2867
target Thresh 19.0
target distance 12.0
model initialize at round 2867
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 3.        , 11.        ,  3.99914944]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 14.422205101855956}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8167518820685451
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.15300521,  3.13225523,  0.03455514]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.20224252767719786}
episode index:2868
target Thresh 19.0
target distance 14.0
model initialize at round 2868
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([15.99938989, 10.00092007,  3.16633487]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 16.124442292594846}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8167099376056595
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.07897613, 2.21633264, 6.05129649]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.230297721258611}
episode index:2869
target Thresh 19.0
target distance 10.0
model initialize at round 2869
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.44936993,  3.59034418,  2.91411155]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.458245605972062}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.8166319782010587
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.77285297, 4.15476882, 0.09996133]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7881973788646944}
episode index:2870
target Thresh 19.0
target distance 6.0
model initialize at round 2870
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([2.        , 9.        , 3.63540506]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8166593924386266
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.31460946, 10.59887789,  0.50266383]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7941405070913706}
episode index:2871
target Thresh 19.0
target distance 12.0
model initialize at round 2871
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.18076192,  8.07335254,  4.83482862]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.181002539661366}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8166541595685363
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.51377016, 8.55548451, 4.85253146]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7566523750177823}
episode index:2872
target Thresh 19.0
target distance 6.0
model initialize at round 2872
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([15.10271024,  7.32016706,  5.78345609]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.458678430863802}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8166878745312633
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.20001829,  3.0132196 ,  4.93390017]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.800090933343461}
episode index:2873
target Thresh 19.0
target distance 2.0
model initialize at round 2873
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.        , 7.        , 5.15889239]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8167447333083924
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.10580217, 8.39230548, 2.87570709]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6168360605832472}
episode index:2874
target Thresh 19.0
target distance 6.0
model initialize at round 2874
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([2.       , 9.       , 3.5481925]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8167720701852447
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.31790663, 10.74138686,  0.41545127]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7294738635746663}
episode index:2875
target Thresh 19.0
target distance 3.0
model initialize at round 2875
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.42537909,  6.10519134,  0.44941204]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 2.147737866824155}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8168254522887964
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.0368604 ,  4.38514684,  0.16622674]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.38690667364383113}
episode index:2876
target Thresh 19.0
target distance 9.0
model initialize at round 2876
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 9.35402369, 10.64907351,  4.36165023]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.352891132879702}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8168091914940506
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.48620309, 1.54256852, 6.09616777]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.6675604884800376}
episode index:2877
target Thresh 19.0
target distance 13.0
model initialize at round 2877
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 2.87526508, 10.67834123,  2.6549803 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 14.726130123450584}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8167747652748373
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.62256578,  4.76290516,  5.82312723]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8511644258600694}
episode index:2878
target Thresh 19.0
target distance 14.0
model initialize at round 2878
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([15.7322037 ,  8.66152738,  2.74059683]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 14.211973864381864}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8167479940376415
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.7657603 , 5.35111157, 6.19192907]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8424180528800623}
episode index:2879
target Thresh 19.0
target distance 7.0
model initialize at round 2879
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([16.51692526,  7.3983832 ,  6.03458643]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.956295320936042}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8167815944728657
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.79489091,  2.84712429,  5.18503051]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.161667392551674}
episode index:2880
target Thresh 19.0
target distance 4.0
model initialize at round 2880
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.64813791, 8.55315979, 2.18546331]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.472010101535032}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8168315127010944
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.09179993, 10.2739343 ,  3.902278  ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.162754817250861}
episode index:2881
target Thresh 19.0
target distance 7.0
model initialize at round 2881
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([4.       , 2.       , 5.0285008]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 7.280109889280519}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8168556464134522
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.14853507, 8.17427482, 3.89575957]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.1860921559827247}
episode index:2882
target Thresh 19.0
target distance 10.0
model initialize at round 2882
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.        , 11.        ,  0.22576254]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.198039027264247}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8168617713232811
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.09693707, 9.15848981, 4.80983601]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.1857843291583855}
episode index:2883
target Thresh 19.0
target distance 8.0
model initialize at round 2883
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.99998235, 2.99999405, 4.47660708]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.062265838598003}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8168767493341402
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.3958541 , 11.354426  ,  3.06068055]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7004356242148728}
episode index:2884
target Thresh 19.0
target distance 1.0
model initialize at round 2884
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.55013908, 6.52728935, 3.2871207 ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.5249760861580095}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8169333258508357
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.23304564, 6.51708305, 1.00393539]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5362080290583255}
episode index:2885
target Thresh 19.0
target distance 12.0
model initialize at round 2885
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 2.75263973, 11.62273997,  2.73206556]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 12.771925403958795}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.816914410185034
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.81282539,  7.51125884,  0.18339779]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5233567188870765}
episode index:2886
target Thresh 19.0
target distance 4.0
model initialize at round 2886
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 7.69251796, 11.75740662,  3.4837333 ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.797019444707978}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8169641786643601
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.48926081, 10.11615571,  5.20054799]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.010226151942429}
episode index:2887
target Thresh 19.0
target distance 14.0
model initialize at round 2887
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([3.49760902, 6.23217496, 0.53624361]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.626756729104889}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8169400124301729
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.280833  ,  8.64099445,  1.98757585]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9633665202006381}
episode index:2888
target Thresh 19.0
target distance 1.0
model initialize at round 2888
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.        ,  8.        ,  3.56951845]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.0000000001371472}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8169999155065211
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.74416401,  6.33951015,  5.56951845]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7083070655930437}
episode index:2889
target Thresh 19.0
target distance 11.0
model initialize at round 2889
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 3.        , 10.        ,  4.63051152]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 11.000000000144997}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8170002293549954
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.0906729 , 10.70400124,  0.64821437]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7098163992674286}
episode index:2890
target Thresh 19.0
target distance 8.0
model initialize at round 2890
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.        ,  6.        ,  5.25716162]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8170062873045267
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.78030419, 11.73578183,  3.55804977]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0724968703393278}
episode index:2891
target Thresh 19.0
target distance 1.0
model initialize at round 2891
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([3.00001257, 3.00000679, 1.50533074]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.4142176493857654}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8170661053241309
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.65650221, 4.01344718, 3.50533074]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.3437608992515693}
episode index:2892
target Thresh 19.0
target distance 7.0
model initialize at round 2892
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 9.55742818, 10.09422359,  5.41072774]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 7.2507988493367455}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8170809661085474
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.45293668, 7.308127  , 3.9948012 ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.6278698357610742}
episode index:2893
target Thresh 19.0
target distance 11.0
model initialize at round 2893
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.       , 3.       , 5.2953403]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.60147050873544}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8170466363112046
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.06932147, 11.03952909,  2.18030192]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.0797998505159015}
episode index:2894
target Thresh 19.0
target distance 4.0
model initialize at round 2894
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([16.        ,  7.        ,  2.35525069]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.472135954999579}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8170929034661576
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.27407925,  3.85292295,  6.07206538]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8958777746031634}
episode index:2895
target Thresh 19.0
target distance 7.0
model initialize at round 2895
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 6.        , 11.        ,  2.09051402]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8171077396026131
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.10980816, 3.45639069, 0.67458748]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.5545889626425917}
episode index:2896
target Thresh 19.0
target distance 1.0
model initialize at round 2896
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.        ,  3.        ,  1.73573988]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.0000000000361347}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8171674193611209
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.53060963,  3.64889368,  3.73573988]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8008684857882635}
episode index:2897
target Thresh 19.0
target distance 2.0
model initialize at round 2897
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.        ,  5.        ,  3.78754151]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.2360679776811376}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8172270579327698
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.10957219,  3.3206007 ,  5.78754151]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.94638601294616}
episode index:2898
target Thresh 19.0
target distance 12.0
model initialize at round 2898
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.31771619,  8.04797749,  4.12308121]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.31782773533916}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8172189127881375
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.86677881, 7.99506643, 6.14078406]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8667928507910719}
episode index:2899
target Thresh 19.0
target distance 13.0
model initialize at round 2899
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([14.11661089,  8.56751553,  5.16979265]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 12.358367214342724}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8172135375042585
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.73806558, 10.07889397,  5.1874955 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.1803292429592445}
episode index:2900
target Thresh 19.0
target distance 13.0
model initialize at round 2900
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([2.08103701, 6.68101787, 2.53262651]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 12.98612022775136}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.817184268228888
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.45062552,  8.03328054,  1.70077344]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.4518528051558633}
episode index:2901
target Thresh 19.0
target distance 12.0
model initialize at round 2901
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 2.        , 10.        ,  4.72401381]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 12.165525060953495}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.817168023872436
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.30704438,  8.10080665,  0.17534604]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.3231690482599573}
episode index:2902
target Thresh 19.0
target distance 8.0
model initialize at round 2902
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.        ,  3.        ,  6.28007889]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.81717399898011
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.61984554, 10.97382361,  4.58096704]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.381054613801786}
episode index:2903
target Thresh 19.0
target distance 8.0
model initialize at round 2903
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([15.        ,  3.        ,  5.98677468]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 8.94427190999916}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8171887663202138
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.52168691, 10.06802481,  4.57084815]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0680519600921552}
episode index:2904
target Thresh 19.0
target distance 12.0
model initialize at round 2904
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.5865212 ,  7.08650902,  4.72534227]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.347862666654375}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8171570250360959
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.56681243, 3.43534806, 5.8934892 ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.7147057200605659}
episode index:2905
target Thresh 19.0
target distance 5.0
model initialize at round 2905
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([13.85472543,  5.32331178,  5.63596106]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 3.5151180706300273}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8171998065654712
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.02245031,  2.51896927,  5.06959044]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.1067666867191024}
episode index:2906
target Thresh 19.0
target distance 4.0
model initialize at round 2906
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.        , 11.        ,  3.94613671]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8172393199955853
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.52346501,  6.78772169,  5.3797661 ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.5216777490995056}
episode index:2907
target Thresh 19.0
target distance 8.0
model initialize at round 2907
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 9.        , 11.        ,  4.24000692]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.817231195842782
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.14410147,  2.76374232,  6.25770977]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.276736195727591}
episode index:2908
target Thresh 19.0
target distance 9.0
model initialize at round 2908
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([15.        ,  2.        ,  0.21466845]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8172489045457474
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.79374977, 10.23480749,  3.08192722]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.792501571260871}
episode index:2909
target Thresh 19.0
target distance 1.0
model initialize at round 2909
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 3.        , 10.        ,  3.33976889]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0000000000066027}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8173082691833604
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.3716661 , 8.43874352, 5.33976889]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8425036049629767}
episode index:2910
target Thresh 19.0
target distance 12.0
model initialize at round 2910
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([15.30267122,  6.06552519,  1.69559306]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.65131884995294}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.817266738763871
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.59514963, 3.38513421, 4.58055468]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.5587774003586284}
episode index:2911
target Thresh 19.0
target distance 4.0
model initialize at round 2911
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.90048475, 6.42395774, 0.60451382]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.706282666671111}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8173061613631647
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.60930908, 11.13504221,  2.03814321]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.41337125581796635}
episode index:2912
target Thresh 19.0
target distance 1.0
model initialize at round 2912
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([2.31968921, 9.94263604, 4.1857183 ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.2573151215155147}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8173620466493429
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.37551869, 11.25293457,  1.90253299]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.4527584177042824}
episode index:2913
target Thresh 19.0
target distance 1.0
model initialize at round 2913
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.        , 3.        , 4.71325874]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.4142135648094132}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8174178935791133
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.4897896 , 4.50858225, 2.43007344]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7203961120887349}
episode index:2914
target Thresh 19.0
target distance 2.0
model initialize at round 2914
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.80099297, 10.13316686,  1.74385851]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.1802497266222176}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8174805289500982
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.80099297, 10.13316686,  1.74385851]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.1802497266222176}
episode index:2915
target Thresh 19.0
target distance 7.0
model initialize at round 2915
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.19025386,  8.52463589,  5.220433  ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 5.527910835375753}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8175041586972743
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.81249413,  3.21803206,  4.08769177]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.2875698709789466}
episode index:2916
target Thresh 19.0
target distance 2.0
model initialize at round 2916
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([2.76950002, 7.45520062, 2.09438046]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.905712215430757}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8175598994724896
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.38121141, 5.88733694, 6.09438046]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.628961272246439}
episode index:2917
target Thresh 19.0
target distance 7.0
model initialize at round 2917
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([12.        , 11.        ,  3.89258671]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8175715162893582
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.74828473,  3.84929717,  4.47666017]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.2933801599304462}
episode index:2918
target Thresh 19.0
target distance 5.0
model initialize at round 2918
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.57872685,  9.58032983,  2.22975734]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.756531653798313}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8176107399384221
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.85675701, 11.00219857,  3.66338672]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.8567598304395667}
episode index:2919
target Thresh 19.0
target distance 8.0
model initialize at round 2919
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.31628647, 10.62098709,  0.13463038]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.292058222091354}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8176165286444194
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.12518952, 3.54958712, 4.71870384]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5636651681609446}
episode index:2920
target Thresh 19.0
target distance 8.0
model initialize at round 2920
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([10.        , 11.        ,  2.53861004]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 10.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8176166280654918
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.47505534,  3.96031899,  4.8394982 ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.0944311090037329}
episode index:2921
target Thresh 19.0
target distance 7.0
model initialize at round 2921
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([16.        , 10.        ,  3.45192277]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 7.280109889280517}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8176432268424232
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.74617641,  3.34720586,  0.31918155]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.823001303606755}
episode index:2922
target Thresh 19.0
target distance 9.0
model initialize at round 2922
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([1.13352744, 6.03380755, 3.50216103]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 11.045829447103769}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.817640518885317
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.82856066, 11.12321432,  1.51986388]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.21112369743418405}
episode index:2923
target Thresh 19.0
target distance 4.0
model initialize at round 2923
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.        ,  6.        ,  1.52622431]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 4.0}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8176828717001308
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.56833228,  2.77255763,  0.9598537 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9590864756408782}
episode index:2924
target Thresh 19.0
target distance 11.0
model initialize at round 2924
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([15.        ,  7.        ,  1.60862177]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8176562110169472
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.24300797, 5.88544486, 5.05995389]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7656107472828221}
episode index:2925
target Thresh 19.0
target distance 14.0
model initialize at round 2925
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.36089529,  9.61826209,  4.38040805]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.635150522469093}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8176425969186499
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.17642728, 7.91882904, 4.11492559]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.935613910112992}
episode index:2926
target Thresh 19.0
target distance 13.0
model initialize at round 2926
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.31773316,  3.95144556,  4.1804471 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 13.333161802643543}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8176060101849135
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.2860622 , 11.06040817,  3.06540872]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.29237087354216823}
episode index:2927
target Thresh 19.0
target distance 3.0
model initialize at round 2927
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.01948808, 3.68285718, 2.56921649]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.3172869789663086}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.817658159430069
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.50811415, 5.29321712, 2.28603118]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.5866483373802658}
episode index:2928
target Thresh 19.0
target distance 5.0
model initialize at round 2928
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.       , 11.       ,  1.3318054]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.000000000000201}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8176940373867088
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.73871512, 10.7792479 ,  4.76543479]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.34205449604014887}
episode index:2929
target Thresh 19.0
target distance 12.0
model initialize at round 2929
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([2.        , 5.        , 0.01852768]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 13.416407864998735}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8176623944851972
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.9646516 , 11.35751008,  1.18667461]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.3592533480404403}
episode index:2930
target Thresh 19.0
target distance 6.0
model initialize at round 2930
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.        , 9.        , 3.69327831]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 6.000000000000078}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8176919815478119
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.62004217, 2.98587361, 4.84372239]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.3802203415878006}
episode index:2931
target Thresh 19.0
target distance 10.0
model initialize at round 2931
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.        ,  3.        ,  0.10828989]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 12.806248474865697}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8176653814086032
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.37946735, 11.72037159,  3.55962213]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8142055629455185}
episode index:2932
target Thresh 19.0
target distance 10.0
model initialize at round 2932
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.86367938, 3.85212764, 0.97206801]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.385398769891772}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8176216663402297
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.95218529,  5.73792563,  5.85702963]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.26640048854303383}
episode index:2933
target Thresh 19.0
target distance 5.0
model initialize at round 2933
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.00061915,  7.35319919,  1.94408339]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.021864872471759}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8176420819355463
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.63509881, 11.85263413,  2.81134216]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.06317235675037}
episode index:2934
target Thresh 19.0
target distance 1.0
model initialize at round 2934
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.86503315, 9.69156748, 5.30990338]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9183751833413714}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8177042141052446
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.86503315, 9.69156748, 5.30990338]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9183751833413714}
episode index:2935
target Thresh 19.0
target distance 9.0
model initialize at round 2935
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.        , 11.        ,  1.14813155]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 9.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8177186399024299
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.29933124,  2.4655487 ,  6.01539032]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.5534751847144989}
episode index:2936
target Thresh 19.0
target distance 1.0
model initialize at round 2936
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([3.00238183, 2.0020321 , 1.71632975]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.4144643030417106}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8177772988605836
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.47649624, 2.7115786 , 3.71632975]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.5976981561768471}
episode index:2937
target Thresh 19.0
target distance 6.0
model initialize at round 2937
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.04939778,  9.31775509,  5.75174475]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 4.318037649229031}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8178098856368337
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.69984329,  4.95750577,  4.90218883]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.3031498149661505}
episode index:2938
target Thresh 19.0
target distance 6.0
model initialize at round 2938
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.88412674,  4.51385574,  1.32419175]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 5.556928910269441}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8178393419789134
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.31354586,  9.77728335,  2.47463582]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.3845955125216254}
episode index:2939
target Thresh 19.0
target distance 10.0
model initialize at round 2939
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([1.40000000e+01, 8.00000000e+00, 5.04558882e-03]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.770329614269007}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8178102481107757
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.22610203, 3.68955507, 5.45637782]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.8338429795801525}
episode index:2940
target Thresh 19.0
target distance 7.0
model initialize at round 2940
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([10.        , 11.        ,  2.14986689]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 7.615773105863908}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8178275672419073
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.35656775, 7.90646897, 5.01712567]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6501946705192377}
episode index:2941
target Thresh 19.0
target distance 12.0
model initialize at round 2941
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([2.86742503, 8.24485421, 3.31900239]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 13.13485740356642}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8178087076726025
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.95660663,  8.32625454,  0.77033463]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.3291276463097383}
episode index:2942
target Thresh 19.0
target distance 12.0
model initialize at round 2942
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.        ,  5.        ,  5.30750322]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.165525060581338}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8177796538710332
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.10830047, 7.66477509, 4.47565015]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.6735390972474782}
episode index:2943
target Thresh 19.0
target distance 6.0
model initialize at round 2943
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([4.        , 5.        , 4.47810006]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 6.000000000002172}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.817812173434081
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.06277379, 10.01117145,  3.62854414]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.362415085954852}
episode index:2944
target Thresh 19.0
target distance 7.0
model initialize at round 2944
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([16.       ,  3.       ,  5.5205462]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8178384980795222
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.5306154 ,  9.22807438,  2.38780497]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.9367079905399517}
episode index:2945
target Thresh 19.0
target distance 12.0
model initialize at round 2945
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([5.38053481, 9.58959051, 1.29026287]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 10.737776252980815}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8178249145293932
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.49244024,  8.06896189,  1.02478042]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.4972455413216272}
episode index:2946
target Thresh 19.0
target distance 9.0
model initialize at round 2946
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([10.6979488 , 10.11620907,  5.53888083]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 9.694565312046052}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8178277457846719
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.26039715,  2.42010292,  5.83976898]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.4942601974981293}
episode index:2947
target Thresh 19.0
target distance 8.0
model initialize at round 2947
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.        , 10.        ,  3.65278721]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8178450178562404
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.65364965,  1.64725538,  0.23686067]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.4943554689800023}
episode index:2948
target Thresh 19.0
target distance 3.0
model initialize at round 2948
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.        , 8.        , 3.47564852]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.0000000001728613}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8178934244320775
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.27579732, 4.92791308, 5.19246322]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.7277815901902587}
episode index:2949
target Thresh 19.0
target distance 13.0
model initialize at round 2949
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.40508464, 2.07450871, 0.42755192]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 11.595154752548442}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.8178151679430963
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.3967755 ,  1.58339311,  5.89658701]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5753191217357612}
episode index:2950
target Thresh 19.0
target distance 13.0
model initialize at round 2950
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([1.39422035, 2.503845  , 3.84755147]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 15.605656632100622}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8177579966318206
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.9048172 ,  8.53480504,  1.88295716]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5432091625041863}
episode index:2951
target Thresh 19.0
target distance 12.0
model initialize at round 2951
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([15.        , 10.        ,  6.25699997]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 14.42220510184658}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8177192736299127
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.5773272 , 1.55718922, 0.85877628]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7275905963790987}
episode index:2952
target Thresh 19.0
target distance 5.0
model initialize at round 2952
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.        , 6.        , 4.80459642]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 5.0}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8177517145286102
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.99909954, 10.95621034,  3.9550405 ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.04379891427057852}
episode index:2953
target Thresh 19.0
target distance 14.0
model initialize at round 2953
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([15.99999857,  3.99999696,  5.28208566]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 14.142134637148574}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8177082810050287
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.64202405, 6.60771912, 3.88386197]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.7053150401885311}
episode index:2954
target Thresh 19.0
target distance 12.0
model initialize at round 2954
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.13190502,  3.44094376,  3.12396228]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.422262390249758}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8176602338876193
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.30398444, 5.8983662 , 5.7257386 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.3205245228277377}
episode index:2955
target Thresh 19.0
target distance 8.0
model initialize at round 2955
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.00000024,  2.99999991,  0.65932482]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 8.246211277639413}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8176804844928668
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.04941362, 10.41465646,  1.52658359]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.5874255407690477}
episode index:2956
target Thresh 19.0
target distance 11.0
model initialize at round 2956
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.22992855, 9.85123377, 0.25870913]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 12.533801015371695}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8176371191230245
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.24977334,  2.04751976,  5.14367075]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7517301149801384}
episode index:2957
target Thresh 19.0
target distance 6.0
model initialize at round 2957
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 9.40103057, 10.12648896,  5.97905517]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 7.446611328306229}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8176485527443808
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.58053004, 5.02665405, 0.27994332]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.5811416091460969}
episode index:2958
target Thresh 19.0
target distance 12.0
model initialize at round 2958
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([3.6566849 , 6.70371764, 0.8330304 ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.347557709133484}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8176197101681422
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([1.40349180e+01, 7.94842409e+00, 1.17732690e-03]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.9490666551256827}
episode index:2959
target Thresh 19.0
target distance 11.0
model initialize at round 2959
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([4.0000019 , 9.99999473, 6.06870127]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.99999809996252}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8176198072044358
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.76287076, 10.20888602,  2.08640412]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.3160120961107482}
episode index:2960
target Thresh 19.0
target distance 7.0
model initialize at round 2960
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 9.        , 11.        ,  2.81068611]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8176199041751865
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.262592  ,  3.63448603,  5.11157427]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8230255346260917}
episode index:2961
target Thresh 19.0
target distance 12.0
model initialize at round 2961
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.39972865,  2.52107549,  3.83680284]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.966187653877208}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8175674606021982
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.6272758 , 5.74923455, 4.15539385]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.44922893900790994}
episode index:2962
target Thresh 19.0
target distance 5.0
model initialize at round 2962
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.        ,  3.        ,  5.88363361]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8176029574749035
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.95451241,  7.85024361,  3.03407769]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.15651229317640694}
episode index:2963
target Thresh 19.0
target distance 14.0
model initialize at round 2963
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.81860373,  6.80144127,  4.94420195]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 13.211638030972555}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8175895358831101
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.56216906, 9.52017028, 4.67871949]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.7391012234738348}
episode index:2964
target Thresh 19.0
target distance 10.0
model initialize at round 2964
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.81851069, 8.01529755, 1.64862698]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.181502056779815}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8175632663510713
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.04878123,  8.73582892,  5.09995921]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.2026060677577615}
episode index:2965
target Thresh 19.0
target distance 3.0
model initialize at round 2965
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([11.        , 11.        ,  0.73856705]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 3.0000000000000004}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.817608251780454
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.80942537, 10.44690748,  4.45538174]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.585004294573784}
episode index:2966
target Thresh 19.0
target distance 12.0
model initialize at round 2966
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 2.      , 10.      ,  4.329175]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8176001648346715
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.80474926,  8.61405982,  0.06369254]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.43251898218906937}
episode index:2967
target Thresh 19.0
target distance 11.0
model initialize at round 2967
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 3.00043083, 11.00041516,  1.77687424]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 13.601366277029104}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8175500878783827
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.58182246,  2.95818638,  0.09546525]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.583323022963711}
episode index:2968
target Thresh 19.0
target distance 14.0
model initialize at round 2968
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([2.       , 8.       , 4.2349298]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 14.560219778561036}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8174955608095703
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.41005547,  4.60841115,  0.2703355 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8474660379321896}
episode index:2969
target Thresh 19.0
target distance 9.0
model initialize at round 2969
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([16.        ,  2.        ,  1.30323475]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 10.8166538269225}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8175041290360264
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.98657253, 10.14112282,  3.88730822]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.3080502173370567}
episode index:2970
target Thresh 19.0
target distance 3.0
model initialize at round 2970
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([4.42741305, 6.87951807, 1.56221884]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.2231626887380944}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8175522919040721
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.94645683, 9.10408967, 3.27903353]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.11705353766852816}
episode index:2971
target Thresh 19.0
target distance 8.0
model initialize at round 2971
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.        , 10.        ,  2.00936093]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 8.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8175695171802715
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.51265924,  2.33484578,  4.8766197 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.591289027628889}
episode index:2972
target Thresh 19.0
target distance 6.0
model initialize at round 2972
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 3.00000001, 11.        ,  1.16617602]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 6.000000001135157}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.817601790214346
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.86145108, 5.76321455, 0.3166201 ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.1509102540287928}
episode index:2973
target Thresh 19.0
target distance 12.0
model initialize at round 2973
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([3.        , 4.        , 0.46423071]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 12.369316876852636}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.817573108835543
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.07967712,  6.7798239 ,  5.91556295]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9462936758927557}
episode index:2974
target Thresh 19.0
target distance 6.0
model initialize at round 2974
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.        , 4.        , 4.96675611]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8175962388398725
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.71800812, 10.62490743,  3.83401488]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.6855864004940413}
episode index:2975
target Thresh 19.0
target distance 9.0
model initialize at round 2975
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.40843551,  1.45352898,  4.48233509]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 11.497963065693522}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8175828736250067
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.27094272, 10.74235026,  4.21685263]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.37388947803607336}
episode index:2976
target Thresh 19.0
target distance 5.0
model initialize at round 2976
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.        ,  3.        ,  5.66246247]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 5.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8176181983884608
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.49854284,  7.98851587,  2.81290655]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5015886475377606}
episode index:2977
target Thresh 19.0
target distance 9.0
model initialize at round 2977
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 3.        , 10.        ,  3.60440969]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8176324496162153
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.31282613, 11.88730571,  2.18848316]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.1222831023897597}
episode index:2978
target Thresh 19.0
target distance 11.0
model initialize at round 2978
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.01052357, 11.36136861,  3.20929956]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.285263432586873}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8176325417571959
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.46141007, 8.62445856, 5.51018772]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6565900405674244}
episode index:2979
target Thresh 19.0
target distance 6.0
model initialize at round 2979
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([1.75538308, 6.07402397, 3.43963671]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.760691420168547}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.817638206596019
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.24940954, 11.3631044 ,  1.74052486]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.4405109785905825}
episode index:2980
target Thresh 19.0
target distance 6.0
model initialize at round 2980
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.        , 11.        ,  1.96241158]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.0}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8176673055119577
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.14744641, 11.6758835 ,  3.11285566]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.691779551136268}
episode index:2981
target Thresh 19.0
target distance 2.0
model initialize at round 2981
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.60359347, 10.48956307,  0.70183247]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.6462847683187732}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8177284499433755
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.60359347, 10.48956307,  0.70183247]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.6462847683187732}
episode index:2982
target Thresh 19.0
target distance 14.0
model initialize at round 2982
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([16.        ,  9.00000001,  2.29945245]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 14.035668851003845}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8177124642564367
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.21324074, 8.45200823, 4.03397   ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.4997830119271286}
episode index:2983
target Thresh 19.0
target distance 1.0
model initialize at round 2983
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.        , 10.        ,  5.29299426]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.4142135633492843}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8177668836719004
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.29993407, 11.53044459,  3.00980895]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6093700920208651}
episode index:2984
target Thresh 19.0
target distance 1.0
model initialize at round 2984
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.10751763,  8.79695035,  4.37531304]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.196517715399214}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8178279332921109
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.10751763,  8.79695035,  4.37531304]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.196517715399214}
episode index:2985
target Thresh 19.0
target distance 3.0
model initialize at round 2985
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([11.        , 11.        ,  2.89225191]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8178630695148623
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.13854532,  7.77603597,  0.04269599]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.26335279207956097}
episode index:2986
target Thresh 19.0
target distance 12.0
model initialize at round 2986
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([1.33351067, 3.91830344, 4.20057654]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 14.009631842140621}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8178154853098223
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.10599552,  6.40558845,  0.51916755]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6037881563788662}
episode index:2987
target Thresh 19.0
target distance 12.0
model initialize at round 2987
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        ,  9.        ,  0.90180033]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.4164078650031}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8177819896763847
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.3462979 , 2.98646381, 4.06994726]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6538422307748009}
episode index:2988
target Thresh 19.0
target distance 10.0
model initialize at round 2988
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.        ,  9.        ,  5.95336485]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.6619037896866}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8177533919446944
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.41433331, 3.12706249, 5.12151178]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.5992915384285771}
episode index:2989
target Thresh 19.0
target distance 2.0
model initialize at round 2989
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.        , 2.        , 4.44819856]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 2.828427124861604}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8177979526998636
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.79114318, 4.76781388, 1.88182794]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7957130941059765}
episode index:2990
target Thresh 19.0
target distance 9.0
model initialize at round 2990
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([10.30138802, 10.11627499,  5.26263738]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 8.376103679952461}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8178092064004302
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.42060492, 8.66725891, 5.84671084]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5363069413665077}
episode index:2991
target Thresh 19.0
target distance 12.0
model initialize at round 2991
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([15.        , 10.        ,  0.64701336]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8177958414783041
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.70867557, 7.72306896, 0.3815309 ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0124474223649431}
episode index:2992
target Thresh 19.0
target distance 5.0
model initialize at round 2992
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([13.18277664,  6.47117547,  3.08791018]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 3.969244681132317}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8178371660048402
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.56999604,  9.45004877,  2.52153956]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6981044016794229}
episode index:2993
target Thresh 19.0
target distance 12.0
model initialize at round 2993
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.33544129,  1.75174131,  4.29964519]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 11.591616192030441}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8177989597686997
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.89914333, 6.62345955, 5.18460681]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.3898137772159303}
episode index:2994
target Thresh 19.0
target distance 4.0
model initialize at round 2994
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([14.        ,  9.        ,  2.13869461]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8178340000807728
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.00187627,  4.34897419,  5.572324  ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.1916734419227326}
episode index:2995
target Thresh 19.0
target distance 13.0
model initialize at round 2995
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([15.64646572,  3.18889756,  1.29428595]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 13.670549138414536}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8177843130842737
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.55344136, 4.9732734 , 5.89606226]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.1196242454803738}
episode index:2996
target Thresh 19.0
target distance 4.0
model initialize at round 2996
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.27152762, 5.10254506, 1.72433966]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.9876273466941763}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8178255863029312
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.05699668, 7.63976012, 1.15796905]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.3647209839188236}
episode index:2997
target Thresh 19.0
target distance 3.0
model initialize at round 2997
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.        ,  9.        ,  4.16137934]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.1622776616508532}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8178732081920896
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.97646117,  5.8140232 ,  5.87819403]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.18746052222487894}
episode index:2998
target Thresh 19.0
target distance 6.0
model initialize at round 2998
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.60888917,  8.05279425,  4.74938369]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.470638088817775}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8179051001691791
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.32396982, 10.65803502,  3.89982777]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.4710589026883313}
episode index:2999
target Thresh 19.0
target distance 9.0
model initialize at round 2999
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 7.       , 11.       ,  4.2566328]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8178815225005666
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.25754044,  2.38653049,  5.70796504]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.46447055804167503}
episode index:3000
target Thresh 19.0
target distance 5.0
model initialize at round 3000
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([15.        ,  9.        ,  2.65900898]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.817916465243628
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.98875299,  3.9914617 ,  6.09263837]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.014120829022317267}
episode index:3001
target Thresh 19.0
target distance 5.0
model initialize at round 3001
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.        , 11.        ,  4.00127029]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 5.3851648071345855}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8179513847070471
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.33982146,  6.61762172,  1.15171437]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.7049363170279351}
episode index:3002
target Thresh 19.0
target distance 3.0
model initialize at round 3002
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.90058414,  8.68003112,  2.63990241]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.5979266655635693}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8179988854147703
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.61883418,  9.9738071 ,  4.35671711]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.3820647189177089}
episode index:3003
target Thresh 19.0
target distance 12.0
model initialize at round 3003
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([3.9983487 , 6.0017752 , 3.33004975]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.043387703569717}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8179448442480458
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.27222147,  4.53930361,  5.64864076]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8613377729365141}
episode index:3004
target Thresh 19.0
target distance 4.0
model initialize at round 3004
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.6929025 ,  8.53371234,  2.15646209]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 2.9914032990764965}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8179923155178467
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.48540942, 10.2666929 ,  3.87327678]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8958475128910939}
episode index:3005
target Thresh 19.0
target distance 3.0
model initialize at round 3005
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.7340958 , 4.92115818, 4.4398396 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 3.1836580426642818}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8180397552033031
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.40101327, 2.12556345, 6.15665429]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.4202116423054749}
episode index:3006
target Thresh 19.0
target distance 1.0
model initialize at round 3006
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.00000001,  6.        ,  1.58201569]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.000000005723649}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8180969418493944
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.60904526,  6.8907283 ,  3.58201569]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.9727499764281488}
episode index:3007
target Thresh 19.0
target distance 11.0
model initialize at round 3007
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([15.        ,  6.        ,  0.20297163]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8180758721503765
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.16785473, 7.36473339, 5.93748918]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.4015042424205499}
episode index:3008
target Thresh 19.0
target distance 4.0
model initialize at round 3008
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([16.        ,  4.        ,  0.06616276]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8181200443596652
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.64745006,  7.37184321,  3.78297745]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9020934184015102}
episode index:3009
target Thresh 19.0
target distance 1.0
model initialize at round 3009
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.       ,  7.       ,  3.4209969]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.0000000000020721}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8181771473349609
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.500444  ,  5.39299853,  5.4209969 ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.7861342025132229}
episode index:3010
target Thresh 19.0
target distance 11.0
model initialize at round 3010
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.48041468,  7.27664079,  4.59587622]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.400380713764504}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8181461952222484
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.45911064, 2.68777293, 5.76402315]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.5552191663439034}
episode index:3011
target Thresh 19.0
target distance 2.0
model initialize at round 3011
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([1.99999994, 4.00000007, 3.27443671]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.236067999072388}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8181967107616833
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.01129159, 5.51122864, 2.99125141]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.1130584200139093}
episode index:3012
target Thresh 19.0
target distance 11.0
model initialize at round 3012
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([2.99257266, 7.01120507, 3.16615558]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 11.18965605127461}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8181494264398442
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.56327771,  4.8152444 ,  5.76793189]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5928038532855987}
episode index:3013
target Thresh 19.0
target distance 13.0
model initialize at round 3013
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.39829278, 6.93657108, 1.60016602]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.762224191215548}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8180933392473851
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.06889358,  5.49524679,  5.91875703]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5000157045679001}
episode index:3014
target Thresh 19.0
target distance 2.0
model initialize at round 3014
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.        ,  5.        ,  5.26672125]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 2.0}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8181470727998738
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.60057443,  6.54142923,  2.98353594]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6081348029287575}
episode index:3015
target Thresh 19.0
target distance 2.0
model initialize at round 3015
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.73446399,  6.36557672,  1.97730273]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9705309098525912}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.818207368863269
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.73446399,  6.36557672,  1.97730273]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9705309098525912}
episode index:3016
target Thresh 19.0
target distance 13.0
model initialize at round 3016
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.41917299,  9.57738319,  3.80140495]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.302436693017572}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8181914045864848
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.46500439, 5.06170475, 5.5359225 ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.46908054727245235}
episode index:3017
target Thresh 19.0
target distance 8.0
model initialize at round 3017
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 6.00188097, 10.97808842,  5.8080225 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 8.239098364352465}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8182024272394033
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.91706677,  8.61622221,  0.10891066]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.39263636568690113}
episode index:3018
target Thresh 19.0
target distance 3.0
model initialize at round 3018
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.36072538,  8.38100762,  3.92322278]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 2.9513885970433704}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8182432611983835
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.71876596, 11.13838828,  3.35685217]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.3134388345778577}
episode index:3019
target Thresh 19.0
target distance 9.0
model initialize at round 3019
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.        ,  2.        ,  4.49773908]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.818257107255815
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.4314975 , 10.3442571 ,  3.08181254]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8678674152107054}
episode index:3020
target Thresh 19.0
target distance 12.0
model initialize at round 3020
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.31703342,  9.00340288,  4.1495707 ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.936583461921398}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8182335769635528
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.60985967, 3.02926099, 5.60090294]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6105612381910747}
episode index:3021
target Thresh 19.0
target distance 7.0
model initialize at round 3021
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([11.        , 11.        ,  0.11826819]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8182389641854213
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.34750544, 6.93955283, 4.70234165]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.143900636978367}
episode index:3022
target Thresh 19.0
target distance 2.0
model initialize at round 3022
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.        , 8.        , 1.84535282]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.0}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8182925073663061
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.20639277, 6.32974697, 5.84535282]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.3890129042805051}
episode index:3023
target Thresh 19.0
target distance 12.0
model initialize at round 3023
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([3.58200146, 2.57415979, 1.35814923]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.696700159076821}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.8182102681781814
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.53573817,  4.98151787,  0.2608137 ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5360568745792046}
episode index:3024
target Thresh 19.0
target distance 13.0
model initialize at round 3024
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.42015114,  8.4199434 ,  4.50347185]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.921235879020337}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8181867844843479
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.78704533, 5.01439921, 5.95480409]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.21344092877526763}
episode index:3025
target Thresh 19.0
target distance 12.0
model initialize at round 3025
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([16.        ,  5.        ,  0.96714991]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 12.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8181374735042043
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.50302265, 5.37032015, 5.56892622]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6246349317754408}
episode index:3026
target Thresh 19.0
target distance 7.0
model initialize at round 3026
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.61359354,  9.0459218 ,  4.7543323 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.943976280979132}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8181689831751587
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.89794218, 10.58842394,  3.90477638]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9877727531252926}
episode index:3027
target Thresh 19.0
target distance 2.0
model initialize at round 3027
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.83939155,  1.43812899,  5.61991835]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.9232941305746631}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8182224610538988
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.68496536,  2.66266342,  3.33673304]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.9530479288539783}
episode index:3028
target Thresh 19.0
target distance 11.0
model initialize at round 3028
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.00003291, 4.99996469, 0.18942374]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.180301192421952}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.8181461369604303
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.61493718,  3.4205942 ,  5.65845882]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7450149080030383}
episode index:3029
target Thresh 19.0
target distance 13.0
model initialize at round 3029
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([2.       , 9.       , 4.3806169]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 13.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8181302613857916
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.19470477,  8.66425343,  6.11513445]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.38811815861401044}
episode index:3030
target Thresh 19.0
target distance 1.0
model initialize at round 3030
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.        ,  7.        ,  4.63322043]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.4142135625795114}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.818183699108858
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.61927227,  8.45649493,  2.35003513]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5944251189737108}
episode index:3031
target Thresh 19.0
target distance 2.0
model initialize at round 3031
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.       , 10.       ,  2.4155167]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8182371015827667
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.07494366,  8.70528163,  0.1323314 ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7092522362277673}
episode index:3032
target Thresh 19.0
target distance 12.0
model initialize at round 3032
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 2.        , 11.        ,  4.10749102]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 13.892443989449802}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8182063542152676
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.3177232 ,  3.84120657,  5.27563794]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.7005119434303447}
episode index:3033
target Thresh 19.0
target distance 13.0
model initialize at round 3033
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([2.13223126, 4.19418438, 3.20919323]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 13.891160807709426}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8181527790229007
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.13475179,  5.45311572,  5.52778424]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9767130198838767}
episode index:3034
target Thresh 19.0
target distance 3.0
model initialize at round 3034
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([4.94578181, 9.29087991, 1.84895581]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.5898182070227125}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8181997125421683
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.32947761, 11.5876768 ,  3.5657705 ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.6737354948568772}
episode index:3035
target Thresh 19.0
target distance 12.0
model initialize at round 3035
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([15.99988431,  5.99964662,  5.40600944]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 12.165352857202814}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8181550336497007
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.54724661, 3.57368877, 6.00778575]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6937003028473343}
episode index:3036
target Thresh 19.0
target distance 11.0
model initialize at round 3036
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([12.51884884,  9.79911149,  3.65683079]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 9.520968404282455}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8181659993189285
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.89441332, 9.96734199, 4.24090425]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8950093472147185}
episode index:3037
target Thresh 19.0
target distance 5.0
model initialize at round 3037
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.60508618, 6.57043268, 2.21302709]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 3.4825366153433253}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8182034908754092
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.8504516 , 10.08458891,  3.64665648]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.1718138772956036}
episode index:3038
target Thresh 19.0
target distance 12.0
model initialize at round 3038
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 4.00027708, 10.00080218,  2.24821913]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 13.892608874234051}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8181680679522098
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.50427595,  3.20219397,  1.13318075]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5433016101413022}
episode index:3039
target Thresh 19.0
target distance 2.0
model initialize at round 3039
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.32186083,  7.89235946,  4.2156477 ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 2.0101982669361256}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8182245916140675
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.11874487,  6.41000866,  6.2156477 ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.4268576420511025}
episode index:3040
target Thresh 19.0
target distance 14.0
model initialize at round 3040
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.2620782 , 4.11334932, 1.73286837]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.738426112166344}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.8181564865985854
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.72772146,  3.85283796,  5.48508876]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.3095032624453955}
episode index:3041
target Thresh 19.0
target distance 3.0
model initialize at round 3041
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([16.01019277,  9.30766471,  1.92303961]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.352659503282533}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8182033108995063
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.90361566, 10.51311239,  3.6398543 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0264408414689676}
episode index:3042
target Thresh 19.0
target distance 14.0
model initialize at round 3042
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([16.        , 11.        ,  1.50549382]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 16.1245154965971}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8181702932267158
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.97017026, 2.86795124, 4.67364074]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.13537609496545544}
episode index:3043
target Thresh 19.0
target distance 13.0
model initialize at round 3043
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.34807048,  8.32173991,  3.95923471]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.82424033965952}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8181469692454757
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.43369596, 4.84256   , 5.41056576]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.5877819437957491}
episode index:3044
target Thresh 19.0
target distance 1.0
model initialize at round 3044
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.97546413, 9.33515165, 3.2353127 ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.33604856143858647}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8182066910946562
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.97546413, 9.33515165, 3.2353127 ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.33604856143858647}
episode index:3045
target Thresh 19.0
target distance 13.0
model initialize at round 3045
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([1.99999084, 9.00004194, 2.79583311]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 13.341682422328333}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.818173704831197
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.66617965,  5.56435592,  5.96398004]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.7959780742783236}
episode index:3046
target Thresh 19.0
target distance 2.0
model initialize at round 3046
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.00000135,  7.00000015,  1.12409705]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.000000154289815}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8182268476914428
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.08463261,  5.65709307,  5.12409705]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.1267957941520106}
episode index:3047
target Thresh 19.0
target distance 12.0
model initialize at round 3047
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4.14267481, 9.67691144, 2.49591884]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.41434002946387}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.818201084412472
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.55519634,  5.53854681,  5.94725108]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.6409284974255604}
episode index:3048
target Thresh 19.0
target distance 4.0
model initialize at round 3048
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.        ,  6.        ,  0.14813679]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 4.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8182384292020733
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.18915411, 10.25255524,  1.58176618]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.3155367241079977}
episode index:3049
target Thresh 19.0
target distance 11.0
model initialize at round 3049
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([3.00001412, 7.9999743 , 6.22479105]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 11.704677853913322}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.818217603253877
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.27934514,  4.81745657,  5.67612329]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.8638685939646643}
episode index:3050
target Thresh 19.0
target distance 6.0
model initialize at round 3050
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.18720082,  8.32747379,  5.83385205]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 4.331520922955998}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8182488387977085
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.64943738,  3.87862944,  4.98429613]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.37097838494710644}
episode index:3051
target Thresh 19.0
target distance 9.0
model initialize at round 3051
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.        , 2.        , 0.59686535]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8182653843330856
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.38340002, 10.46864858,  3.46412412]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.6552327064291421}
episode index:3052
target Thresh 19.0
target distance 5.0
model initialize at round 3052
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.09515501,  5.58097469,  5.15474296]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 4.0560835972579286}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8183026591328151
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.19759491,  1.94019694,  0.30518703]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.20644649492920628}
episode index:3053
target Thresh 19.0
target distance 5.0
model initialize at round 3053
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 7.32475945, 11.16111235,  4.05571485]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 4.587663535016666}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8183399095220667
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.95252311, 8.60728461, 5.48934423]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1296436919958295}
episode index:3054
target Thresh 19.0
target distance 7.0
model initialize at round 3054
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([11.        , 11.        ,  3.59381342]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 7.6157731058639095}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8183479629701393
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.04919642,  3.5762519 ,  6.17788688]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.4265943553452098}
episode index:3055
target Thresh 19.0
target distance 11.0
model initialize at round 3055
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.17866777, 10.46894569,  3.09062243]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 11.116508137577425}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8183398694886844
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.69927997, 6.386851  , 5.10832527]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.4899859530814731}
episode index:3056
target Thresh 19.0
target distance 1.0
model initialize at round 3056
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.        , 5.        , 1.67819041]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8183927841535557
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.92563461, 3.31867599, 5.67819041]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6853704225629039}
episode index:3057
target Thresh 19.0
target distance 4.0
model initialize at round 3057
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.31710642,  7.01604045,  4.14206147]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.9965597930313774}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8184330350905235
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.79447073, 10.56263823,  3.57569086]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.4832469358440071}
episode index:3058
target Thresh 19.0
target distance 9.0
model initialize at round 3058
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.62102756,  3.63974631,  2.80792487]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 9.900067690351548}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8184328631070343
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.81234034, 10.21594797,  5.10881302]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.1289970850794906}
episode index:3059
target Thresh 19.0
target distance 13.0
model initialize at round 3059
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 2.        , 10.        ,  4.08043361]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 15.264337522473749}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8183929874060352
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.45050734,  1.74540044,  0.68220992]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5174725050951337}
episode index:3060
target Thresh 19.0
target distance 11.0
model initialize at round 3060
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.17160023,  8.53502836,  5.20774508]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.185661815812487}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8183955276335522
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.97966589, 8.69022818, 5.50863323]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1983990150709656}
episode index:3061
target Thresh 19.0
target distance 1.0
model initialize at round 3061
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.        , 8.        , 1.31800621]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.414213562373226}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8184483377159711
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.2963457 , 6.49846078, 5.31800609]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5825481644843118}
episode index:3062
target Thresh 19.0
target distance 5.0
model initialize at round 3062
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.44025285, 6.8706663 , 1.55373877]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.4140919722211835}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8184793755578802
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.23170215, 10.53594321,  0.70418285]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.5186854459079481}
episode index:3063
target Thresh 19.0
target distance 2.0
model initialize at round 3063
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.        ,  7.        ,  1.80506199]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.236067977511514}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8185321238034553
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.13897776,  5.32277811,  5.80506199]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9195352131937479}
episode index:3064
target Thresh 19.0
target distance 3.0
model initialize at round 3064
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.50943898,  8.09910414,  1.07556218]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.1406965070182706}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8185753400925571
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.79051899,  5.39042948,  4.79237687]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.443077277693952}
episode index:3065
target Thresh 19.0
target distance 5.0
model initialize at round 3065
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 3.        , 11.        ,  2.92240977]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 5.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8186093157462867
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.94692659, 5.78853566, 0.07285385]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.2180228288056452}
episode index:3066
target Thresh 19.0
target distance 1.0
model initialize at round 3066
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.        , 10.        ,  6.05088329]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.4142135638781492}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8186651979387398
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.22254307, 11.15662289,  1.76769799]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.2721325961826601}
episode index:3067
target Thresh 19.0
target distance 4.0
model initialize at round 3067
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.50333505, 10.56665961,  2.26993349]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.524136433847599}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8187021601779733
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.65511602, 11.37318102,  3.70356288]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5081427258287395}
episode index:3068
target Thresh 19.0
target distance 6.0
model initialize at round 3068
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.28144249, 7.59559971, 5.15809131]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 4.651436509598332}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.818739098329726
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.99058291, 3.76950659, 0.30853539]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.2543503904096054}
episode index:3069
target Thresh 19.0
target distance 1.0
model initialize at round 3069
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.        , 11.        ,  2.88204437]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.4142135623730951}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8187916588840162
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.52427096, 10.3108749 ,  0.59885907]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6095106587683293}
episode index:3070
target Thresh 19.0
target distance 7.0
model initialize at round 3070
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.        , 3.        , 5.00511026]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8188136690477517
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.49029955, 9.60257545, 3.87236904]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6463287261664993}
episode index:3071
target Thresh 19.0
target distance 8.0
model initialize at round 3071
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.45112572, 3.59095101, 2.91300774]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 6.571276500488464}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8188327795145328
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.17556293, 10.20940883,  3.78026651]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.27326617259081143}
episode index:3072
target Thresh 19.0
target distance 10.0
model initialize at round 3072
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.951049  ,  9.61170058,  0.03982275]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 12.792187415951425}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8188046215549297
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.4885038 , 3.81192849, 5.49115499]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9596125461598591}
episode index:3073
target Thresh 19.0
target distance 12.0
model initialize at round 3073
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.11525041,  7.46901327,  4.47502208]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 11.125141131927656}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8187912893291146
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.48981192, 7.51824648, 4.20953962]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7130884442092141}
episode index:3074
target Thresh 19.0
target distance 11.0
model initialize at round 3074
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.        ,  8.        ,  0.20961397]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8187883418750957
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.19782962, 10.16283096,  4.51050213]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8602258776668296}
episode index:3075
target Thresh 19.0
target distance 9.0
model initialize at round 3075
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([11.        , 11.        ,  2.03824922]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8188045829254513
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.85424585, 8.72406261, 4.90550799]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8977067501833028}
episode index:3076
target Thresh 19.0
target distance 14.0
model initialize at round 3076
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.76969612,  3.14836425,  3.40062857]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 13.660289472846939}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8187670928741267
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.78071788, 8.26266645, 4.28559019]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.34216708029264997}
episode index:3077
target Thresh 19.0
target distance 2.0
model initialize at round 3077
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.70875865, 5.0546649 , 1.08697527]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.0704268823479857}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8188227240980143
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.89278934, 6.52659634, 3.08697527]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.48539174959319614}
episode index:3078
target Thresh 19.0
target distance 4.0
model initialize at round 3078
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.        , 3.        , 4.26211596]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 4.472135955000187}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8188564759558674
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.36059035, 6.8196632 , 1.41256003]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.40317088720418776}
episode index:3079
target Thresh 19.0
target distance 12.0
model initialize at round 3079
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([14.50385168,  6.60577756,  2.27675098]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 13.25350887925676}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8188406274720522
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.29532607, 10.66854081,  4.01126852]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.44393995351425364}
episode index:3080
target Thresh 19.0
target distance 13.0
model initialize at round 3080
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.58374393,  4.09082079,  4.72229481]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.584099962975284}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8187898132602742
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.26855702, 3.90841745, 5.04088581]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.28374325507400827}
episode index:3081
target Thresh 19.0
target distance 9.0
model initialize at round 3081
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.        , 2.        , 4.69679093]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 9.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8188032034424225
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.41457518, 10.51898389,  3.2808644 ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7576930247815649}
episode index:3082
target Thresh 19.0
target distance 2.0
model initialize at round 3082
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([14.86776409,  5.55799668,  6.2641139 ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.2622671854496958}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8188491952706928
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.59171972,  5.00046042,  1.69774329]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.5917198984902314}
episode index:3083
target Thresh 19.0
target distance 11.0
model initialize at round 3083
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.        , 6.        , 5.19901109]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8188187578973747
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.40542881,  8.45200346,  0.08397271]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8085883417862545}
episode index:3084
target Thresh 19.0
target distance 1.0
model initialize at round 3084
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.       ,  7.       ,  2.3078548]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.414213562382147}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8188710370682346
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.92998847,  5.59732061,  0.02466949]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.4087203297248343}
episode index:3085
target Thresh 19.0
target distance 5.0
model initialize at round 3085
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.50057868, 3.76200373, 1.47987336]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.495814578577822}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.818907717013419
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.24297036, 8.08887006, 2.91350274]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.25871313187436185}
episode index:3086
target Thresh 19.0
target distance 4.0
model initialize at round 3086
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.        ,  5.        ,  0.13370102]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8189505036453875
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.49516026,  8.29719437,  3.85051572]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8653316789113555}
episode index:3087
target Thresh 19.0
target distance 13.0
model initialize at round 3087
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.32695121,  7.18247143,  4.04295683]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.53529389233745}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8189200728916026
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.32922483, 5.81592785, 5.21110376]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8798450102834168}
episode index:3088
target Thresh 19.0
target distance 14.0
model initialize at round 3088
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([16.00002273,  5.00002735,  1.88734787]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 14.560234120523056}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8188968459642605
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.40929732, 9.68946231, 3.33868011]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8017995870915913}
episode index:3089
target Thresh 19.0
target distance 6.0
model initialize at round 3089
at step 0:
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([2.        , 5.        , 5.10320497]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8189129783159772
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.38483943, 11.71566219,  1.68727843]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9437133571537316}
episode index:3090
target Thresh 19.0
target distance 3.0
model initialize at round 3090
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.        ,  5.        ,  0.39842385]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.1622776601684}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8189588155957196
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.43057905,  7.74813743,  2.11523854]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.4988317068946735}
episode index:3091
target Thresh 19.0
target distance 11.0
model initialize at round 3091
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.        ,  3.        ,  5.59317994]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 12.529964086141668}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8189237555736227
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.64704948, 8.87753573, 4.47814144]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.3735927847324843}
episode index:3092
target Thresh 19.0
target distance 12.0
model initialize at round 3092
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([3.655069  , 7.31196562, 3.41633594]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.634193690409255}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8189054623174874
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.05537697, 10.91614964,  0.86766818]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9178217562387142}
episode index:3093
target Thresh 19.0
target distance 13.0
model initialize at round 3093
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([2.        , 4.        , 5.34343743]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 13.60147050873544}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8188681456506103
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.33958602,  7.92515187,  6.22839905]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.3477368385845515}
episode index:3094
target Thresh 19.0
target distance 11.0
model initialize at round 3094
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([3.15248973, 9.67602325, 2.49006307]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.45345475815479}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8188425664027065
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.44174768,  5.86461468,  5.94139531]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.46202834998891734}
episode index:3095
target Thresh 19.0
target distance 2.0
model initialize at round 3095
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.9977729 , 9.64484603, 0.07386607]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.0992672430587196}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8188946521370726
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.31988789, 9.77552557, 4.07386607]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8389089202934801}
episode index:3096
target Thresh 19.0
target distance 7.0
model initialize at round 3096
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([13.83232961,  5.67459688,  2.68058956]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.4519145717681}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8189252051223314
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.80122468, 10.07315139,  1.83103364]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.2251568605427285}
episode index:3097
target Thresh 19.0
target distance 1.0
model initialize at round 3097
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.        ,  8.        ,  4.02460194]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.4142135623870238}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.818977230556443
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.25211754,  9.12374036,  1.74141663]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.280846811158081}
episode index:3098
target Thresh 19.0
target distance 2.0
model initialize at round 3098
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([13.00000693, 11.00000882,  1.91460675]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.828428458722867}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8190198290783351
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.49987598,  8.62925689,  5.63142144]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.6225548087000168}
episode index:3099
target Thresh 19.0
target distance 1.0
model initialize at round 3099
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.99072875,  5.65889813,  5.07724595]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.34122784910205395}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8190782097786324
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.99072875,  5.65889813,  5.07724595]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.34122784910205395}
episode index:3100
target Thresh 19.0
target distance 9.0
model initialize at round 3100
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([3.34427675, 4.48889263, 0.64666956]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 10.050105353157374}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8190804962713951
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.38743885, 11.55637265,  0.94755772]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8275153701420347}
episode index:3101
target Thresh 19.0
target distance 3.0
model initialize at round 3101
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.25109724, 8.66413288, 2.43103829]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.8302418596514949}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8191292449831065
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.39026645, 9.74218789, 2.14785298]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.4677338829889145}
episode index:3102
target Thresh 19.0
target distance 13.0
model initialize at round 3102
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([15.00000001,  9.00000005,  2.39524385]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.000000009561997}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.819121022307844
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.80601708, 9.20440649, 4.4129467 ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8315320440572005}
episode index:3103
target Thresh 19.0
target distance 10.0
model initialize at round 3103
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([11.72450166, 11.76816409,  3.44927776]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 9.503472515416448}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8191314400748502
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.16754472, 8.7307172 , 4.03335122]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7496791688753833}
episode index:3104
target Thresh 19.0
target distance 11.0
model initialize at round 3104
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([14.        , 10.        ,  1.77573365]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8191131506301713
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.57349165, 3.97073756, 5.51025119]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.1274857392490414}
episode index:3105
target Thresh 19.0
target distance 14.0
model initialize at round 3105
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([2.        , 9.        , 5.60214901]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 14.142135623730988}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8190973521740139
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.9330463 , 11.74571835,  1.05348125]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.7487180114021884}
episode index:3106
target Thresh 19.0
target distance 14.0
model initialize at round 3106
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([16.02160097,  8.94023625,  6.06921816]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 15.645196481049686}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8190556243766837
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.14764284, 2.24490365, 4.67099447]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8868430106381274}
episode index:3107
target Thresh 19.0
target distance 8.0
model initialize at round 3107
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.33254355,  9.4550437 ,  5.31458497]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 6.666933341708764}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8190744356375017
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.40616841,  2.66108114,  6.18184374]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5289978881771695}
episode index:3108
target Thresh 19.0
target distance 6.0
model initialize at round 3108
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 6.3207023 , 11.11111855,  4.08551931]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 7.484266053639876}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8190932347971549
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.7884095 , 4.90189361, 4.95277809]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.7944900229118351}
episode index:3109
target Thresh 19.0
target distance 13.0
model initialize at round 3109
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.20927622,  5.52186789,  5.23117542]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.467006195396511}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8190428131914271
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.26221867, 2.38376769, 5.54976643]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6697020878685306}
episode index:3110
target Thresh 19.0
target distance 12.0
model initialize at round 3110
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.69310964,  4.65475267,  2.76417279]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.809615551422562}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.8189779618020832
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.84126628, 3.30663506, 4.51639318]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.3452845965150527}
episode index:3111
target Thresh 19.0
target distance 11.0
model initialize at round 3111
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 5.        , 11.        ,  4.67731547]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8189776176426343
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.10650204,  8.63487432,  0.69501832]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.38034122716223084}
episode index:3112
target Thresh 19.0
target distance 5.0
model initialize at round 3112
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.05126992, 5.6821889 , 2.55032766]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.318207216318788}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8190139452141938
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.83098029, 8.95261314, 3.98395704]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.17553682328940803}
episode index:3113
target Thresh 19.0
target distance 8.0
model initialize at round 3113
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([3.61005398, 4.6371714 , 2.81462234]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 9.017593800472858}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8190109631727702
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.57444826, 10.8254997 ,  0.83232507]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6003675156608114}
episode index:3114
target Thresh 19.0
target distance 11.0
model initialize at round 3114
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([11.52166891, 11.6838351 ,  3.66327679]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 9.546193443871655}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8190213794834996
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.88454885, 10.12940065,  4.24735026]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.2411163894803794}
episode index:3115
target Thresh 19.0
target distance 1.0
model initialize at round 3115
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.63156956, 11.81786075,  3.00138748]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0333326226768658}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8190794599137039
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.63156956, 11.81786075,  3.00138748]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.0333326226768658}
episode index:3116
target Thresh 19.0
target distance 1.0
model initialize at round 3116
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.20615374,  9.17370148,  1.78176277]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.26957664676000836}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8191375030770296
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.20615374,  9.17370148,  1.78176277]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.26957664676000836}
episode index:3117
target Thresh 19.0
target distance 11.0
model initialize at round 3117
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.99988026,  3.99913263,  5.58520961]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 11.1800669536229}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.8190668868176932
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.60155068, 2.50102581, 0.77105939]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7828729607798116}
episode index:3118
target Thresh 19.0
target distance 5.0
model initialize at round 3118
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.        , 6.        , 4.63565254]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8190828146554461
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.53455   , 11.43238648,  1.21972493]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6875331002847391}
episode index:3119
target Thresh 19.0
target distance 10.0
model initialize at round 3119
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 5.43509761, 11.87681677,  1.56001538]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.387229101536583}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8190798162751788
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.10235664,  6.37014153,  5.86090353]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.3840333726079739}
episode index:3120
target Thresh 19.0
target distance 7.0
model initialize at round 3120
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.1094493 ,  5.31612419,  3.27879298]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.478820815446721}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.819098541429528
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.76291992, 10.21594231,  4.14605175]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0939804667043231}
episode index:3121
target Thresh 19.0
target distance 12.0
model initialize at round 3121
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([15.        , 11.        ,  1.29998797]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8190955399326644
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.55232664, 9.42135282, 5.60087612]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7999357974801717}
episode index:3122
target Thresh 19.0
target distance 3.0
model initialize at round 3122
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.       ,  8.       ,  3.3474133]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.1622776601683786}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8191408490809408
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.04919611,  5.19949168,  5.06422799]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9715065409788427}
episode index:3123
target Thresh 19.0
target distance 1.0
model initialize at round 3123
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.00000001, 10.99999999,  0.45821064]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.414213564677943}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8191923724967279
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.3959925 , 10.49216723,  4.45821064]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.779136466946394}
episode index:3124
target Thresh 19.0
target distance 13.0
model initialize at round 3124
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.89199549,  3.2667731 ,  3.29943764]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 11.959275512138394}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.8191338102930694
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.43122432, 2.25238883, 5.33484334]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.4996544189270918}
episode index:3125
target Thresh 19.0
target distance 12.0
model initialize at round 3125
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([2.00000078, 6.99999945, 0.39483803]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 12.649110076398507}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8191256466569052
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.30610455, 11.4941206 ,  2.41254088]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.8518486139915343}
episode index:3126
target Thresh 19.0
target distance 5.0
model initialize at round 3126
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 3.99999516, 10.00000134,  3.88231313]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 5.000001336432423}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8191617642460483
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.39571859, 5.12244618, 5.31594252]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6165623138074398}
episode index:3127
target Thresh 19.0
target distance 7.0
model initialize at round 3127
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.00000001, 3.        , 1.00881737]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 7.000000000011694}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8191890085909213
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.97924776, 9.39903144, 2.15926145]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6013267573114562}
episode index:3128
target Thresh 19.0
target distance 6.0
model initialize at round 3128
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 8.1620817 , 10.20599426,  0.20758885]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 7.331402572549264}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8192020700629732
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.18770111, 4.57076736, 5.07483987]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.46847877892070433}
episode index:3129
target Thresh 19.0
target distance 13.0
model initialize at round 3129
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([16.26484782,  4.82176659,  5.94349766]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 13.56165750491594}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.8191456513831079
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.65038999, 1.90496165, 5.97890336]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.36229745551210524}
episode index:3130
target Thresh 19.0
target distance 6.0
model initialize at round 3130
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([16.        ,  5.        ,  0.26955765]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.708203932499369}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8191699862930011
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.5798759 , 10.94470707,  3.42000173]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.42374706130103595}
episode index:3131
target Thresh 19.0
target distance 13.0
model initialize at round 3131
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 2.        , 11.        ,  4.77474785]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8191618267455397
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.30725264, 10.59746034,  0.50926539]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.5064013808676477}
episode index:3132
target Thresh 19.0
target distance 2.0
model initialize at round 3132
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.1515518 ,  7.05706058,  4.10233235]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.120112720827284}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8192131954570796
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.38091618,  8.27090692,  1.81914705]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8226018804787184}
episode index:3133
target Thresh 19.0
target distance 10.0
model initialize at round 3133
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.00815554,  5.68294986,  2.57595038]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.664300118109006}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.8191527648367051
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.59037247, 2.45214577, 0.32817078]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7436231942208671}
episode index:3134
target Thresh 19.0
target distance 3.0
model initialize at round 3134
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.        , 8.        , 5.76241994]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8191948181971719
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.73523161, 10.43387138,  3.19604932]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.6249831288728165}
episode index:3135
target Thresh 19.0
target distance 13.0
model initialize at round 3135
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([15.67899821,  9.46008133,  6.13767576]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 14.387748871468498}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8191742583977478
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.82669266, 5.6718221 , 5.589008  ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.065253812529657}
episode index:3136
target Thresh 19.0
target distance 11.0
model initialize at round 3136
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.13401796,  6.95841906,  3.54779589]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 11.282031715118649}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.819128531521963
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.37010355, 2.39090394, 6.14957221]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.5383145266709913}
episode index:3137
target Thresh 19.0
target distance 10.0
model initialize at round 3137
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.14268899, 3.67691023, 2.49591038]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.998930355600862}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.819018974960613
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.38973558,  1.47683588,  5.98264831]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6523760554808467}
episode index:3138
target Thresh 19.0
target distance 12.0
model initialize at round 3138
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.35302945,  3.55364642,  2.97537786]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 12.19267558405478}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8189890168086529
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.10164522, 8.22898949, 4.14352479]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.25053530123654105}
episode index:3139
target Thresh 19.0
target distance 11.0
model initialize at round 3139
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.        , 11.        ,  1.46900528]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 11.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8189966455273074
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.8087852 , 10.77391478,  4.05307875]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8397904710444111}
episode index:3140
target Thresh 19.0
target distance 8.0
model initialize at round 3140
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 7.99999998, 11.00000084,  2.60740286]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.630146458902491}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8189810602042504
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.60160986,  3.3356649 ,  4.3419204 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.52094686281851}
episode index:3141
target Thresh 19.0
target distance 12.0
model initialize at round 3141
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([2.67120907, 3.07161206, 4.09775209]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 15.508578366137801}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8189488353386852
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.38962804, 11.53241205,  0.98271372]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.809948473896694}
episode index:3142
target Thresh 19.0
target distance 11.0
model initialize at round 3142
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.36431406,  8.60386926,  4.38919806]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.033855261254558}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8189332751447516
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.76596035, 5.33613061, 6.12371561]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.4095831377939046}
episode index:3143
target Thresh 19.0
target distance 2.0
model initialize at round 3143
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.00000005, 8.99999994, 0.13780707]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.236068009595749}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8189876856806468
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.72139185, 10.52052023,  2.13780707]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5545478799525656}
episode index:3144
target Thresh 19.0
target distance 1.0
model initialize at round 3144
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.44007624, 9.13894713, 0.47111004]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.4467639335331453}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8190389137615116
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.84229028, 8.61031766, 4.47111004]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.42038634944384645}
episode index:3145
target Thresh 19.0
target distance 2.0
model initialize at round 3145
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([1.22224499, 7.49246523, 3.0612067 ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.8237412157743953}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8190808562714095
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.99897204, 7.91981637, 0.49483608]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.08019022300494494}
episode index:3146
target Thresh 19.0
target distance 12.0
model initialize at round 3146
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([15.20104247,  6.48123426,  3.07545435]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.29855775332296}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8190486508936932
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.38288076, 5.36932817, 6.24360128]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.5319783561766905}
episode index:3147
target Thresh 19.0
target distance 14.0
model initialize at round 3147
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([2.        , 6.        , 5.95998192]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 14.317821063276355}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8190187689639168
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.17063899,  9.11542689,  0.84494342]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.2060122101319531}
episode index:3148
target Thresh 19.0
target distance 2.0
model initialize at round 3148
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([4.        , 8.        , 1.56254166]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8190699221017499
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.73163132, 6.33881564, 5.56254166]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.806275653976435}
episode index:3149
target Thresh 19.0
target distance 10.0
model initialize at round 3149
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([5.3254448 , 9.4667859 , 1.19661921]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 8.690927752103104}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8190721756578558
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.40709247, 10.50000587,  1.49750737]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6447713919751283}
episode index:3150
target Thresh 19.0
target distance 13.0
model initialize at round 3150
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([15.        ,  7.        ,  0.65149563]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8190377361312339
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.40995124, 5.60962235, 5.81964256]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5660872130267044}
episode index:3151
target Thresh 19.0
target distance 1.0
model initialize at round 3151
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.        , 11.        ,  2.74556738]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.000000004567557}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8190919754281465
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.04133559, 10.35258742,  4.74556738]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0214476701622501}
episode index:3152
target Thresh 19.0
target distance 13.0
model initialize at round 3152
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([16.        ,  8.        ,  0.22064179]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 13.341664064126332}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8190644497047803
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.45663891, 5.34284845, 5.67197402]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5710202786770826}
episode index:3153
target Thresh 19.0
target distance 5.0
model initialize at round 3153
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.24437553, 5.68760635, 5.05992842]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 4.0841961079036775}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.819100277510171
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.94265076, 1.84647197, 0.2103725 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.16388957069129637}
episode index:3154
target Thresh 19.0
target distance 7.0
model initialize at round 3154
at step 0:
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.9331043 ,  7.68163999,  2.6205554 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.686315656956328}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8191105334510853
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.97861231, 11.59129211,  3.20462886]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.1433758808783427}
episode index:3155
target Thresh 19.0
target distance 2.0
model initialize at round 3155
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.        , 6.        , 5.95722151]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8191646809373175
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.32535764, 7.03721512, 1.6740362 ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.0162737384670661}
episode index:3156
target Thresh 19.0
target distance 3.0
model initialize at round 3156
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.43670224, 4.62320858, 3.77225065]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.4426301056468684}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8192125537023041
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.60237938, 6.43371091, 3.48906535]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.6919432677882756}
episode index:3157
target Thresh 19.0
target distance 8.0
model initialize at round 3157
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([15.3885007 ,  3.63751497,  2.34785303]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 6.512230791663149}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.819231017435457
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.3248657 , 10.19332268,  3.21511181]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.7022677430766416}
episode index:3158
target Thresh 19.0
target distance 11.0
model initialize at round 3158
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.4836624 , 10.60180635,  2.89262688]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.085133573261478}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8192305982902092
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.86417844, 7.95018497, 5.19351504]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.28438929010807}
episode index:3159
target Thresh 19.0
target distance 9.0
model initialize at round 3159
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 7.37046182, 11.89512301,  2.33169571]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.39322970707993}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8192199804239171
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.03764447,  3.54710491,  0.06621325]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5483984787042349}
episode index:3160
target Thresh 19.0
target distance 11.0
model initialize at round 3160
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.        ,  7.        ,  5.59623504]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8191924838687858
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.0431622 , 5.46779096, 4.76438197]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.0650666411442495}
episode index:3161
target Thresh 19.0
target distance 11.0
model initialize at round 3161
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([15.      ,  6.      ,  0.764095]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8191818847723085
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.66143419, 10.72612529,  4.78179785]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9822184757416899}
episode index:3162
target Thresh 19.0
target distance 13.0
model initialize at round 3162
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([3.08616648, 9.68076275, 2.52957487]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 13.022751609675296}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8191639147532047
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.70070797,  8.16416011,  6.26409241]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.3413564995967353}
episode index:3163
target Thresh 19.0
target distance 1.0
model initialize at round 3163
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.        , 7.        , 0.59496039]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.414213562395204}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8192179084590349
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.97632375, 8.67146067, 2.59496039]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.1849335380820614}
episode index:3164
target Thresh 19.0
target distance 3.0
model initialize at round 3164
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.        ,  7.        ,  2.99156982]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8192595426269468
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.2169968 ,  4.31683418,  0.4251992 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.384020190876942}
episode index:3165
target Thresh 19.0
target distance 3.0
model initialize at round 3165
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([16.00000004,  7.99999999,  0.8285138 ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.6055512914589873}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.819298146735214
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.46649622,  5.14366504,  0.26214318]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.48811716724536053}
episode index:3166
target Thresh 19.0
target distance 10.0
model initialize at round 3166
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.80688993, 8.09700711, 1.71322172]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.700759028480523}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8192593450526483
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.86296207,  5.52809285,  4.59818335]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.545583592168886}
episode index:3167
target Thresh 19.0
target distance 7.0
model initialize at round 3167
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.        , 4.        , 0.04488772]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8192833598598471
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.92046771, 10.58475953,  3.1953318 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.42278839797259316}
episode index:3168
target Thresh 19.0
target distance 13.0
model initialize at round 3168
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([2.68059933, 8.46078831, 6.13871574]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 12.562766888063717}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8192702762371079
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.04725227,  6.60040063,  5.87323329]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.1261479250695188}
episode index:3169
target Thresh 19.0
target distance 13.0
model initialize at round 3169
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.99944119,  7.99887947,  5.25978255]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.037761752971642}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8192572008690202
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.79293088, 7.82304499, 4.9943001 ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.1428658845401132}
episode index:3170
target Thresh 19.0
target distance 12.0
model initialize at round 3170
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.58459613,  7.63089762,  2.8302018 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.880600746350321}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8192056089483974
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.20839826, 2.38064415, 5.14879281]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.8783639767009244}
episode index:3171
target Thresh 19.0
target distance 13.0
model initialize at round 3171
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.39800889,  7.48423598,  4.46306515]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 12.648772706830357}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8191561374578664
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.10479471, 1.92418874, 4.78165615]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.1293417161360426}
episode index:3172
target Thresh 19.0
target distance 7.0
model initialize at round 3172
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.7257221 , 5.38740005, 1.99922722]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.6192976949152005}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.819177325208972
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.50761524, 10.65406518,  0.86648587]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.6142834290403176}
episode index:3173
target Thresh 19.0
target distance 3.0
model initialize at round 3173
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([14.3969903 ,  6.63547754,  2.34266537]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.1051274618144884}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8192249375828823
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.23514645,  8.09488787,  2.05948007]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7707169829754986}
episode index:3174
target Thresh 19.0
target distance 1.0
model initialize at round 3174
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.        , 6.        , 5.70461249]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8192787250041159
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.87013282, 6.67219601, 1.42142719]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9298314763975788}
episode index:3175
target Thresh 19.0
target distance 6.0
model initialize at round 3175
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 3.65418355, 10.69005668,  0.82477825]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 5.925628092834095}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8193083970829823
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.71643706, 5.46746627, 6.25840764]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8554570534368048}
episode index:3176
target Thresh 19.0
target distance 13.0
model initialize at round 3176
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([15.        ,  3.        ,  6.21256828]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 13.92838827718412}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8192810111757023
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.62952103, 8.67052577, 5.38071521]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9197290568559283}
episode index:3177
target Thresh 19.0
target distance 7.0
model initialize at round 3177
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.66652269, 11.23471303,  1.14991968]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.356577627434846}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8193021262985909
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.39005858,  4.57070503,  0.01717846]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.6912669019672785}
episode index:3178
target Thresh 19.0
target distance 4.0
model initialize at round 3178
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([16.28120027,  7.09060568,  1.71521204]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 3.6970866872679595}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8193465723142251
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.06243899,  9.1680481 ,  3.43202674]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8342916707259685}
episode index:3179
target Thresh 19.0
target distance 13.0
model initialize at round 3179
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.4389472 , 6.87282246, 1.55523651]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.192485214131285}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8193057240480475
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.16967054,  3.98914583,  0.15701283]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.0035923329606593}
episode index:3180
target Thresh 19.0
target distance 3.0
model initialize at round 3180
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.45368919, 5.01395036, 4.12606263]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 3.0630625633680624}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8193501409879882
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.31158296, 1.79722429, 5.84287732]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.3717552072384523}
episode index:3181
target Thresh 19.0
target distance 12.0
model initialize at round 3181
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.00000007, 8.99999975, 5.9831419 ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.64911049596178}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8193298295945928
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.82071052,  5.28994939,  5.43447413]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.3409037512030699}
episode index:3182
target Thresh 19.0
target distance 10.0
model initialize at round 3182
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 7.35118203, 11.89573736,  1.66857642]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 10.467175897989105}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8193268129557698
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.09942876,  6.41720832,  5.96946457]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.4288926016760539}
episode index:3183
target Thresh 19.0
target distance 4.0
model initialize at round 3183
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.        ,  6.        ,  2.00301062]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 4.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8193681644749107
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.46166169,  2.69991703,  5.71982531]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8384601137324176}
episode index:3184
target Thresh 19.0
target distance 10.0
model initialize at round 3184
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.43559578,  5.62050586,  3.77398586]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 10.00491046223882}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8193625953775995
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.94880874, 10.97345594,  3.79168859]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.05766395849580825}
episode index:3185
target Thresh 19.0
target distance 12.0
model initialize at round 3185
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.08013556, 9.29061817, 1.88394898]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.157525308118437}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8193520227302139
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.21905998,  6.36795956,  5.90165183]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.0046603566141383}
episode index:3186
target Thresh 19.0
target distance 7.0
model initialize at round 3186
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 7.        , 11.        ,  4.00187278]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.219544457292885}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8193365508516682
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.46707556,  4.56876046,  5.7363902 ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6357099295283557}
episode index:3187
target Thresh 19.0
target distance 11.0
model initialize at round 3187
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([3.       , 2.       , 6.0554719]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 13.601470508735446}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8193069535446125
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.23736921, 10.70086113,  0.94043352]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.0357664077864839}
episode index:3188
target Thresh 19.0
target distance 11.0
model initialize at round 3188
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.        , 4.        , 0.11023157]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.038404810405194}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8192773747996809
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.48322405, 11.49408135,  1.2783785 ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7149641689697747}
episode index:3189
target Thresh 19.0
target distance 12.0
model initialize at round 3189
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([15.61389323,  9.58536482,  2.81968999]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 13.351012914173726}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.819245541933787
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.52647123, 2.57989147, 5.98783692]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.6735452015266438}
episode index:3190
target Thresh 19.0
target distance 12.0
model initialize at round 3190
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([16.        ,  5.        ,  5.58669233]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8192114797856636
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.31271058, 6.90247862, 4.47165395]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.1343872407507076}
episode index:3191
target Thresh 19.0
target distance 1.0
model initialize at round 3191
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.33445702, 6.10950229, 0.42155116]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.951234812880457}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8192681177932496
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.33445702, 6.10950229, 0.42155116]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.951234812880457}
episode index:3192
target Thresh 19.0
target distance 4.0
model initialize at round 3192
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.        ,  6.        ,  5.01281166]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 4.47213595499958}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.819300525114463
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.2696125 , 10.40657496,  2.16325574]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.4878463912554751}
episode index:3193
target Thresh 19.0
target distance 12.0
model initialize at round 3193
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([16.        ,  5.        ,  0.37893742]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 12.165525060651712}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.8192412023549417
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.34559478, 3.80456038, 4.41434312]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0370938201342594}
episode index:3194
target Thresh 19.0
target distance 9.0
model initialize at round 3194
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([11.56830086, 11.87686913,  2.85275757]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.443139543706188}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8192233937514963
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.54972554, 2.31343906, 0.30408981]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.6328050318438131}
episode index:3195
target Thresh 19.0
target distance 12.0
model initialize at round 3195
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 3.9800108 , 11.36819842,  1.95922869]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 12.25796542514601}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8192128977399368
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.65342012,  6.33827889,  5.97693154]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.48430385169937157}
episode index:3196
target Thresh 19.0
target distance 2.0
model initialize at round 3196
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.70462157,  4.471636  ,  6.15437746]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.378566567024402}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8192601564519356
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.54921587,  3.50015911,  5.87119215]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.6730878492761129}
episode index:3197
target Thresh 19.0
target distance 6.0
model initialize at round 3197
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 8.        , 11.        ,  2.23927701]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 6.0827625303605615}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8192867736872567
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.26945965, 9.90516214, 3.38972108]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.28566190390685514}
episode index:3198
target Thresh 19.0
target distance 3.0
model initialize at round 3198
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.65920609,  3.28035592,  1.17738884]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 2.7409130573452516}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8193309466276483
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.7487097 ,  6.09284612,  2.89420354]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.2678940418584464}
episode index:3199
target Thresh 19.0
target distance 1.0
model initialize at round 3199
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.        ,  4.        ,  6.00579929]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.414213562373095}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.819384280706827
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.27250303,  5.09954962,  1.72261399]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.2901172674471554}
episode index:3200
target Thresh 19.0
target distance 2.0
model initialize at round 3200
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([3.94713819, 5.68213862, 2.61221135]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.9729123121142655}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8194314268234447
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.85893164, 6.56378213, 2.32902604]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.0274307046450415}
episode index:3201
target Thresh 19.0
target distance 10.0
model initialize at round 3201
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.        , 6.        , 3.99086642]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.198039027275694}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.8193682971276667
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.95500218,  4.61586513,  5.74308681]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.13636219348197}
episode index:3202
target Thresh 19.0
target distance 12.0
model initialize at round 3202
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        ,  3.        ,  1.49307698]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.000000000015325}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.819309119898218
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.49823436, 2.99548846, 5.52848268]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.49825478864070744}
episode index:3203
target Thresh 19.0
target distance 4.0
model initialize at round 3203
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.        , 4.        , 5.33586979]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8193502188151973
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.33703765, 7.11225523, 2.76949917]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.107975567915708}
episode index:3204
target Thresh 19.0
target distance 4.0
model initialize at round 3204
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([1.38911377, 2.48727022, 3.85785687]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.8644824990598887}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8193824791820031
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.60174844, 6.04586504, 1.00830095]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.603493813731921}
episode index:3205
target Thresh 19.0
target distance 12.0
model initialize at round 3205
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([3.        , 7.        , 3.79286349]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.649110641703045}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8193376714994949
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.11072913,  3.26057358,  0.11145449]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.28312458414157426}
episode index:3206
target Thresh 19.0
target distance 11.0
model initialize at round 3206
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([15.        ,  5.        ,  0.77132576]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8192907847165419
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.07462764, 3.65598142, 5.37310208]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.1342952115264158}
episode index:3207
target Thresh 19.0
target distance 11.0
model initialize at round 3207
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([3.        , 8.        , 3.88849056]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8192827808196989
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.57845115,  9.88740272,  5.90619341]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.4363273735091688}
episode index:3208
target Thresh 19.0
target distance 10.0
model initialize at round 3208
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([2.29156981, 5.65752078, 2.40667057]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 11.08132216758737}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8192772799810324
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.03448006, 11.80187118,  2.42437342]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.255080136202221}
episode index:3209
target Thresh 19.0
target distance 2.0
model initialize at round 3209
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.38546026, 6.42641563, 3.89337897]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.6893273355336138}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8193273805168638
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.77410773, 7.37683464, 1.61019367]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9937695082473739}
episode index:3210
target Thresh 19.0
target distance 3.0
model initialize at round 3210
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.28182508, 7.09051366, 1.71492976]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.2998290441944764}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.819371375730032
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.90081263, 9.54413035, 3.43174445]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.553096708174736}
episode index:3211
target Thresh 19.0
target distance 13.0
model initialize at round 3211
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([16.        ,  6.        ,  5.15219593]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 13.341664064125649}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8193512474334795
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.12945608, 8.58326749, 4.60352387]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.43637697499686534}
episode index:3212
target Thresh 19.0
target distance 5.0
model initialize at round 3212
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.45667378,  5.38017373,  5.99718428]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 3.42356215489361}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8193863280747723
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.62579727,  2.3162211 ,  1.14762836]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7011547684442757}
episode index:3213
target Thresh 19.0
target distance 9.0
model initialize at round 3213
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([12.39520001,  9.64021611,  5.28028083]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.703824228299034}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8193709754978371
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.20439415, 3.71582902, 0.73161306]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.7444381473617923}
episode index:3214
target Thresh 19.0
target distance 14.0
model initialize at round 3214
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([16.        , 10.        ,  1.95817297]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 15.231546211727819}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8193349184899061
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.67740273, 4.52098692, 2.84313424]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.612777581150368}
episode index:3215
target Thresh 19.0
target distance 11.0
model initialize at round 3215
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 4.601666  , 11.51677277,  1.32210224]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.427363913763141}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8193171970333941
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.75034041,  6.9557759 ,  5.05661979]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.2535462122412537}
episode index:3216
target Thresh 19.0
target distance 1.0
model initialize at round 3216
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.46188195, 10.6830599 ,  3.73366594]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.624517464693232}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.819373362032762
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.46188195, 10.6830599 ,  3.73366594]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.624517464693232}
episode index:3217
target Thresh 19.0
target distance 7.0
model initialize at round 3217
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.11828199,  9.74229039,  0.16598719]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 5.8095891015677825}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8194026174353259
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.90330973,  4.89054442,  5.59961658]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8957780770381261}
episode index:3218
target Thresh 19.0
target distance 8.0
model initialize at round 3218
at step 0:
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([15.        ,  5.        ,  0.62786501]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 10.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8194073117950696
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.88788183, 10.13214513,  5.21193848]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.2415740930882577}
episode index:3219
target Thresh 19.0
target distance 13.0
model initialize at round 3219
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.44217709,  2.36315163,  4.53966784]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 12.006251811336249}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8193648109513477
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.62041463, 5.64586934, 5.14144416]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.5191277064241663}
episode index:3220
target Thresh 19.0
target distance 14.0
model initialize at round 3220
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([2.        , 9.        , 5.15602899]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8193494984194799
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.3129695 , 10.06812911,  0.60736122]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.3202990515514928}
episode index:3221
target Thresh 19.0
target distance 2.0
model initialize at round 3221
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 3.        , 11.        ,  3.03060925]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8193993899469724
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.55193116, 9.26735174, 0.74742394]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5217687656010846}
episode index:3222
target Thresh 19.0
target distance 2.0
model initialize at round 3222
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.        , 2.        , 5.76043868]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8194492505147829
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.1879942 , 3.80869619, 3.47725337]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.8342365145878474}
episode index:3223
target Thresh 19.0
target distance 12.0
model initialize at round 3223
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([3.        , 6.        , 5.19759631]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8194245145107113
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.34679219, 11.70201372,  2.36574324]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9589075583523567}
episode index:3224
target Thresh 19.0
target distance 7.0
model initialize at round 3224
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 9.10289299, 10.14674347,  5.836169  ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 8.047705663040933}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8194344473034506
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.52205079,  6.74855366,  0.13705716]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8881261336944377}
episode index:3225
target Thresh 19.0
target distance 6.0
model initialize at round 3225
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 7.        , 11.        ,  2.97472107]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 7.211102551186508}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8194579760718806
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.21885408, 4.83946488, 6.12516515]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.2714196614188051}
episode index:3226
target Thresh 19.0
target distance 12.0
model initialize at round 3226
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.82422283,  8.67376536,  2.68543178]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.577606183466921}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8194264522282259
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.58407989, 2.265477  , 5.85357871]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.6415819172919697}
episode index:3227
target Thresh 19.0
target distance 11.0
model initialize at round 3227
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([3.00324885, 3.99930299, 0.79866427]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 13.0360382923974}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8194040685981464
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.81272347, 11.11079941,  2.2499965 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.21759826838685978}
episode index:3228
target Thresh 19.0
target distance 8.0
model initialize at round 3228
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.       , 2.       , 0.6094405]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 8.062257748299858}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8194220670355578
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.77936797, 9.47557974, 1.47669927]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.5689420871716007}
episode index:3229
target Thresh 19.0
target distance 11.0
model initialize at round 3229
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.        ,  5.        ,  4.65288997]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.704699910429124}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8194020352151763
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.05793609, 9.80955514, 4.1042222 ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.8116256047693278}
episode index:3230
target Thresh 19.0
target distance 13.0
model initialize at round 3230
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.31703571, 11.00439316,  4.14898229]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.37413626557139}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8193891659252301
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.21659475, 6.58140156, 3.88349983]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.6204361814357185}
episode index:3231
target Thresh 19.0
target distance 7.0
model initialize at round 3231
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 2.        , 10.        ,  3.48667943]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.071067811865475}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8194071522671466
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.69975728, 3.15314326, 4.35393821]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.33704383881795713}
episode index:3232
target Thresh 19.0
target distance 6.0
model initialize at round 3232
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.59180059, 9.57548698, 2.22147064]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.592661103256078}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8194278691615013
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.78340169, 11.70302357,  1.08872941]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.7356337138407171}
episode index:3233
target Thresh 19.0
target distance 11.0
model initialize at round 3233
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.       ,  2.       ,  5.4978981]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 12.52996408614167}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8193876778246761
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.63782533, 8.0614915 , 4.09967442]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.36735772374783754}
episode index:3234
target Thresh 19.0
target distance 12.0
model initialize at round 3234
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.        ,  8.        ,  1.43271416]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8193797107785615
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.07086452, 8.93269529, 3.45041701]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.09773282419255172}
episode index:3235
target Thresh 19.0
target distance 2.0
model initialize at round 3235
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([2.        , 8.        , 4.36188245]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 2.8284271247471935}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8194203814643223
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.00015124, 10.66457793,  1.7955116 ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6645779433718169}
episode index:3236
target Thresh 19.0
target distance 1.0
model initialize at round 3236
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.99999903, 6.99999982, 4.33503509]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0000001795419788}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8194700199006941
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.84926511, 8.45297164, 2.05184978]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9625146921348542}
episode index:3237
target Thresh 19.0
target distance 2.0
model initialize at round 3237
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.        , 8.        , 3.13265514]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.0000000003142584}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8195226851199959
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.27098598, 6.62051832, 5.13265514]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9573423724754547}
episode index:3238
target Thresh 19.0
target distance 11.0
model initialize at round 3238
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.93123398,  4.68156455,  2.62166762]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.423829928475598}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8194934963737278
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.06045978, 8.96934311, 3.78981455]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.971226775156329}
episode index:3239
target Thresh 19.0
target distance 11.0
model initialize at round 3239
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([15.        ,  7.        ,  1.02262991]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.180339887502313}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8194665858407898
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.43523936, 5.53393684, 0.19077684]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6888554616683052}
episode index:3240
target Thresh 19.0
target distance 12.0
model initialize at round 3240
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.00022494, 1.99996282, 0.84620159]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.999806665344963}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8194264693644023
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.51762928,  6.43160382,  5.73116322]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7454902593688048}
episode index:3241
target Thresh 19.0
target distance 10.0
model initialize at round 3241
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([4.86853978, 6.90871836, 0.32271927]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 9.58353840246688}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8193995961072431
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.91604776,  4.90179172,  5.77405151]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.2854461489405797}
episode index:3242
target Thresh 19.0
target distance 8.0
model initialize at round 3242
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.        , 8.        , 3.91456759]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8194068559275567
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.64911451, 10.61589196,  0.21545575]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.5202495643177278}
episode index:3243
target Thresh 19.0
target distance 1.0
model initialize at round 3243
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.        , 10.        ,  6.25485325]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0000000000019842}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8194594432099463
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.95853314, 11.37400021,  1.97166794]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0289129867083409}
episode index:3244
target Thresh 19.0
target distance 2.0
model initialize at round 3244
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([16.       ,  7.       ,  2.0440141]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8195089472336106
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.91568236,  5.5778829 ,  6.0440141 ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5840018014499366}
episode index:3245
target Thresh 19.0
target distance 5.0
model initialize at round 3245
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.12075492, 6.22694771, 3.36240625]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.874144483523303}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8195465230814748
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.7199234 , 9.47492434, 2.79603564]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5951028077264895}
episode index:3246
target Thresh 19.0
target distance 6.0
model initialize at round 3246
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([1.56516051, 5.87955883, 3.601668  ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.669864295152323}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8195754638650911
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.26320438, 10.43486819,  2.75211208]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6234184106633996}
episode index:3247
target Thresh 19.0
target distance 4.0
model initialize at round 3247
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 9.62962098, 10.57965266,  0.7575404 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.4073613285529554}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8196218688946892
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.06263984, 10.27285924,  0.47435509]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.1863295274118795}
episode index:3248
target Thresh 19.0
target distance 2.0
model initialize at round 3248
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.        , 6.        , 5.05627751]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8196712619790554
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.06502111, 7.3171487 , 2.77309221]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.685939971575319}
episode index:3249
target Thresh 19.0
target distance 10.0
model initialize at round 3249
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([12.        , 11.        ,  2.28894269]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8196784225733339
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.8588234 , 10.10308681,  4.87301616]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.17480767569294955}
episode index:3250
target Thresh 19.0
target distance 12.0
model initialize at round 3250
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.55255106, 5.6495947 , 1.40627116]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 11.737803292775553}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8196584412951518
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.31511367, 10.99583998,  0.85760315]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.31514112974968067}
episode index:3251
target Thresh 19.0
target distance 14.0
model initialize at round 3251
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.86058762,  6.76140099,  4.97867656]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.98064895469076}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8196315793420027
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.56674948, 5.2806088 , 4.14682349]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.6324130542841062}
episode index:3252
target Thresh 19.0
target distance 5.0
model initialize at round 3252
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([16.69611996,  7.4677457 ,  6.14882231]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.3925303723167515}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8196632771947804
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.85481854,  4.85602292,  3.29926639]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.2097479010236378}
episode index:3253
target Thresh 19.0
target distance 12.0
model initialize at round 3253
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.        , 8.        , 3.46812367]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.369316876876931}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8196341797942773
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.37514175,  5.29147394,  4.6362706 ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6894961135574696}
episode index:3254
target Thresh 19.0
target distance 8.0
model initialize at round 3254
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([3.        , 6.        , 4.12687683]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 9.433981132056637}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8196237478314549
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.98884633, 11.53038101,  1.86139437]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5304982801637215}
episode index:3255
target Thresh 19.0
target distance 1.0
model initialize at round 3255
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.41193006, 2.9158579 , 1.58543032]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.4204358025294269}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8196791459433003
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.41193006, 2.9158579 , 1.58543032]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.4204358025294269}
episode index:3256
target Thresh 19.0
target distance 1.0
model initialize at round 3256
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.        ,  6.        ,  3.26998699]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0000000000015996}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8197314397271678
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.26546119,  4.48866855,  5.26998699]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8949900064830142}
episode index:3257
target Thresh 19.0
target distance 10.0
model initialize at round 3257
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.        , 9.        , 4.05563617]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8197234234116112
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.61923762,  6.81016179,  6.07333902]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0197143533624715}
episode index:3258
target Thresh 19.0
target distance 2.0
model initialize at round 3258
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([16.        ,  9.        ,  1.23952931]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.236067977506072}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8197726337757071
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.89566751,  7.9265932 ,  5.23952931]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8986705990009273}
episode index:3259
target Thresh 19.0
target distance 4.0
model initialize at round 3259
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([1.76826379, 8.33306079, 5.58425522]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.6382469093116185}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8198099673694572
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.82795627, 6.00166814, 5.01788448]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.17205181921182167}
episode index:3260
target Thresh 19.0
target distance 4.0
model initialize at round 3260
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([1.49122655e+01, 4.58572998e+00, 1.20865663e-02]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.8052033464071475}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8198561154935389
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.39302675,  2.97288882,  6.01208657]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.1467036150767074}
episode index:3261
target Thresh 19.0
target distance 3.0
model initialize at round 3261
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.37443718,  9.64078788,  2.35643274]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.4962574693257893}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8199022353232466
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.19241877, 11.11159939,  2.07324743]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.2224396739511528}
episode index:3262
target Thresh 19.0
target distance 9.0
model initialize at round 3262
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([15.08073363,  3.2901174 ,  1.88348549]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.302602563901837}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8199198935480937
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.86751958, 10.47937632,  2.75074426]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.0117505751994071}
episode index:3263
target Thresh 19.0
target distance 2.0
model initialize at round 3263
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.65569704, 10.69833121,  0.82977646]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.45776477210614547}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.819975065149335
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.65569704, 10.69833121,  0.82977646]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.45776477210614547}
episode index:3264
target Thresh 19.0
target distance 5.0
model initialize at round 3264
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([1.60000000e+01, 3.00000000e+00, 1.23082956e-02]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8200065412991907
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.72658078,  7.62678967,  3.44593768]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.46264891750685055}
episode index:3265
target Thresh 19.0
target distance 4.0
model initialize at round 3265
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 2.99998856, 10.00000355,  3.85086405]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 4.000003546652328}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8200495876766251
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.95627791, 6.66803537, 5.56767875]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.6694646176032948}
episode index:3266
target Thresh 19.0
target distance 4.0
model initialize at round 3266
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([13.        , 11.        ,  1.53855246]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.00000000001064}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8200896674018235
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.54104507, 10.09102752,  5.25536715]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.057809409040213}
episode index:3267
target Thresh 19.0
target distance 1.0
model initialize at round 3267
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.63014013, 6.41794991, 1.26098269]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7561472790333297}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8201447195231816
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.63014013, 6.41794991, 1.26098269]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7561472790333297}
episode index:3268
target Thresh 19.0
target distance 12.0
model initialize at round 3268
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.       , 6.       , 5.6077435]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8201156083627149
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.07950149,  5.95445857,  0.49270512]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.9577638777781002}
episode index:3269
target Thresh 19.0
target distance 3.0
model initialize at round 3269
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.28074321,  7.09115011,  1.71563452]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 2.2986976903493517}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8201585687301881
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.9642008 ,  9.5736089 ,  3.43244921]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5747249327750876}
episode index:3270
target Thresh 19.0
target distance 10.0
model initialize at round 3270
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.0000001 ,  1.99999996,  0.6087901 ]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 11.661903893686416}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8201250605243006
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.00393007, 7.90446815, 5.77693702]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.09561265516302613}
episode index:3271
target Thresh 19.0
target distance 12.0
model initialize at round 3271
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([16.        ,  4.        ,  0.86177462]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8200788645273707
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.1065327 , 6.89318687, 5.46355093]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8995176515727393}
episode index:3272
target Thresh 19.0
target distance 7.0
model initialize at round 3272
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.        , 11.        ,  2.95937937]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8200937337446763
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.32687546,  3.95480885,  5.82663755]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.3299845517850546}
episode index:3273
target Thresh 19.0
target distance 12.0
model initialize at round 3273
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 3.        , 11.        ,  3.50266278]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 13.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8200714607943363
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.0326593 ,  6.46209337,  4.95399501]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.4632460554422211}
episode index:3274
target Thresh 19.0
target distance 13.0
model initialize at round 3274
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.22706537,  8.49497131,  3.05796945]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.405067593247184}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8200402116559559
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.58299171, 3.31173252, 6.22611638]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6611024845531341}
episode index:3275
target Thresh 19.0
target distance 3.0
model initialize at round 3275
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.        , 6.        , 1.51943844]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8200801841340524
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.87447788, 2.86359118, 5.23625313]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.18537305152467437}
episode index:3276
target Thresh 19.0
target distance 1.0
model initialize at round 3276
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.99999992, 8.00000007, 3.46220863]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0000000696157336}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.820132036381799
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.56704882, 6.37367684, 5.46220863]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.7613983366099453}
episode index:3277
target Thresh 19.0
target distance 1.0
model initialize at round 3277
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.        , 7.        , 4.48659444]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.414213562376559}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8201808368588028
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.64106418, 8.54273556, 2.20340687]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.6506894915717704}
episode index:3278
target Thresh 19.0
target distance 11.0
model initialize at round 3278
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([2.        , 9.        , 4.36951351]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8201776490062144
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.04641378, 11.43324317,  2.38721636]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.4357222567710101}
episode index:3279
target Thresh 19.0
target distance 1.0
model initialize at round 3279
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.38641368, 9.7073495 , 1.48177498]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.416964081244801}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8202294241132245
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.06759593, 10.75179704,  3.48177498]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.754829783079589}
episode index:3280
target Thresh 19.0
target distance 12.0
model initialize at round 3280
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([16.        ,  6.        ,  6.04198313]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8202094575978603
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.6761517 , 8.53258627, 5.49331524]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5686416410372722}
episode index:3281
target Thresh 19.0
target distance 2.0
model initialize at round 3281
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.60721212,  6.50074395,  0.70881527]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6352471299597756}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8202642383846983
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.60721212,  6.50074395,  0.70881527]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6352471299597756}
episode index:3282
target Thresh 19.0
target distance 4.0
model initialize at round 3282
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.        ,  9.        ,  3.33326101]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8203040573952117
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.62851217,  5.78848988,  0.7668904 ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0083371677073625}
episode index:3283
target Thresh 19.0
target distance 12.0
model initialize at round 3283
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([3.        , 8.        , 4.78210068]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8202887526109273
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.50788652,  9.44845995,  0.23343291]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7391698756377592}
episode index:3284
target Thresh 19.0
target distance 11.0
model initialize at round 3284
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([15.0051567 ,  2.0022321 ,  1.41850489]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 11.005156925587114}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.8202232175468562
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.21137236, 2.68230374, 0.88753997]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.7142945243570918}
episode index:3285
target Thresh 19.0
target distance 8.0
model initialize at round 3285
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([5.25849954, 9.25510207, 0.4728586 ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 7.363445329930244}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8202433519516551
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.96251265, 2.64330167, 5.62330268]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.644393004587623}
episode index:3286
target Thresh 19.0
target distance 10.0
model initialize at round 3286
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.        , 3.        , 0.08851212]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.770329614267911}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8202035605716482
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.69751238,  7.3916797 ,  4.97347374]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.4948855863599003}
episode index:3287
target Thresh 19.0
target distance 12.0
model initialize at round 3287
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 4.        , 10.        ,  3.73321354]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.369316876853134}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8201954738085923
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.01732863,  7.60587222,  5.75091639]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.1544367360728167}
episode index:3288
target Thresh 19.0
target distance 12.0
model initialize at round 3288
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([12.88525294, 10.26084379,  3.30477273]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.084669812088956}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8201732715040995
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.45855672, 3.33148008, 4.75610497]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.6348542090064988}
episode index:3289
target Thresh 19.0
target distance 6.0
model initialize at round 3289
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([1.45518022, 5.66777236, 3.74358416]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 5.908363504438862}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8202016435332725
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.53514142, 10.55275816,  2.89402823]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6450726827388061}
episode index:3290
target Thresh 19.0
target distance 9.0
model initialize at round 3290
at step 0:
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([1.60000000e+01, 2.00000000e+00, 7.55041441e-03]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 12.727922061357855}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8201911532559325
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.5498547 , 10.61408712,  4.02525326]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.6717655361038604}
episode index:3291
target Thresh 19.0
target distance 11.0
model initialize at round 3291
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.        , 11.        ,  1.59473341]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8201904593872634
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.01046871, 8.43716614, 3.89562157]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.4372914671040942}
episode index:3292
target Thresh 19.0
target distance 2.0
model initialize at round 3292
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.        , 5.        , 1.99460095]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 2.0}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8202390198308143
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.44771619, 3.3797547 , 5.99460095]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.5870804198153301}
episode index:3293
target Thresh 19.0
target distance 9.0
model initialize at round 3293
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([15.        ,  2.        ,  0.80399197]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 11.401754250992015}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.82023582883154
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.85950485, 10.28735643,  5.10488013]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.1165166566067495}
episode index:3294
target Thresh 19.0
target distance 5.0
model initialize at round 3294
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 4.        , 10.        ,  3.48202741]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 5.385164807181461}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8202697679875569
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.54279902, 5.74349121, 4.9156568 ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8728183714012531}
episode index:3295
target Thresh 19.0
target distance 12.0
model initialize at round 3295
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.79175179, 3.18275602, 1.7875368 ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 11.354611555294962}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.8202119869994549
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.35868264,  4.46927526,  5.82294238]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6405638011930348}
episode index:3296
target Thresh 19.0
target distance 10.0
model initialize at round 3296
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([10.45734518, 11.63905351,  3.74167538]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 8.85953107987877}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8202214640343639
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.95444731, 9.60844256, 4.32574884]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.6101453942196821}
episode index:3297
target Thresh 19.0
target distance 6.0
model initialize at round 3297
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([16.        , 11.        ,  1.16243714]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 6.082762530298692}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8202442405019881
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.44008764,  5.59216881,  4.31288122]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8149636440915069}
episode index:3298
target Thresh 19.0
target distance 3.0
model initialize at round 3298
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 2.        , 11.        ,  2.48366478]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.000000000001151}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8202838724539123
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.08309917, 7.71600258, 6.20047947]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.2959054016944143}
episode index:3299
target Thresh 19.0
target distance 10.0
model initialize at round 3299
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([3.        , 8.        , 5.60674524]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8202831521706224
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.46063167, 11.19814068,  1.62444809]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.5746111099057201}
episode index:3300
target Thresh 19.0
target distance 12.0
model initialize at round 3300
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.39617565, 10.51003286,  3.84369552]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 12.266254355858209}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8202679325382791
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.92857489, 4.55835246, 5.57821306]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.5629023145327106}
episode index:3301
target Thresh 19.0
target distance 4.0
model initialize at round 3301
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.68288267, 7.01166463, 1.01693123]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 4.069371183025996}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8203046412653725
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.82125339, 3.49274093, 0.45056062]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.9577320912179919}
episode index:3302
target Thresh 19.0
target distance 9.0
model initialize at round 3302
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([2.61545308, 2.95677474, 3.54690671]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 9.679195113837187}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.820301439093697
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.08714732, 11.34841631,  1.56460956]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9770844061980971}
episode index:3303
target Thresh 19.0
target distance 1.0
model initialize at round 3303
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.30862725, 10.17400015,  6.0799675 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0771592426762324}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8203558272779907
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.30862725, 10.17400015,  6.0799675 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0771592426762324}
episode index:3304
target Thresh 19.0
target distance 9.0
model initialize at round 3304
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.92071024,  4.68110119,  2.62792677]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 10.93195103348203}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8203406040763348
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.54292513, 10.5417753 ,  4.36244419]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7104488557002681}
episode index:3305
target Thresh 19.0
target distance 12.0
model initialize at round 3305
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.91364598,  1.13169526,  5.03845024]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.94813325550886}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.8202579712990554
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.47870688, 1.89204886, 5.6579294 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.49072775331220964}
episode index:3306
target Thresh 19.0
target distance 3.0
model initialize at round 3306
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.39216319, 6.76329772, 4.52294302]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.5234017689084514}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8202946275367639
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.61089744, 9.89235981, 3.95657241]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.4037167467401596}
episode index:3307
target Thresh 19.0
target distance 7.0
model initialize at round 3307
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([13.92223519,  8.70740142,  5.02737713]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 6.073840432417623}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8203200469586115
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.01558542,  3.22983722,  6.17782121]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.23036504017923157}
episode index:3308
target Thresh 19.0
target distance 5.0
model initialize at round 3308
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([10.01149819, 11.86606751,  1.94545334]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 7.093581237573884}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8203346815206576
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.8419656 ,  6.61978224,  4.81271211]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6396130816353676}
episode index:3309
target Thresh 19.0
target distance 1.0
model initialize at round 3309
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.        , 9.        , 4.73649406]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0000000001092253}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8203859399250318
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.427551  , 8.111564  , 0.45330876]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9859606422805637}
episode index:3310
target Thresh 19.0
target distance 1.0
model initialize at round 3310
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.        , 6.        , 2.08794004]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.414213562373095}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8204341773336925
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.13835544, 4.52149457, 6.08794004]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.49810609026861485}
episode index:3311
target Thresh 19.0
target distance 2.0
model initialize at round 3311
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([12.51595536,  9.6512139 ,  5.945714  ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.5244803174194246}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8204853747439177
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.85332141, 10.67328136,  1.66252869]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6890735794179239}
episode index:3312
target Thresh 19.0
target distance 6.0
model initialize at round 3312
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.51963725, 6.60073896, 2.26690513]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 4.4254091428921924}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8205134555989554
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.15473646, 10.7364198 ,  1.41734921]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.3056434065259161}
episode index:3313
target Thresh 19.0
target distance 14.0
model initialize at round 3313
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([2.00071169, 9.99728901, 5.97911787]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 14.558790902131364}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8204868381922128
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.77835041,  6.79276931,  5.1472648 ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8231717484839014}
episode index:3314
target Thresh 19.0
target distance 11.0
model initialize at round 3314
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.10309701,  7.41837193,  3.15087354]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.111755743972394}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8204716213920961
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.46222902, 7.18781241, 4.88539108]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.4989280167598146}
episode index:3315
target Thresh 19.0
target distance 11.0
model initialize at round 3315
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.       , 8.       , 4.6558485]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.70469991076634}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.820436330702593
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.80529011,  3.63448158,  5.54081012]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.41414448961583367}
episode index:3316
target Thresh 19.0
target distance 5.0
model initialize at round 3316
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 5.        , 11.        ,  2.45945299]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.820467174345561
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.22949022, 6.20095871, 5.89308237]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.30504125356970585}
episode index:3317
target Thresh 19.0
target distance 9.0
model initialize at round 3317
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([13.11987285,  2.27845302,  3.99132311]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 11.258684254933758}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8204543214778859
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.77254516, 11.17327378,  3.72584066]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.7917384860422486}
episode index:3318
target Thresh 19.0
target distance 3.0
model initialize at round 3318
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.72190625,  4.3964995 ,  2.00850749]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.758511437127232}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8204994690158558
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.32662568,  5.74027181,  1.72532218]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.4173045284416114}
episode index:3319
target Thresh 19.0
target distance 13.0
model initialize at round 3319
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([15.        ,  4.        ,  5.58518362]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8204599960088839
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.35592054, 5.90995933, 4.18695993]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.6503427343123213}
episode index:3320
target Thresh 19.0
target distance 2.0
model initialize at round 3320
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([13.99999553,  7.99999837,  4.5012157 ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 2.8284291330932994}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8205110468983723
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.18218912,  6.80216433,  0.21803039]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.145548881203601}
episode index:3321
target Thresh 19.0
target distance 11.0
model initialize at round 3321
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.11704838,  4.2590496 ,  3.33443642]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.195090668396382}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.8204498726340869
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.23546689, 3.43615662, 5.08665681]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8801951309446887}
episode index:3322
target Thresh 19.0
target distance 7.0
model initialize at round 3322
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.40439079,  5.57405138,  2.9425379 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.604750832097127}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8204778796683478
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.76260906, 10.01161754,  2.09298198]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.2483879443768813}
episode index:3323
target Thresh 19.0
target distance 6.0
model initialize at round 3323
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.        , 5.        , 5.17811656]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8205004008400056
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.3662177 , 11.05052375,  2.04537485]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.635792935673368}
episode index:3324
target Thresh 19.0
target distance 3.0
model initialize at round 3324
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.        , 7.        , 1.52140921]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 3.162277660419277}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8205396458472417
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.51965808, 4.24206195, 5.2382239 ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.5378869301496244}
episode index:3325
target Thresh 19.0
target distance 3.0
model initialize at round 3325
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([12.33905089, 10.06927309,  0.32926958]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.9753725961986273}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8205846727125914
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.54714208,  8.96490534,  0.04608428]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.45421573409551047}
episode index:3326
target Thresh 19.0
target distance 4.0
model initialize at round 3326
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([13.12713517,  5.80448685,  3.633834  ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 4.297052086930872}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8206153790611682
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.16478205,  8.77455525,  0.78427808]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.2792462325234442}
episode index:3327
target Thresh 19.0
target distance 1.0
model initialize at round 3327
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.57385572,  9.40391134,  0.64795082]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.701751202162003}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8206692806900561
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.57385572,  9.40391134,  0.64795082]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.701751202162003}
episode index:3328
target Thresh 19.0
target distance 6.0
model initialize at round 3328
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 4.        , 10.        ,  2.61429167]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 6.08276253029822}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8206999431754084
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.45824795, 4.93634577, 6.04792105]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0424656254213527}
episode index:3329
target Thresh 19.0
target distance 14.0
model initialize at round 3329
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([15.62060033,  8.36035248,  5.49499989]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.688364117254459}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.820680129164606
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.3952844 , 6.6023055 , 4.94633212]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5607233487269084}
episode index:3330
target Thresh 19.0
target distance 7.0
model initialize at round 3330
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.        ,  4.        ,  0.93430155]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.280109890040588}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8207025422913229
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.98857245, 10.68447232,  4.08474563]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.3157345512526743}
episode index:3331
target Thresh 19.0
target distance 7.0
model initialize at round 3331
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.        , 4.        , 5.43920422]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8207249419647824
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.53164838, 10.41583886,  2.30646299]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7487305736788405}
episode index:3332
target Thresh 19.0
target distance 12.0
model initialize at round 3332
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.3998767 ,  5.47847007,  4.46666646]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.34026756210137}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8207051382879864
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.52711676, 10.54758135,  3.91799858]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7235080477051782}
episode index:3333
target Thresh 19.0
target distance 7.0
model initialize at round 3333
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.08158775,  7.58971529,  5.14515328]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.972499646022086}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8207248382680188
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.58985082, 10.45808659,  4.01241194]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.8009957153988638}
episode index:3334
target Thresh 19.0
target distance 3.0
model initialize at round 3334
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.       , 10.       ,  3.5267005]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 3.1622776601683795}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8207667786493477
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.53334437,  7.24464881,  5.24351519]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5268970633611774}
episode index:3335
target Thresh 19.0
target distance 12.0
model initialize at round 3335
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([1.71525162, 7.65572982, 2.75110787]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 12.568529893030771}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8207295024021655
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.74941462,  4.80954693,  5.63606949]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.3147465689966355}
episode index:3336
target Thresh 19.0
target distance 7.0
model initialize at round 3336
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 1.48772236, 10.59415929,  2.89172029]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.52323215312625}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8207491773704945
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.26268567, 11.37384426,  1.75897907]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.8266752408913366}
episode index:3337
target Thresh 19.0
target distance 6.0
model initialize at round 3337
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.00155748, 4.99907892, 0.47593659]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.484831549569885}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8207583770690338
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.62385455, 11.4101904 ,  1.06000993]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5565443096255682}
episode index:3338
target Thresh 19.0
target distance 8.0
model initialize at round 3338
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 7.        , 11.        ,  5.63860583]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 8.062257748298531}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8207753769629931
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.53126576,  9.33582857,  0.2226793 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8129178817894733}
episode index:3339
target Thresh 19.0
target distance 6.0
model initialize at round 3339
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.64240203, 7.44445903, 6.11403179]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 4.647180682620652}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.820803143990095
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.41673901, 3.02813579, 5.26447587]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.5839392145234349}
episode index:3340
target Thresh 19.0
target distance 12.0
model initialize at round 3340
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.17535815,  2.03774826,  4.7588768 ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.561295993376769}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.820761764598003
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.25103723, 5.94703442, 5.36065311]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9797417412250354}
episode index:3341
target Thresh 19.0
target distance 12.0
model initialize at round 3341
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.3162966 , 11.86867288,  2.99399108]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.469546942268517}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8207633824493608
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.82459427, 10.23550458,  3.29487923]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8575652257888057}
episode index:3342
target Thresh 19.0
target distance 2.0
model initialize at round 3342
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([14.24578426,  4.33507412,  5.86895514]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.7859304384275825}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8208081134148262
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.42806361,  3.58413495,  5.58576983]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.7071456515713751}
episode index:3343
target Thresh 19.0
target distance 1.0
model initialize at round 3343
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.0000003 ,  9.00000146,  2.38120648]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.0000002959543837}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.82085870907469
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.37057965,  8.61914711,  4.38120648]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.735675812868317}
episode index:3344
target Thresh 19.0
target distance 7.0
model initialize at round 3344
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.65774548, 8.9847459 , 4.7991643 ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 6.133419169422075}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8208836786908139
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.66878162, 3.19282205, 5.94960837]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.3832570416392648}
episode index:3345
target Thresh 19.0
target distance 14.0
model initialize at round 3345
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([16.        ,  5.        ,  1.29347914]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 14.000000000000002}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8208464789117819
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.63294913, 5.41230435, 4.17844076]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.7553935930606731}
episode index:3346
target Thresh 19.0
target distance 2.0
model initialize at round 3346
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.        ,  8.        ,  3.67771494]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 2.236067977499928}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8208940598861136
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.47115463,  6.94477118,  1.39452964]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 1.055736360687423}
episode index:3347
target Thresh 19.0
target distance 13.0
model initialize at round 3347
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.35438909, 10.64736305,  4.3626895 ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 11.372828606956482}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8208883061613982
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.83301576, 9.36755289, 4.38039234]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.654120080963058}
episode index:3348
target Thresh 19.0
target distance 4.0
model initialize at round 3348
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([1.13093444, 6.8614886 , 3.60199857]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 4.252268930246202}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8209187201322153
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.26236616, 9.73833787, 0.75244265]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.37054429427644997}
episode index:3349
target Thresh 19.0
target distance 12.0
model initialize at round 3349
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([1.79468505, 4.17456543, 3.37911439]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 13.206468722898313}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8208693889383174
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.34717469,  4.82154514,  5.6977054 ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.8918893899511436}
episode index:3350
target Thresh 19.0
target distance 2.0
model initialize at round 3350
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.        ,  4.        ,  5.19417739]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 2.2360679775002397}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8209169062797266
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.45433392,  5.59205409,  2.91099208]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.6106055825223924}
episode index:3351
target Thresh 19.0
target distance 10.0
model initialize at round 3351
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 5.        , 11.        ,  5.36879063]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.049875621120986}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8209184730212409
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.0460745 ,  9.41384004,  5.66967878]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.587967989138816}
episode index:3352
target Thresh 19.0
target distance 2.0
model initialize at round 3352
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 9.33883982, 11.88872732,  2.36033735]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.8839557819747057}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8209659473806142
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.35394542, 10.449501  ,  0.07715204]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8487848250647404}
episode index:3353
target Thresh 19.0
target distance 2.0
model initialize at round 3353
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.99999965,  8.9999994 ,  5.18932986]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.8284272980988736}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8210047142567382
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.36728001, 11.35092877,  2.62295925]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.5079818917278717}
episode index:3354
target Thresh 19.0
target distance 3.0
model initialize at round 3354
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([15.00000001,  8.        ,  0.88656729]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.162277665338859}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8210463212003277
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.45489664, 11.28696281,  2.60338198]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5378462652214481}
episode index:3355
target Thresh 19.0
target distance 5.0
model initialize at round 3355
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.        , 4.        , 5.48747683]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.821076624648846
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.68399958, 8.75389786, 2.63792078]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.40052780965763807}
episode index:3356
target Thresh 19.0
target distance 9.0
model initialize at round 3356
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 6.        , 11.        ,  4.98231387]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8210908245857301
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.38932253, 10.88050871,  1.56638734]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6222581019037156}
episode index:3357
target Thresh 19.0
target distance 3.0
model initialize at round 3357
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.00000061,  9.00000055,  1.74220436]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.6055513930627057}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8211295080953532
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.11257594,  6.42948798,  5.45901906]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9858911664601433}
episode index:3358
target Thresh 19.0
target distance 11.0
model initialize at round 3358
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 6.57370979, 11.5963    ,  1.37219733]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.106143925273518}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8211189539520701
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.06776909,  4.23987887,  5.38990018]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.9625987414945273}
episode index:3359
target Thresh 19.0
target distance 2.0
model initialize at round 3359
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 9.        , 11.        ,  3.33078337]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.000000000000058}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8211662697395844
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.68178029, 11.06326069,  1.04759806]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.32444675762499836}
episode index:3360
target Thresh 19.0
target distance 7.0
model initialize at round 3360
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 9.        , 11.        ,  0.69726485]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 8.602325267042628}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8211702410254251
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.09211622, 3.83569592, 5.28133831]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9226314509983198}
episode index:3361
target Thresh 19.0
target distance 7.0
model initialize at round 3361
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 8.       , 11.       ,  5.2159934]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.821179249808908
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.53421646,  5.38661899,  5.80006687]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6053334215139204}
episode index:3362
target Thresh 19.0
target distance 8.0
model initialize at round 3362
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.4798095 ,  3.72208651,  3.70814645]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.293712493873116}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8211986389322821
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.22938918, 10.35317921,  2.57540522]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.6862918650776063}
episode index:3363
target Thresh 19.0
target distance 12.0
model initialize at round 3363
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.3934245 ,  9.49869844,  4.4540503 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.81615196557162}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8211766334196184
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.70548488, 2.69838582, 5.90538254]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9926991829272678}
episode index:3364
target Thresh 19.0
target distance 11.0
model initialize at round 3364
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([4.02684964, 5.68275583, 2.56484193]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 12.193568567699813}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8211591493426281
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.34423756, 10.44093321,  0.01617416]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8617308430740844}
episode index:3365
target Thresh 19.0
target distance 2.0
model initialize at round 3365
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.6709336 ,  5.90498964,  0.95320052]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.199822793932729}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8212093100231561
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.05857508,  7.47260091,  2.95320052]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5306419090858194}
episode index:3366
target Thresh 19.0
target distance 12.0
model initialize at round 3366
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([2.        , 5.        , 4.58181596]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.16552506059644}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8211681295316168
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.2908579 ,  3.45022161,  5.18359227]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.839989293912279}
episode index:3367
target Thresh 19.0
target distance 5.0
model initialize at round 3367
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.60751983, 9.50173492, 0.70943182]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.5425430756943905}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8212038516277775
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.07923759, 5.95114406, 0.14306121]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.954438907038511}
episode index:3368
target Thresh 19.0
target distance 5.0
model initialize at round 3368
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.67200698,  8.96616122,  4.81308389]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 4.197634501496028}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8212395525175884
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.66170509, 10.27842262,  4.24671327]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9790441962938254}
episode index:3369
target Thresh 19.0
target distance 11.0
model initialize at round 3369
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 5.      , 11.      ,  5.302598]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 11.000000000000995}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8212459927671037
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.0850505 , 11.22311294,  1.60348616]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9417600411416321}
episode index:3370
target Thresh 19.0
target distance 11.0
model initialize at round 3370
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([15.        ,  9.        ,  1.01384228]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 12.08304597359457}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8212262577610036
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.84710517, 4.81661365, 0.46517452]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.1766244175324538}
episode index:3371
target Thresh 19.0
target distance 1.0
model initialize at round 3371
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.        , 10.        ,  5.84118104]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0000000000345375}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8212763092859854
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.83617478, 10.87924351,  1.55799573]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8448493323774291}
episode index:3372
target Thresh 19.0
target distance 7.0
model initialize at round 3372
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.09894945, 8.57855848, 5.15741587]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 5.650858944648819}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8213009478171811
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.35493606, 3.13046246, 0.02467464]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.37815348742501154}
episode index:3373
target Thresh 19.0
target distance 5.0
model initialize at round 3373
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.46889146, 7.30136224, 4.57966948]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 4.326843688568589}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8213337766257436
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.10304188, 3.12280362, 6.01329886]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.16030707052254756}
episode index:3374
target Thresh 19.0
target distance 2.0
model initialize at round 3374
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.        , 10.        ,  3.37398505]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 2.2360679774998844}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8213808184697065
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.67708925, 10.13580405,  1.09079974]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9225540622305265}
episode index:3375
target Thresh 19.0
target distance 2.0
model initialize at round 3375
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 5.06628665, 10.11782216,  1.81898945]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 2.2467261217606858}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8214307649097331
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.47558096, 10.66730931,  3.81898945]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.5803967161030822}
episode index:3376
target Thresh 19.0
target distance 10.0
model initialize at round 3376
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.        , 6.        , 4.75597262]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8214087753715105
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.03426447,  7.65854471,  6.20730426]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.34317017245683146}
episode index:3377
target Thresh 19.0
target distance 4.0
model initialize at round 3377
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.00000207,  3.99999963,  0.83403891]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.123106485288986}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8214499794078125
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.62872848,  7.31150712,  2.5508536 ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9323743611099048}
episode index:3378
target Thresh 19.0
target distance 3.0
model initialize at round 3378
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 2.        , 11.        ,  1.86384362]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.000000001234314}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8214883162147057
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.99946462, 7.66776704, 5.58065831]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.33223338793507884}
episode index:3379
target Thresh 19.0
target distance 12.0
model initialize at round 3379
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([15.00000147, 10.99999713,  6.19629383]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 12.649111130031391}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8214777214882538
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.05561615, 7.73014796, 3.93081137]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.7322630654932553}
episode index:3380
target Thresh 19.0
target distance 3.0
model initialize at round 3380
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.23111819,  6.79932145,  4.40667987]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 2.9029949701395483}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8215188685715166
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.62546676,  3.95522409,  6.12349456]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6270674185780457}
episode index:3381
target Thresh 19.0
target distance 6.0
model initialize at round 3381
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.40129691,  5.02662314,  1.02899664]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.9868414324033745}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8215433698152887
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.25142918, 11.42393224,  2.17944072]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.49288454303675683}
episode index:3382
target Thresh 19.0
target distance 11.0
model initialize at round 3382
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([16.34627431,  4.00988651,  1.65358704]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 13.31659574009979}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8215258703605243
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.77800087, 9.56955989, 5.38810458]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9642011371676273}
episode index:3383
target Thresh 19.0
target distance 1.0
model initialize at round 3383
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([2.9999999 , 6.99999999, 4.25065374]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.4142136289654188}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8215756558598267
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.84805052, 5.54683442, 6.25065374]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.4779620122034842}
episode index:3384
target Thresh 19.0
target distance 9.0
model initialize at round 3384
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 5.00000007, 10.9999992 ,  5.81293654]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.219544212055466}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8215844836634412
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.80719889,  8.85717831,  0.1138247 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.23993812232469874}
episode index:3385
target Thresh 19.0
target distance 10.0
model initialize at round 3385
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([ 4.33302437, 11.64961203,  2.38159335]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 11.733190447349164}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8215603005239623
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.50068258,  4.96401639,  5.83292558]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5019739736084115}
episode index:3386
target Thresh 19.0
target distance 14.0
model initialize at round 3386
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 2.        , 10.        ,  5.03863263]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 14.31782106327666}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8215383376641477
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.25028791,  7.62281814,  0.20677932]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6712275853247532}
episode index:3387
target Thresh 19.0
target distance 10.0
model initialize at round 3387
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([12.        , 11.        ,  2.18051347]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 10.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8215446555082209
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.82585843, 10.75870686,  4.76458694]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.29756960252630377}
episode index:3388
target Thresh 19.0
target distance 8.0
model initialize at round 3388
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([13.45936744, 10.16176689,  0.48863095]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 7.599060385671287}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.821556019833725
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.02976323,  2.63682674,  5.35588972]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.0359798307997545}
episode index:3389
target Thresh 19.0
target distance 12.0
model initialize at round 3389
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([3.        , 7.        , 5.42866135]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8215408242956633
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.69296711,  8.61490635,  0.87999358]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6872983506086349}
episode index:3390
target Thresh 19.0
target distance 12.0
model initialize at round 3390
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([16.        ,  8.        ,  5.46073127]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 12.041594578791587}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8215211187406377
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.52113173, 7.12575194, 4.9120635 ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.4951044003142922}
episode index:3391
target Thresh 19.0
target distance 1.0
model initialize at round 3391
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.43558449, 11.67212993,  1.44787949]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8009322605976041}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8215737363353486
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.43558449, 11.67212993,  1.44787949]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8009322605976041}
episode index:3392
target Thresh 19.0
target distance 14.0
model initialize at round 3392
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.33313273,  7.2322533 ,  4.01314878]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.533519767084517}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.821545267900224
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.80211625, 5.63857287, 5.18129571]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 1.0252637627912777}
episode index:3393
target Thresh 19.0
target distance 7.0
model initialize at round 3393
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 9.        , 11.        ,  1.85759228]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 9.219544457292887}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8215515725335428
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.17014828, 4.28090967, 4.44166563]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.3284214953315043}
episode index:3394
target Thresh 19.0
target distance 6.0
model initialize at round 3394
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 7.        , 11.        ,  3.65589702]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8215733064604132
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.3862737 , 10.60136337,  0.52315508]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.5550842580627812}
episode index:3395
target Thresh 19.0
target distance 6.0
model initialize at round 3395
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([16.        ,  9.        ,  1.21455258]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.324555320336759}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8215950275875622
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.50394248, 10.50419942,  4.36499666]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.7013496072212362}
episode index:3396
target Thresh 19.0
target distance 4.0
model initialize at round 3396
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.        ,  4.        ,  6.28256321]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8216331185567447
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.09104752,  7.46366383,  3.7161926 ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.0553914381761753}
episode index:3397
target Thresh 19.0
target distance 4.0
model initialize at round 3397
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([3.        , 7.        , 0.49676436]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 4.000000000418236}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8216740140515778
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.74538203, 10.24755317,  2.21357905]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.059136726946258}
episode index:3398
target Thresh 19.0
target distance 11.0
model initialize at round 3398
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.25510601, 1.22820748, 0.29617518]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.904655853191985}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.8216157179000154
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.36317202,  3.20868642,  0.04839557]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.41886028686121124}
episode index:3399
target Thresh 19.0
target distance 12.0
model initialize at round 3399
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([1.60901716, 6.94737945, 3.55367351]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 13.424453404267366}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8215851634337502
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.26632767,  6.34639281,  0.43863501]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.43694210862079624}
episode index:3400
target Thresh 19.0
target distance 13.0
model initialize at round 3400
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.31803806,  8.94175823,  4.1862061 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 13.277273546379488}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8215567586035603
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.98346875, 1.77520702, 5.35435303]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.22540001216546762}
episode index:3401
target Thresh 19.0
target distance 8.0
model initialize at round 3401
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.64641351, 3.3488693 , 1.21880787]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 9.338130927781933}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8215439028130828
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.41148693, 11.47407968,  0.95332398]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6277523685390197}
episode index:3402
target Thresh 19.0
target distance 13.0
model initialize at round 3402
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([15.        , 10.        ,  1.26943558]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8215356954609907
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.34680813, 9.77214499, 3.28713843]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8464536356306331}
episode index:3403
target Thresh 19.0
target distance 1.0
model initialize at round 3403
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([2.99997957, 5.00000587, 3.87173069]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.4142238581336204}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.821582277219081
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.40905688, 5.92026217, 1.58854538]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.41675610828585724}
episode index:3404
target Thresh 19.0
target distance 12.0
model initialize at round 3404
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 3.        , 10.        ,  3.65505946]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8215740634177373
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.50422226,  9.26543539,  5.67276231]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.5623624423710623}
episode index:3405
target Thresh 19.0
target distance 12.0
model initialize at round 3405
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.37839585,  2.45031992,  3.88071775]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.968652109811687}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8215332473671186
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.1285749 , 6.1407253 , 4.48249407]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.19061772193908377}
episode index:3406
target Thresh 19.0
target distance 11.0
model initialize at round 3406
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([15.00135875, 10.9990011 ,  0.37606448]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 13.601981948954954}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8215070572063576
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.06418225, 2.73518612, 5.82739671]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.27248073179478544}
episode index:3407
target Thresh 19.0
target distance 4.0
model initialize at round 3407
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.52298992,  6.19329942,  4.65149164]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 5.428613868753378}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8215340555016267
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.64540136, 10.44615839,  3.80193572]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6576326674438764}
episode index:3408
target Thresh 19.0
target distance 13.0
model initialize at round 3408
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([15.        ,  4.        ,  4.85892177]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.92838827718412}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8215078804691106
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.42660511, 9.89230557, 4.02706869]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9890405197688276}
episode index:3409
target Thresh 19.0
target distance 4.0
model initialize at round 3409
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.        , 7.        , 5.60644841]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.472135954999579}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8215375686843478
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.57332076, 11.63758499,  2.75689249]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.7671830220323631}
episode index:3410
target Thresh 19.0
target distance 12.0
model initialize at round 3410
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.00000194, 10.99998385,  5.84207296]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.369314844289644}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.821527055806049
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.93786272, 8.04805117, 3.57659051]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.07854907444059765}
episode index:3411
target Thresh 19.0
target distance 4.0
model initialize at round 3411
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 3.39681403, 10.06122492,  0.41825693]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.0868364639030976}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8215622120468447
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.49502428, 6.78608113, 6.13507163]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5392683214458348}
episode index:3412
target Thresh 19.0
target distance 3.0
model initialize at round 3412
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.76091124,  4.87000094,  4.89097619]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 2.2432664699517852}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8216057915334997
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.03221776,  3.58436929,  4.60779088]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.1305263959838794}
episode index:3413
target Thresh 19.0
target distance 1.0
model initialize at round 3413
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.        , 6.        , 2.79946166]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.414213562373095}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8216551161405489
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.65834779, 4.98457412, 4.79946166]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.3420002770429427}
episode index:3414
target Thresh 19.0
target distance 12.0
model initialize at round 3414
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([2.99999999, 8.        , 3.9384377 ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 12.041594592442992}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.821640002825663
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.76926149,  8.96754213,  5.67295524]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.2330102401556666}
episode index:3415
target Thresh 19.0
target distance 12.0
model initialize at round 3415
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([15.      ,  2.      ,  5.996562]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.000000000011608}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.8215713250677917
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.51038765, 2.6285873 , 5.18241178]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7967698821264402}
episode index:3416
target Thresh 19.0
target distance 8.0
model initialize at round 3416
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([11.        , 11.        ,  3.66907644]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8215775796385604
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.98118016,  3.76507675,  6.25314991]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.7653081847620132}
episode index:3417
target Thresh 19.0
target distance 10.0
model initialize at round 3417
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.31870469,  6.9249959 ,  4.19617391]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.263180129936185}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8215789228346391
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.17298906, 11.0387429 ,  4.49706147]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.1772744386129231}
episode index:3418
target Thresh 19.0
target distance 8.0
model initialize at round 3418
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([1.37134466, 7.56114718, 2.96361786]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 9.288670612240812}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8215827060573988
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.17666303, 11.87285468,  1.26449505]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.1998996047265345}
episode index:3419
target Thresh 19.0
target distance 4.0
model initialize at round 3419
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([2.00000212, 4.99999857, 0.4163925 ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.472136287065774}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8216233532222943
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.00384316, 8.17744639, 2.13319825]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.2918679815451504}
episode index:3420
target Thresh 19.0
target distance 12.0
model initialize at round 3420
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([3.        , 7.        , 5.26752543]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 12.000000000003324}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8215951032903257
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.10180111,  7.67576807,  0.15248527]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6833929726257937}
episode index:3421
target Thresh 19.0
target distance 6.0
model initialize at round 3421
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.33933549, 5.08045653, 1.06999987]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.069169177824738}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8216192958595012
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.90641   , 11.41072731,  2.22044395]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9951261322831215}
episode index:3422
target Thresh 19.0
target distance 3.0
model initialize at round 3422
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 3.34508786, 10.69605127,  4.33323288]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.77445532177717}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8216598967108422
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.29596355, 7.78747602, 6.05004757]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.36436364646897934}
episode index:3423
target Thresh 19.0
target distance 6.0
model initialize at round 3423
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 3.10680552, 10.67957753,  2.51729113]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.901898993233528}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8216867242081473
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.10977172, 11.27293217,  1.66773521]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9311274661149238}
episode index:3424
target Thresh 19.0
target distance 5.0
model initialize at round 3424
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([10.15565259, 10.20296365,  0.19239491]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 3.9261016261646775}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8217189515435338
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.20000268, 11.34322555,  1.62602417]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.397246588320634}
episode index:3425
target Thresh 19.0
target distance 12.0
model initialize at round 3425
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.59116818, 1.45336929, 0.6782152 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.423175381369534}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.8216504512021428
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.40526462,  2.97005846,  6.14725029]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0513100554013708}
episode index:3426
target Thresh 19.0
target distance 1.0
model initialize at round 3426
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.95869503,  3.99283509,  4.92032623]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.008011540127984}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8216966868452119
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.86393545,  5.67312709,  2.63714093]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.686741320637922}
episode index:3427
target Thresh 19.0
target distance 2.0
model initialize at round 3427
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.32188952, 7.04163118, 1.67737263]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.242911640095706}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8217428955129934
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.24615936, 5.36055527, 5.67737263]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9885167070150773}
episode index:3428
target Thresh 19.0
target distance 2.0
model initialize at round 3428
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.42552565,  4.37171378,  5.97800565]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.565016120873981}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8217948806703241
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.42552565,  4.37171378,  5.97800565]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.565016120873981}
episode index:3429
target Thresh 19.0
target distance 12.0
model initialize at round 3429
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.20958025,  8.48580776,  3.06969929]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.48189573636184}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8217731247559397
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.20420076, 5.99990578, 4.52103152]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.2042007784352258}
episode index:3430
target Thresh 19.0
target distance 6.0
model initialize at round 3430
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([2.21662473, 6.48953391, 3.06496406]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.3001398292979145}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8217998645177372
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.27020895, 10.99936268,  2.21540813]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7297913312440246}
episode index:3431
target Thresh 19.0
target distance 1.0
model initialize at round 3431
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([16.00000645,  8.00000027,  1.05106514]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.4142183124340149}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8218459892658383
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.95326705,  6.68237107,  5.05106514]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.32104844551710265}
episode index:3432
target Thresh 19.0
target distance 1.0
model initialize at round 3432
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([1.9999965 , 9.99998277, 5.521927  ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0000035017889215}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8218949709176687
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.63878885, 10.3815206 ,  1.23874169]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7440491723099999}
episode index:3433
target Thresh 19.0
target distance 5.0
model initialize at round 3433
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([4.38032141, 9.36056604, 5.95034027]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 4.118171213916562}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8219216518368783
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.29802276, 5.01274161, 5.10078387]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.0312597584635925}
episode index:3434
target Thresh 19.0
target distance 13.0
model initialize at round 3434
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([3.        , 8.        , 5.76479673]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8219065489239142
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.14088831,  9.27622246,  1.21612897]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9024254792695571}
episode index:3435
target Thresh 19.0
target distance 10.0
model initialize at round 3435
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([11.73229736, 10.01844224,  5.47791195]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 10.189632247978468}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8219077893415253
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.59106518, 6.35514769, 5.77880011]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8747528473949296}
episode index:3436
target Thresh 19.0
target distance 11.0
model initialize at round 3436
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.42855588,  7.6274913 ,  2.32331821]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.726704569308136}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8218904587989025
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.23134683, 5.58643006, 6.05783576]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.6304138124552576}
episode index:3437
target Thresh 19.0
target distance 14.0
model initialize at round 3437
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([16.        ,  2.        ,  6.07032013]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 16.64331697709324}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8218519360028206
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.64393863, 10.75524992,  4.67209632]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.4320674707638392}
episode index:3438
target Thresh 19.0
target distance 6.0
model initialize at round 3438
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.68311859,  4.47966529,  2.09393402]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 4.571660194213106}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.821878590644135
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.30187959,  8.54898023,  1.2443781 ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5427247219895582}
episode index:3439
target Thresh 19.0
target distance 4.0
model initialize at round 3439
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.64680526, 7.53462867, 2.15983261]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 2.9647973530136427}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8219189154753431
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.43711326, 9.25949591, 3.8766473 ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.9301547128258137}
episode index:3440
target Thresh 19.0
target distance 11.0
model initialize at round 3440
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.58169952,  3.63015729,  2.83197826]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.448488550339032}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8218907438451433
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.22055711, 8.93437132, 4.00012519]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.960049581984466}
episode index:3441
target Thresh 19.0
target distance 4.0
model initialize at round 3441
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.96125519, 11.49028684,  2.60678869]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.991481412113995}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8219310417144503
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.79208844, 10.46448498,  4.32360339]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9561278349362031}
episode index:3442
target Thresh 19.0
target distance 7.0
model initialize at round 3442
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([10.       , 11.       ,  1.5453903]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 7.615773105863909}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8219523624267779
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.6358495 , 8.27369956, 4.69583426]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6922543131385901}
episode index:3443
target Thresh 19.0
target distance 10.0
model initialize at round 3443
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.        ,  9.        ,  0.09457606]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8219560097551819
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.43975973, 9.70193321, 4.67864892]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.5312555222585336}
episode index:3444
target Thresh 19.0
target distance 11.0
model initialize at round 3444
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.13864485, 8.23930464, 1.8377034 ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.166764901713732}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8219077378279886
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.6255606 ,  3.00434266,  4.15629441]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.37446457797921645}
episode index:3445
target Thresh 19.0
target distance 6.0
model initialize at round 3445
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 4.24448663, 10.13297887,  1.74853056]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.833904536495268}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.821934322131429
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.0866757 , 10.97494481,  0.89897464]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.09022438729147876}
episode index:3446
target Thresh 19.0
target distance 1.0
model initialize at round 3446
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.00001237,  4.00000116,  1.10362643]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0000011614366195}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8219802071554698
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.25780074,  2.68598702,  5.10362643]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8058932285805774}
episode index:3447
target Thresh 19.0
target distance 3.0
model initialize at round 3447
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([12.99999988, 10.99999997,  4.39630055]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.60555131796856}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8220204089544386
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.6907855 ,  8.12989514,  6.11311525]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.3353898549464933}
episode index:3448
target Thresh 19.0
target distance 12.0
model initialize at round 3448
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([12.6337485 , 10.98272322,  3.52804112]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 11.743259257889632}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8220121729076684
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.93634362, 6.27564648, 5.54574385]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.2829012438405673}
episode index:3449
target Thresh 19.0
target distance 10.0
model initialize at round 3449
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 7.29505406, 10.24800179,  0.35918492]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 10.164526872162819}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8220062652023441
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.09977069,  5.01180802,  0.37688777]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.10046700668757026}
episode index:3450
target Thresh 19.0
target distance 6.0
model initialize at round 3450
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([1.61799574, 6.63904266, 2.80977422]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 4.574700506820941}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.822032782438589
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.24068271, 11.27067523,  1.96021818]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.36220607704484226}
episode index:3451
target Thresh 19.0
target distance 2.0
model initialize at round 3451
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.37023368, 8.17859229, 4.04244661]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.235375552193761}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8220814403811039
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.89973153, 6.581089  , 6.04244661]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.99247329657308}
episode index:3452
target Thresh 19.0
target distance 9.0
model initialize at round 3452
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 6.61273363, 11.48112196,  1.29992121]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 7.402917183299881}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8220974958640515
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.35552867, 11.89458022,  2.16717998]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.102550258739657}
episode index:3453
target Thresh 19.0
target distance 8.0
model initialize at round 3453
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.64630277, 3.06488852, 1.04939431]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 7.942990359400956}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8221135420502516
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.39828448, 10.77647832,  1.91665308]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.4567192446281644}
episode index:3454
target Thresh 19.0
target distance 11.0
model initialize at round 3454
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([12.71232368, 11.08364098,  3.45202613]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.19009680518852}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8221147157352835
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.19217684, 8.94834296, 3.75291429]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9676188879483841}
episode index:3455
target Thresh 19.0
target distance 9.0
model initialize at round 3455
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([2.37856353, 3.63984077, 2.35391715]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 9.26134395803623}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.822108788615435
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.25445802, 11.87776495,  2.37162   ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 1.1516527943114079}
episode index:3456
target Thresh 19.0
target distance 12.0
model initialize at round 3456
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.50794899, 6.74731381, 1.47010773]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.516081053194823}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8220644909498157
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.47162005,  1.74107767,  6.07188404]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.5380206695873846}
episode index:3457
target Thresh 19.0
target distance 1.0
model initialize at round 3457
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.        , 11.        ,  2.37689596]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.000000000234893}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.822110192369437
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.4467317 , 9.3032259 , 0.09370958]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8897190393828809}
episode index:3458
target Thresh 19.0
target distance 3.0
model initialize at round 3458
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.88324518, 9.56744163, 6.27488947]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.7991651704705631}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8221530338865317
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.03343367, 8.1567117 , 5.99170416]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.16023847189201482}
episode index:3459
target Thresh 19.0
target distance 13.0
model initialize at round 3459
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.97774077,  9.68282281,  2.59402287]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.49017936987004}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8221379732252365
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.89725655, 6.90590559, 4.32854041]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.275042844474194}
episode index:3460
target Thresh 19.0
target distance 12.0
model initialize at round 3460
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([3.00620744, 1.99856436, 0.78271931]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 12.643676171411617}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8220976428645851
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.28509467,  6.58258756,  1.38449562]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.6486040732547108}
episode index:3461
target Thresh 19.0
target distance 13.0
model initialize at round 3461
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.66827393,  8.3500467 ,  5.52398229]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.740007475550502}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8220781806012514
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.33275836, 6.24728844, 4.97531416]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8229840955387815}
episode index:3462
target Thresh 19.0
target distance 5.0
model initialize at round 3462
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.        ,  7.        ,  6.24029398]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.4031242374328485}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8220967502492776
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.3644202 , 10.29601604,  5.10755275]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7927140124493145}
episode index:3463
target Thresh 19.0
target distance 12.0
model initialize at round 3463
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.47388263,  5.61487564,  2.29536098]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.925046029755519}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8220687143329117
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.03461536, 8.91343218, 3.46350791]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.09323202059853695}
episode index:3464
target Thresh 19.0
target distance 11.0
model initialize at round 3464
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.        ,  2.        ,  4.70908093]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 12.083045973594574}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8220344802724981
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.60906488, 7.64947075, 5.59404256]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.890377601395621}
episode index:3465
target Thresh 19.0
target distance 1.0
model initialize at round 3465
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.        , 11.        ,  4.67474484]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8220829411841332
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.34818142, 10.2596932 ,  0.39155954]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.8180980795936397}
episode index:3466
target Thresh 19.0
target distance 14.0
model initialize at round 3466
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([2.        , 7.        , 4.28444219]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.560219778561038}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8220349389572483
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.61706087,  3.36269925,  0.31984789]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.7157617415744918}
episode index:3467
target Thresh 19.0
target distance 1.0
model initialize at round 3467
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.9999998 , 3.99999901, 5.52126479]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9999997971986709}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.822086255295496
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.9999998 , 3.99999901, 5.52126479]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.9999997971986709}
episode index:3468
target Thresh 19.0
target distance 12.0
model initialize at round 3468
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.15217678,  7.193539  ,  4.64327502]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.181298061918318}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8220734951640759
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.00364955, 8.00704903, 4.37779221]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.007937761993389044}
episode index:3469
target Thresh 19.0
target distance 3.0
model initialize at round 3469
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.65220931, 5.64664195, 2.78894913]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.3973318738462186}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8221162114478903
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.77837662, 7.32482235, 2.50576382]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.3932257417233287}
episode index:3470
target Thresh 19.0
target distance 7.0
model initialize at round 3470
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.79280446,  3.67016708,  2.7042225 ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.388474534517647}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8221347273396414
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.18727162,  9.28857881,  1.57148127]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.3440180062399405}
episode index:3471
target Thresh 19.0
target distance 1.0
model initialize at round 3471
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.54319609,  3.71706507,  3.69419003]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8502071058919543}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8221859558167901
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.54319609,  3.71706507,  3.69419003]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8502071058919543}
episode index:3472
target Thresh 19.0
target distance 11.0
model initialize at round 3472
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([3.81931179, 5.47007356, 2.07234514]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.286278663518525}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8221380068576072
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.034528  ,  4.10936824,  4.39093614]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.1146891235912527}
episode index:3473
target Thresh 19.0
target distance 2.0
model initialize at round 3473
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.99970845,  7.00004   ,  4.01523197]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.0000400253565696}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8221863263720407
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.48554039,  5.38871899,  6.01523197]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.6219742178158681}
episode index:3474
target Thresh 19.0
target distance 9.0
model initialize at round 3474
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 8.20868832, 11.87733257,  1.80670136]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 8.014294434374284}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.822197224797442
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.09092865,  9.8895117 ,  0.39077458]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.14309327337953143}
episode index:3475
target Thresh 19.0
target distance 13.0
model initialize at round 3475
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([2.00000037, 3.99999939, 6.27019072]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.999999630712635}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8221512192785082
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.81975666,  4.95006967,  0.3055963 ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.967016049904255}
episode index:3476
target Thresh 19.0
target distance 6.0
model initialize at round 3476
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.        , 4.        , 4.42302871]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.082762530298219}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8221774965371235
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.23701298, 9.09010605, 3.57347279]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.1874578713230421}
episode index:3477
target Thresh 19.0
target distance 8.0
model initialize at round 3477
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.        , 9.        , 5.20345807]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8221908859322447
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.22010327, 11.47478374,  1.78753153]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.91304901810668}
episode index:3478
target Thresh 19.0
target distance 9.0
model initialize at round 3478
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([15.        ,  2.        ,  4.78378558]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.219544457292889}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8222017705165243
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.69308596, 10.0672496 ,  3.36785905]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9819468111990598}
episode index:3479
target Thresh 19.0
target distance 5.0
model initialize at round 3479
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.        , 6.        , 4.91088414]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8222306621613265
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.15046843, 10.77819733,  2.06132822]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.2680245773033058}
episode index:3480
target Thresh 19.0
target distance 13.0
model initialize at round 3480
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([2.58998217, 4.63226023, 2.82690066]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 13.479587914943387}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8221945181095851
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.85480737,  5.84851068,  5.71186228]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.2098330646297481}
episode index:3481
target Thresh 19.0
target distance 8.0
model initialize at round 3481
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 6.79809103, 11.7782803 ,  3.41808975]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.213442272366006}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8222078872349898
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.00702417e+00, 3.80092613e+00, 2.16321153e-03]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8009569283082273}
episode index:3482
target Thresh 19.0
target distance 12.0
model initialize at round 3482
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 3.91244931, 10.96434043,  4.538378  ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 13.067270470173735}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8221863437974637
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.38331136,  5.53527692,  5.98971012]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.602407786647777}
episode index:3483
target Thresh 19.0
target distance 7.0
model initialize at round 3483
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.00117073, 3.99979318, 0.83514708]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 9.218939641800299}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8221874868169925
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.58322203, 11.90480323,  1.13603452]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9961790771636256}
episode index:3484
target Thresh 19.0
target distance 12.0
model initialize at round 3484
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([16.        ,  7.        ,  0.56396263]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8221770106775349
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.69515654, 8.66327993, 4.58166513]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.45421354465874375}
episode index:3485
target Thresh 19.0
target distance 12.0
model initialize at round 3485
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([2.00710956, 4.35888066, 3.21180964]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 14.591767130456969}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8221470489798643
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.37342598, 10.4523342 ,  0.09677126]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8321855801923832}
episode index:3486
target Thresh 19.0
target distance 14.0
model initialize at round 3486
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([2.        , 3.        , 4.59408355]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 15.231546211727817}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8221050593039483
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.53827926,  9.28591053,  0.91267455]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.6094992954142957}
episode index:3487
target Thresh 19.0
target distance 13.0
model initialize at round 3487
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([15.45661088,  9.59283281,  2.90956205]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 12.47070989610501}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8220968910769815
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.45967386, 8.85618897, 4.9272649 ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.4816447517599108}
episode index:3488
target Thresh 19.0
target distance 8.0
model initialize at round 3488
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([16.        ,  3.        ,  0.20897692]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8221102613612153
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.37332748, 11.09838852,  3.07623569]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.6343490731630501}
episode index:3489
target Thresh 19.0
target distance 13.0
model initialize at round 3489
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.49517372,  3.75358207,  3.68732381]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 13.082689161235209}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.822082430437031
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.04785121, 9.87087176, 4.8554705 ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.13770926555872895}
episode index:3490
target Thresh 19.0
target distance 8.0
model initialize at round 3490
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([16.        ,  3.        ,  1.16800183]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 9.433981132089855}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8220884060208027
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.52493254, 10.83219114,  3.75207529]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.5038341995087169}
episode index:3491
target Thresh 19.0
target distance 13.0
model initialize at round 3491
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 2.        , 11.        ,  4.53152895]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.000000000000002}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8220779792552776
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.04508033, 11.33475658,  2.26604649]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.33777833969277016}
episode index:3492
target Thresh 19.0
target distance 7.0
model initialize at round 3492
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.2740132 , 4.09967197, 1.72208231]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 6.036305202726028}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8221015418363694
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.79807296, 9.80044858, 2.87252639]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.28389311040576987}
episode index:3493
target Thresh 19.0
target distance 11.0
model initialize at round 3493
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 2.99997965, 11.0003099 ,  2.64635354]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 11.045409340325197}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8220980004300685
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.02711931,  9.77299001,  0.66404686]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.2286241282748467}
episode index:3494
target Thresh 19.0
target distance 6.0
model initialize at round 3494
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([1.23209342, 6.72368106, 5.0209403 ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.474894486613365}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8221215437990468
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.59083616, 1.68784129, 6.17138438]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.6682293175912031}
episode index:3495
target Thresh 19.0
target distance 8.0
model initialize at round 3495
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 3.96548381, 11.85058646,  2.64277637]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 8.079415064703259}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8221323952895623
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.45149733, 11.45917418,  1.2268254 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6439648792369583}
episode index:3496
target Thresh 19.0
target distance 2.0
model initialize at round 3496
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.        , 9.        , 5.79254103]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.236067977499935}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.82217756761004
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.12016561, 10.80861698,  3.50935572]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9004088060584434}
episode index:3497
target Thresh 19.0
target distance 11.0
model initialize at round 3497
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([2.31738489, 1.96543891, 4.17212987]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 15.571513097799546}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8221477085377096
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.08323626, 11.44873818,  1.05709125]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0206966785510372}
episode index:3498
target Thresh 19.0
target distance 9.0
model initialize at round 3498
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([1.13276946, 7.08042364, 3.42036521]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 10.617217966218151}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8221512427054468
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.02182134, 11.32793257,  1.72125337]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0316846696198263}
episode index:3499
target Thresh 19.0
target distance 12.0
model initialize at round 3499
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 2.        , 11.        ,  4.06415558]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 13.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.822136354677761
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.79154379,  6.72625492,  5.79867312]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7555793779387938}
episode index:3500
target Thresh 19.0
target distance 2.0
model initialize at round 3500
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([4.        , 3.        , 0.03691309]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8221814742565449
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.32858451, 3.19323013, 4.03691297]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.8711173296739341}
episode index:3501
target Thresh 19.0
target distance 12.0
model initialize at round 3501
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([3.07453858, 5.40567039, 3.16303813]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 13.14742571637658}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8221339236415586
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.1046374 ,  3.60930101,  5.48162914]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.0830151885609531}
episode index:3502
target Thresh 19.0
target distance 3.0
model initialize at round 3502
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 1.99984238, 10.99913167,  5.54282022]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 3.0001577438984826}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8221734503576186
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.31262424, 11.35844572,  0.97644912]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.4756230115562203}
episode index:3503
target Thresh 19.0
target distance 2.0
model initialize at round 3503
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.75976727, 6.66567212, 2.72403431]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.7912442568371292}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8222157236309183
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.57997482, 7.42061815, 2.440849  ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7164428955159402}
episode index:3504
target Thresh 19.0
target distance 3.0
model initialize at round 3504
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.20279007, 9.48217557, 3.0742749 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.936619468174099}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8222579727825216
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.8515106 , 11.12790548,  2.79108959]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8610633632291991}
episode index:3505
target Thresh 19.0
target distance 8.0
model initialize at round 3505
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.        , 11.        ,  2.25869492]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.82227123229193
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.71387267, 3.25207497, 5.12595369]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3813274735318699}
episode index:3506
target Thresh 19.0
target distance 3.0
model initialize at round 3506
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 6.        , 11.        ,  1.29497307]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.242640687119285}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8222998816965881
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.77662708, 7.29599578, 4.72860234]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7385914969450577}
episode index:3507
target Thresh 19.0
target distance 9.0
model initialize at round 3507
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 7.09309079, 10.19119521,  5.83627605]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 10.309293834645171}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8222939896520733
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.57084953,  4.83221306,  5.85397867]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.5949971802180253}
episode index:3508
target Thresh 19.0
target distance 9.0
model initialize at round 3508
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 9.59860197, 11.71983443,  3.58426416]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 8.460255316239373}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8223023007895607
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.69194018, 8.30415147, 4.16833762]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.43290757808053154}
episode index:3509
target Thresh 19.0
target distance 4.0
model initialize at round 3509
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([12.9780214 ,  8.8680422 ,  4.95651984]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 4.166295801657981}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.822333572312956
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.6154574 ,  5.84214366,  0.10696297]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.6353789737816593}
episode index:3510
target Thresh 19.0
target distance 14.0
model initialize at round 3510
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([2.        , 3.        , 4.90906024]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8222879865734718
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.41159692,  4.60650993,  5.22765125]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8450280960034569}
episode index:3511
target Thresh 19.0
target distance 13.0
model initialize at round 3511
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.31784353,  6.9476771 ,  4.18268728]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.484207774425084}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8222602793836609
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.53164314, 5.22621859, 5.35083421]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5777709554266325}
episode index:3512
target Thresh 19.0
target distance 12.0
model initialize at round 3512
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([3.        , 3.        , 6.14117575]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 11.999999999999986}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8222147404601199
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.04067598,  3.79340609,  0.17658145]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7944480860891432}
episode index:3513
target Thresh 19.0
target distance 14.0
model initialize at round 3513
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.78341192,  8.16288501,  3.38875997]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.965095139637134}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.822193385125422
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.31535988, 5.9957699 , 4.84009221]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.315388250128158}
episode index:3514
target Thresh 19.0
target distance 8.0
model initialize at round 3514
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 6.        , 11.        ,  1.79615706]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.54400374531753}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8222041575207323
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.83229455, 3.24744579, 0.38023052]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8682992828849243}
episode index:3515
target Thresh 19.0
target distance 13.0
model initialize at round 3515
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([1.6669849 , 5.646697  , 2.78033727]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 13.822723705357127}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8221624979335707
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.11632387,  2.79599438,  5.38211359]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.1893235685960795}
episode index:3516
target Thresh 19.0
target distance 12.0
model initialize at round 3516
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([4.37186016, 9.64137385, 2.35800305]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.520223137371776}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8221390512106405
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.57274655,  5.49546129,  5.80933529]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6542380292330694}
episode index:3517
target Thresh 19.0
target distance 11.0
model initialize at round 3517
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 4.43430538, 11.87795243,  1.56094569]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.227325522343717}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8221286871087634
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.50936604,  6.82350804,  5.57864854]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9585860298993402}
episode index:3518
target Thresh 19.0
target distance 12.0
model initialize at round 3518
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([14.        ,  6.        ,  5.64894009]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 12.16552506059644}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8220990201140176
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.89330875, 8.53114365, 2.53390159]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.5417532613269244}
episode index:3519
target Thresh 19.0
target distance 10.0
model initialize at round 3519
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.        , 10.        ,  2.19806233]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.049875621120892}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8221049417541512
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.11754659, 9.29904238, 4.78213579]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.32131533773352006}
episode index:3520
target Thresh 19.0
target distance 7.0
model initialize at round 3520
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 4.        , 10.        ,  1.70814341]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8221257407636667
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.93981699, 3.95587911, 4.85858749]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9577718244325126}
episode index:3521
target Thresh 19.0
target distance 8.0
model initialize at round 3521
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([16.76452177,  9.50070643,  6.19394684]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 6.7359276418038645}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8221439858320803
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.95407574,  3.27908261,  5.06120561]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.2828358920968851}
episode index:3522
target Thresh 19.0
target distance 3.0
model initialize at round 3522
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.80251673,  6.52069104,  6.21945143]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.5334604317084264}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8221860394835614
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.15972712,  4.87606759,  5.93626612]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.20216822775631538}
episode index:3523
target Thresh 19.0
target distance 3.0
model initialize at round 3523
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.05778488, 3.3180223 , 5.75673079]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.6201703924219029}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8222253158656602
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.46778753, 2.07967851, 1.19036018]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.4745248508235932}
episode index:3524
target Thresh 19.0
target distance 11.0
model initialize at round 3524
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 3.       , 11.       ,  3.3462075]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 11.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.822221770490442
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.45705071, 11.6311264 ,  1.36390999]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.7792405840719367}
episode index:3525
target Thresh 19.0
target distance 4.0
model initialize at round 3525
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.99989663, 3.99997237, 4.41282868]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.123107366423964}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8222555930596167
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.63528263, 7.26160696, 3.84645807]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8235551258390421}
episode index:3526
target Thresh 19.0
target distance 2.0
model initialize at round 3526
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([2.12435048, 2.46341198, 5.9501121 ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.9320485925173796}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8222975673740314
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.36374924, 1.81085875, 5.6669268 ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.6637691184452896}
episode index:3527
target Thresh 19.0
target distance 6.0
model initialize at round 3527
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.66132493, 6.3514598 , 5.51976681]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 4.364619482752471}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8223234232924298
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.92031607, 2.16780473, 4.67021089]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.18576316985801322}
episode index:3528
target Thresh 19.0
target distance 4.0
model initialize at round 3528
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.11425754, 11.15852123,  3.97449768]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 4.160090577364707}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8223626050965407
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.48176178,  7.84660492,  5.69131201]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.974081265324989}
episode index:3529
target Thresh 19.0
target distance 7.0
model initialize at round 3529
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.5772516 , 9.371001  , 5.46845412]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 5.387612450865086}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8223807417159796
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.51008064, 3.89641647, 4.33571289]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.50074996521433}
episode index:3530
target Thresh 19.0
target distance 13.0
model initialize at round 3530
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([2.        , 8.        , 4.98259115]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 13.34166406543726}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8223470591765527
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.8287914 ,  4.95499559,  5.86754932]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8300123980860445}
episode index:3531
target Thresh 19.0
target distance 2.0
model initialize at round 3531
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([15.        ,  5.        ,  0.17763394]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 2.2360679774997894}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8223917230895832
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.98144719,  6.49836285,  4.17763394]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.5019801202144928}
episode index:3532
target Thresh 19.0
target distance 14.0
model initialize at round 3532
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([2.        , 3.        , 4.67994213]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.000000000000002}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.8223336699952873
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.03097431,  3.37714337,  0.14897722]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.3784131699027714}
episode index:3533
target Thresh 19.0
target distance 3.0
model initialize at round 3533
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.5414955 ,  5.83973291,  3.62919545]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.2083886099957084}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.822375539075651
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.755345  ,  7.53538928,  3.34601014]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5250896952947207}
episode index:3534
target Thresh 19.0
target distance 3.0
model initialize at round 3534
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.16161887, 9.78259225, 0.20115584]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 2.204932380276236}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8224173844677087
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.23057625, 10.48785778,  6.20115584]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9242848930145873}
episode index:3535
target Thresh 19.0
target distance 3.0
model initialize at round 3535
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 6.4855719 , 10.2171548 ,  0.52463501]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.3307421154119794}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8224564621333004
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.97912852, 8.93231992, 2.24144971]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.3520033572460024}
episode index:3536
target Thresh 19.0
target distance 2.0
model initialize at round 3536
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 5.99999924, 11.00000466,  2.742966  ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 2.828429880761233}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.822495517702389
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.01050154, 9.54782437, 4.45978069]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.1310255308980548}
episode index:3537
target Thresh 19.0
target distance 1.0
model initialize at round 3537
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.        , 7.        , 0.96259468]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.4142135648185656}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8225428621010034
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.38069407, 8.54248256, 2.96259468]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6627332041266132}
episode index:3538
target Thresh 19.0
target distance 13.0
model initialize at round 3538
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([16.76921941,  7.10671483,  1.91069096]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 13.798165153321895}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8225258516043223
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.53249492, 7.79750747, 5.6452085 ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.5696964696267273}
episode index:3539
target Thresh 19.0
target distance 13.0
model initialize at round 3539
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([16.       , 10.       ,  1.7738983]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 14.764823060233402}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.822496248406863
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.45086177, 3.28921182, 4.94204523]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6206418207061847}
episode index:3540
target Thresh 19.0
target distance 11.0
model initialize at round 3540
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 5.        , 11.        ,  4.84383321]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.083045973659944}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8224880919638348
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.98719181,  6.03742585,  0.57835063]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.039556846508571004}
episode index:3541
target Thresh 19.0
target distance 9.0
model initialize at round 3541
at step 0:
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.42732156,  9.599225  ,  3.78755295]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.558258801043411}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8225036292114449
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.79490072, 10.88601032,  4.65481172]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8030322517089551}
episode index:3542
target Thresh 19.0
target distance 10.0
model initialize at round 3542
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([12.53577161, 10.82971278,  3.63605225]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 8.992589800245485}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.822509398210647
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.87195175, 8.28290314, 6.22012571]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.9166973564364436}
episode index:3543
target Thresh 19.0
target distance 9.0
model initialize at round 3543
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([11.        , 11.        ,  0.35901326]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.041594578792305}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8224924211553808
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.61337871, 2.55482383, 4.0935308 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6762436709168986}
episode index:3544
target Thresh 19.0
target distance 8.0
model initialize at round 3544
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 2.        , 10.        ,  1.76563376]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 7.999999999999999}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8225054686565411
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.81708186, 2.5517964 , 4.63289253]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5813246184547788}
episode index:3545
target Thresh 19.0
target distance 2.0
model initialize at round 3545
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.        ,  9.        ,  4.19496012]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 2.8284271248764834}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8225499115587812
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.04380942, 10.32017016,  1.91177481]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.1732301693823168}
episode index:3546
target Thresh 19.0
target distance 1.0
model initialize at round 3546
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.44707838,  8.64863496,  3.75593257]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.852320144453882}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8225999397765543
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.44707838,  8.64863496,  3.75593257]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.852320144453882}
episode index:3547
target Thresh 19.0
target distance 4.0
model initialize at round 3547
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.10065397,  8.41751463,  3.15254283]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.035255348224108}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8226307924845957
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.35012636, 10.71623248,  4.5861721 ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.450680013463638}
episode index:3548
target Thresh 19.0
target distance 3.0
model initialize at round 3548
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([11.1542087 , 10.14556471,  5.86092901]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.5639831731173524}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8226642806099596
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.37067151,  8.57591672,  5.2945584 ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8530735117560251}
episode index:3549
target Thresh 19.0
target distance 1.0
model initialize at round 3549
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.23496571, 2.51272996, 0.63419062]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5409630092926957}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8227142343337314
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.23496571, 2.51272996, 0.63419062]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5409630092926957}
episode index:3550
target Thresh 19.0
target distance 6.0
model initialize at round 3550
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.02243261,  9.63005472,  5.10260606]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.2059177252365485}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8227398054441649
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.11250587, 10.37893063,  4.25305002]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.631177260494387}
episode index:3551
target Thresh 19.0
target distance 13.0
model initialize at round 3551
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([3.00000122, 8.99999763, 6.19686985]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 13.152944871332677}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8227249696728702
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.60856445,  7.50011812,  1.64820161]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6350904848879108}
episode index:3552
target Thresh 19.0
target distance 10.0
model initialize at round 3552
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.25933698,  5.64898592,  5.10243607]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.630807222051967}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8226855183993936
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.42070902, 3.45086502, 5.70421238]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.6166646905758041}
episode index:3553
target Thresh 19.0
target distance 2.0
model initialize at round 3553
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.63055927,  8.41235709,  1.25769966]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 2.440482161111573}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8227270528624214
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.54822621,  6.85002569,  0.97451269]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0114819058130586}
episode index:3554
target Thresh 19.0
target distance 10.0
model initialize at round 3554
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.        , 6.        , 5.70234179]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.049875621058295}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8226935003004348
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.62369555,  5.62265981,  0.30411787]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8813066330696179}
episode index:3555
target Thresh 19.0
target distance 5.0
model initialize at round 3555
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.41764001,  9.42682905,  4.4991169 ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.762330404511746}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.822726904869923
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.41520434, 10.52142369,  3.93274628]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6335849778437382}
episode index:3556
target Thresh 19.0
target distance 14.0
model initialize at round 3556
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([2.        , 2.        , 4.52443242]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 14.000000000000002}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.8226691492432917
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.057825  ,  2.76076912,  6.27665281]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.7629635540232491}
episode index:3557
target Thresh 19.0
target distance 11.0
model initialize at round 3557
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([5.7513084 , 9.7408361 , 6.27828455]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 10.608857716323769}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8226632362135827
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([1.63262375e+01, 7.21835897e+00, 1.28013770e-02]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.3925704694796325}
episode index:3558
target Thresh 19.0
target distance 1.0
model initialize at round 3558
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.        ,  6.        ,  2.91159421]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.414213562373095}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8227074724495441
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.77662899,  4.62366878,  0.6284089 ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.4376297513915909}
episode index:3559
target Thresh 19.0
target distance 10.0
model initialize at round 3559
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.43114393, 10.5839163 ,  2.92559505]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.778706425939864}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8227131566408179
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.94721673, 8.89833547, 5.50966851]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.305460133947284}
episode index:3560
target Thresh 19.0
target distance 4.0
model initialize at round 3560
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.00000012,  8.00000006,  1.4968993 ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.000000056780548}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8227438649225551
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.77505572, 11.69766427,  2.93052869]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0428071750284418}
episode index:3561
target Thresh 19.0
target distance 12.0
model initialize at round 3561
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([16.        ,  2.        ,  6.25425148]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 12.999999999999998}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8227123683931755
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.91879422, 7.2056994 , 5.1392131 ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.22114841263708473}
episode index:3562
target Thresh 19.0
target distance 4.0
model initialize at round 3562
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([1.25221776, 8.66691867, 5.08279634]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.8291986663865196}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8227510671418723
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.49093575, 6.84486008, 0.5164256 ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9863746531923733}
episode index:3563
target Thresh 19.0
target distance 5.0
model initialize at round 3563
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.       , 4.       , 5.8778441]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8227791237151848
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.49373131, 9.01247864, 3.0282877 ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.5064224526258122}
episode index:3564
target Thresh 19.0
target distance 7.0
model initialize at round 3564
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([4.53976505, 4.6793465 , 1.42551297]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.352534127486184}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8227731914475337
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.96676808, 11.44443915,  1.44321534]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.44567983409986306}
episode index:3565
target Thresh 19.0
target distance 3.0
model initialize at round 3565
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([11.28902937, 10.23685051,  0.3244068 ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.8734507301682826}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.822814561556494
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.64714096, 10.41350754,  0.04122125]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.6844581073449613}
episode index:3566
target Thresh 19.0
target distance 3.0
model initialize at round 3566
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.00000041,  2.99999979,  0.53212326]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.000000212232}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8228531882591694
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.63018626,  6.27209964,  2.24893796]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6864203843983911}
episode index:3567
target Thresh 19.0
target distance 1.0
model initialize at round 3567
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([3.00000143, 2.00001509, 2.4860864 ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.0000014334866474}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8229000343386932
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.46357921, 1.466026  , 4.4860864 ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7568853859214248}
episode index:3568
target Thresh 19.0
target distance 5.0
model initialize at round 3568
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.00033723, 3.99989319, 0.70325917]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.099058128914333}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8229306214257115
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.78686915, 8.85139462, 2.13688831]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.25982363102916656}
episode index:3569
target Thresh 19.0
target distance 13.0
model initialize at round 3569
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([16.        ,  5.        ,  0.18952578]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8228932316398413
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.06598451, 6.15343012, 5.0744874 ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.16701723822490155}
episode index:3570
target Thresh 19.0
target distance 12.0
model initialize at round 3570
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([16.38150766,  2.96115798,  1.61785525]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 13.722219505070338}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8228578129857975
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.23715048, 6.9434687 , 4.50281687]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9728173161025417}
episode index:3571
target Thresh 19.0
target distance 2.0
model initialize at round 3571
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.66676697,  7.7671106 ,  0.87117356]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8363629178191234}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8229074048634611
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.66676697,  7.7671106 ,  0.87117356]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8363629178191234}
episode index:3572
target Thresh 19.0
target distance 10.0
model initialize at round 3572
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.        , 9.        , 3.90870404]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.44030650891055}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8228992063968449
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.22035985,  6.53293731,  5.92640689]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.5766980526969634}
episode index:3573
target Thresh 19.0
target distance 1.0
model initialize at round 3573
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.15301012,  3.4482829 ,  3.10998189]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.4736767361338277}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8229487589412219
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.15301012,  3.4482829 ,  3.10998189]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.4736767361338277}
episode index:3574
target Thresh 19.0
target distance 2.0
model initialize at round 3574
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.        , 5.        , 1.80347222]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8229927173303293
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.97806508, 3.37998679, 5.80347222]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.049286072717003}
episode index:3575
target Thresh 19.0
target distance 10.0
model initialize at round 3575
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([3.77137852, 3.66736928, 2.7170617 ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 10.757840163138999}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8229573203786289
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.53661609,  6.92488103,  5.60202332]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.4694331800295369}
episode index:3576
target Thresh 19.0
target distance 6.0
model initialize at round 3576
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([10.10483215, 11.86924193,  1.88421934]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.23991733632507}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8229826376632542
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.97684714, 10.67384532,  1.03465556]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6742429611816748}
episode index:3577
target Thresh 19.0
target distance 6.0
model initialize at round 3577
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([12.70277285,  9.37701775,  5.52722192]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 5.479962711485604}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8230053876457432
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.14796937,  4.93067053,  0.39448057]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.1634059669784076}
episode index:3578
target Thresh 19.0
target distance 10.0
model initialize at round 3578
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.29659147,  8.52892267,  3.01199126]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.61939895055485}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8230062714781516
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.52406326, 11.44928337,  3.31287942]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.6902882354291859}
episode index:3579
target Thresh 19.0
target distance 13.0
model initialize at round 3579
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([1.99693603, 7.99651152, 5.00168681]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 14.319141841559796}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8229613764417008
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.3535278 ,  1.73790035,  5.32027782]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.44008877958375364}
episode index:3580
target Thresh 19.0
target distance 11.0
model initialize at round 3580
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.        ,  9.        ,  1.08637398]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8229599649815376
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.21427708, 10.8893312 ,  3.38726214]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.2411685038644937}
episode index:3581
target Thresh 19.0
target distance 6.0
model initialize at round 3581
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.83327943, 5.21291022, 3.3467865 ]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 4.789992098830862}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8229852461882662
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.24781661, 10.10467799,  2.49723058]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.2690177567391706}
episode index:3582
target Thresh 19.0
target distance 8.0
model initialize at round 3582
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([15.93246792,  4.68161455,  2.62093383]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 8.015686773316263}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8229980177669937
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.90971029, 10.03116428,  5.4881926 ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.3289903890188053}
episode index:3583
target Thresh 19.0
target distance 1.0
model initialize at round 3583
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.30813848, 7.52922875, 3.0056653 ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8710656865199954}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8230474044807864
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.30813848, 7.52922875, 3.0056653 ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8710656865199954}
episode index:3584
target Thresh 19.0
target distance 12.0
model initialize at round 3584
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 5.65834449, 10.71315225,  0.8387224 ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.365018380477075}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8230347891265098
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.25566969,  5.86581529,  0.57323864]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.28874301346643305}
episode index:3585
target Thresh 19.0
target distance 4.0
model initialize at round 3585
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([12.8075328 ,  9.1865925 ,  3.37062371]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.34225779367504}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8230678190652366
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.76290638, 11.60462735,  2.80425309]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.973447673480068}
episode index:3586
target Thresh 19.0
target distance 8.0
model initialize at round 3586
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.43883136, 10.12698659,  0.46463078]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 7.140483933689842}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8230854708780748
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.82975427,  3.53134122,  5.61507474]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9852997733106226}
episode index:3587
target Thresh 19.0
target distance 12.0
model initialize at round 3587
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.11369462,  7.31766322,  3.95282292]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.598323041383813}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8230581283098138
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.26471624, 4.06610338, 5.12096985]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.2728449085757906}
episode index:3588
target Thresh 19.0
target distance 10.0
model initialize at round 3588
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([10.32737722, 10.813664  ,  4.26253867]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 8.329461711192911}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8230660412779902
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.81220265, 10.33239426,  4.84661214]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.693516595475198}
episode index:3589
target Thresh 19.0
target distance 3.0
model initialize at round 3589
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.        ,  5.        ,  0.42758959]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.1622776611881216}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8231043504614782
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.96817223,  8.18848719,  2.14440428]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.1911555042693587}
episode index:3590
target Thresh 19.0
target distance 12.0
model initialize at round 3590
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.39634505,  7.5105672 ,  3.84336245]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.973061212713661}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8230832053052182
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.93637232, 4.70590635, 5.29469469]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.708768127210778}
episode index:3591
target Thresh 19.0
target distance 14.0
model initialize at round 3591
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 3.30942759, 11.88959862,  1.71500986]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 13.273266575815912}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8230662953689827
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.41873089,  8.3922259 ,  5.44952741]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.7012238782446838}
episode index:3592
target Thresh 19.0
target distance 7.0
model initialize at round 3592
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([16.08549797,  5.68079689,  2.52997261]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.428832984472506}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8230889270916768
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.05418754, 10.58833635,  3.68041669]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.0315174108346261}
episode index:3593
target Thresh 19.0
target distance 12.0
model initialize at round 3593
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.38078596,  8.4588398 ,  3.87545991]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.687292089819575}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8230897839911604
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.46557469, 10.2553496 ,  4.17634735]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8782163778569895}
episode index:3594
target Thresh 19.0
target distance 4.0
model initialize at round 3594
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([4.87970395, 7.76886425, 0.21727246]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 3.3466245900205425}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8231227159425956
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.35798557, 4.72562064, 5.93408715]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.4510406825545803}
episode index:3595
target Thresh 19.0
target distance 5.0
model initialize at round 3595
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.        ,  6.        ,  0.25522583]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8231530114464789
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.4882279 , 10.15889517,  1.68885522]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9845648867548894}
episode index:3596
target Thresh 19.0
target distance 11.0
model initialize at round 3596
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.59785424, 7.528441  , 1.32939595]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.736188107296899}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.823123703000872
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.40081839,  5.5615136 ,  4.49754287]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.8211675369888423}
episode index:3597
target Thresh 19.0
target distance 5.0
model initialize at round 3597
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([11.47728794, 11.80616771,  1.50957936]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 5.1861751753551335}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8231488262761589
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.86744171,  8.39394496,  0.66002332]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.4156492880431424}
episode index:3598
target Thresh 19.0
target distance 1.0
model initialize at round 3598
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.        ,  9.        ,  2.02429586]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.4142135623730951}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8231924359382107
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.84399112,  7.60586682,  6.02429586]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.4238864596515298}
episode index:3599
target Thresh 19.0
target distance 11.0
model initialize at round 3599
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 5.63867936, 11.3835597 ,  1.23992699]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 9.369175106607873}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8232002874201986
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.20645629, 11.48741305,  1.82400033]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9312803552425986}
episode index:3600
target Thresh 19.0
target distance 1.0
model initialize at round 3600
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.        , 7.        , 5.94400859]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.4142135623730951}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8232438585705957
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.58495604, 7.75705769, 3.66082328]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.48091834616863594}
episode index:3601
target Thresh 19.0
target distance 4.0
model initialize at round 3601
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([16.        ,  9.        ,  2.57526869]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 4.472135954999579}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8232740699779629
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.76952353,  5.73106297,  4.00889807]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.7665327532723988}
episode index:3602
target Thresh 19.0
target distance 2.0
model initialize at round 3602
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.27211487,  8.48257785,  5.27512479]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5540107279195347}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8233231196393623
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.27211487,  8.48257785,  5.27512479]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5540107279195347}
episode index:3603
target Thresh 19.0
target distance 2.0
model initialize at round 3603
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.72443686,  9.09787374,  3.44092095]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.1569963460108423}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8233721420811938
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.72443686,  9.09787374,  3.44092095]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 1.1569963460108423}
episode index:3604
target Thresh 19.0
target distance 5.0
model initialize at round 3604
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.39110068, 4.49379255, 3.85380554]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 4.523147728302833}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8233971476582818
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.66285177, 9.42177575, 3.0042495 ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5399664022828421}
episode index:3605
target Thresh 19.0
target distance 13.0
model initialize at round 3605
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([15.        ,  7.        ,  1.54184645]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 13.038404810405295}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.823380216312383
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.2783019 , 7.60341046, 5.27636399]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.484494800226959}
episode index:3606
target Thresh 19.0
target distance 6.0
model initialize at round 3606
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 9.        , 11.        ,  4.28279352]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 7.810249675906655}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8233880004972408
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.61259086,  6.16736074,  4.86686699]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.42201357478448853}
episode index:3607
target Thresh 19.0
target distance 1.0
model initialize at round 3607
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.        ,  3.        ,  5.96838427]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8234314350869035
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.27612018,  4.55695695,  3.68519669]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.6216457182388364}
episode index:3608
target Thresh 19.0
target distance 3.0
model initialize at round 3608
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.5805704 , 4.52131643, 1.32859248]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.5457679065874292}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8234694413420748
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.09260154, 7.30873832, 3.04540717]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.3223265328973918}
episode index:3609
target Thresh 19.0
target distance 12.0
model initialize at round 3609
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.        ,  4.        ,  0.16989201]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8234381626124155
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.92717109, 5.30095969, 5.33803894]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9747938099753471}
episode index:3610
target Thresh 19.0
target distance 14.0
model initialize at round 3610
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([16.        ,  5.        ,  5.01496053]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 14.866068747176946}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8234129245085777
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 1.80236196, 10.2119725 ,  4.18310746]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.28981569368129856}
episode index:3611
target Thresh 19.0
target distance 3.0
model initialize at round 3611
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([15.00000163,  9.00000003,  1.03061884]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.162278206770195}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8234482448644448
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.17776312,  6.79882674,  4.74743354]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1463845966164476}
episode index:3612
target Thresh 19.0
target distance 7.0
model initialize at round 3612
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 9.45574325, 11.90702864,  2.25898728]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 6.606814462941368}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.823465664357069
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.21255682, 11.64061007,  1.12624212]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6749530798632751}
episode index:3613
target Thresh 19.0
target distance 4.0
model initialize at round 3613
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([13.59876156,  5.34868818,  4.58669531]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 4.1206381263625955}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8235036085589624
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.80275944,  2.84944665,  0.0203247 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8720455506078396}
episode index:3614
target Thresh 19.0
target distance 10.0
model initialize at round 3614
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.        ,  8.        ,  1.48796004]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8234997978424098
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.14142838, 7.46665854, 5.78884819]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.4876188871908273}
episode index:3615
target Thresh 19.0
target distance 9.0
model initialize at round 3615
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 4.68254119, 11.03562323,  1.03116911]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 9.425418520671567}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8235099080627635
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.47544709, 2.87073704, 5.89842788]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.9920851448509267}
episode index:3616
target Thresh 19.0
target distance 2.0
model initialize at round 3616
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 3.        , 10.        ,  3.51889813]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.0000000000006803}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8235559379471806
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.65988963, 8.35175456, 5.51889813]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.4892916654957989}
episode index:3617
target Thresh 19.0
target distance 6.0
model initialize at round 3617
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([2.32651727, 2.17844411, 4.04536366]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 6.057314376136329}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8235757782778361
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.53654436, 7.29811316, 0.91262244]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8834732547276305}
episode index:3618
target Thresh 19.0
target distance 5.0
model initialize at round 3618
at step 0:
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([3.63758083, 9.388223  , 1.24277371]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.728764910135163}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8236083575459
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.23599412, 10.34916498,  0.67640309]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0036389825258625}
episode index:3619
target Thresh 19.0
target distance 1.0
model initialize at round 3619
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.99979218,  1.99994368,  4.41622949]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.0000563395537614}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8236515872813845
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.66284503,  3.51341245,  2.13304418]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8384246391122406}
episode index:3620
target Thresh 19.0
target distance 2.0
model initialize at round 3620
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([16.        ,  7.        ,  0.14147394]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 2.0}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8236947931396333
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.31850763,  7.01701642,  4.14147383]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.3189618581697384}
episode index:3621
target Thresh 19.0
target distance 6.0
model initialize at round 3621
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 8.       , 11.       ,  1.2498781]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8237072313007678
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.46932952, 6.78794307, 4.11713688]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5714711663030099}
episode index:3622
target Thresh 19.0
target distance 13.0
model initialize at round 3622
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 4.14734824, 11.87197583,  1.85393732]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 13.227450765620764}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8236945661415348
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.82462353,  6.93024768,  1.58845487]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.9466349097766938}
episode index:3623
target Thresh 19.0
target distance 3.0
model initialize at round 3623
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.59757904, 10.47072759,  0.69008368]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.5032853320684736}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8237296918268986
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.29279118,  8.84901104,  4.40689825]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.1049724244158987}
episode index:3624
target Thresh 19.0
target distance 2.0
model initialize at round 3624
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.91866781, 5.71038425, 5.0246141 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9632381999093722}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8237783181188084
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.91866781, 5.71038425, 5.0246141 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9632381999093722}
episode index:3625
target Thresh 19.0
target distance 3.0
model initialize at round 3625
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.        ,  5.        ,  5.24693966]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.1622776601683786}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8238134013322065
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.39175888,  8.18057102,  2.68056904]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.6344786500009866}
episode index:3626
target Thresh 19.0
target distance 6.0
model initialize at round 3626
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.        , 2.        , 4.69132066]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 6.082762530298655}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8238306529093733
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.94453914, 8.74980894, 3.55857943]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7518572701294011}
episode index:3627
target Thresh 19.0
target distance 10.0
model initialize at round 3627
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 3.        , 10.        ,  4.75749016]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.04987562112089}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8238359209745537
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.95585083, 11.66805562,  1.05834303]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.6695128553223808}
episode index:3628
target Thresh 19.0
target distance 2.0
model initialize at round 3628
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.40143976, 2.52516257, 3.83417833]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.6610216334299499}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8238844643967156
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.40143976, 2.52516257, 3.83417833]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.6610216334299499}
episode index:3629
target Thresh 19.0
target distance 13.0
model initialize at round 3629
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([3.        , 4.        , 0.09417361]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 13.341664064122993}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8238455305483998
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.7824946 ,  6.84688433,  0.69594992]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.26599437958830213}
episode index:3630
target Thresh 19.0
target distance 13.0
model initialize at round 3630
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([16.       ,  2.       ,  6.0863843]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 13.341664064126332}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.823797405265508
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.45328502, 5.22882833, 4.12179   ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.592671641608102}
episode index:3631
target Thresh 19.0
target distance 3.0
model initialize at round 3631
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([11.        , 11.        ,  4.57405448]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8238350700795869
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([1.34975467e+01, 8.97830678e+00, 7.68386524e-03]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.5029213887928622}
episode index:3632
target Thresh 19.0
target distance 14.0
model initialize at round 3632
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 2.       , 11.       ,  2.9964453]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 15.652475842498527}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8237999966273354
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.52662146,  3.73117344,  5.88140632]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.5912680305784646}
episode index:3633
target Thresh 19.0
target distance 14.0
model initialize at round 3633
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([16.        ,  9.        ,  2.26908842]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8237916901570592
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.87945145, 10.7220955 ,  4.28675777]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.9223154344482134}
episode index:3634
target Thresh 19.0
target distance 9.0
model initialize at round 3634
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([1.10779485, 5.32753878, 3.2664814 ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 11.40318108890395}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8237812049990537
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.27764494, 11.67546527,  1.00099763]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.988964192204776}
episode index:3635
target Thresh 19.0
target distance 4.0
model initialize at round 3635
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.        , 10.        ,  2.10541967]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.9999999999999996}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8238161909299946
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.7473201 ,  6.77026368,  5.82223377]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.0732164154018626}
episode index:3636
target Thresh 19.0
target distance 6.0
model initialize at round 3636
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([ 2.00000005, 10.00000003,  1.59286135]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 6.00000003095866}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.823835856056013
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.11832573, 4.1488892 , 4.74330543]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.1901814213738422}
episode index:3637
target Thresh 19.0
target distance 2.0
model initialize at round 3637
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.        ,  9.        ,  2.85130799]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.2360679775046752}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8238788093666078
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.70901939,  7.39596406,  0.56811779]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.4913830021736421}
episode index:3638
target Thresh 19.0
target distance 11.0
model initialize at round 3638
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.06417405, 6.75844779, 5.98206401]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.926236399587456}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.8238289969985721
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.6158071 ,  1.83412893,  6.01746947]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6377551215920965}
episode index:3639
target Thresh 19.0
target distance 9.0
model initialize at round 3639
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 7.57017115, 10.40233698,  0.64195984]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 9.807766083554347}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8238121051626789
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.60186691,  3.72505667,  4.37647738]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.4838427324040495}
episode index:3640
target Thresh 19.0
target distance 1.0
model initialize at round 3640
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.0027055 , 5.99709092, 0.18835753]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.4143630911616238}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8238577486383276
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.64634551, 7.55212005, 2.18835753]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6556737381146074}
episode index:3641
target Thresh 19.0
target distance 1.0
model initialize at round 3641
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.        ,  6.        ,  3.64196789]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0000000000971117}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8239033670489156
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.86479963,  4.32247   ,  5.64196789]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.690887868642113}
episode index:3642
target Thresh 19.0
target distance 12.0
model initialize at round 3642
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 4.54793825, 11.59127363,  2.24917321]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 11.467315418588246}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8238972531928876
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.04562425, 11.32292729,  2.26687606]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.3261343425663706}
episode index:3643
target Thresh 19.0
target distance 11.0
model initialize at round 3643
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.92774857,  9.68141839,  2.62374043]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 13.357390361586917}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8238622685509713
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.02814989, 1.43958165, 5.50870206]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.5611248915939676}
episode index:3644
target Thresh 19.0
target distance 3.0
model initialize at round 3644
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.        ,  6.        ,  4.95867682]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.1622776601721383}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8238945368310399
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.54224557,  8.92530287,  4.3923062 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.4638089924648752}
episode index:3645
target Thresh 19.0
target distance 7.0
model initialize at round 3645
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([5.28078596, 9.2876759 , 0.49440306]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.688561344222935}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8239116762536632
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.1824047 , 2.86358902, 5.64484714]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.22777056421311914}
episode index:3646
target Thresh 19.0
target distance 1.0
model initialize at round 3646
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.97718996, 1.31719079, 5.70883512]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.6831901020531215}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8239599593147399
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.97718996, 1.31719079, 5.70883512]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.6831901020531215}
episode index:3647
target Thresh 19.0
target distance 5.0
model initialize at round 3647
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([10.1039553 , 10.19418906,  0.14817112]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.471423698519095}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8239845090099616
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.26579204,  8.525061  ,  5.58180027]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9026352421907127}
episode index:3648
target Thresh 19.0
target distance 6.0
model initialize at round 3648
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 9.        , 11.        ,  0.49263399]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 6.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8240040633386129
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.62358618, 11.53543219,  3.64307807]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.6545036244618058}
episode index:3649
target Thresh 19.0
target distance 1.0
model initialize at round 3649
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.00015479,  9.9904777 ,  5.73864293]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0002001175857465}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8240441441431776
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.26685742,  9.45342391,  5.45545762]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9144634797884794}
episode index:3650
target Thresh 19.0
target distance 5.0
model initialize at round 3650
at step 0:
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([1.10982347, 8.31350449, 3.28155315]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.4739043588047345}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8240636714261454
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.34209413, 10.4877653 ,  0.14881192]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8338012452450035}
episode index:3651
target Thresh 19.0
target distance 2.0
model initialize at round 3651
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.98193757, 7.68287229, 2.59152901]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0318775969318585}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8241118467625567
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.98193757, 7.68287229, 2.59152901]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.0318775969318585}
episode index:3652
target Thresh 19.0
target distance 11.0
model initialize at round 3652
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 4.19035809, 10.18972085,  1.79513043]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.666903735850362}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8241013256276113
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.04142622,  6.14880798,  5.81283328]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.15446665245689006}
episode index:3653
target Thresh 19.0
target distance 5.0
model initialize at round 3653
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 5.        , 11.        ,  2.78921634]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 5.385164807271011}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8241283216234516
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.85043728, 5.8224436 , 6.22284573]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8687749043646886}
episode index:3654
target Thresh 19.0
target distance 6.0
model initialize at round 3654
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.89353674,  8.67959926,  2.64409775]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.333879930384955}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8241502788747198
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.25314282, 10.66804961,  3.79454182]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.41745940056787073}
episode index:3655
target Thresh 19.0
target distance 4.0
model initialize at round 3655
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([10.91304533, 10.8509365 ,  5.19430971]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 4.983075095873426}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8241797961255493
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.29815674,  8.86370401,  0.34475367]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.9137188020575938}
episode index:3656
target Thresh 19.0
target distance 13.0
model initialize at round 3656
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([2.24097531, 2.91903879, 0.68587559]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.759281556124797}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.8241215345568885
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.63257863,  3.70546508,  4.43809598]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7954114961795175}
episode index:3657
target Thresh 19.0
target distance 13.0
model initialize at round 3657
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([3.        , 4.        , 4.61486816]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 14.7648230602334}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8240924500839637
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.54489062, 11.44853719,  1.49982955]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6389915153783966}
episode index:3658
target Thresh 19.0
target distance 6.0
model initialize at round 3658
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 2.58292081, 10.09210427,  4.72138882]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 5.109156583678478}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8241219589382472
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.74859723, 5.90295457, 6.1550182 ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9372994744394413}
episode index:3659
target Thresh 19.0
target distance 14.0
model initialize at round 3659
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([16.        ,  8.        ,  0.47978037]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 14.560219778561034}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8240968718919948
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.68306679, 4.89004312, 5.93111261]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.1219434016421264}
episode index:3660
target Thresh 19.0
target distance 4.0
model initialize at round 3660
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([16.        ,  4.        ,  0.64859789]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.472135954999579}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8241263634178115
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.08897855,  7.71391351,  2.08222728]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.2996041715163294}
episode index:3661
target Thresh 19.0
target distance 12.0
model initialize at round 3661
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([16.        ,  4.        ,  6.05837822]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 12.16552506059645}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8240934091118536
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.37335303, 5.41815935, 0.66015453]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.6913255556185885}
episode index:3662
target Thresh 19.0
target distance 13.0
model initialize at round 3662
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.61335027,  3.9537246 ,  3.54910803]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.613442465883567}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.8240421101975626
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.41311765, 3.73673825, 5.58451373]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.48987033066816926}
episode index:3663
target Thresh 19.0
target distance 3.0
model initialize at round 3663
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([14.        , 11.        ,  3.36779237]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8240767575610185
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.77863754,  7.90130732,  0.80142175]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.24236662621431423}
episode index:3664
target Thresh 19.0
target distance 11.0
model initialize at round 3664
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.        , 5.        , 4.39445066]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8240419243988053
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.28062473,  8.30946119,  0.99622698]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.4177516807572305}
episode index:3665
target Thresh 19.0
target distance 1.0
model initialize at round 3665
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.93777825, 11.62751283,  2.6190089 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.1283618229464838}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8240899216916588
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.93777825, 11.62751283,  2.6190089 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.1283618229464838}
episode index:3666
target Thresh 19.0
target distance 12.0
model initialize at round 3666
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.6116176 ,  8.36245696,  5.48951793]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.907405109685639}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8240794467036893
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.70991817, 11.33461728,  3.22403547]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.4428500801091327}
episode index:3667
target Thresh 19.0
target distance 1.0
model initialize at round 3667
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.        ,  7.99999999,  5.22201324]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.4142135649119334}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.824121982296191
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.41023471,  9.57624986,  2.93882793]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7073587594618227}
episode index:3668
target Thresh 19.0
target distance 12.0
model initialize at round 3668
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([15.04993215,  3.31530626,  1.90712851]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 14.234341296152818}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8240929848991625
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.19392662, 9.22702808, 5.07527544]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.2985787730956193}
episode index:3669
target Thresh 19.0
target distance 7.0
model initialize at round 3669
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.13326647,  3.91993266,  3.57126343]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 6.141534493001847}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8241099581653251
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.69762668, 10.52043241,  2.4385222 ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6018965954483606}
episode index:3670
target Thresh 19.0
target distance 8.0
model initialize at round 3670
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 8.81868453, 10.12526259,  4.96997905]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 8.60169097709193}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8241105189568452
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.62636669, 3.43554058, 5.2708672 ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.5738444421221944}
episode index:3671
target Thresh 19.0
target distance 14.0
model initialize at round 3671
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([2.        , 5.        , 4.91764641]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.824083522719645
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.08017813,  6.97217502,  6.08579334]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9202426327368808}
episode index:3672
target Thresh 19.0
target distance 6.0
model initialize at round 3672
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([2.        , 8.        , 5.84552526]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.708203932499364}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.824098071453726
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.21172854, 10.85460595,  0.42959873]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.2568431564318935}
episode index:3673
target Thresh 19.0
target distance 1.0
model initialize at round 3673
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.21343072,  4.9321725 ,  4.98020911]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.787856357281987}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8241432271229002
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.81574002,  4.41395129,  0.6970238 ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.6143328318552485}
episode index:3674
target Thresh 19.0
target distance 3.0
model initialize at round 3674
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.33071113,  4.79214577,  4.27547169]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.811662829531919}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8241803571318463
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.19594396,  1.9738419 ,  5.99228638]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.19768227698576749}
episode index:3675
target Thresh 19.0
target distance 9.0
model initialize at round 3675
at step 0:
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.31711328,  1.98351956,  4.1613853 ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 11.00921612836606}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8241677458702216
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.39303073, 11.32777351,  3.89590284]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.5117700885368532}
episode index:3676
target Thresh 19.0
target distance 1.0
model initialize at round 3676
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.21221748,  7.1664349 ,  1.77615326]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.2696976706629905}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8242155653573388
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.21221748,  7.1664349 ,  1.77615326]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.2696976706629905}
episode index:3677
target Thresh 19.0
target distance 6.0
model initialize at round 3677
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 6.90514245, 10.13128078,  5.03269672]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 7.0986241608139125}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8242300584127062
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.594914  , 4.89070924, 5.8999555 ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.604869516031001}
episode index:3678
target Thresh 19.0
target distance 3.0
model initialize at round 3678
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.4311798 ,  3.44446795,  4.49192262]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.581581243842455}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8242645134253421
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.39193763,  6.25078489,  1.925552  ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8455402910150509}
episode index:3679
target Thresh 19.0
target distance 12.0
model initialize at round 3679
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([15.       ,  8.       ,  0.1004154]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.16552506059644}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8242540279980003
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.73967882, 9.95220226, 4.11811825]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.26467289007302336}
episode index:3680
target Thresh 19.0
target distance 2.0
model initialize at round 3680
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.73674848,  6.3148298 ,  0.26085966]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.1783187795534946}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8242937033503508
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.93782988,  4.69748121,  6.26085966]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9854148872142814}
episode index:3681
target Thresh 19.0
target distance 2.0
model initialize at round 3681
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([4.28627052, 5.08390301, 1.71022385]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.579153427430911}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8243387077763826
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.83809393, 5.76809952, 3.71022385]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.28282752180235254}
episode index:3682
target Thresh 19.0
target distance 3.0
model initialize at round 3682
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.34927122,  7.35367206,  5.93144107]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.3980051648166272}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8243783385915397
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.22462352,  5.67535192,  5.64825576]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.39478108283878827}
episode index:3683
target Thresh 19.0
target distance 13.0
model initialize at round 3683
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([2.38766209, 3.63771371, 2.34836513]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 14.126208072225635}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.82434744143863
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.82074765,  9.69971349,  1.23332651]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.3497190145848054}
episode index:3684
target Thresh 19.0
target distance 13.0
model initialize at round 3684
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.56514487, 9.61863529, 1.38641136]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.740433601730526}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8243165610548671
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.45585553,  2.82455394,  0.27137299]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9421748594992555}
episode index:3685
target Thresh 19.0
target distance 13.0
model initialize at round 3685
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([16.67221789,  6.49864638,  2.11552853]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 13.754107864064313}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8242876446065609
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.65565249, 5.59988895, 5.28367546]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.8886770697193966}
episode index:3686
target Thresh 19.0
target distance 3.0
model initialize at round 3686
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.49033562,  9.25616667,  4.60939479]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 2.2939169438779103}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8243272462760465
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.24159468, 10.53405012,  4.32620948]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.5248592944328856}
episode index:3687
target Thresh 19.0
target distance 5.0
model initialize at round 3687
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.        , 4.        , 0.26904648]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.824356459427248
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.80801737, 8.12142257, 1.70267587]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.193645832576833}
episode index:3688
target Thresh 19.0
target distance 3.0
model initialize at round 3688
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.        , 6.        , 1.88744467]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 3.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8243907867762512
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.07811831, 2.66867976, 5.60425936]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.3404050108072598}
episode index:3689
target Thresh 19.0
target distance 4.0
model initialize at round 3689
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.4153126 ,  5.87568261,  4.22988141]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 3.9195376093560026}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8244276987608646
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.60694973,  2.76414991,  5.9466961 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.975865289813476}
episode index:3690
target Thresh 19.0
target distance 11.0
model initialize at round 3690
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.98564372,  9.01850032,  3.24073184]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 13.03624619025956}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8243987913736626
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.22976614, 2.91774568, 0.12569346]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.9460706189841838}
episode index:3691
target Thresh 19.0
target distance 11.0
model initialize at round 3691
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.12026294,  5.27586902,  3.99387407]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 11.626925539038236}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8243799182685244
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.76735518, 11.77004498,  3.44520631]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.8044208408738293}
episode index:3692
target Thresh 19.0
target distance 4.0
model initialize at round 3692
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.00000718,  3.99999815,  0.75785416]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 4.000001850190519}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8244168032107749
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.88185292,  7.33015813,  2.47466885]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6801814931070445}
episode index:3693
target Thresh 19.0
target distance 8.0
model initialize at round 3693
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.        ,  2.        ,  5.30800438]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8244288034840717
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.58027109,  9.72143725,  1.89207784]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.6436705270139709}
episode index:3694
target Thresh 19.0
target distance 9.0
model initialize at round 3694
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([14.        ,  3.        ,  0.14437598]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8244058729211229
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.72525846, 11.57086308,  3.59570821]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.6335357657839221}
episode index:3695
target Thresh 19.0
target distance 12.0
model initialize at round 3695
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([2.01146621, 3.68293096, 2.57398319]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.007969731666336}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.8243353911347405
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.25094493,  3.92856333,  3.47664766]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.9618748436984716}
episode index:3696
target Thresh 19.0
target distance 13.0
model initialize at round 3696
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([2.        , 6.        , 4.39970946]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8243145192665223
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.74065305,  8.82021783,  5.8510417 ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.31556689915596287}
episode index:3697
target Thresh 19.0
target distance 3.0
model initialize at round 3697
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([13.11512209,  7.27442922,  3.3202796 ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 3.3615643752726996}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8243487744127185
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.77580281,  8.41642701,  0.75390898]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.6251574277883413}
episode index:3698
target Thresh 19.0
target distance 8.0
model initialize at round 3698
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([16.0128839,  8.6829207,  2.5731408]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.34117286756406}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8243607768561778
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.57292568, 10.20006395,  5.44039957]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9839418296137273}
episode index:3699
target Thresh 19.0
target distance 12.0
model initialize at round 3699
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([16.00000005,  5.99999996,  0.32516306]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.649110674741664}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8243187798242085
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.61486123, 2.25103323, 4.92693938]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.4597276928251346}
episode index:3700
target Thresh 19.0
target distance 8.0
model initialize at round 3700
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([1.91986391, 9.31893894, 5.6747551 ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 6.410591491630858}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.8243355499111827
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.81042448, 3.93983864, 4.54201387]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.9587677268403392}
episode index:3701
target Thresh 19.0
target distance 2.0
model initialize at round 3701
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.91865814, 1.82684545, 4.31037402]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 2.1746763479306597}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8243776256675548
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.80349534, 3.25843566, 2.02718871]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.0933994843824852}
episode index:3702
target Thresh 19.0
target distance 13.0
model initialize at round 3702
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([16.       ,  2.       ,  0.4045183]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8243356581096023
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.62551285, 6.2501244 , 5.00629462]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.4503363652451365}
episode index:3703
target Thresh 19.0
target distance 12.0
model initialize at round 3703
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([3.        , 5.        , 3.84470773]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.041594580768697}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8242901191145874
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.06374954,  3.50291717,  6.16329873]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.5011540127762965}
episode index:3704
target Thresh 19.0
target distance 5.0
model initialize at round 3704
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([4.00000014, 9.        , 1.02403706]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 5.09901954397599}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8243166925492198
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.96775045, 4.93896094, 4.45766644]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9395145966109646}
episode index:3705
target Thresh 19.0
target distance 6.0
model initialize at round 3705
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.23053331, 6.33289398, 5.85980105]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 4.339022453910203}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8243407617761314
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.86463793, 1.86375614, 5.01024513]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.1920554057165184}
episode index:3706
target Thresh 19.0
target distance 4.0
model initialize at round 3706
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([16.1314094 ,  3.24588711,  1.84351557]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.4825341140127932}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8243775179801301
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.39343763,  5.46925435,  3.5603274 ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.6606694445109706}
episode index:3707
target Thresh 19.0
target distance 3.0
model initialize at round 3707
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.        ,  3.        ,  5.39147782]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8244116637546501
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.93785964,  5.96821201,  2.82510721]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.06979900302319958}
episode index:3708
target Thresh 19.0
target distance 11.0
model initialize at round 3708
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.90173618, 8.41763217, 0.60924357]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 10.115042479860225}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8244055216478247
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.03530412,  9.69956363,  0.62694642]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7004538910329349}
episode index:3709
target Thresh 19.0
target distance 5.0
model initialize at round 3709
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.        ,  4.        ,  0.35196369]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8244345404689188
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.73075781,  8.42372062,  1.78559308]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9306475688721654}
episode index:3710
target Thresh 19.0
target distance 12.0
model initialize at round 3710
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 3.81297239, 10.67254559,  2.69215584]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 13.052062066083876}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8244178086375737
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.70988387,  6.3402346 ,  0.143487  ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.4471319168983912}
episode index:3711
target Thresh 19.0
target distance 9.0
model initialize at round 3711
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.       , 11.       ,  0.3272106]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.816653827763709}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8244116698393251
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.86385239, 5.32079049, 0.34491345]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9214919886222587}
episode index:3712
target Thresh 19.0
target distance 2.0
model initialize at round 3712
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.6701977 ,  5.84224178,  0.91582459]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.1828170070104607}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8244562667502221
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.11684413,  7.43164038,  2.91582459]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.05023661670585}
episode index:3713
target Thresh 19.0
target distance 10.0
model initialize at round 3713
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 5.51608126, 11.73067303,  1.45910328]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.23804134688838}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8244523011071071
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.82158609,  6.41205671,  5.75999144]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.4490236714316159}
episode index:3714
target Thresh 19.0
target distance 3.0
model initialize at round 3714
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([1.99994066, 2.00001275, 3.93996382]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.605573583045384}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8244863624123004
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.59062562, 4.32781641, 1.3735932 ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.7870312393400405}
episode index:3715
target Thresh 19.0
target distance 13.0
model initialize at round 3715
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([15.        ,  5.        ,  1.24056118]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8244518970881985
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.47024842, 5.87143058, 4.1255228 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5451301084783281}
episode index:3716
target Thresh 19.0
target distance 5.0
model initialize at round 3716
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.16651714,  7.14609336,  1.78656691]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.445579553014371}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8244734010370607
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.68822773, 11.64825647,  2.93701099]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.945459602046444}
episode index:3717
target Thresh 19.0
target distance 3.0
model initialize at round 3717
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.15037019, 11.02479507,  1.17342299]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 3.141855511422511}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8245074291836081
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.11105979,  8.63040595,  4.89023768]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.089782713158806}
episode index:3718
target Thresh 19.0
target distance 10.0
model initialize at round 3718
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.24748986,  8.6646732 ,  2.43320566]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.88305462932472}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8244846254579303
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.2985836 , 4.50943294, 5.8845379 ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.5742893026881192}
episode index:3719
target Thresh 19.0
target distance 3.0
model initialize at round 3719
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.87880479,  3.43530144,  2.03139711]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.5693851837046613}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8245238228704417
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.81993916,  4.54244172,  1.74821109]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9389674141932478}
episode index:3720
target Thresh 19.0
target distance 2.0
model initialize at round 3720
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.        ,  8.        ,  5.08850098]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.828427124751301}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8245656331841019
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.25039146,  9.30337584,  2.80531567]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.023327015508668}
episode index:3721
target Thresh 19.0
target distance 9.0
model initialize at round 3721
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 6.15691082, 11.8645848 ,  2.53528833]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.09279007700582}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.824568305975146
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.05081921,  7.91878098,  0.83617649]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.9201853558041082}
episode index:3722
target Thresh 19.0
target distance 8.0
model initialize at round 3722
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([16.55850322,  2.63518116,  1.39700526]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 7.796569451369982}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8245825291062295
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.64574639, 10.14817776,  2.26426403]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.6625292848847003}
episode index:3723
target Thresh 19.0
target distance 3.0
model initialize at round 3723
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.62846936,  9.57539596,  0.75494068]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.6509692761080306}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8246216581263407
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.68849789,  7.75852061,  0.47175538]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.0243938926947824}
episode index:3724
target Thresh 19.0
target distance 4.0
model initialize at round 3724
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.67154868, 6.80439806, 0.89351147]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 3.8632139738678877}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8246530295333943
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.58796718, 3.21259425, 0.32714086]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.6252213326314997}
episode index:3725
target Thresh 19.0
target distance 13.0
model initialize at round 3725
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([3.        , 7.        , 5.09845185]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8246262789446729
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.02905791,  8.15154276,  6.26659878]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9826971805947708}
episode index:3726
target Thresh 19.0
target distance 6.0
model initialize at round 3726
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 8.08857427, 10.82762227,  6.1970458 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 7.770242816494621}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8246357858069473
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.97130329, 5.46839214, 4.78111927]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5323818364633177}
episode index:3727
target Thresh 19.0
target distance 12.0
model initialize at round 3727
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([16.        ,  4.        ,  0.88283318]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 12.64911064067352}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8246110185279364
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.8585935 , 8.04568098, 0.05098011]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8598078608676804}
episode index:3728
target Thresh 19.0
target distance 14.0
model initialize at round 3728
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([2.        , 2.        , 5.11739779]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 14.142135623730953}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.8245622120338514
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.61485853,  4.03147387,  5.15280348]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.38642535888684837}
episode index:3729
target Thresh 19.0
target distance 5.0
model initialize at round 3729
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([16.        ,  6.        ,  0.86771935]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 5.83095189484556}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8245836114609224
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.68532158, 11.37608665,  2.01816343]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7817332270545492}
episode index:3730
target Thresh 19.0
target distance 10.0
model initialize at round 3730
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([13.66591934,  8.64947816,  2.78063029]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 10.33192564560332}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8245669293657432
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.45348934, 5.4010986 , 0.23196253]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.605419416912539}
episode index:3731
target Thresh 19.0
target distance 12.0
model initialize at round 3731
at step 0:
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([14.        , 10.        ,  1.16635722]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8245502562105935
{'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.06592427, 3.95256901, 4.90087476]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.0812139678744842}
episode index:3732
target Thresh 19.0
target distance 7.0
model initialize at round 3732
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([1.16747928, 8.09040514, 4.71477199]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 6.716860007250067}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8245716416428995
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.250354  , 2.29545575, 5.86521606]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.8057687212143438}
episode index:3733
target Thresh 19.0
target distance 12.0
model initialize at round 3733
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.10371014,  6.3548206 ,  3.23493469]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.599448270925699}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8245317802094281
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.24980149, 2.48083516, 5.836711  ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.5761361996350248}
episode index:3734
target Thresh 19.0
target distance 11.0
model initialize at round 3734
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.09381709,  4.53193775,  3.78000712]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.406529307304153}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.8244865666727118
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.29870936, 2.7045555 , 6.09859813]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7652618766356615}
episode index:3735
target Thresh 19.0
target distance 4.0
model initialize at round 3735
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.42939843, 8.37273079, 5.98038483]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.4403765213648123}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8245255956966216
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.38700976, 6.69029469, 5.69719952]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9231813458371915}
episode index:3736
target Thresh 19.0
target distance 6.0
model initialize at round 3736
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 8.        , 11.        ,  4.75004101]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 7.810249675906654}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8245194691228572
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.1610143 ,  6.54219283,  4.76774386]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.998934465610415}
episode index:3737
target Thresh 19.0
target distance 5.0
model initialize at round 3737
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([12.7460313 , 10.12247521,  3.42147017]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 3.8474407444975025}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8245482400909644
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.32814717, 10.51581544,  4.85509955]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5849061946572874}
episode index:3738
target Thresh 19.0
target distance 12.0
model initialize at round 3738
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([1.36835295, 2.41245111, 3.9039973 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 15.60104802527964}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8245139702268185
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.25204742, 10.58282174,  0.50577362]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9482163507061465}
episode index:3739
target Thresh 19.0
target distance 5.0
model initialize at round 3739
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([12.67764757, 10.86625872,  0.93044871]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 5.04272643857221}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.824530513248607
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.96646307,  5.75304919,  6.08089243]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.24921763756949736}
episode index:3740
target Thresh 19.0
target distance 5.0
model initialize at round 3740
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.        , 2.        , 5.49727821]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 5.0}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8245567667052174
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.81922607, 6.92818376, 2.64772229]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.19451680670545082}
episode index:3741
target Thresh 19.0
target distance 14.0
model initialize at round 3741
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 2.71340625, 10.52428331,  2.14305241]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 13.746059299445154}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8245401408227059
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.24782524,  6.8585755 ,  5.87756996]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.7653546673054122}
episode index:3742
target Thresh 19.0
target distance 13.0
model initialize at round 3742
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([1.10514696, 5.34533613, 3.24629295]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 14.146171969006033}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8245116049936317
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.35868626,  8.78128271,  0.13125457]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0107848314687182}
episode index:3743
target Thresh 19.0
target distance 1.0
model initialize at round 3743
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 5.        , 11.        ,  2.01985199]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.0000000000004512}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8245558059538364
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.33183651, 11.22104512,  4.01985199]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7037779477754894}
episode index:3744
target Thresh 19.0
target distance 6.0
model initialize at round 3744
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 5.99764414, 11.00165145,  3.54019797]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 7.211170418477158}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8245699488689352
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.69399637, 4.73676847, 4.40745674]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.4036447188816035}
episode index:3745
target Thresh 19.0
target distance 8.0
model initialize at round 3745
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([12.        , 11.        ,  0.99848669]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8245817416783052
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.7324557 , 9.9512732 , 3.86574546]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9881804762352787}
episode index:3746
target Thresh 19.0
target distance 14.0
model initialize at round 3746
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([14.33976514,  8.72430401,  4.31614947]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 12.889540098342142}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8245610825783996
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.97720017, 4.68125862, 5.76748171]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.31955578852368877}
episode index:3747
target Thresh 19.0
target distance 2.0
model initialize at round 3747
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.31871298, 9.07094148, 4.10942292]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.9552096411282482}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8245999667612761
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.8583001 , 10.81536476,  3.82623761]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.23274241837408488}
episode index:3748
target Thresh 19.0
target distance 1.0
model initialize at round 3748
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.34652444,  8.87914897,  0.92048949]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.173194873769372}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8246440852017239
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.78576265, 10.46594857,  2.92048949]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5128408263466203}
episode index:3749
target Thresh 19.0
target distance 12.0
model initialize at round 3749
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.77717531,  6.49277816,  2.10078641]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.787239065905947}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8246155749210295
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.33553249, 6.55242258, 5.26893334]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8011508118456433}
episode index:3750
target Thresh 19.0
target distance 7.0
model initialize at round 3750
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([ 9.        , 11.        ,  3.60658312]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 8.602325267042627}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8246137864280081
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.37769447,  4.69770472,  5.90747116]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.7933756908350531}
episode index:3751
target Thresh 19.0
target distance 7.0
model initialize at round 3751
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([16.38716342,  2.9529773 ,  1.6119458 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 6.501156254985665}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8246350466328538
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.19450468,  8.69105541,  2.76238976]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.36507373040626373}
episode index:3752
target Thresh 19.0
target distance 4.0
model initialize at round 3752
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.01894677, 4.36745115, 3.20312607]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.8228648517290966}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8246661804198956
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.36492826, 7.63487782, 2.63675546]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7322857898443267}
episode index:3753
target Thresh 19.0
target distance 1.0
model initialize at round 3753
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.        , 3.00000002, 2.81026125]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 1.0000000035649677}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8247075852732734
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.45719594, 2.23594394, 0.52707594]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8903986679682183}
episode index:3754
target Thresh 19.0
target distance 1.0
model initialize at round 3754
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.        ,  6.        ,  5.48517632]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.0000000000003606}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.824751604558154
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.84511634,  6.19263329,  1.20199102]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.1687868176388738}
episode index:3755
target Thresh 19.0
target distance 6.0
model initialize at round 3755
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.09502747, 6.72199259, 0.14764613]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 4.722948682337066}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.824775237583427
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.28472029, 1.86261035, 5.58127551]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7283549831351266}
episode index:3756
target Thresh 19.0
target distance 3.0
model initialize at round 3756
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([11.39436302, 10.49570052,  4.45591688]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.4827562824415292}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8248139716165429
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.73122303, 10.50512484,  4.17273157]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5631540490555834}
episode index:3757
target Thresh 19.0
target distance 13.0
model initialize at round 3757
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.31704483,  9.00706869,  4.14739251]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 12.812508648483885}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.824793311191507
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.30434475, 3.39001861, 5.59872475]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.4947122798237782}
episode index:3758
target Thresh 19.0
target distance 11.0
model initialize at round 3758
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([3.00000001, 4.99999999, 0.44546192]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 12.529964078854382}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8247808153277687
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.91757921, 10.48887128,  0.17997947]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5177313523574956}
episode index:3759
target Thresh 19.0
target distance 1.0
model initialize at round 3759
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.48622903,  5.22248104,  0.5280053 ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5347115877622948}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8248274161747561
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.48622903,  5.22248104,  0.5280053 ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5347115877622948}
episode index:3760
target Thresh 19.0
target distance 4.0
model initialize at round 3760
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([13.10818292,  2.63848582,  3.72297895]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.8572982029382796}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8248584325888018
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.199096  ,  6.08349104,  3.15660834]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8052440458199146}
episode index:3761
target Thresh 19.0
target distance 8.0
model initialize at round 3761
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.53754668, 2.68435246, 1.42876643]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 7.330249791100039}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.824872431150846
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 3.44609191, 10.30041344,  2.29602424]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6301288856606325}
episode index:3762
target Thresh 19.0
target distance 1.0
model initialize at round 3762
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.98000452, 6.31714877, 5.71050763]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6831439254931988}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8249189704994639
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.98000452, 6.31714877, 5.71050763]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6831439254931988}
episode index:3763
target Thresh 19.0
target distance 12.0
model initialize at round 3763
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([2.89394272, 6.57407791, 6.28236961]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.666985090710433}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8248885864019009
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.26996046,  3.24827854,  5.16733123]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7711030799728197}
episode index:3764
target Thresh 19.0
target distance 4.0
model initialize at round 3764
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 5.6414929 , 10.0065985 ,  4.78298211]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 4.002139336110548}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8249170530052223
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.91313199, 6.69540425, 6.2166115 ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9625947258619818}
episode index:3765
target Thresh 19.0
target distance 13.0
model initialize at round 3765
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([15.       ,  4.       ,  0.9398095]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8248810816915908
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.55241231, 5.52745057, 5.82477112]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.7637823396229603}
episode index:3766
target Thresh 19.0
target distance 7.0
model initialize at round 3766
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([15.        ,  4.        ,  0.06386393]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 9.899494936611665}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8248836388139053
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.78931613, 10.89325039,  4.64793739]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7965020008332048}
episode index:3767
target Thresh 19.0
target distance 6.0
model initialize at round 3767
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.94602809, 4.39191197, 1.98385161]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.608404091155018}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8249071615338281
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.06514831, 8.46239265, 1.13429569]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.5415403652114489}
episode index:3768
target Thresh 19.0
target distance 3.0
model initialize at round 3768
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.83943328,  5.84451787,  1.55289548]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.833658058877128}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8249431628202346
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.36702043,  8.09483863,  3.26971018]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.6400449187230787}
episode index:3769
target Thresh 19.0
target distance 2.0
model initialize at round 3769
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.        , 3.        , 6.21354866]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 2.2360679774997902}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8249869444746589
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.01909057, 4.33934405, 1.93036335]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.6609317150891544}
episode index:3770
target Thresh 19.0
target distance 4.0
model initialize at round 3770
at step 0:
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([15.98868695,  8.36194083,  1.95287245]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.8172429889331556}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8250229055103325
{'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.1765165 , 10.50494923,  3.66968714]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9608331499841846}
episode index:3771
target Thresh 19.0
target distance 12.0
model initialize at round 3771
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([3.32333079, 6.03979984, 1.6759879 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 11.079442694608797}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8250042672234007
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.5360695 ,  9.35472164,  1.12732013]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.642804754259982}
episode index:3772
target Thresh 19.0
target distance 3.0
model initialize at round 3772
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 2.99956079, 10.00122277,  2.9256351 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.162307921736492}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8250376586314783
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.79279885, 10.06634759,  0.35926449]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9563676769754839}
episode index:3773
target Thresh 19.0
target distance 10.0
model initialize at round 3773
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([5.14008938, 9.11256491, 0.96728485]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 8.904243781202972}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8250379583042934
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.22086252,  9.93188431,  1.26817289]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.23112766598835352}
episode index:3774
target Thresh 19.0
target distance 2.0
model initialize at round 3774
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([13.99974006,  7.99785475,  5.60180581]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.0002610923119515}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8250816568583849
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.60319352,  8.50904714,  1.3186205 ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.6454334764970453}
episode index:3775
target Thresh 19.0
target distance 14.0
model initialize at round 3775
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.78734921, 11.7850899 ,  3.38674676]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 15.514319332935074}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8250551469746719
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.41242668, 3.44430662, 4.55489369]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7366483435007039}
episode index:3776
target Thresh 19.0
target distance 2.0
model initialize at round 3776
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.       , 4.       , 3.3019495]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 2.0000000000002247}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8250988178385916
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.35354256, 2.46498274, 5.3019495 ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7963141141886719}
episode index:3777
target Thresh 19.0
target distance 1.0
model initialize at round 3777
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.25461316, 1.6609314 , 5.08953094]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8188828059433516}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8251451124871256
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.25461316, 1.6609314 , 5.08953094]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8188828059433516}
episode index:3778
target Thresh 19.0
target distance 1.0
model initialize at round 3778
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.67848391,  8.90287724,  0.95220106]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.1293926337715767}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8251913826346549
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.67848391,  8.90287724,  0.95220106]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.1293926337715767}
episode index:3779
target Thresh 19.0
target distance 12.0
model initialize at round 3779
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([2.84904325, 6.45310483, 2.05200397]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 11.257740500838354}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8251629538383488
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.72934451,  8.08144906,  5.2201509 ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.28264525802097695}
episode index:3780
target Thresh 19.0
target distance 10.0
model initialize at round 3780
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 5.46209017, 10.8334749 ,  1.52811163]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 8.7325560063454}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8251721829842898
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.01652785,  9.55440088,  0.1121851 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.1289720130631324}
episode index:3781
target Thresh 19.0
target distance 10.0
model initialize at round 3781
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.32132358, 11.12013977,  4.08014631]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.32219080017156}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8251791331662334
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.05641788, 10.57154572,  4.66421978]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.43215280733337746}
episode index:3782
target Thresh 19.0
target distance 12.0
model initialize at round 3782
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([15.        ,  3.        ,  0.54989689]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 14.422205101855956}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8251507301526019
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.69698062, 10.9426262 ,  3.7180437 ]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.30840314303624955}
episode index:3783
target Thresh 19.0
target distance 14.0
model initialize at round 3783
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([16.82628427,  2.95670382,  1.6054632 ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 14.96642123030115}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.825111242393328
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.28093217, 5.92102235, 4.20723952]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9629148758760143}
episode index:3784
target Thresh 19.0
target distance 11.0
model initialize at round 3784
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.75298153,  7.6647426 ,  2.72810388]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 12.153843858652833}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8250717754994489
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.30356074, 1.48049111, 5.32988019]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.86885967600895}
episode index:3785
target Thresh 19.0
target distance 4.0
model initialize at round 3785
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.        ,  5.        ,  0.02654856]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 4.0}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8251000358196833
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.26541668,  9.15111607,  1.46017795]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.30542115771250034}
episode index:3786
target Thresh 19.0
target distance 10.0
model initialize at round 3786
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.01464457,  8.73326954,  4.49586916]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 9.018589791957142}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8251003179923836
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.34502402, 9.25613059, 4.79675732]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.4297027540567926}
episode index:3787
target Thresh 19.0
target distance 13.0
model initialize at round 3787
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([15.        ,  4.        ,  1.06438464]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8250645072130479
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.57427492, 4.43116385, 5.94934626]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8083107368736947}
episode index:3788
target Thresh 19.0
target distance 8.0
model initialize at round 3788
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 8.        , 11.        ,  1.61897057]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.433981132056603}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8250692257894985
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.75133095, 3.81259892, 4.20304403]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8497960379176107}
episode index:3789
target Thresh 19.0
target distance 11.0
model initialize at round 3789
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([14.9719141 ,  5.96672813,  5.02131057]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 11.020460505628913}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.82504867245666
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.41743517, 6.87325434, 0.1894575 ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.4362528850074856}
episode index:3790
target Thresh 19.0
target distance 10.0
model initialize at round 3790
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([3.        , 6.        , 4.88798857]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8250403806104947
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.80425318, 10.98758178,  0.62250611]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.19614033522690774}
episode index:3791
target Thresh 19.0
target distance 12.0
model initialize at round 3791
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([15.        ,  6.        ,  0.42821616]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 12.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8250218360183514
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.81777854, 6.39556172, 6.1627337 ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9084221576912859}
episode index:3792
target Thresh 19.0
target distance 6.0
model initialize at round 3792
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.45605623, 9.40735652, 5.39327502]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.669962935680801}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8250451672631354
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.2212865 , 5.44404917, 4.5437191 ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8964231031562863}
episode index:3793
target Thresh 19.0
target distance 11.0
model initialize at round 3793
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.3481716 ,  5.67774269,  4.34426379]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.757078367959913}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8250327202921643
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.15762412, 10.68706375,  4.07878098]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.3503918703313064}
episode index:3794
target Thresh 19.0
target distance 10.0
model initialize at round 3794
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.        , 6.        , 4.40018368]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 10.049875621120888}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.8250063560275176
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.70315266,  5.00002356,  5.56833061]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.2968473459250174}
episode index:3795
target Thresh 19.0
target distance 14.0
model initialize at round 3795
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([15.08288194,  5.41112811,  3.15711415]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 14.226640111506981}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8249858517436146
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.47452508, 10.33792521,  4.60844615]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8145655751963338}
episode index:3796
target Thresh 19.0
target distance 10.0
model initialize at round 3796
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 4.79352092, 10.48415384,  2.08980501]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.535736854522561}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.8249818333123472
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.35077949,  8.42216593,  0.10750786]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.5488809785265422}
episode index:3797
target Thresh 19.0
target distance 6.0
model initialize at round 3797
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([16.77904929,  6.23611971,  1.83978146]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 6.747878746763349}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8250027391158481
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.26573541, 10.66964213,  2.99022542]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.4239712643498505}
episode index:3798
target Thresh 19.0
target distance 11.0
model initialize at round 3798
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 4.42655549, 11.62801672,  2.32454714]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 10.698045021890458}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.8250052426752938
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.23172107, 10.67642421,  0.6254353 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.0236221126669631}
episode index:3799
target Thresh 19.0
target distance 5.0
model initialize at round 3799
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.        ,  9.        ,  1.25341957]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.385164807134504}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8250309635836498
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.27282758, 10.84055167,  4.68704896]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.3160042083284755}
episode index:3800
target Thresh 19.0
target distance 3.0
model initialize at round 3800
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.85059267,  5.77145854,  1.50934916]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.8967378699407154}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8250666292101734
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.45422532,  8.12203452,  3.22616386]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5592516638446213}
episode index:3801
target Thresh 19.0
target distance 12.0
model initialize at round 3801
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([15.        , 10.        ,  1.18672579]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 13.416407864998739}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8250501315997412
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.70424754, 4.9762689 , 4.92124333]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.0200835695839134}
episode index:3802
target Thresh 19.0
target distance 1.0
model initialize at round 3802
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.00000043,  9.00000012,  1.27416247]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.000000117342445}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8250909020095231
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.2755939 ,  7.49354908,  5.27416247]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8838872902507136}
episode index:3803
target Thresh 19.0
target distance 6.0
model initialize at round 3803
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 4.40254087, 10.0698026 ,  0.42438525]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 5.085758307772267}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8251141476313616
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.47745264, 5.13008636, 5.85801463]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.49485703521147734}
episode index:3804
target Thresh 19.0
target distance 5.0
model initialize at round 3804
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([2.93296278, 4.40070287, 1.99320858]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.754132157485344}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8251422556997653
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.12073734, 7.88181982, 3.42683797]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8871693105754631}
episode index:3805
target Thresh 19.0
target distance 9.0
model initialize at round 3805
at step 0:
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([12.97164451,  9.31728843,  5.70552635]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.266079588904653}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8251425253708466
{'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.96673492, 7.62644878, 6.00641451]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6273313598485561}
episode index:3806
target Thresh 19.0
target distance 6.0
model initialize at round 3806
at step 0:
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.1331828 ,  7.04701859,  3.49366915]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 6.478860060905504}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8251494377022688
{'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.48413286, 10.40465378,  4.07774262]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.7877537862925879}
episode index:3807
target Thresh 19.0
target distance 13.0
model initialize at round 3807
at step 0:
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([16.        ,  6.        ,  1.36727351]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 13.038404810405295}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8251212289561806
{'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.24348742, 5.96284763, 4.53542043]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9931574296232236}
episode index:3808
target Thresh 19.0
target distance 5.0
model initialize at round 3808
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.99966882, 7.0000492 , 4.00409842]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.00004921480501}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8251493056479503
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.99239912, 2.08521629, 5.4377278 ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.08555460267568746}
episode index:3809
target Thresh 19.0
target distance 13.0
model initialize at round 3809
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([ 4.22507382, 11.15394203,  1.7655074 ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 13.28607876445223}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8251328209783175
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.10299133,  5.19563504,  5.50002494]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9180945658567501}
episode index:3810
target Thresh 19.0
target distance 10.0
model initialize at round 3810
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([13.13105196,  5.09935098,  3.45879483]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.87169575920045}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8251204065302516
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.63494702, 11.52446925,  3.19331237]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.8235445967869808}
episode index:3811
target Thresh 19.0
target distance 4.0
model initialize at round 3811
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.13198094,  9.32224921,  5.80089259]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.980324909989094}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8251509311217707
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.26350045,  6.87091705,  5.23452198]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7477258868745406}
episode index:3812
target Thresh 19.0
target distance 6.0
model initialize at round 3812
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([10.04364777, 10.18535644,  0.09966534]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.096125720557073}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8251717103360081
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.54570726,  9.28599974,  1.25010942]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.616110589638652}
episode index:3813
target Thresh 19.0
target distance 12.0
model initialize at round 3813
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.54735961,  7.84983762,  3.62225354]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.70834692616453}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8251512594665785
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.74322566, 6.68066732, 5.07358577]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7274895674493628}
episode index:3814
target Thresh 19.0
target distance 6.0
model initialize at round 3814
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 7.92709148, 10.13228141,  5.04747295]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 5.838443058112768}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8251720277013209
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.98055622, 7.13624578, 6.19791703]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9899764745471169}
episode index:3815
target Thresh 19.0
target distance 7.0
model initialize at round 3815
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.        , 3.        , 4.80154657]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.825188068803002
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.23133128, 9.62174473, 3.66880534]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8566963543966002}
episode index:3816
target Thresh 19.0
target distance 8.0
model initialize at round 3816
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.       ,  3.       ,  4.9548676]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.000000000000002}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8251883256945484
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.67149352, 10.11210242,  5.25575576]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.1132230962759737}
episode index:3817
target Thresh 19.0
target distance 3.0
model initialize at round 3817
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([ 7.        , 11.        ,  0.34286517]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 3.6055512754644683}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8252187845273682
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.77329363, 8.50752141, 6.05967986]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9167977966917568}
episode index:3818
target Thresh 19.0
target distance 11.0
model initialize at round 3818
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([4.47586672, 5.1911853 , 0.50867289]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.558414932933886}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8251944547512821
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.46502317,  6.02744198,  5.96000512]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.46583217233374447}
episode index:3819
target Thresh 19.0
target distance 13.0
model initialize at round 3819
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([4.25672939, 7.11938346, 1.73765963]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.796500495914952}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8251644439587483
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.84439672,  6.20562629,  0.62262125]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.25786537546808885}
episode index:3820
target Thresh 19.0
target distance 3.0
model initialize at round 3820
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.       ,  3.       ,  4.7270565]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 3.605551275463989}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.825197373978623
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.27986804,  5.97268689,  2.16068589]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.7206497415688535}
episode index:3821
target Thresh 19.0
target distance 4.0
model initialize at round 3821
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([10.       , 11.       ,  0.0935046]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.0}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8252302867666715
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.73694081, 11.61847276,  3.8103193 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.9620760416859424}
episode index:3822
target Thresh 19.0
target distance 9.0
model initialize at round 3822
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 6.27800509, 11.88005937,  2.42096895]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.518949252553266}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.825226231726508
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.0191833 ,  6.18199745,  0.4386718 ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.18300566082087197}
episode index:3823
target Thresh 19.0
target distance 4.0
model initialize at round 3823
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([16.        , 10.        ,  0.82406872]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.825259119754273
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.76250337, 10.99591633,  4.54087352]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7625143064399661}
episode index:3824
target Thresh 19.0
target distance 8.0
model initialize at round 3824
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([16.        , 11.        ,  2.49423328]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 8.246211251235321}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8252704888243422
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.32265168,  2.74300792,  5.36149205]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.41249125188341695}
episode index:3825
target Thresh 19.0
target distance 13.0
model initialize at round 3825
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([16.        , 10.        ,  1.91490572]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.999999999999998}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.825262214855398
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.45862447, 10.46056931,  3.93260845]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7107823566589697}
episode index:3826
target Thresh 19.0
target distance 11.0
model initialize at round 3826
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([15.00000001, 10.99999999,  0.56839054]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 13.038404817160623}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.825243781898081
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.88739681, 4.16876229, 0.01972278]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.2028797412889631}
episode index:3827
target Thresh 19.0
target distance 1.0
model initialize at round 3827
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.41187073,  6.36858447,  5.96968365]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8628914244510774}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8252894339926741
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.41187073,  6.36858447,  5.96968365]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.8628914244510774}
episode index:3828
target Thresh 19.0
target distance 7.0
model initialize at round 3828
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([4.        , 2.        , 0.60656041]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 7.280109889280518}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8253077282784578
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.60455208, 8.44289838, 3.75700448]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.6831846563189581}
episode index:3829
target Thresh 19.0
target distance 14.0
model initialize at round 3829
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([1.09577728, 3.55011084, 3.76976275]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 16.66243390072702}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.825274074359338
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.63321462, 10.14334752,  0.37153907]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9318717675619265}
episode index:3830
target Thresh 19.0
target distance 12.0
model initialize at round 3830
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([1.34927618, 7.55207813, 2.97779489]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 13.139941869874061}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8252350388528648
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.7987361 ,  4.88854521,  5.5795712 ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.9110541913266907}
episode index:3831
target Thresh 19.0
target distance 10.0
model initialize at round 3831
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.        , 3.        , 6.14699078]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8252051114490075
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.36616574,  8.07057168,  5.0319524 ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.6377509133118519}
episode index:3832
target Thresh 19.0
target distance 9.0
model initialize at round 3832
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 5.        , 11.        ,  3.58664787]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 12.727922061357857}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8251847532394805
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.89968307,  2.53754894,  5.0379801 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.5468293572001006}
episode index:3833
target Thresh 19.0
target distance 8.0
model initialize at round 3833
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([11.        , 11.        ,  0.08816212]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.630145812734105}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8251723997199604
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.72211598, 2.53267611, 6.10586497]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8601413283914301}
episode index:3834
target Thresh 19.0
target distance 2.0
model initialize at round 3834
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([2.        , 7.        , 3.45145178]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.2360679777332026}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8252127980511939
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.5164725 , 6.74156085, 1.16826647]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8852747254449589}
episode index:3835
target Thresh 19.0
target distance 14.0
model initialize at round 3835
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([15.99934095, 10.00116208,  3.09669447]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 15.652406126802935}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8251792215183467
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.5193398 , 2.72844117, 5.98165609]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.5860529223080914}
episode index:3836
target Thresh 19.0
target distance 3.0
model initialize at round 3836
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 2.       , 11.       ,  2.0963473]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 3.0000000000000004}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8252120103711957
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.45164958, 7.86216777, 5.81316092]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.47221294435357136}
episode index:3837
target Thresh 19.0
target distance 5.0
model initialize at round 3837
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.       , 7.       , 3.2011857]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 5.099019516195742}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8252374227432793
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.06814952, 1.81242834, 0.35162978]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.1995682436797696}
episode index:3838
target Thresh 19.0
target distance 13.0
model initialize at round 3838
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 4.66999496, 10.20857834,  1.13425415]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.991060451087812}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8252075492878297
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.15265289,  2.03885531,  0.01921577]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.15752028638222443}
episode index:3839
target Thresh 19.0
target distance 2.0
model initialize at round 3839
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([4.        , 7.        , 0.86410969]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.828427124747055}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8252428066994734
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.255799  , 9.19862384, 2.58092438]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.323858857952719}
episode index:3840
target Thresh 19.0
target distance 3.0
model initialize at round 3840
at step 0:
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.8814611 , 3.75313766, 0.2013604 ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.962260261300084}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8252805719151205
{'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.01722271, 2.17185798, 6.2013604 ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.1727188125379472}
episode index:3841
target Thresh 19.0
target distance 2.0
model initialize at round 3841
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.68195439,  6.05763942,  1.04425591]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 2.0820742077050642}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8253183174716237
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.27536238,  4.35496196,  0.7610706 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.4492465162663528}
episode index:3842
target Thresh 19.0
target distance 5.0
model initialize at round 3842
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.98538252,  3.99986099,  4.16110253]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 5.000160381132439}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8253436691179823
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.98176443,  8.03950608,  1.31154661]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 1.3734664061758404}
episode index:3843
target Thresh 19.0
target distance 4.0
model initialize at round 3843
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([10.00508833, 11.16916049,  2.55072552]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.008659098900848}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8253788544303865
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.78302673, 10.32133517,  4.26754009]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.036203072639535}
episode index:3844
target Thresh 19.0
target distance 5.0
model initialize at round 3844
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([1.71723657, 5.65904579, 2.74961156]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 3.578750769471762}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8254090498256974
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.56088828, 8.88273527, 2.18324094]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.4544998546687521}
episode index:3845
target Thresh 19.0
target distance 4.0
model initialize at round 3845
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([2.5578811 , 8.62385928, 2.8466174 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.18261010219945}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8254392295187747
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.2179846 , 10.56095834,  2.28024679]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8968309039864449}
episode index:3846
target Thresh 19.0
target distance 12.0
model initialize at round 3846
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([1.1333073 , 4.92142321, 3.57039917]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 14.230280283259802}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8254169423193646
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.21061991, 11.13731135,  0.73854097]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.25142623732468306}
episode index:3847
target Thresh 19.0
target distance 13.0
model initialize at round 3847
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([15.        ,  5.        ,  0.28229111]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.152946437965907}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.825394666703738
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.24977928, 7.53087932, 3.73362335]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.586704819673699}
episode index:3848
target Thresh 19.0
target distance 10.0
model initialize at round 3848
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([5.14619752, 9.07087747, 0.35438555]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 8.854086175641811}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.825394867783793
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.90163061,  9.52175591,  0.65527371]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.5309479874483354}
episode index:3849
target Thresh 19.0
target distance 1.0
model initialize at round 3849
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.99999931, 2.99999743, 5.45988083]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9999993088214555}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8254402197661868
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.99999931, 2.99999743, 5.45988083]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.9999993088214555}
episode index:3850
target Thresh 19.0
target distance 5.0
model initialize at round 3850
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.97565038,  3.33532615,  3.23516178]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.664754741823339}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8254703521810491
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.08148164,  6.52994573,  2.66879105]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.47706422322044695}
episode index:3851
target Thresh 19.0
target distance 1.0
model initialize at round 3851
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.00000001, 4.00000001, 1.90525406]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.4142135641010152}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8255104948725909
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.05776718, 3.68762855, 5.90525406]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.1664629054650473}
episode index:3852
target Thresh 19.0
target distance 12.0
model initialize at round 3852
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.32796532,  3.80847144,  4.2656436 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.485105465629566}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.8254613578342289
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.74564865, 2.87675001, 0.01786399]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.1509485200161347}
episode index:3853
target Thresh 19.0
target distance 12.0
model initialize at round 3853
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 3.38333858, 10.56592362,  2.95594686]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 13.1109098325001}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8254449803968944
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.24578184,  6.73856699,  0.4072791 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.3588257687045218}
episode index:3854
target Thresh 19.0
target distance 14.0
model initialize at round 3854
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 1.9999998 , 10.99999975,  5.03780627]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 16.64331701341768}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8254133338896579
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.61932321,  2.71772924,  5.92276789]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.812434664598711}
episode index:3855
target Thresh 19.0
target distance 6.0
model initialize at round 3855
at step 0:
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.86693817,  6.67770159,  2.65994263]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.287196101533435}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.8254245715657157
{'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.92286973, 10.14036987,  5.5272014 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.261210728744856}
episode index:3856
target Thresh 19.0
target distance 2.0
model initialize at round 3856
at step 0:
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 5.        , 11.        ,  3.94297433]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 2.000000000000049}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8254646740879958
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.35673116, 10.81271299,  1.65978902]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.669978522355693}
episode index:3857
target Thresh 19.0
target distance 3.0
model initialize at round 3857
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([15.02225631,  9.682808  ,  2.5675714 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.2968208851871106}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8254997003544323
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.29989593, 10.36868517,  4.28438609]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6989248779724605}
episode index:3858
target Thresh 19.0
target distance 11.0
model initialize at round 3858
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 5.        , 11.        ,  4.81386852]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8255042205651163
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.18608131, 10.79089982,  1.11475143]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.1348947827175684}
episode index:3859
target Thresh 19.0
target distance 8.0
model initialize at round 3859
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([ 9.35174912, 10.65991603,  4.35506749]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 6.565061922845034}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8255176964206692
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.041343  , 8.62119358, 5.22232626]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.38105583203023685}
episode index:3860
target Thresh 19.0
target distance 2.0
model initialize at round 3860
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.        ,  8.        ,  0.15227383]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 2.0}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8255602973798971
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.69931849,  9.53079356,  2.15227383]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8421407431028866}
episode index:3861
target Thresh 19.0
target distance 4.0
model initialize at round 3861
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.98402007,  5.36531751,  1.95629519]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.6347309469603473}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8255903128775722
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.87802695,  8.050897  ,  1.38992457]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.1321662907414358}
episode index:3862
target Thresh 19.0
target distance 7.0
model initialize at round 3862
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([14.73923635,  5.6626456 ,  2.73636562]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 5.484238955617361}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8256130741860386
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.70128   , 10.1685508 ,  1.88680969]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.0877046527220269}
episode index:3863
target Thresh 19.0
target distance 6.0
model initialize at round 3863
at step 0:
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.19075902, 8.52435874, 5.22077537]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 4.528378409308883}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8256406239463184
{'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.59035702, 4.9244634 , 0.37121944]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.096883761806039}
episode index:3864
target Thresh 19.0
target distance 8.0
model initialize at round 3864
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 6.59327199, 10.45790082,  0.68204373]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.429621708925188}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8256540470767332
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.44724394, 10.80877781,  1.54930238]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.48640833770911696}
episode index:3865
target Thresh 19.0
target distance 1.0
model initialize at round 3865
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.        ,  7.        ,  5.86628938]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.4142135623730951}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8256965576698327
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.41404309,  7.91256211,  1.58310407]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.4231749854588547}
episode index:3866
target Thresh 19.0
target distance 10.0
model initialize at round 3866
at step 0:
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([15.        ,  8.        ,  0.79093283]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 10.440306508910549}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8257010176221767
{'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.90284138, 11.40291495,  3.37500629]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9886672885147131}
episode index:3867
target Thresh 19.0
target distance 2.0
model initialize at round 3867
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.        , 6.        , 4.78270888]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.23606797749979}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8257409346289963
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.14077185, 7.67509496, 2.49952358]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9186056275648579}
episode index:3868
target Thresh 19.0
target distance 4.0
model initialize at round 3868
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([3.65735864, 3.64772115, 2.78582299]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 2.8775082041237257}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8257708491326851
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.80354044, 6.32114816, 2.21945237]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8653400357109954}
episode index:3869
target Thresh 19.0
target distance 1.0
model initialize at round 3869
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.25720638,  9.56286857,  3.50313568]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.583891818284227}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8258132856057774
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.89116253,  7.92020469,  5.50313568]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.13495512207540167}
episode index:3870
target Thresh 19.0
target distance 14.0
model initialize at round 3870
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([16.        ,  9.        ,  1.70371884]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 15.231546211727817}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8257798575335593
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.85022264, 3.47001529, 4.58868046]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.4933027801162553}
episode index:3871
target Thresh 19.0
target distance 13.0
model initialize at round 3871
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([2.        , 2.        , 6.16125965]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 15.264337522473747}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8257482634833181
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.43034137, 10.55173658,  0.76303561]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.6997191919710529}
episode index:3872
target Thresh 19.0
target distance 11.0
model initialize at round 3872
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([15.        ,  2.        ,  1.23090285]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8257077841378717
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.96265004, 5.02758142, 5.83267916]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.04643010170581916}
episode index:3873
target Thresh 19.0
target distance 3.0
model initialize at round 3873
at step 0:
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 3.        , 11.        ,  4.77815604]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.0}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8257426029881201
{'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.77763053, 10.60048629,  0.21178543]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.45723012930501633}
episode index:3874
target Thresh 19.0
target distance 11.0
model initialize at round 3874
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([ 4.        , 11.        ,  5.22042942]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.401754251396744}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8257363805330364
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.52245963,  7.81123588,  5.23813227]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5134945941521146}
episode index:3875
target Thresh 19.0
target distance 7.0
model initialize at round 3875
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 4.        , 11.        ,  2.05184205]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 7.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8257543376728005
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.36181729, 4.63344622, 5.20228612]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.8991836779205137}
episode index:3876
target Thresh 19.0
target distance 14.0
model initialize at round 3876
at step 0:
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 2.        , 10.        ,  3.68159604]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 14.14213562373095}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8257379818246381
{'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.29541209,  8.75024857,  1.13292828]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.8063133501649743}
episode index:3877
target Thresh 19.0
target distance 10.0
model initialize at round 3877
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([3.99999997, 1.99999998, 4.59668803]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 12.206555650384248}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8257064474546472
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.1083346 ,  9.07854454,  5.48164965]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8951181120602554}
episode index:3878
target Thresh 19.0
target distance 13.0
model initialize at round 3878
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([16.42663075,  5.89276484,  1.56916397]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 14.9999018431838}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8256901123855295
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.89591408, 9.0837583 , 5.30368152]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.2814682528634833}
episode index:3879
target Thresh 19.0
target distance 12.0
model initialize at round 3879
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.66880765,  3.02971599,  3.49320221]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.718384751799931}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.825622642044825
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.73736344, 2.1410281 , 4.39586668]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.2981054982593061}
episode index:3880
target Thresh 19.0
target distance 13.0
model initialize at round 3880
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([15.        ,  8.        ,  5.71538329]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 13.341664064126332}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8256123497229395
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.8907053 , 11.01223045,  3.44990083]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8907892686543956}
episode index:3881
target Thresh 19.0
target distance 4.0
model initialize at round 3881
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.95878445,  8.38233946,  1.97438162]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 3.269401045297591}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8256471214025575
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.95268628, 10.36569389,  3.69119631]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.6360682559938212}
episode index:3882
target Thresh 19.0
target distance 5.0
model initialize at round 3882
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([10.        , 11.        ,  3.50778782]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.0990195135927845}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8256721272158527
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.64588297, 10.11922817,  0.65823166]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.373649870319494}
episode index:3883
target Thresh 19.0
target distance 14.0
model initialize at round 3883
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([16.        , 10.        ,  0.32467478]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8256618301029771
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.80320792, 8.91279264, 4.34237763]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.2158673334306367}
episode index:3884
target Thresh 19.0
target distance 11.0
model initialize at round 3884
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.98966839,  8.68293831,  2.58693528]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 11.23127713866493}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8256598339916501
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.01311058, 10.60719703,  4.88782344]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.3930217078460863}
episode index:3885
target Thresh 19.0
target distance 3.0
model initialize at round 3885
at step 0:
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.        ,  9.        ,  1.48245018]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.162277660168379}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8256920857198818
{'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.75928372,  5.90738914,  5.19926488]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.25791684097218653}
episode index:3886
target Thresh 19.0
target distance 3.0
model initialize at round 3886
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([4.        , 6.        , 4.96139717]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8257218742621203
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.37502989, 8.2611312 , 4.39502656]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9677369201907112}
episode index:3887
target Thresh 19.0
target distance 12.0
model initialize at round 3887
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 5.42924384, 11.88486846,  1.56691235]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.187571281005047}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8257036122798521
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.40670547,  3.99503488,  1.01824458]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.4067357808171383}
episode index:3888
target Thresh 19.0
target distance 14.0
model initialize at round 3888
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([3.42413964, 6.89678001, 1.57197159]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.495581438044521}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8256633109546502
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.03609474,  1.85166421,  6.17374791]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.1526641304088282}
episode index:3889
target Thresh 19.0
target distance 12.0
model initialize at round 3889
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.00000334, 5.9999971 , 0.29476279]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.369312934697632}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8256282944443454
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.81338214,  2.92044478,  5.17972441]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.20286759188923612}
episode index:3890
target Thresh 19.0
target distance 11.0
model initialize at round 3890
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([15.46419466,  4.82977231,  1.52558106]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 13.907834423009453}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8256081309387908
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.00301125, 11.43713239,  2.97691329]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.4371427608207847}
episode index:3891
target Thresh 19.0
target distance 12.0
model initialize at round 3891
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([2.60254744, 6.63536526, 2.81920993]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.504849042452317}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.8255678852110495
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.0824368 ,  5.80091518,  1.13780094]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8051465450625799}
episode index:3892
target Thresh 19.0
target distance 6.0
model initialize at round 3892
at step 0:
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([3.12088976, 7.56488562, 5.17277694]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 4.7004866360350555}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8255904768787281
{'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.56324567, 3.68505306, 4.32322102]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8124358727334154}
episode index:3893
target Thresh 19.0
target distance 9.0
model initialize at round 3893
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 6.        , 11.        ,  5.40206432]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 9.055385138137417}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8255971197380543
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.81281251,  9.9559367 ,  5.98613779]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.19230374920432636}
episode index:3894
target Thresh 19.0
target distance 1.0
model initialize at round 3894
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.        ,  3.        ,  3.72149169]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.000000004071552}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8256367867162987
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.53083679,  3.69893841,  1.43830639]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8776688374393977}
episode index:3895
target Thresh 19.0
target distance 6.0
model initialize at round 3895
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([5.55889014, 9.57237236, 5.59134007]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 5.794159822249027}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8256593433027379
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.28264846, 5.88636085, 4.74178414]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9303363438525789}
episode index:3896
target Thresh 19.0
target distance 14.0
model initialize at round 3896
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([16.        ,  3.        ,  1.09184092]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 14.035668847618199}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.8256042901583269
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.33760305, 2.43270727, 4.84406131]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.7912049707085875}
episode index:3897
target Thresh 19.0
target distance 4.0
model initialize at round 3897
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.99940267,  4.99996655,  4.20752954]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 4.000033492393567}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8256340171617241
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.27683201,  8.52737594,  3.64115893]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.8639128683896747}
episode index:3898
target Thresh 19.0
target distance 11.0
model initialize at round 3898
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([16.32138488,  4.04227159,  1.6778571 ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 13.686162836816694}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8256177844603098
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.50217821, 9.61334607, 5.41237464]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6337856215000405}
episode index:3899
target Thresh 19.0
target distance 2.0
model initialize at round 3899
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 3.00000002, 10.00000001,  1.23145026]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 2.000000005603635}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8256573952848072
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.20655716, 8.51580443, 5.23145026]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9463644864479253}
episode index:3900
target Thresh 19.0
target distance 3.0
model initialize at round 3900
at step 0:
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([1.20763931, 9.82066573, 4.92645454]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.3334819462094103}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8256919860601764
{'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.11945924, 8.19886028, 0.36008393]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.23198258776213065}
episode index:3901
target Thresh 19.0
target distance 3.0
model initialize at round 3901
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.3443509 , 4.98828749, 0.3648588 ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 2.0178861956761303}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8257290457767166
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.91560523, 3.32964499, 0.08167349]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.34027679311962944}
episode index:3902
target Thresh 19.0
target distance 6.0
model initialize at round 3902
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([10.08344953, 11.8681713 ,  1.89881914]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.259518274665615}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8257515382701079
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.97425803, 10.70571559,  1.04926287]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7061849259408962}
episode index:3903
target Thresh 19.0
target distance 6.0
model initialize at round 3903
at step 0:
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([3.        , 5.        , 4.32322383]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.211102550927979}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8257559418702909
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.71419535, 10.66816582,  0.62411199]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.4379477365516682}
episode index:3904
target Thresh 19.0
target distance 12.0
model initialize at round 3904
at step 0:
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.50602091,  4.22514449,  4.63005042]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.988554192794298}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8257377506655106
{'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.98501905, 9.89218364, 4.08138242]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.10885217633158459}
episode index:3905
target Thresh 19.0
target distance 5.0
model initialize at round 3905
at step 0:
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([10.73768975, 10.12227215,  5.57805729]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 6.684686747763002}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8257510080316995
{'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.76945494,  5.71354717,  0.16213051]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.3677040247520255}
episode index:3906
target Thresh 19.0
target distance 5.0
model initialize at round 3906
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.14300464, 6.77324479, 4.32469082]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 4.849567699722437}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8257782192781483
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.34655342, 2.15287716, 5.7583202 ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.37877525883844865}
episode index:3907
target Thresh 19.0
target distance 2.0
model initialize at round 3907
at step 0:
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([15.        , 11.        ,  1.60613554]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 2.0000000000002234}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8258202412281794
{'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.57686199, 11.71043351,  3.60613554]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.9151423562511856}
episode index:3908
target Thresh 19.0
target distance 11.0
model initialize at round 3908
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([4.63565651, 8.39625179, 1.24767941]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 9.961197480594281}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8257982100519604
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.37879966,  5.98149774,  4.69901164]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1615626027996928}
episode index:3909
target Thresh 19.0
target distance 6.0
model initialize at round 3909
at step 0:
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([16.00594453, 10.03234051,  2.39901471]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.083398279055232}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8258183082271412
{'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.70945615, 11.19475279,  3.54945879]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.3497776164663377}
episode index:3910
target Thresh 19.0
target distance 14.0
model initialize at round 3910
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([2.        , 4.        , 4.57303166]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 15.65247584249853}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.8257852207584178
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.02924686, 10.70591898,  1.17480786]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.295531764829654}
episode index:3911
target Thresh 19.0
target distance 13.0
model initialize at round 3911
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([2.91083942, 5.41518891, 2.00892148]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.328055039160589}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.8257400564965593
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.2560018 ,  3.02689334,  0.04432718]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.2574105200764693}
episode index:3912
target Thresh 19.0
target distance 12.0
model initialize at round 3912
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.       ,  2.       ,  5.8401525]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8257052261948401
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.80747008, 7.31279904, 4.44192882]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.3673023423534464}
episode index:3913
target Thresh 19.0
target distance 5.0
model initialize at round 3913
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.22965096, 4.03744046, 1.17160159]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 4.967870443572796}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8257324004722321
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.03318696, 8.52844937, 2.60522847]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.4727170088598869}
episode index:3914
target Thresh 19.0
target distance 2.0
model initialize at round 3914
at step 0:
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.00007761,  7.00004249,  1.51087921]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.236071272552498}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.825769326806722
{'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.29002179,  5.71827171,  1.22769391]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.774614029122847}
episode index:3915
target Thresh 19.0
target distance 12.0
model initialize at round 3915
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([3.57557274, 7.40842698, 0.65082138]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 10.519142092030892}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.8257454590954981
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.4042823 ,  6.83750787,  6.10215362]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9299804376172824}
episode index:3916
target Thresh 19.0
target distance 12.0
model initialize at round 3916
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([16.00000003,  2.00000002,  1.5331189 ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 12.041594609930776}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.825686031213779
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.67217203, 3.0203729 , 5.00215398]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.32846040244754326}
episode index:3917
target Thresh 19.0
target distance 2.0
model initialize at round 3917
at step 0:
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.        , 4.        , 4.22650456]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 2.00000000029626}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8257254426402176
{'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.81453802, 3.87890513, 1.94331926]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.22149517731443788}
episode index:3918
target Thresh 19.0
target distance 5.0
model initialize at round 3918
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([10.30144165, 10.13653716,  5.9308846 ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 5.161515435844536}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8257455132277064
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.09399266,  7.72322897,  0.79814337]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.29229577825127045}
episode index:3919
target Thresh 19.0
target distance 5.0
model initialize at round 3919
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([15.25776798,  7.51045679,  3.03753686]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.505043140936564}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8257655735750996
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.16142773, 10.60280644,  4.18798094]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.4287442508439101}
episode index:3920
target Thresh 19.0
target distance 10.0
model initialize at round 3920
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.        ,  8.        ,  5.36089993]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8257533460274904
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.60305337, 6.37548355, 5.09541747]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.5464014311107722}
episode index:3921
target Thresh 19.0
target distance 11.0
model initialize at round 3921
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.99784276,  6.99738458,  5.03269815]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 11.400361605201283}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8257471954011547
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.16305245, 9.93020261, 5.050401  ]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.17736340413609197}
episode index:3922
target Thresh 19.0
target distance 5.0
model initialize at round 3922
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.        , 2.        , 0.38137978]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 5.000000000000035}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8257742966380922
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.61843275, 6.44238032, 1.81500916]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.8327056937397052}
episode index:3923
target Thresh 19.0
target distance 12.0
model initialize at round 3923
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.        , 7.        , 6.12509346]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.649110640673516}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8257600940002653
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.08568963, 10.97450755,  1.5764257 ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.08940122050063076}
episode index:3924
target Thresh 19.0
target distance 1.0
model initialize at round 3924
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.66967531, 8.20913436, 1.13460558]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7015712344854198}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8258044863330041
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.66967531, 8.20913436, 1.13460558]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7015712344854198}
episode index:3925
target Thresh 19.0
target distance 1.0
model initialize at round 3925
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.00008669, 11.0000005 ,  1.01580733]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.0000005072051936}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8258437872789203
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([1.23057159, 9.72875628, 5.01580733]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.8158389738309156}
episode index:3926
target Thresh 19.0
target distance 5.0
model initialize at round 3926
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([15.4715813 ,  6.58562141,  1.38874119]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 5.615925128737303}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.8258661130900241
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.84051931, 10.06073476,  0.53918527]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.952708389507205}
episode index:3927
target Thresh 19.0
target distance 2.0
model initialize at round 3927
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.        , 3.        , 4.97003531]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.0000000000620792}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8259053783361825
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.82184931, 4.67351436, 2.68685001]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.3719281448643988}
episode index:3928
target Thresh 19.0
target distance 1.0
model initialize at round 3928
at step 0:
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.99999542,  5.99999913,  4.33994055]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.000000873884752}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8259446235949415
{'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.7008186 ,  7.44422209,  2.05675524]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.8297469356873016}
episode index:3929
target Thresh 19.0
target distance 9.0
model initialize at round 3929
at step 0:
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 5.       , 11.       ,  2.2560451]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 9.486832980505138}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8259489489307658
{'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.17923727, 2.12284081, 4.84011857]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8299044042996974}
episode index:3930
target Thresh 19.0
target distance 5.0
model initialize at round 3930
at step 0:
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.70066195,  8.93513958,  4.36497355]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.944209338244545}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8259759436901085
{'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.10458406,  4.45195587,  5.79860293]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.46389862282540517}
episode index:3931
target Thresh 19.0
target distance 9.0
model initialize at round 3931
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([10.58301417, 10.43178492,  0.66367215]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.097845302642305}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.8259759926931973
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.9446449 ,  2.96939199,  0.9645603 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.3535415854141746}
episode index:3932
target Thresh 19.0
target distance 2.0
model initialize at round 3932
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.        , 9.        , 3.36943638]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.000000000002746}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8260176972462883
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.42237032, 7.42081715, 5.36943638]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7146629447326639}
episode index:3933
target Thresh 19.0
target distance 12.0
model initialize at round 3933
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.09475886,  8.49332412,  4.61653829]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 11.106322285473722}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.8260114981848476
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.81005697, 9.61685217, 4.63424114]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0181841188489822}
episode index:3934
target Thresh 19.0
target distance 5.0
model initialize at round 3934
at step 0:
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.86184662,  4.72156092,  1.44852322]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 4.364380892107815}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8260384496079028
{'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.36390649,  9.04878368,  2.88215261]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.6379614443485435}
episode index:3935
target Thresh 19.0
target distance 1.0
model initialize at round 3935
at step 0:
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.        ,  4.        ,  5.94355917]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.0}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.826077591261966
{'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.65054305,  5.6550337 ,  3.66037386]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.7424212466577819}
episode index:3936
target Thresh 19.0
target distance 5.0
model initialize at round 3936
at step 0:
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.        ,  7.        ,  1.24050921]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.099019513608145}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8261021447552771
{'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.32563257,  2.89753738,  4.67413859]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.1226507840370976}
episode index:3937
target Thresh 19.0
target distance 8.0
model initialize at round 3937
at step 0:
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 7.99992008, 11.00012314,  3.15646124]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 8.944346312598755}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.8261064213039385
{'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.51329642, 2.8427874 , 5.7405347 ]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.536832387819524}
episode index:3938
target Thresh 19.0
target distance 14.0
model initialize at round 3938
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([3.31655043, 7.04837162, 1.68248194]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.847790448921971}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.8260770856365022
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.54008269,  5.97474282,  0.56744356]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.0777975197387417}
episode index:3939
target Thresh 19.0
target distance 3.0
model initialize at round 3939
at step 0:
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.72548833,  9.48143002,  6.16807413]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.506649121034671}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8261136901832951
{'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.99776829,  7.82063395,  5.88488882]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.17937993286522372}
episode index:3940
target Thresh 19.0
target distance 5.0
model initialize at round 3940
at step 0:
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.6467026 ,  4.55375797,  2.18638726]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 3.506395352010689}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8261429635807113
{'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.76956795,  7.18430647,  1.62001664]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.8476171691881539}
episode index:3941
target Thresh 19.0
target distance 9.0
model initialize at round 3941
at step 0:
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([ 8.11701751, 10.19627338,  0.15909355]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 7.92384940805846}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8261559970813248
{'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.47965983, 11.37782292,  1.02635161]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.6430428082149366}
episode index:3942
target Thresh 19.0
target distance 2.0
model initialize at round 3942
at step 0:
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.92901017, 10.29821706,  3.27057838]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7053643428160422}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8262000863541928
{'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.92901017, 10.29821706,  3.27057838]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.7053643428160422}
episode index:3943
target Thresh 19.0
target distance 8.0
model initialize at round 3943
at step 0:
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([2.51042924, 4.61018898, 2.87596047]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 10.625558644229237}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8261898120272286
{'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.49508028, 10.62425892,  0.61047801]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.6215189798126861}
episode index:3944
target Thresh 19.0
target distance 12.0
model initialize at round 3944
at step 0:
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.50026817, 11.60706402,  2.88227713]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 13.263086483510774}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.8261755796657021
{'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.28529485, 5.39469746, 4.61679467]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.8164493488967008}
episode index:3945
target Thresh 19.0
target distance 1.0
model initialize at round 3945
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.        ,  6.        ,  0.66343373]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 1.0000000000519105}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8262170962446007
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.86108197,  7.67722684,  2.66343373]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.691328003823062}
episode index:3946
target Thresh 19.0
target distance 9.0
model initialize at round 3946
at step 0:
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 9.55210759, 11.69912412,  3.63121653]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 7.584398700089293}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.8262300944525447
{'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.53624897, 10.40757268,  4.49847518]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7990826581949083}
episode index:3947
target Thresh 19.0
target distance 1.0
model initialize at round 3947
at step 0:
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.21007683, 9.37548842, 2.07070686]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8746257324188457}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8262741091196033
{'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.21007683, 9.37548842, 2.07070686]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8746257324188457}
episode index:3948
target Thresh 19.0
target distance 1.0
model initialize at round 3948
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([3.        , 7.        , 2.09429482]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 1.4142135623730951}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.826313062244668
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.41395322, 5.50078057, 6.09429482]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.6485193161935255}
episode index:3949
target Thresh 19.0
target distance 6.0
model initialize at round 3949
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4.25210092, 3.06409323, 1.25896185]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 6.066526679388042}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8263328265516969
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.6513493 , 9.25870978, 2.40940533]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.4341521170381427}
episode index:3950
target Thresh 19.0
target distance 1.0
model initialize at round 3950
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 5.46257687, 10.79356184,  1.50712698]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.477074073454673}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8263742507920027
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.11726339, 11.77187701,  3.50712698]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7807335155477378}
episode index:3951
target Thresh 19.0
target distance 2.0
model initialize at round 3951
at step 0:
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.55456981, 7.37708292, 5.45452237]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.247342239589056}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8264131490078954
{'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.61747495, 8.77502449, 3.17133706]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.6571828454929116}
episode index:3952
target Thresh 19.0
target distance 3.0
model initialize at round 3952
at step 0:
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.99915556, 9.0001567 , 3.9681133 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 3.000156817760226}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.8264470935717689
{'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.34473519, 5.68634793, 5.684928  ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.46606863303486984}
episode index:3953
target Thresh 19.0
target distance 12.0
model initialize at round 3953
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([15.        , 11.        ,  1.30914515]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 13.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.8264367827592337
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.13855131, 6.58645251, 5.326848  ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.6025968926231613}
episode index:3954
target Thresh 19.0
target distance 6.0
model initialize at round 3954
at step 0:
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([ 6.31878513, 11.07684166,  4.10591841]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 5.302721905697366}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.8264564907977292
{'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.75167573, 8.2020686 , 5.25636249]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.320150998967183}
episode index:3955
target Thresh 19.0
target distance 11.0
model initialize at round 3955
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.86249346, 4.71876026, 1.4451558 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.282177575938604}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.8264067858281653
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.99804517,  2.86687361,  5.48056149]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.1331407382775113}
episode index:3956
target Thresh 19.0
target distance 1.0
model initialize at round 3956
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.        ,  3.        ,  5.50664544]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.0}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8264481285661415
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.85696811,  3.18575851,  1.22346013]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 1.1821097868912183}
episode index:3957
target Thresh 19.0
target distance 9.0
model initialize at round 3957
at step 0:
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 3.63722485, 10.61027846,  0.7763111 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 8.633825956536464}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8264566202857158
{'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.66042144, 2.1138236 , 5.64356987]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.358147190966939}
episode index:3958
target Thresh 19.0
target distance 12.0
model initialize at round 3958
at step 0:
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([16.53431609,  2.69156504,  1.43346232]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 14.538128966087484}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8264291572678609
{'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.87418935, 8.12240476, 4.60160925]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.17553132715854022}
episode index:3959
target Thresh 19.0
target distance 12.0
model initialize at round 3959
at step 0:
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([15.        ,  7.        ,  1.13156479]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 12.000000000000002}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8264129738226789
{'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.5053429 , 6.97385925, 4.86608233]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.49534733531667763}
episode index:3960
target Thresh 19.0
target distance 11.0
model initialize at round 3960
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([3.99999448, 5.00000053, 4.05589628]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 12.083050781358047}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8263910822295372
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.088526  , 10.51194002,  1.22404309]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5195377187177236}
episode index:3961
target Thresh 19.0
target distance 2.0
model initialize at round 3961
at step 0:
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 6.00000928, 10.99997049,  6.02692604]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.000009277123154}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8264298780189795
{'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.4559556 , 11.63748537,  3.74374073]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.783762153201792}
episode index:3962
target Thresh 19.0
target distance 2.0
model initialize at round 3962
at step 0:
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.60082901,  6.93522682,  3.56238067]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1115955645951177}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.826473675677819
{'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.60082901,  6.93522682,  3.56238067]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1115955645951177}
episode index:3963
target Thresh 19.0
target distance 10.0
model initialize at round 3963
at step 0:
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([2.78248316, 8.50999725, 6.20595789]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 9.547917589224538}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8264799784264106
{'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.55154482, 10.33652493,  0.50684605]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8008190931587722}
episode index:3964
target Thresh 19.0
target distance 7.0
model initialize at round 3964
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 7.        , 11.        ,  3.33555508]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 7.0710678118654755}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8264973449524716
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.12192563,  9.55576142,  0.20281385]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.9840541225561118}
episode index:3965
target Thresh 19.0
target distance 12.0
model initialize at round 3965
at step 0:
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.35765608,  4.36755186,  3.93142343]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.346392171144357}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.8264773436285633
{'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.49574468, 8.96731195, 5.38275566]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.4968211898886231}
episode index:3966
target Thresh 19.0
target distance 11.0
model initialize at round 3966
at step 0:
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([2.9398723 , 7.30710265, 3.26223981]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 12.178365304534868}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.8264611765932012
{'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.95029939,  9.7492967 ,  0.71357205]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.750943200048583}
episode index:3967
target Thresh 19.0
target distance 3.0
model initialize at round 3967
at step 0:
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([4.        , 7.        , 5.87030935]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 3.6055512754639896}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8264925598778048
{'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.27740938, 10.08262147,  3.30393874]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.2894516736916062}
episode index:3968
target Thresh 19.0
target distance 7.0
model initialize at round 3968
at step 0:
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([12.68000481, 10.90014057,  0.95062989]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.279719602848696}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.8265010168681709
{'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.77336989,  4.09318945,  5.81788866]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7789642255771843}
episode index:3969
target Thresh 19.0
target distance 14.0
model initialize at round 3969
at step 0:
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([1.99999998, 6.9999999 , 5.56251001]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.560219766204186}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.8264647583236224
{'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.37402447,  2.70306789,  6.16428633]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.47755940190675256}
episode index:3970
target Thresh 19.0
target distance 2.0
model initialize at round 3970
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([2.       , 7.       , 2.4647541]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 2.2360679775003427}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8265034476315238
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.13351529, 5.76447472, 0.1815688 ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8979242260079532}
episode index:3971
target Thresh 19.0
target distance 10.0
model initialize at round 3971
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 4.3989019 , 10.63501172,  2.34149629]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.266804338332513}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.826476062708303
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.12355912,  2.95349819,  5.50964322]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.13201997981098945}
episode index:3972
target Thresh 19.0
target distance 1.0
model initialize at round 3972
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.99999991, 4.99999998, 4.35124731]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.0000000184968278}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8265147296947847
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.82560029, 6.46655015, 2.068062  ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9483063225846309}
episode index:3973
target Thresh 19.0
target distance 2.0
model initialize at round 3973
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.        ,  5.        ,  2.37917656]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.000000000029331}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8265533772212833
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.25732931,  3.33793954,  0.09599125]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.4247605302077042}
episode index:3974
target Thresh 19.0
target distance 4.0
model initialize at round 3974
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([ 2.        , 11.        ,  2.84952712]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 3.9999999999999996}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.8265846820445988
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.45020819, 7.39177394, 0.28315651]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.59680334216347}
episode index:3975
target Thresh 19.0
target distance 2.0
model initialize at round 3975
at step 0:
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([4.62627148, 4.59111473, 0.76368063]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 2.275171418591104}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8266232925370423
{'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.24884478, 3.62411939, 4.76368063]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.6718993510455101}
episode index:3976
target Thresh 19.0
target distance 1.0
model initialize at round 3976
at step 0:
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.        , 8.        , 2.25686085]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.414213562373095}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.8266643729261454
{'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.34761239, 7.82541067, 4.25686085]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.052099023206872}
episode index:3977
target Thresh 19.0
target distance 9.0
model initialize at round 3977
at step 0:
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([2.99999979, 1.99999995, 4.38194203]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 9.848857932428226}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.8266621714592451
{'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.28032213, 10.05694516,  0.39964488]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.1862919861601742}
episode index:3978
target Thresh 19.0
target distance 12.0
model initialize at round 3978
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([2.       , 9.       , 5.1842792]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 12.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.8266538658830161
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.99588977,  9.68583704,  0.91879675]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.685849357709986}
episode index:3979
target Thresh 19.0
target distance 14.0
model initialize at round 3979
at step 0:
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([3.6600005 , 7.27710361, 1.17540472]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.636841072934331}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.8266415964090252
{'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.28257151, 10.1619312 ,  0.90992191]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.7354762768515157}
episode index:3980
target Thresh 19.0
target distance 13.0
model initialize at round 3980
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([3.67479302, 8.16569978, 1.10861653]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.89481162314844}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.8266071345877392
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.58256031,  1.88404729,  5.99357815]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5939878290729776}
episode index:3981
target Thresh 19.0
target distance 12.0
model initialize at round 3981
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([2.99999998, 4.99999973, 5.65111041]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.165525035460941}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.8265658676129516
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.42936997,  3.76602923,  5.96970142]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8781567882126478}
episode index:3982
target Thresh 19.0
target distance 13.0
model initialize at round 3982
at step 0:
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([2.99999999, 8.00000001, 3.358392  ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.601470519428872}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.8265385426480972
{'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.90202595,  4.23441821,  0.24335362]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.25406851531529906}
episode index:3983
target Thresh 19.0
target distance 3.0
model initialize at round 3983
at step 0:
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.9758639 , 3.68279694, 2.59513819]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.6684959283493903}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.8265746271002439
{'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.43777522, 5.2928234 , 2.31195288]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.6339102810039767}
episode index:3984
target Thresh 19.0
target distance 2.0
model initialize at round 3984
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.74954349,  9.12638652,  3.41834664]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9088063431213784}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8266181466417495
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.74954349,  9.12638652,  3.41834664]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9088063431213784}
episode index:3985
target Thresh 19.0
target distance 11.0
model initialize at round 3985
at step 0:
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([2.08119552, 6.41003064, 3.1583097 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 12.19696033532796}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.8265963408782638
{'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.01113642,  9.88825573,  0.32645663]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8883255414194957}
episode index:3986
target Thresh 19.0
target distance 2.0
model initialize at round 3986
at step 0:
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([13.       , 11.       ,  4.4252944]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 2.236067977563408}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.826637324991412
{'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.0878874 ,  9.71600442,  0.14210909]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9553025113239138}
episode index:3987
target Thresh 19.0
target distance 2.0
model initialize at round 3987
at step 0:
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([16.00000001,  4.        ,  0.66407507]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.2360679861622947}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8266758061035004
{'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.53326937,  3.17478451,  4.66407507]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.5611825446049232}
episode index:3988
target Thresh 19.0
target distance 11.0
model initialize at round 3988
at step 0:
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.39434174, 10.49576828,  4.45587468]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 9.407414250202999}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.8266820186793318
{'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.82809443, 10.37243125,  5.03994815]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9079897710764014}
episode index:3989
target Thresh 19.0
target distance 5.0
model initialize at round 3989
at step 0:
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([14.        , 10.        ,  2.76299787]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 5.385164807134505}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.8267060945379153
{'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.29923642,  5.6612125 ,  6.19662725]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.7257715883376098}
episode index:3990
target Thresh 19.0
target distance 1.0
model initialize at round 3990
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.        ,  3.        ,  3.06649184]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.9999999999999999}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8267445294929298
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.58716862,  1.46870797,  0.78330654]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.7918574399550029}
episode index:3991
target Thresh 19.0
target distance 13.0
model initialize at round 3991
at step 0:
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([2.        , 9.        , 3.90058541]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 14.317821063276352}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.8267066999637632
{'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.61846094,  3.33482075,  0.21917642]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.7032772323197031}
episode index:3992
target Thresh 19.0
target distance 2.0
model initialize at round 3992
at step 0:
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.68183321, 11.06184767,  1.0467574 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.3241222596328699}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8267500992375014
{'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.68183321, 11.06184767,  1.0467574 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.3241222596328699}
episode index:3993
target Thresh 19.0
target distance 1.0
model initialize at round 3993
at step 0:
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.07502633,  9.70608754,  0.13246506]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.3033372417920092}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.8267934767790043
{'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.07502633,  9.70608754,  0.13246506]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.3033372417920092}
episode index:3994
target Thresh 19.0
target distance 5.0
model initialize at round 3994
at step 0:
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.00263979,  1.99942535,  0.7956559 ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 5.00057534759145}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.8268198276854193
{'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.3381889 ,  6.902803  ,  2.22928529]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.35187922603765537}
episode index:3995
target Thresh 19.0
target distance 5.0
model initialize at round 3995
at step 0:
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([2.90911799, 9.58370463, 0.0098626 ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.746059590617532}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.8268485214596224
{'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.39350485, 6.2868442 , 5.72667729]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.6709068205290175}
episode index:3996
target Thresh 19.0
target distance 10.0
model initialize at round 3996
at step 0:
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.        , 2.        , 5.24902964]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 11.180339887498947}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.8268176480979862
{'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.58821516,  7.08800331,  6.13399126]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.5947618478897726}
episode index:3997
target Thresh 19.0
target distance 11.0
model initialize at round 3997
at step 0:
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.20941148,  6.81165266,  4.93426299]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.241624027464189}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.8267996144909591
{'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.94567812, 6.51594165, 4.38559523]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.5187934578819867}
episode index:3998
target Thresh 19.0
target distance 2.0
model initialize at round 3998
at step 0:
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.66393555,  4.10483403,  1.0729205 ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 2.2070651856800745}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.8268379491710064
{'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.64620118,  2.76459191,  5.0729205 ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.8424810902923808}
episode index:3999
target Thresh 19.0
target distance 14.0
model initialize at round 3999
at step 0:
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([2.49388872, 1.74362974, 6.09693956]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.508544244286128}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.826782491200498
{'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.21594572,  2.93065764,  5.56597465]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.9553827499427234}

Process finished with exit code 0
