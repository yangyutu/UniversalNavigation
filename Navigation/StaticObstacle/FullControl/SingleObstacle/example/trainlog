/home/yangyutu/anaconda3/bin/python /snap/pycharm-community/132/helpers/pydev/pydevconsole.py --mode=client --port=32907
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel'])
Python 3.7.3 (default, Mar 27 2019, 22:11:17)
Type 'copyright', 'credits' or 'license' for more information
IPython 7.4.0 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.4.0
Python 3.7.3 (default, Mar 27 2019, 22:11:17)
[GCC 7.3.0] on linux
runfile('/home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/StaticObstacle/FullControl/SingleObstacle/DDPGHER_CNN.py', wdir='/home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/StaticObstacle/FullControl/SingleObstacle')
Backend Qt5Agg is interactive backend. Turning interactive mode on.
episode index:0
target Thresh 3.799999999999999
target distance 2.0
model initialize at round 0
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.15099654, 1.14190861, 6.27088308]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 2.8620772766188565}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([10.90900758, 10.6917154 ]), 'previousTarget': array([10.90905158, 10.69155129]), 'currentState': array([-0.08397203, -0.17629469,  1.37452224]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 15.458306640197621}
episode index:1
target Thresh 4.100980165737321
target distance 4.0
model initialize at round 1
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([2.00000331, 9.00000204, 1.55431431]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.472132079511647}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([1.99915259, 9.00044203, 4.77556979]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.472696280928697}
episode index:2
target Thresh 4.396000524884688
target distance 2.0
model initialize at round 2
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.99999873, 10.00000168,  3.22197247]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.236066089236515}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([14.00007264, 10.00017474,  0.16004264]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.2360548143897914}
episode index:3
target Thresh 4.685179089519418
target distance 2.0
model initialize at round 3
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.00000002, 9.99999979, 5.81190586]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.2360677815980545}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 2.99986401, 10.00006884,  2.74997603]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 2.2361903639098153}
episode index:4
target Thresh 4.968631534923135
target distance 3.0
model initialize at round 4
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.99999994,  4.99999979,  5.43533063]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.9999997908944724}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.00009464,  5.00001992,  2.3734008 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 3.0000199264307543}
episode index:5
target Thresh 5.246471245853414
target distance 2.0
model initialize at round 5
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.00000194,  6.99999931,  0.66154593]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.9999993111047991}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.00011847,  7.00009144,  3.88280141]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.00009144375591}
episode index:6
target Thresh 5.518809361899206
target distance 4.0
model initialize at round 6
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.99999983,  5.00000039,  2.98565912]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.9999996142949614}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.00010796,  5.00002336,  6.2069146 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.999976638135776}
episode index:7
target Thresh 5.785754821938149
target distance 1.0
model initialize at round 7
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 8.00000028, 11.00000021,  1.64682501]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0000002810356774}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.12375
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.9999916 , 11.00000492,  3.64682501]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9999916041888333}
episode index:8
target Thresh 6.047414407713585
target distance 6.0
model initialize at round 8
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 9.00000024, 10.99999993,  0.72268933]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 5.999999755126698}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 8.99996697, 11.00004984,  3.9439448 ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 6.000033031565236}
episode index:9
target Thresh 6.303892786548666
target distance 3.0
model initialize at round 9
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.99999984,  5.00000018,  3.30484307]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.605551034496957}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.099
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([16.00055191,  4.99990377,  0.24291324]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.6059375094415596}
episode index:10
target Thresh 6.555292553214675
target distance 1.0
model initialize at round 10
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.99999985,  3.00000017,  3.3053894 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9999998460104166}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.1809090909090909
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.99999985,  3.00000017,  3.3053894 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9999998460104166}
episode index:11
target Thresh 6.801714270970326
target distance 6.0
model initialize at round 11
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.00000007,  4.99999931,  5.81954432]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.211103161423167}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16583333333333333
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([14.00012732,  5.00003088,  2.75761449]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.211147484862334}
episode index:12
target Thresh 7.043256511788387
target distance 4.0
model initialize at round 12
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.0000001 ,  7.9999998 ,  6.15655327]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 3.999999796500774}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15307692307692308
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.00007208,  8.00012062,  3.09462344]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.000120618200147}
episode index:13
target Thresh 7.280015895785792
target distance 5.0
model initialize at round 13
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 9.99999594, 11.00000204,  3.67775095]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.099023899371544}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14214285714285715
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([ 9.99996092, 11.00008824,  0.61582112]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.099075135536808}
episode index:14
target Thresh 7.512087129872971
target distance 4.0
model initialize at round 14
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.00000087, 6.99999929, 0.31832283]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.472136199160872}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13266666666666668
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([3.99981182, 6.9999403 , 3.53957831]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 4.4722735162779745}
episode index:15
target Thresh 7.739563045637888
target distance 2.0
model initialize at round 15
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.99999994, 7.00000045, 2.69514161]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.0000004528138}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12437500000000001
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.99977005, 6.99993231, 5.91639709]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.9999323252703565}
episode index:16
target Thresh 7.962534636479896
target distance 4.0
model initialize at round 16
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.99996879, 6.99998681, 4.54350209]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.000013192728266}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11705882352941177
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([1.99931598, 6.99991901, 1.48157226]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.00008105345962}
episode index:17
target Thresh 8.181091094008332
target distance 3.0
model initialize at round 17
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([13.99999988,  9.99999989,  4.89188838]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 3.162277594327997}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11055555555555557
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.00024674, 10.000496  ,  1.82995855]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 3.162670206700728}
episode index:18
target Thresh 8.395319843720328
target distance 1.0
model initialize at round 18
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.0000002 , 9.99999971, 0.03513401]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9999997128753934}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.1573684210526316
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.0000002 , 9.99999971, 0.03513401]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.9999997128753934}
episode index:19
target Thresh 8.605306579972192
target distance 4.0
model initialize at round 19
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([14.99999966,  5.99999974,  4.80732942]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 4.123105451821366}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14950000000000002
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.00063951,  6.00024722,  1.74539959]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 4.123190414468359}
episode index:20
target Thresh 8.81113530025828
target distance 2.0
model initialize at round 20
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 9.99999722, 11.00000792,  2.91039461]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.0000027810859256}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14238095238095239
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 9.99988629, 11.00035753,  6.13165009]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.000113745786641}
episode index:21
target Thresh 9.012888338811136
target distance 3.0
model initialize at round 21
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([13.99999452,  1.99998601,  5.34130192]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.162292662667831}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13590909090909092
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.01040524,  1.99463999,  2.27937209]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 3.1640933212616664}
episode index:22
target Thresh 9.21064639953625
target distance 1.0
model initialize at round 22
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.97465343, 3.9874416 , 4.60360765]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0128755924969686}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.94432781, 3.92281313, 1.54167782]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0786245615542214}
episode index:23
target Thresh 9.404488588294726
target distance 4.0
model initialize at round 23
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([1.99996875, 9.99998917, 4.47702241]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 4.123102703978787}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12458333333333334
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 1.99947419, 10.00021418,  1.41509258]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 4.123440965768392}
episode index:24
target Thresh 9.59449244454666
target distance 4.0
model initialize at round 24
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 7.99999974, 11.00000142,  2.75118655]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 3.999999744326548}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11960000000000001
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 7.99977191, 11.00019062,  5.97244203]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 3.999771912832314}
episode index:25
target Thresh 9.78073397236797
target distance 5.0
model initialize at round 25
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 9.00000005, 10.99999942,  5.79866123]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.999999951307291}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.115
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 8.99945387, 11.00052911,  2.7367314 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 5.000546161864946}
episode index:26
target Thresh 9.963287670853045
target distance 3.0
model initialize at round 26
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.99999573,  9.99999924,  4.32012439]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.9999992382235092}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11074074074074075
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.00161903, 10.00399827,  1.25819456]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 3.00399870340603}
episode index:27
target Thresh 10.142226563915358
target distance 2.0
model initialize at round 27
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([15.00009847,  9.99997732,  0.77561491]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 2.236166190654143}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10678571428571429
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([15.30894075, 10.15804963,  3.99687039]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 2.4576590067891684}
episode index:28
target Thresh 10.317622229498014
target distance 2.0
model initialize at round 28
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.99999768,  8.99999587,  5.20236421]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.999995874238818}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10310344827586207
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.00064984,  9.00000239,  2.14043438]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.000002492998678}
episode index:29
target Thresh 10.48954482820589
target distance 2.0
model initialize at round 29
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.9998941 , 5.99995482, 4.54683709]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.2360610303682695}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09966666666666667
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.99605825, 5.99976844, 1.48490726]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.234515238774593}
episode index:30
target Thresh 10.658063131370797
target distance 1.0
model initialize at round 30
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.00000581, 9.99999923, 0.87085312]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.4142171294305317}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09645161290322582
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 2.95266783, 10.18645266,  4.09210848]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.521593213184799}
episode index:31
target Thresh 10.823244548560947
target distance 2.0
model initialize at round 31
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([16.07196978,  3.02586898,  1.34706169]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.312026366905072}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0934375
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([16.0512275 ,  3.00555414,  4.56831717]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.2844415944997207}
episode index:32
target Thresh 10.985155154545662
target distance 8.0
model initialize at round 32
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.00000233, 11.00000188,  1.68219822]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 8.062259328829319}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0906060606060606
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.00013542, 11.00004508,  4.9034537 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 8.06228568647455}
episode index:33
target Thresh 11.143859715726173
target distance 7.0
model initialize at round 33
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 1.99999813, 11.00000069,  3.79003704]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 7.280111069603714}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08794117647058824
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([ 2.00024812, 11.00046519,  0.72810721]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 7.280489022092481}
episode index:34
target Thresh 11.299421716043039
target distance 2.0
model initialize at round 34
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.0000001 , 5.99999973, 6.07747507]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 2.000000269075467}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08542857142857144
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.99925309, 6.00012545, 3.01554524]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.999874686580061}
episode index:35
target Thresh 11.451903382370576
target distance 8.0
model initialize at round 35
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 3.00000191, 11.00000021,  1.11298722]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 8.062257722654577}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08305555555555556
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([ 3.00024127, 11.00041615,  4.33424269]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 8.062640761737299}
episode index:36
target Thresh 11.60136570940843
target distance 7.0
model initialize at round 36
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.99999835,  4.00000111,  3.55087781]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 9.219542542853493}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08081081081081082
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([14.99994913,  3.99987964,  0.48894798]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 9.219602739469254}
episode index:37
target Thresh 11.747868484080277
target distance 3.0
model initialize at round 37
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.00000454, 7.99999755, 0.50668352]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 2.999997547859351}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0786842105263158
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.9998588 , 8.00014402, 3.727939  ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.000144028029271}
episode index:38
target Thresh 11.891470309449378
target distance 8.0
model initialize at round 38
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.00003764,  5.99997885,  0.49018544]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.434024262200298}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07666666666666667
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.00038619,  5.99971137,  3.71144091]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.434461592191703}
episode index:39
target Thresh 12.032228628160603
target distance 12.0
model initialize at round 39
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([15.00000156,  9.00000397,  2.19805454]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 12.041596464373063}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07475000000000001
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([15.00314167,  8.99891348,  5.41931001]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 12.04463523674721}
episode index:40
target Thresh 12.170199745418232
target distance 1.0
model initialize at round 40
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.0000701 , 8.00004076, 1.5286228 ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.4142919534580571}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07292682926829269
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.41566746, 8.06080326, 4.7498778 ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.7690160828365349}
episode index:41
target Thresh 12.305438851508809
target distance 2.0
model initialize at round 41
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.00068896, 2.00005097, 1.07584875]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 2.2354289749446217}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07119047619047619
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.76583917, 7.51757589, 4.29688429]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 4.683123406631546}
episode index:42
target Thresh 12.438000043877988
target distance 11.0
model initialize at round 42
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.95163798,  5.98964575,  4.35449922]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.66281765069184}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06953488372093024
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.51206667, -0.24080293,  1.29075646]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 14.675748367964601}
episode index:43
target Thresh 12.567936348770218
target distance 11.0
model initialize at round 43
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([15.0009296 ,  5.00041154,  1.41876906]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 12.083721978138659}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06795454545454546
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.13231502,  4.35144874,  4.63968634]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 10.738030962344633}
episode index:44
target Thresh 12.695299742439962
target distance 11.0
model initialize at round 44
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([ 3.10539537, 12.67962367,  2.51012856]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 11.023318270560091}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.08443839707158353
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([14.30592586, 11.99827589,  0.52769277]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0441002723661672}
episode index:45
target Thresh 12.820141171942895
target distance 1.0
model initialize at round 45
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.09553268,  7.14247786,  1.98213797]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9156205928658881}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.10434191017872303
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.09553268,  7.14247786,  1.98213797]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9156205928658881}
episode index:46
target Thresh 12.942510575515385
target distance 5.0
model initialize at round 46
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.00091853,  2.99885172,  0.10589653]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 5.001148362542939}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.11969992536291162
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.00172821,  7.59100631,  0.40672794]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0788060057215991}
episode index:47
target Thresh 13.062456902550478
target distance 12.0
model initialize at round 47
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.0012481 ,  2.99877189,  0.22467476]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 13.894140893580435}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11720617691785096
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([13.12822477,  4.84272754,  3.44581472]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 12.265188369591868}
episode index:48
target Thresh 13.180028133178295
target distance 7.0
model initialize at round 48
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([ 8.87804907, 10.16223607,  6.2541368 ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 6.179007325621235}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.1332709911646051
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.68383561, 11.56653596,  1.12137662]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.6487857194146087}
episode index:49
target Thresh 13.295271297458726
target distance 1.0
model initialize at round 49
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.45281925,  4.57844865,  3.33042181]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6907331664955393}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.150605571341313
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.45281925,  4.57844865,  3.33042181]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.6907331664955393}
episode index:50
target Thresh 13.408232494194078
target distance 2.0
model initialize at round 50
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.65723639,  3.95289904,  0.97358578]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.3459846716159255}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.1672603640601108
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.65723639,  3.95289904,  0.97358578]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.3459846716159255}
episode index:51
target Thresh 13.518956909369209
target distance 10.0
model initialize at round 51
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([14.02656713,  3.02258855,  1.70663529]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 12.215413167770086}
done in step count: 99
reward sum = 0.36972963764972644
running average episode reward sum: 0.17115400393683416
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.9250726 , 10.23212023,  4.92781579]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.9537500264921435}
episode index:52
target Thresh 13.62748883422654
target distance 11.0
model initialize at round 52
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([2.88471064, 5.36827205, 2.87618423]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.614426982465949}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16792468310783729
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.90293686, 2.39735711, 6.09729367]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.105737226121791}
episode index:53
target Thresh 13.733871682983127
target distance 5.0
model initialize at round 53
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.70418305, 5.92622025, 4.83556092]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 3.9373484841888136}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.18207543615967345
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.21894436, 1.84368711, 6.26917564]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.2690173784436035}
episode index:54
target Thresh 13.838148010196925
target distance 12.0
model initialize at round 54
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 2.99999639, 11.00000748,  3.02220309]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 13.89245087101697}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17876497368404304
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.90685912, 3.57320522, 6.24338501]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.102160493037806}
episode index:55
target Thresh 13.940359527789191
target distance 11.0
model initialize at round 55
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 3.2579739 , 10.11794415,  1.72852915]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.78187584842294}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.1903257531510349
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.11484095, 11.76536888,  2.0293974 ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.1701692461621387}
episode index:56
target Thresh 14.0405471217298
target distance 11.0
model initialize at round 56
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.68286105, 6.01627909, 1.01167315]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.793212844025575}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18698670485013955
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.62270201, 2.98674733, 4.23269211]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.377307356730357}
episode index:57
target Thresh 14.13875086839218
target distance 11.0
model initialize at round 57
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.00000061,  5.99999976,  0.62041014]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 11.045361643859387}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.19529679198323507
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.37212428, 7.82001139, 5.22216523]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.900497171998407}
episode index:58
target Thresh 14.2350100505844
target distance 9.0
model initialize at round 58
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([11.03113278, 11.31437703,  2.47408786]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 9.126277783922436}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.20671118216604428
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.81392575, 9.56495576, 5.34131791]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9228968641982696}
episode index:59
target Thresh 14.329363173262806
target distance 3.0
model initialize at round 59
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([11.19870717, 10.21076775,  0.22477847]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.966607080759965}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.2194376457966102
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.51597714, 10.25765448,  6.22476905]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8862025729559437}
episode index:60
target Thresh 14.421847978934528
target distance 4.0
model initialize at round 60
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([10.999895  , 11.00004935,  3.70425367]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 5.656963388699837}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.2305179836402513
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.07099266,  6.8512139 ,  0.57148777]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.16485527896475052}
episode index:61
target Thresh 14.512501462754983
target distance 13.0
model initialize at round 61
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([2.80548737, 7.52233759, 6.21346951]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 12.203864125495858}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.22679995164605368
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.77498339, 6.96190777, 3.1515051 ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.277577545298028}
episode index:62
target Thresh 14.601359887326431
target distance 7.0
model initialize at round 62
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([13.48263132, 11.0872484 ,  1.180844  ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.24786158532525}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.23726955355192791
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.72014593,  4.36933666,  0.04809169]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.4633873802879466}
episode index:63
target Thresh 14.68845879720349
target distance 11.0
model initialize at round 63
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([4.00001038, 8.00004145, 2.32742649]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.529974823176232}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.23356221677767905
{'scaleFactor': 20, 'currentTarget': array([14.91999071,  3.04721034]), 'previousTarget': array([14.89314979,  2.88701343]), 'currentState': array([ 3.38962311, -0.36516295,  3.83161777]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.024710741025325}
episode index:64
target Thresh 14.77383303311145
target distance 12.0
model initialize at round 64
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([2.68863909, 5.53560073, 2.1512327 ]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 11.40575955042464}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2299689519041763
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.79893658, 6.56880039, 5.37245945]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.21116176635322}
episode index:65
target Thresh 14.857516745883007
target distance 1.0
model initialize at round 65
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.58841254, 11.16351671,  1.10458177]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.610710268668223}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.24163608899653727
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.58841254, 11.16351671,  1.10458177]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.610710268668223}
episode index:66
target Thresh 14.939543410119073
target distance 13.0
model initialize at round 66
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([3.68542704, 6.46296154, 6.13386154]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.792216767989467}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.23802958020554416
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.78310397, 2.80700588, 3.07183432]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.2185562007458}
episode index:67
target Thresh 15.019945837579046
target distance 5.0
model initialize at round 67
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.15476452, 7.05813522, 4.07492065]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 5.128270171016018}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.24823598855409484
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.48234198, 2.21482133, 5.50854003]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5604623389624231}
episode index:68
target Thresh 15.09875619030595
target distance 10.0
model initialize at round 68
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4.68082477, 7.91563505, 0.95184964]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 8.872540994749828}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.25591113885619704
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.77351116, 10.48907713,  0.68635943]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.5588733097601833}
episode index:69
target Thresh 15.176005993491701
target distance 11.0
model initialize at round 69
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([15.00000023,  4.99999972,  0.11811083]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 11.401754401060956}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.25225526544396565
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.12060328,  3.76482241,  3.33933543]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.289779460094731}
episode index:70
target Thresh 15.251726148087581
target distance 2.0
model initialize at round 70
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([12.69004554, 10.86812958,  4.24392176]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 2.281641692176099}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.26264603635320555
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.53977743,  9.41631347,  6.2439208 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.6205817605507956}
episode index:71
target Thresh 15.325946943165045
target distance 11.0
model initialize at round 71
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([14.56220622,  3.3750138 ,  5.45122349]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.651331518060719}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2589981747371888
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.19947232,  3.54797088,  2.38926064]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.328799749736998}
episode index:72
target Thresh 15.398698068031749
target distance 9.0
model initialize at round 72
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([ 9.75367391, 12.13068789,  3.40680456]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 10.534048076750354}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.2666544591599291
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.94834862, 5.47919439, 5.70768628]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.4819700490926708}
episode index:73
target Thresh 15.470008624107667
target distance 6.0
model initialize at round 73
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.00009535,  8.00001745,  1.1829521 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.708281417707666}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.2751501861207235
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.30407411, 11.43650985,  4.33339022]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5319792453308095}
episode index:74
target Thresh 15.539907136566047
target distance 13.0
model initialize at round 74
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.66317003, 6.74278131, 0.84856003]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.406327746002376}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2714815169724472
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.70042549, 7.71291363, 4.0698049 ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.303220901596738}
episode index:75
target Thresh 15.608421565743868
target distance 10.0
model initialize at round 75
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([3.99999992, 6.        , 4.11987305]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.19803910857647}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.26790939174912554
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.88330208, 7.67140009, 1.05793201]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.122617980460515}
episode index:76
target Thresh 15.675579318326335
target distance 9.0
model initialize at round 76
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 8.864531  , 10.13727439,  5.6866653 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.030351852355896}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.27453162509522977
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.49625823, 2.01512992, 5.42117044]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.5039689288268129}
episode index:77
target Thresh 15.741407258309936
target distance 2.0
model initialize at round 77
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([15.14267898,  6.78569565,  0.18622249]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.386734621563765}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.28357737349144474
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.50320515,  6.74392365,  4.18622118]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.8945543721317074}
episode index:78
target Thresh 15.805931717748377
target distance 7.0
model initialize at round 78
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.00115285, 9.00006646, 1.05958145]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 7.00006655361541}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.2912078481525167
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([1.71072149, 2.13349596, 6.21000598]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.31859571648462737}
episode index:79
target Thresh 15.869178507285772
target distance 4.0
model initialize at round 79
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.99992775, 9.0003155 , 2.79793364]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 4.4723858340277145}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.2992185668994476
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([1.48256964, 5.6708702 , 4.23155671]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.847231370665774}
episode index:80
target Thresh 15.931172926481239
target distance 12.0
model initialize at round 80
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([3.99999999, 3.        , 3.4473933 ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 12.00000000530245}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2955245105179729
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.88659837, 0.70840053, 0.38530528]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.347207760405283}
episode index:81
target Thresh 15.991939773929056
target distance 12.0
model initialize at round 81
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([15.       ,  8.       ,  0.8107869]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 12.165525064717464}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.30058583633204994
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.94471207, 5.69980556, 5.97892298]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.305243274889701}
episode index:82
target Thresh 16.051503357178444
target distance 13.0
model initialize at round 82
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([4.10805603, 8.73330774, 0.14989966]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.894934049406393}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.30605732369194855
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.65923687,  8.54959106,  5.88440897]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5647899806667165}
episode index:83
target Thresh 16.109887502456885
target distance 11.0
model initialize at round 83
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([15.00001345,  6.00000826,  1.55300682]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 11.704709724648264}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.3114892926283194
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.51731551, 9.85080599, 5.28751507]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.5383996546007817}
episode index:84
target Thresh 16.16711556420097
target distance 13.0
model initialize at round 84
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([2.00044101, 5.99784175, 5.9159503 ]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.928751599171536}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.31538484921015936
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.03969462, 10.09125246,  5.95134598]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 1.3221227267994535}
episode index:85
target Thresh 16.223210434398432
target distance 13.0
model initialize at round 85
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([3.09139595, 6.66055983, 2.5178057 ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 12.977910405326824}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.3117175835216692
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.79706372, 6.51525592, 5.73905128]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.300895813896513}
episode index:86
target Thresh 16.2781945517453
target distance 10.0
model initialize at round 86
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([14.00000308, 10.00002777,  2.4622454 ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 11.1803550615179}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.31689718272655926
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.80409471, 5.38309336, 6.19675675]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.43027828469769996}
episode index:87
target Thresh 16.332089910621647
target distance 1.0
model initialize at round 87
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.12069489,  3.05733227,  1.445463  ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.0641986555723673}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.32443357837739384
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.45798063,  1.63985791,  5.44545656]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.6507590361738614}
episode index:88
target Thresh 16.384918069889633
target distance 7.0
model initialize at round 88
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 7.33631962, 11.25389301,  3.99215186]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 7.488637983384879}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.33064804404729914
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.16469453, 6.6741225 , 4.85940705]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.6939491541878471}
episode index:89
target Thresh 16.436700161517347
target distance 11.0
model initialize at round 89
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([3.99999977, 4.99999982, 4.81422949]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.704700191585706}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.33305369514000427
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.09380654,  8.59257858,  5.43369637]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.9935687169704445}
episode index:90
target Thresh 16.487456899031883
target distance 11.0
model initialize at round 90
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([15.0000014 ,  5.00000018,  1.13345021]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 11.70470116130877}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.33681935836989746
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.63215116, 9.9027469 , 3.73521913]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.9748152324679957}
episode index:91
target Thresh 16.53720858580501
target distance 13.0
model initialize at round 91
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([2.99998847, 7.99996959, 5.35191464]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 13.03841864195369}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.340073338018745
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.00582042,  9.07983128,  1.10412776]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9973796068980798}
episode index:92
target Thresh 16.585975123174805
target distance 10.0
model initialize at round 92
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.00000589,  9.00000276,  1.44102448]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 10.198044258671395}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.34548054076460716
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.100034  , 11.36794288,  4.02509067]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.3812987863043593}
episode index:93
target Thresh 16.63377601840644
target distance 11.0
model initialize at round 93
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([3.42598235, 3.77523924, 6.04911005]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 11.386766564471564}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.34180521586285606
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([4.78217904, 7.81834733, 2.98717093]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 9.219610671138751}
episode index:94
target Thresh 16.680630392495363
target distance 4.0
model initialize at round 94
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([15.00003129,  8.00000768,  1.24258476]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.123120663612193}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.3481175835843102
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.76819285,  4.74489323,  0.67621295]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.0700402648335852}
episode index:95
target Thresh 16.726556987815947
target distance 8.0
model initialize at round 95
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 3.53343654, 10.1744814 ,  4.65628076]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 7.194285085652999}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.35363220274487955
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.85599193, 2.85142764, 5.52353512]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.20691078386974385}
episode index:96
target Thresh 16.771574175618678
target distance 14.0
model initialize at round 96
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.44459624,  7.64266367,  3.75177503]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.461179405684415}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.35723854802585575
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.69722441, 7.0348618 , 4.63672902]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.304775988481777}
episode index:97
target Thresh 16.815699963378915
target distance 7.0
model initialize at round 97
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 1.99999948, 10.00000076,  3.17156184]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.0710682188759195}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.3626380003084096
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.47147715, 11.73039358,  2.03881608]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9015604144492877}
episode index:98
target Thresh 16.858952002000116
target distance 13.0
model initialize at round 98
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([2.38954741, 3.36277397, 5.94797707]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.61566960036069}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.35897499020428425
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.77499074, 2.96920279, 2.8859986 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.22505563667897}
episode index:99
target Thresh 16.90134759287443
target distance 13.0
model initialize at round 99
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 3.41993776, 10.90333768,  1.56859928]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 14.020006702139199}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.3553852403022414
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.79184156, 2.62408513, 4.78982448]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.215077612677375}
episode index:100
target Thresh 16.942903694803487
target distance 10.0
model initialize at round 100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([13.31844407,  7.51537059,  5.2329917 ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 9.33268485835674}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.3593390427468096
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.44201537, 7.83498397, 4.68432107]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.0042634457867254}
episode index:101
target Thresh 16.98363693078215
target distance 12.0
model initialize at round 101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([3.68150221, 9.06962586, 1.0433833 ]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 10.360356781744418}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.3632153196532491
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.61511736,  9.81785196,  0.49471196]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.42580812395073137}
episode index:102
target Thresh 17.023563594647925
target distance 1.0
model initialize at round 102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.01847171, 8.0560366 , 2.2543765 ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.441738909386622}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.36920449130710103
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.79131158, 6.60690066, 6.25437519]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.4450594832713564}
episode index:103
target Thresh 17.062699657598724
target distance 11.0
model initialize at round 103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([3.99999992, 6.00000022, 2.930453  ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 11.180339928507165}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.36565444812145587
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([4.89268493, 6.66312231, 6.15169978]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 10.195345011190897}
episode index:104
target Thresh 17.101060774581548
target distance 13.0
model initialize at round 104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.67086407, 4.20125926, 1.12187451]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.72692873790866}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.36722826807225256
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.11352444, 10.09747085,  1.45815954]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.14962742018614164}
episode index:105
target Thresh 17.138662290554674
target distance 3.0
model initialize at round 105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.00182056, 8.00135465, 1.64170473]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 3.162987819212339}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.372735454693269
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.60077929, 5.26048726, 5.35851716]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.4766873068195332}
episode index:106
target Thresh 17.17551924662586
target distance 3.0
model initialize at round 106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([14.94948361,  7.0342218 ,  3.54816866]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 3.179311418153636}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.3782294785746404
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.82760323,  4.01323636,  5.26498252]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.1729041544523094}
episode index:107
target Thresh 17.211646386068956
target distance 9.0
model initialize at round 107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.2873844 ,  7.59753682,  5.17008603]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.958654799983034}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.38245433304570936
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.12393338, 11.54482652,  3.47097311]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.5587445067901201}
episode index:108
target Thresh 17.24705816022145
target distance 4.0
model initialize at round 108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([1.99999905, 6.99999997, 4.17773414]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 5.656854945611787}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.3874111253519682
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.72792172, 10.02224062,  1.32817738]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0149088588488644}
episode index:109
target Thresh 17.28176873426516
target distance 11.0
model initialize at round 109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.3287112 ,  8.54306661,  2.98313624]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.637173633683547}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.3838892060305866
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.22210602,  6.118711  ,  6.20438921]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.222870040181027}
episode index:110
target Thresh 17.315791992892525
target distance 1.0
model initialize at round 110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.84906189,  7.14952076,  3.35881031]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 1.1593878949642944}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.3893496636339147
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.37852728,  5.60605517,  5.35881019]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.735813065054719}
episode index:111
target Thresh 17.34914154586064
target distance 1.0
model initialize at round 111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([15.01085209,  2.99327655,  0.44732719]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 1.4266443320155944}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.39462422020861193
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.61569535,  4.12072138,  4.4473254 ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.4028197031252909}
episode index:112
target Thresh 17.381830733435358
target distance 4.0
model initialize at round 112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([11.99975529, 11.00002091,  4.0583719 ]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 5.00020831570172}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.39938033638293385
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.01650197,  8.1878346 ,  5.49199938]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.001274295804401}
episode index:113
target Thresh 17.41387263172757
target distance 8.0
model initialize at round 113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([3.17296868, 7.67402339, 2.46983656]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 8.504383556824688}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.40297987613590164
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.97936442, 10.6346275 ,  0.48753703]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.3659547651779099}
episode index:114
target Thresh 17.44528005792383
target distance 5.0
model initialize at round 114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.99751656,  7.00087736,  3.80399513]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.0008779746698435}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4075806193686937
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.3108586 ,  2.24772087,  5.2376238 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.7323124296503908}
episode index:115
target Thresh 17.476065575413383
target distance 10.0
model initialize at round 115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.00000237, 6.0000066 , 2.22833405]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.770329867003428}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.40406699333965324
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.77121072, 2.21835777, 5.44952957]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.231372140847013}
episode index:116
target Thresh 17.506241498813704
target distance 4.0
model initialize at round 116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.99999724,  5.99999343,  5.31674647]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 4.0000065699878276}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.40874154937863055
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.9080041,  9.1608391,  2.7503755]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.8441885234645717}
episode index:117
target Thresh 17.535819898896506
target distance 6.0
model initialize at round 117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([11.38743658, 11.95252844,  1.60363406]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.709888712661301}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.41301930953206284
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.00187897, 10.67807536,  0.7540742 ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.321930127051316}
episode index:118
target Thresh 17.564812607416204
target distance 3.0
model initialize at round 118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.99996193,  8.00045651,  2.65598786]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 3.605910009482724}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4176207944099447
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.1090034 ,  5.9183067 ,  4.37280112]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9247534453874433}
episode index:119
target Thresh 17.59323122184278
target distance 4.0
model initialize at round 119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([1.83429574, 9.32525484, 5.615767  ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.601091404925427}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.4219862890348702
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.53495852, 7.0448329 , 5.04939353]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.46719756717061545}
episode index:120
target Thresh 17.62108711000093
target distance 12.0
model initialize at round 120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 5.16330899, 11.21614374,  1.8095991 ]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 11.060974875384753}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.42519076489591473
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.28147951,  8.55665885,  6.11048523]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.8442884939002899}
episode index:121
target Thresh 17.648391414617326
target distance 2.0
model initialize at round 121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.30128218,  3.34452502,  5.89441013]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.7333031253111617}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.4296588651836531
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.40668966,  2.69163751,  5.6112228 ]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.6686587978238746}
episode index:122
target Thresh 17.675155057777882
target distance 11.0
model initialize at round 122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([15.00003427,  4.00005696,  2.03115653]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 11.180363415483754}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.4261657036780949
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.31229733,  5.81493117,  5.25240915]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.314136135055564}
episode index:123
target Thresh 17.70138874529672
target distance 12.0
model initialize at round 123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([2.99999683, 3.99999239, 5.31940413]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 12.369321797333258}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.42272888348714255
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.61059519, 6.05009759, 2.25746798]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.432739185704365}
episode index:124
target Thresh 17.727102970998622
target distance 3.0
model initialize at round 124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.80398711,  6.99743887,  4.1566577 ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 3.0089523775497593}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4269549728184454
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.68032474,  9.59712951,  1.59028577]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.7906619941213039}
episode index:125
target Thresh 17.75230802091674
target distance 11.0
model initialize at round 125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([13.37052892,  9.78913275,  6.0113405 ]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 12.759427331593562}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.42955627690086756
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.44852009, 4.97426721, 5.46267011]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 1.0725515688467595}
episode index:126
target Thresh 17.777013977407105
target distance 11.0
model initialize at round 126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.87260312, 4.84471303, 0.26928204]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.193078801562804}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.42617394401188435
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.77410644, 5.99087678, 3.49053096]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.22589763178923}
episode index:127
target Thresh 17.801230723181742
target distance 13.0
model initialize at round 127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([16.        ,  7.        ,  6.13686895]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 13.000000000125418}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.4282307810576437
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.48130786, 7.79480212, 4.73864336]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9291757967881392}
episode index:128
target Thresh 17.824967945261843
target distance 5.0
model initialize at round 128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.98662161,  7.00597297,  3.72368288]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 5.102272786631401}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.4321364753743053
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([13.64405966,  2.33696623,  5.15731119]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.49014259408039196}
episode index:129
target Thresh 17.848235138852747
target distance 10.0
model initialize at round 129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([ 5.37075494, 10.02361466,  0.38306826]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 8.62927737074061}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.4351674919009305
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([14.06281791,  9.65718756,  0.68395308]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.3485203872531839}
episode index:130
target Thresh 17.871041611142125
target distance 1.0
model initialize at round 130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([14.60705934,  6.58596353,  2.81566656]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.5111707172571442}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4393272820390913
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.93118393,  5.68658859,  0.53248101]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.32087748443082587}
episode index:131
target Thresh 17.89339648502298
target distance 13.0
model initialize at round 131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.3856891 , 9.04492767, 0.39853209]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.661221694983606}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.4359990450539467
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.77436191, 7.07951631, 3.61978256]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.26331392854449}
episode index:132
target Thresh 17.915308702742934
target distance 4.0
model initialize at round 132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.84106933, 6.01933444, 1.02438074]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 4.068549608823903}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4399433831362479
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([3.97376875, 9.11983737, 2.74119507]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8805534283916282}
episode index:133
target Thresh 17.936787029481238
target distance 1.0
model initialize at round 133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.00875251, 9.0115055 , 1.92246681]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.4285394652411234}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.44397440266508187
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.77413705, 7.7235958 , 5.92246633]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.35695007152430785}
episode index:134
target Thresh 17.957840056854977
target distance 3.0
model initialize at round 134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 4.34609434, 11.30973   ,  3.9584651 ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.5729935105701607}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4478012293860813
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.24487744, 8.72497923, 5.6752792 ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.7652188193413033}
episode index:135
target Thresh 17.978476206355804
target distance 3.0
model initialize at round 135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.99998053, 2.99995367, 5.31657612]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.00004632608249}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.45150114718397777
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.37322048, 6.07159835, 2.75020479]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.3800261200088242}
episode index:136
target Thresh 17.998703732718674
target distance 2.0
model initialize at round 136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([16.01696278,  6.98291734,  0.2130807 ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 2.2437168657206787}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.455359532970956
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.34040233,  6.87102447,  4.21307987]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9351777173563929}
episode index:137
target Thresh 18.018530727223837
target distance 6.0
model initialize at round 137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([16.00006299, 10.99921965,  5.79493654]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.000063043385681}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.4586795164094537
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.71237089, 10.55402177,  4.94536715]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8404575354078587}
episode index:138
target Thresh 18.03796512093346
target distance 2.0
model initialize at round 138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.0008589 ,  7.99370599,  5.85001385]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.2413162637500235}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4624307429101051
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.02210051,  9.80474023,  3.56682819]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.9972029804154834}
episode index:139
target Thresh 18.057014687864154
target distance 4.0
model initialize at round 139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 9.99999909, 11.00000175,  3.05334473]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.000001778427106}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4657186997066609
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.78105855,  8.89005102,  0.20378809]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9165839718419566}
episode index:140
target Thresh 18.075687048096686
target distance 13.0
model initialize at round 140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([ 1.99999995, 10.99999999,  4.25476551]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 13.152946483513698}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.4678770291116148
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.32589332,  9.13892937,  5.98928091]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.6882740624198187}
episode index:141
target Thresh 18.093989670824122
target distance 13.0
model initialize at round 141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([15.55507268,  5.58187405,  2.23532355]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 14.59781093930578}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.4695359774629384
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.5819698 , 11.53461676,  3.12027826]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7902556055986709}
episode index:142
target Thresh 18.111929877339588
target distance 13.0
model initialize at round 142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([ 4.1887811 , 11.19125827,  1.78843897]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.012764237855832}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.4717467618219893
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.6998093 ,  8.79110501,  5.80613789]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.36572062009306344}
episode index:143
target Thresh 18.1295148439649
target distance 10.0
model initialize at round 143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([12.00000625, 11.00000581,  1.75050991]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 10.049882419756322}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4743245148189473
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.0878191 , 10.27126109,  4.33457682]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.2851223801986463}
episode index:144
target Thresh 18.146751604921167
target distance 3.0
model initialize at round 144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.99942007, 8.00138383, 2.96963483]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.0013838893605156}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.4776781113374373
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.0252092 , 5.4341023 , 4.68644857]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.0670810246360352}
episode index:145
target Thresh 18.163647055142608
target distance 11.0
model initialize at round 145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([15.        , 10.        ,  0.78582877]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 11.40175425149054}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.4795756536378907
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.55737464, 6.98383097, 0.23715874]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5576091138517648}
episode index:146
target Thresh 18.18020795303459
target distance 8.0
model initialize at round 146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.99987889, 10.0000634 ,  3.66135824]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 8.000063398735934}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.48222306968640144
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.99689196,  1.5152865 ,  0.24542896]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.4847234664291104}
episode index:147
target Thresh 18.196440923177075
target distance 9.0
model initialize at round 147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([5.74119303, 9.72345936, 5.74527633]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 9.287028589219325}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.48471789874997223
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.31365953,  8.60439124,  0.04616318]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.7921928676368075}
episode index:148
target Thresh 18.212352458974568
target distance 1.0
model initialize at round 148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.3669481 ,  6.32436569,  3.94751859]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7113141421937319}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4881761678858784
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.3669481 ,  6.32436569,  3.94751859]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.7113141421937319}
episode index:149
target Thresh 18.227948925253543
target distance 11.0
model initialize at round 149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.12165674,  4.2655923 ,  3.99739337]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 11.633201051606285}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.489205737447204
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.91513105, 10.34386166,  4.03279263]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.35418015273490855}
episode index:150
target Thresh 18.24323656080847
target distance 13.0
model initialize at round 150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([15.        ,  8.        ,  0.37562292]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.038404810445442}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.49067161486326416
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.77613224, 6.60080965, 5.54376711]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.45767861077487865}
episode index:151
target Thresh 18.258221480897404
target distance 2.0
model initialize at round 151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.86205689, 5.67684528, 2.6548748 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9206362198198322}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.4940224595023216
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.86205689, 5.67684528, 2.6548748 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9206362198198322}
episode index:152
target Thresh 18.272909679688183
target distance 11.0
model initialize at round 152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([ 4.6470999 , 10.34548424,  1.20875567]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.933227320406278}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.49577618665816986
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.52167399,  7.53872793,  4.94327106]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.7204328954947797}
episode index:153
target Thresh 18.287307032656177
target distance 6.0
model initialize at round 153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([14.00000002, 11.0000008 ,  2.54965693]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 6.324556072321486}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.49837074553869287
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.52746412,  4.89474506,  5.70009922]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.48411647285049636}
episode index:154
target Thresh 18.301419298934526
target distance 13.0
model initialize at round 154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([14.33356828,  3.76483458,  4.28378606]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 13.44609194711509}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.49955902843851024
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.35750389, 10.88754111,  4.88555725]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.6522639465326849}
episode index:155
target Thresh 18.31525212361792
target distance 13.0
model initialize at round 155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([15.22753968,  7.5048238 ,  5.23752213]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 12.237562096960314}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5013427613292836
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.6186241 , 8.64935041, 4.97203848]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8968565867348013}
episode index:156
target Thresh 18.328811040020668
target distance 14.0
model initialize at round 156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([3.57855951, 6.41652135, 0.64794796]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.435136962589302}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.4981494953335556
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.78001678, 6.69640648, 3.86919843]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.22408983350235}
episode index:157
target Thresh 18.342101471890118
target distance 4.0
model initialize at round 157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([11.00000023, 11.00000138,  2.41035578]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 5.656855067147717}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5006633482381453
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.62438992,  6.96093681,  5.5607989 ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.37763588910212836}
episode index:158
target Thresh 18.355128735576226
target distance 4.0
model initialize at round 158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.34353853,  6.71329148,  4.31497908]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 4.492322973331806}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5034357809498613
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.21646397, 10.12974507,  3.74860822]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8967721563628424}
episode index:159
target Thresh 18.36789804215817
target distance 5.0
model initialize at round 159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.01913781, 9.79365121, 5.80686927]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 5.124867369539608}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5061147157433433
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.69613056, 10.74960327,  0.95731239]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.3937450404856852}
episode index:160
target Thresh 18.380414499528833
target distance 14.0
model initialize at round 160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([1.99999999, 6.99999999, 4.66168618]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 14.560219793264967}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.507168220919225
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.29136416, 11.00674441,  0.9802748 ]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.7086679365699734}
episode index:161
target Thresh 18.39268311443806
target distance 14.0
model initialize at round 161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([16.        , 10.        ,  0.83760947]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 14.035668847908262}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5088874179555706
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.57463984, 11.18948539,  4.85531065]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.6050749248786047}
episode index:162
target Thresh 18.404708794495395
target distance 4.0
model initialize at round 162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.99999321, 1.9999874 , 5.22020817]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 4.123116205714949}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5115997040411192
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.32266335, 5.23465169, 2.65383696]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.8305839336107743}
episode index:163
target Thresh 18.416496350133208
target distance 12.0
model initialize at round 163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.00000785,  3.00000566,  1.62651795]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.649116297733762}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5120958427140484
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.75790207, 7.89608647, 5.09555172]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.1736211108173764}
episode index:164
target Thresh 18.428050496530922
target distance 2.0
model initialize at round 164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.49380421, 10.60743407,  2.27474692]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.5445255003740568}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5149922315460844
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.82719565, 10.38758954,  4.27474657]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.6363237468100102}
episode index:165
target Thresh 18.43937585550115
target distance 12.0
model initialize at round 165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 4.48712351, 10.78788395,  1.48920554]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 11.17446361233845}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5163013347877008
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.54343542,  6.53747485,  0.65735103]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7136186397868994}
episode index:166
target Thresh 18.450476957338495
target distance 9.0
model initialize at round 166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([12.50467594,  9.51147645,  3.92541754]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.650873365307196}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5184643269326784
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.8342158 , 10.94082873,  4.792675  ]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.8363116867500406}
episode index:167
target Thresh 18.4613582426317
target distance 12.0
model initialize at round 167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([1.9999973 , 6.99999901, 4.49492359]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 13.000002109093696}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5153782297485554
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.85560101, 1.77506051, 1.43296372]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.14716516595908}
episode index:168
target Thresh 18.472024064039974
target distance 6.0
model initialize at round 168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 9.00000013, 10.99999996,  0.71905726]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 7.81024973163947}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5174177571147845
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.822506  , 5.44600754, 5.58631519]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.4800279645004107}
episode index:169
target Thresh 18.48247868803409
target distance 7.0
model initialize at round 169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([2.99999999, 4.        , 4.31574154]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 9.219544461622913}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5189957593717989
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.02959271, 11.64351876,  2.0502586 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.1643911272335576}
episode index:170
target Thresh 18.492726296603045
target distance 1.0
model initialize at round 170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([2.78535181, 8.85834028, 4.72693944]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 1.2228808982046335}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5216922753988643
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.41854503, 9.6331725 , 2.44375342]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.759004189053093}
episode index:171
target Thresh 18.502770988926898
target distance 2.0
model initialize at round 171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([13.63497177, 11.01707866,  2.91737658]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 2.36508989357174}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5243574365884057
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.19809735, 10.40515918,  0.63419079]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.9984405153947161}
episode index:172
target Thresh 18.512616783016515
target distance 1.0
model initialize at round 172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.9724896 ,  4.123423  ,  2.79210621]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 1.1237597872081124}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5269917866659294
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.14730486,  2.34135197,  0.50892078]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.674919218050069}
episode index:173
target Thresh 18.52226761732081
target distance 2.0
model initialize at round 173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.60023332,  6.51569829,  1.31375569]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.6010720221526515}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5296527534092286
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.46506125,  7.75652683,  3.31375545]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5877402951019438}
episode index:174
target Thresh 18.531727352302184
target distance 9.0
model initialize at round 174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([4.29531105, 3.65683076, 2.39640954]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 10.643512870508923}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5305658753089991
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.29116949, 10.91815982,  0.99818502]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7135394225807927}
episode index:175
target Thresh 18.540999771980758
target distance 1.0
model initialize at round 175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.3401599 , 10.76423336,  4.2846905 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7006959862099175}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5332331146538344
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.3401599 , 10.76423336,  4.2846905 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7006959862099175}
episode index:176
target Thresh 18.55008858544804
target distance 11.0
model initialize at round 176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.16986058,  7.06613719,  4.72229743]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.622017769178072}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5302204981868636
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([13.21264525,  4.3452903 ,  1.66036617]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 10.218480726471544}
episode index:177
target Thresh 18.558997428350605
target distance 6.0
model initialize at round 177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.93261907,  2.00381194,  4.08707881]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 5.996566635798404}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5323738507110027
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.18762196,  7.30128462,  3.2375217 ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7234674735078994}
episode index:178
target Thresh 18.567729864344397
target distance 12.0
model initialize at round 178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([2.99997859, 9.99997569, 4.9925487 ]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 12.649123258291347}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5293996951204385
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.77325253, 5.77257668, 1.93061673]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.229275887034897}
episode index:179
target Thresh 18.576289386520244
target distance 7.0
model initialize at round 179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([13.34218708,  5.71025944,  4.31661773]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.517999597631901}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5311889066536298
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.90387715, 10.63735543,  4.90068976]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9739122006378469}
episode index:180
target Thresh 18.58467941880115
target distance 12.0
model initialize at round 180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([3.00053722, 6.01600732, 2.53924733]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 12.039740387225649}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5282541613130021
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.78544672, 5.65056249, 5.76050138]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.303304343907765}
episode index:181
target Thresh 18.5929033173119
target distance 14.0
model initialize at round 181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.98155242, 12.33977226,  3.22277117]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 14.446917163401523}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5294570070988186
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.20484297, 6.49347638, 4.67410316]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5343028910394878}
episode index:182
target Thresh 18.600964371721542
target distance 9.0
model initialize at round 182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([10.71067797, 10.11808672,  5.54487228]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 9.251939017141945}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5311239838985523
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.24350929, 7.53928072, 3.84575961]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.5917097885034533}
episode index:183
target Thresh 18.60886580655933
target distance 9.0
model initialize at round 183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([4.86254956, 2.72041307, 1.43797224]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 11.609033525638429}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5317298948669553
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.07789145, 10.89767085,  1.47337663]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.12860144522374298}
episode index:184
target Thresh 18.616610782504544
target distance 4.0
model initialize at round 184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([5.72865812, 9.89728427, 4.85808754]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.979928554331284}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5339447611076799
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.12288555, 7.99852844, 4.29171621]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.0060615821690473}
episode index:185
target Thresh 18.62420239765084
target distance 2.0
model initialize at round 185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.45579983, 8.19888256, 0.49893111]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.853491325610451}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5362385850264558
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.60300101, 10.82073027,  2.2157452 ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6290849293600246}
episode index:186
target Thresh 18.631643688745495
target distance 11.0
model initialize at round 186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.00054415, 7.00157252, 2.23965496]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 11.401643150923892}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5333709990102715
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.77552367, 4.25963225, 5.46090614]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.227772244533817}
episode index:187
target Thresh 18.638937632404165
target distance 12.0
model initialize at round 187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.13132342,  4.09321686,  3.45493174]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.18487748037978}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5305339192283021
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.19342372,  3.60031397,  0.39299869]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.209509864872429}
episode index:188
target Thresh 18.646087146301564
target distance 12.0
model initialize at round 188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([13.54806208,  5.85098398,  3.61345482]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 10.764751904103305}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5309929468564184
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.15620168, 8.79151293, 5.3656745 ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8067785850962748}
episode index:189
target Thresh 18.653095090338585
target distance 3.0
model initialize at round 189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([17.20994224,  3.16933076,  1.77033084]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.1943814784688005}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5332540156098057
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.79411058,  5.53654755,  3.48714315]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.5746944621126603}
episode index:190
target Thresh 18.659964267786282
target distance 8.0
model initialize at round 190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.79366115, 3.67024601, 2.69571155]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 6.37931686120578}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5351028682595771
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.97488382, 9.47619572, 1.56296961]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 1.106692993352135}
episode index:191
target Thresh 18.66669742640722
target distance 8.0
model initialize at round 191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([0.67690991, 3.04005928, 3.47739518]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 7.084584828610483}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.53693246202758
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.02029474, 9.44147302, 2.34465133]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.5588955793648129}
episode index:192
target Thresh 18.673297259554634
target distance 2.0
model initialize at round 192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.00149333,  5.9964975 ,  6.11741066]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 2.0035030596834376}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5392286668875408
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.53150692,  7.75149017,  3.83422464]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.5303233956182127}
episode index:193
target Thresh 18.67976640724977
target distance 11.0
model initialize at round 193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([3.36071057, 8.64383169, 2.35678712]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 10.897064225684298}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5403787392971261
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.68357977, 10.49192874,  6.09130335]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5985467114038235}
episode index:194
target Thresh 18.68610745723797
target distance 4.0
model initialize at round 194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.71648495,  7.47719426,  6.15416193]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 2.493365636835217}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5425834585827819
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.92591635,  5.83017637,  5.87097638]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8334753702776381}
episode index:195
target Thresh 18.692322946023772
target distance 12.0
model initialize at round 195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([15.06539073,  4.60844521,  5.12295651]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 12.309010324006818}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5434404471985447
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.28251532, 10.2835088 ,  4.00791706]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.40024010755672074}
episode index:196
target Thresh 18.698415359885573
target distance 2.0
model initialize at round 196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([16.00121654,  3.00141236,  1.86174756]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.2377878248781236}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5456569931518515
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.63274973,  1.80533562,  5.86174577]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.6620169497526591}
episode index:197
target Thresh 18.704387135870142
target distance 11.0
model initialize at round 197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([16.00763997, 11.14572676,  2.56612813]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 11.008604544471352}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5470319928712725
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.38202313, 10.96541519,  4.86701546]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.6189438796593074}
episode index:198
target Thresh 18.710240662767504
target distance 1.0
model initialize at round 198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([15.66796644,  3.82291009,  4.63356245]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.223023683067227}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5492082140126228
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.56342291,  5.06339917,  2.35037654]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.5669786832130972}
episode index:199
target Thresh 18.715978282066462
target distance 7.0
model initialize at round 199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([12.99997721, 11.00001938,  3.43874168]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 7.2801347879389}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5509388642138532
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.08741293,  4.47565181,  0.3060001 ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.4836172744406783}
episode index:200
target Thresh 18.72160228889124
target distance 12.0
model initialize at round 200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([15.99999987,  5.9999985 ,  5.62561214]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 12.16552517567542}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5514598610116656
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.97941375, 8.92795537, 1.66101665]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.3492043791490136}
episode index:201
target Thresh 18.727114932919555
target distance 10.0
model initialize at round 201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([12.43233772,  9.59969294,  3.78510106]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.70795341584999}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5518792947990527
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.68186442, 2.97679348, 5.82050509]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.31898085984332514}
episode index:202
target Thresh 18.732518419282524
target distance 12.0
model initialize at round 202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([ 3.53911483, 10.68075178,  1.41843289]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 11.460347107356995}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5525569784989051
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.200377  ,  5.82880029,  0.02020825]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.26355319289266943}
episode index:203
target Thresh 18.737814909446737
target distance 12.0
model initialize at round 203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([2.63787578, 8.44262876, 6.10313082]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 11.375786980836047}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5536612156601808
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.29872949,  9.53051949,  5.837648  ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8793356881065185}
episode index:204
target Thresh 18.74300652207888
target distance 1.0
model initialize at round 204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.96009024, 8.67716664, 2.59658766]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6783416851001424}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.555838478022814
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.96009024, 8.67716664, 2.59658766]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.6783416851001424}
episode index:205
target Thresh 18.74809533389323
target distance 13.0
model initialize at round 205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.53073662,  8.38380406,  5.43180823]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.638354238594534}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5531402329838684
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.22279373,  3.27110082,  2.36987554]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.226067647823236}
episode index:206
target Thresh 18.753083380482362
target distance 1.0
model initialize at round 206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.7606077 ,  5.86338559,  4.88576114]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.27563049066252093}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5552989758196951
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.7606077 ,  5.86338559,  4.88576114]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.27563049066252093}
episode index:207
target Thresh 18.757972657131422
target distance 9.0
model initialize at round 207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([12.37582579,  9.65451476,  5.25858951]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.138377084840592}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5564447224919257
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.8535854 , 6.98782696, 0.99310669]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.3055305953078076}
episode index:208
target Thresh 18.76276511961626
target distance 5.0
model initialize at round 208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.56352678,  3.62535736,  2.83514673]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 3.4027520950118926}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5582869972618255
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.2611384 ,  6.73143356,  2.26877611]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.37459470473359524}
episode index:209
target Thresh 18.767462684985773
target distance 2.0
model initialize at round 209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.00305959,  5.99260424,  6.10664511]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.0073980924820436}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5602956306081978
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.55204284,  7.75360486,  3.82345956]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.5112496324117345}
episode index:210
target Thresh 18.77206723232874
target distance 8.0
model initialize at round 210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 5.0000001 , 11.00000002,  1.1538493 ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 8.246211289950102}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5617163070254162
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([2.46420671, 2.91795703, 6.02110628]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.5420382775468054}
episode index:211
target Thresh 18.776580603525492
target distance 12.0
model initialize at round 211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.82363437,  3.67365358,  2.67778647]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.897794947385735}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5590667018035982
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.19774883,  5.35215722,  5.8990404 ]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.2032849494759}
episode index:212
target Thresh 18.78100460398469
target distance 13.0
model initialize at round 212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([3.       , 2.       , 3.9788214]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 13.928388277185338}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5564419755040507
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.87627709, 7.74211354, 0.9168899 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.148450287679315}
episode index:213
target Thresh 18.785341003365506
target distance 3.0
model initialize at round 213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 9.02191128, 11.09445921,  2.34486201]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.023387223186757}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5583305457587047
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.91795169, 11.19753633,  4.06167647]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.2138984045006022}
episode index:214
target Thresh 18.78959153628551
target distance 7.0
model initialize at round 214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.00426254,  4.00003265,  1.00965947]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.071639566781815}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5599400877552168
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.77214162, 10.30535819,  2.16010342]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0386192386194604}
episode index:215
target Thresh 18.793757903014548
target distance 4.0
model initialize at round 215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([4.09018605, 8.68052409, 2.51918232]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.004556175443463}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5617064769295028
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.0336503 , 10.93298402,  1.95281147]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.07498989402866177}
episode index:216
target Thresh 18.797841770154854
target distance 12.0
model initialize at round 216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        ,  2.        ,  6.01692867]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 12.041594578896074}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5591179678192286
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.08996469,  4.45920881,  2.95499455]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.194933933095227}
episode index:217
target Thresh 18.801844771307746
target distance 13.0
model initialize at round 217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([2.73836777, 4.43605408, 5.54863405]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.483517677173344}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5565532064989569
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.66488246, 2.16176044, 2.48664414]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.336271701342817}
episode index:218
target Thresh 18.805768507727056
target distance 11.0
model initialize at round 218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([15.00000019, 10.00000009,  1.42139357]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 11.000000191902403}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5578609233340481
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([ 4.95819052, 10.44015394,  4.00546668]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.0544498894783898}
episode index:219
target Thresh 18.80961454895967
target distance 9.0
model initialize at round 219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([10.99469784, 10.99730084,  4.61446846]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 10.295847322905374}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5591567518342749
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.41458626,  2.83679868,  0.9153559 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.933870334312404}
episode index:220
target Thresh 18.81338443347336
target distance 4.0
model initialize at round 220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 3.00000124, 11.00000373,  2.25282434]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 4.000003731825482}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5609297531829887
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.08970195, 7.82670016, 5.96963879]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8315525227781978}
episode index:221
target Thresh 18.8170796692722
target distance 1.0
model initialize at round 221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 5.87861261, 11.00338351,  4.11572623]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.1213924954983898}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5628625020425247
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.25775309, 10.01330428,  6.11572599]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.2347060003535224}
episode index:222
target Thresh 18.820701734499778
target distance 2.0
model initialize at round 222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.18405068, 3.44760282, 3.64185011]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 2.3890602664140768}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5647335222127376
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.76090111, 4.03436121, 1.35866421]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.9947997534930704}
episode index:223
target Thresh 18.82425207803048
target distance 2.0
model initialize at round 223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.37179228, 11.42421844,  3.88871586]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.564084021984533}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5666766761314308
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.37179228, 11.42421844,  3.88871586]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.564084021984533}
episode index:224
target Thresh 18.827732120049056
target distance 2.0
model initialize at round 224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.33541004, 5.34463145, 3.66520548]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.3475742309969116}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5683847355704021
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.15472869, 6.68712616, 1.09883391]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.34904299020845664}
episode index:225
target Thresh 18.831143252618716
target distance 6.0
model initialize at round 225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([2.99999757, 6.99999657, 5.09848499]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.211106473053698}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5694888161103439
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.17210918, 10.73448489,  1.11618701]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.31641719914857375}
episode index:226
target Thresh 18.83448684023797
target distance 12.0
model initialize at round 226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([15.        , 10.        ,  0.49280995]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 12.649110640680744}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.570304809375072
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.81541041, 5.67270844, 6.22732594]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8786432169626942}
episode index:227
target Thresh 18.837764220386454
target distance 12.0
model initialize at round 227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.1457449 ,  3.99142136,  4.20867014]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.219389892960182}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5703781952196441
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.14083224, 9.10720623, 3.39451849]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.1769940535116869}
episode index:228
target Thresh 18.840976704059912
target distance 4.0
model initialize at round 228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([16.00000177,  9.00001101,  2.41390094]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 4.123116739525432}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5720402557204316
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.56509282,  5.74440234,  6.13071492]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.8621363596634798}
episode index:229
target Thresh 18.84412557629466
target distance 2.0
model initialize at round 229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.69889738,  6.90473354,  0.86652523]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 2.208755093975269}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5738574719999081
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.21166562,  8.51553265,  2.86652523]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5286879514862298}
episode index:230
target Thresh 18.84721209668157
target distance 14.0
model initialize at round 230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([2.71345121, 5.52423253, 2.13501543]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 13.515243364864169}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.571373240519389
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.88613793, 8.74146799, 5.35626995]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.138568350630544}
episode index:231
target Thresh 18.85023749986995
target distance 11.0
model initialize at round 231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([ 4.58838651, 11.55617061,  1.3388068 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 9.752562572003622}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5724006311560349
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.71239422,  9.28070321,  5.63969401]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.4018847789002809}
episode index:232
target Thresh 18.853202996061427
target distance 3.0
model initialize at round 232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([16.0000276 ,  9.00006573,  2.17526962]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 3.162348745349098}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5740254784467816
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.57152109,  6.02724967,  5.89208372]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.42934452744590523}
episode index:233
target Thresh 18.856109771494005
target distance 10.0
model initialize at round 233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([14.        ,  8.        ,  0.24433344]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.180339887858361}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5715723781115389
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.0948437 ,  4.41372443,  3.46558534]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.204064250481592}
episode index:234
target Thresh 18.858958988916616
target distance 14.0
model initialize at round 234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([16.        ,  9.        ,  0.89973992]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 14.560219778561013}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.569140155225958
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([13.13798354,  5.12724586,  4.12099432]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 11.138710373238396}
episode index:235
target Thresh 18.861751788054224
target distance 13.0
model initialize at round 235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([1.99999799, 7.99999541, 5.30159545]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 13.000002009527385}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.5684093825615345
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.41884957,  7.43679686,  0.80603468]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.7018780089798572}
episode index:236
target Thresh 18.86448928606372
target distance 2.0
model initialize at round 236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.90554288, 9.19867882, 3.31277633]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 2.3778554300920427}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5701882459262537
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.24769427, 7.64968972, 5.31277633]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.695305102539448}
episode index:237
target Thresh 18.867172577980806
target distance 12.0
model initialize at round 237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([3.        , 6.00000002, 2.68577099]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 12.041594582699842}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5677924969937905
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.86786053, 4.89353829, 5.90702384]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.132698764885575}
episode index:238
target Thresh 18.86980273715803
target distance 9.0
model initialize at round 238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 5.00000001, 11.00000005,  2.47285181]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 9.486833032578875}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5690517158882474
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.99355051, 2.86387093, 5.34011046]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 1.3165924196093486}
episode index:239
target Thresh 18.87238081569413
target distance 6.0
model initialize at round 239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.32131267, 4.91158085, 4.19621563]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.096891800717158}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5704869889365616
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.01291169, 10.21154007,  3.34665935]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.7885656419106533}
episode index:240
target Thresh 18.874907844854896
target distance 12.0
model initialize at round 240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([15.        , 10.        ,  0.83771771]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 13.000000000002935}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5710681766474982
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([2.8730392 , 4.98383901, 6.00586344]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.1279852391877008}
episode index:241
target Thresh 18.877384835485685
target distance 2.0
model initialize at round 241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([13.95189292,  2.07464966,  3.14525104]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 2.0494670532346304}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5727583907935829
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.61213938,  1.83035906,  0.86206561]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.42333663518798964}
episode index:242
target Thresh 18.87981277841578
target distance 11.0
model initialize at round 242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([3.        , 4.        , 3.64819121]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 13.038404810413057}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5731821383584665
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.40281049, 10.97379903,  6.24996669]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5977640003739868}
episode index:243
target Thresh 18.882192644854722
target distance 8.0
model initialize at round 243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 9.        , 11.        ,  2.39696792]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.000000000486972}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.573957386620715
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.84566976,  3.729946  ,  6.13148474]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.746082428122762}
episode index:244
target Thresh 18.88452538678082
target distance 3.0
model initialize at round 244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.00016718, 3.99979276, 0.11000079]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.0002072446468655}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5755355034508346
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.23362749, 6.7493432 , 1.82681512]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.34265235470317323}
episode index:245
target Thresh 18.88681193732195
target distance 13.0
model initialize at round 245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([16.        ,  6.        ,  6.02197933]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 13.341664064136726}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5757561868969825
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.58794469, 9.8551457 , 3.77419877]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.0377635173061703}
episode index:246
target Thresh 18.889053211128815
target distance 12.0
model initialize at round 246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([14.        ,  8.        ,  0.96190327]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.000000000002427}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5734251901888976
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.14362616,  2.0507394 ,  4.18315291]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.183984072060294}
episode index:247
target Thresh 18.89125010474082
target distance 2.0
model initialize at round 247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.00202093,  5.99709713,  0.03936261]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.0029038896765723}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5751049273252327
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.85742727,  7.44561547,  2.03936261]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0210405141709433}
episode index:248
target Thresh 18.8934034969447
target distance 4.0
model initialize at round 248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([15.79152137,  4.66997481,  2.69699317]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 2.9391437873359325}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.576576313759272
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.02231163,  6.56853207,  2.13062232]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.4320444220945247}
episode index:249
target Thresh 18.895514249126055
target distance 2.0
model initialize at round 249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([2.99973246, 1.99937239, 5.31143105]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 2.0006276276271584}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5781904085042349
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.64280603, 3.54704677, 3.02824562]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7863626499151053}
episode index:250
target Thresh 18.8975832056139
target distance 4.0
model initialize at round 250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([15.00005155,  6.00001261,  1.24198693]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 4.123130365382562}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5795631347429746
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.09186532,  1.95381017,  4.67561512]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.1028238147718414}
episode index:251
target Thresh 18.899611194018416
target distance 5.0
model initialize at round 251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 4.48913533, 11.6102332 ,  2.27788737]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 4.551953930332444}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.58099931337257
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.01702104, 11.32500683,  1.7115164 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.0353149666184611}
episode index:252
target Thresh 18.901599025562
target distance 5.0
model initialize at round 252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([15.00003932, 11.00000548,  1.14052993]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 5.099032599987972}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5823500856297058
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.05972587,  6.90863052,  4.5741586 ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.910591346832898}
episode index:253
target Thresh 18.90354749540378
target distance 6.0
model initialize at round 253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.37607509, 7.56113143, 4.40753913]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 5.5960222556445505}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5836179281075763
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.90691999, 2.10931454, 5.55798237]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.14357421940275378}
episode index:254
target Thresh 18.90545738295767
target distance 10.0
model initialize at round 254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([15.00000039,  9.00000015,  1.36849755]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 10.198039380664204}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5846348899321894
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.34469223, 10.65858922,  3.95256922]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.4851536364318123}
episode index:255
target Thresh 18.907329452204163
target distance 1.0
model initialize at round 255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.59705957, 3.42888788, 6.07755649]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6989492418915163}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5862574098933918
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.59705957, 3.42888788, 6.07755649]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.6989492418915163}
episode index:256
target Thresh 18.90916445199591
target distance 8.0
model initialize at round 256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.44095246, 10.366227  ,  4.52970099]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.472058157776826}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5874252210288888
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.36748762, 11.34117348,  3.39695977]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.5014444108138103}
episode index:257
target Thresh 18.9109631163573
target distance 9.0
model initialize at round 257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([2.90154197, 3.421098  , 2.0074673 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.187807868559231}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5884486030058889
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.57795283, 11.37422336,  2.59153934]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.5640628823443354}
episode index:258
target Thresh 18.912726164778057
target distance 12.0
model initialize at round 258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.88226019, 6.28816549, 0.53503388]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.916060660851294}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5861766006776807
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([4.80173941, 2.58160434, 3.75628566]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 11.213353820102407}
episode index:259
target Thresh 18.91445430250106
target distance 11.0
model initialize at round 259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([14.00000033,  8.99999864,  5.9488219 ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 11.045361218102645}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5870052698656092
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.95927046, 7.96997304, 5.96652308]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9597402962912891}
episode index:260
target Thresh 18.916148220804434
target distance 2.0
model initialize at round 260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.0000408 , 3.99952981, 5.80094242]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.2364702975247583}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5885113799427524
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.10135466, 5.81426894, 3.51775652]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9176379838248929}
episode index:261
target Thresh 18.917808597278096
target distance 12.0
model initialize at round 261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([13.14653072,  2.9418366 ,  4.23453784]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 13.75424096152425}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5885057517060911
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.13233041, 10.75764955,  3.42038428]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.2761251134385906}
episode index:262
target Thresh 18.91943609609477
target distance 9.0
model initialize at round 262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([ 8.44509238, 10.09628215,  5.32978272]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 10.348381535478687}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5893161123062164
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.69749709, 2.40316712, 5.347482  ]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8056338595952321}
episode index:263
target Thresh 18.92103136827568
target distance 5.0
model initialize at round 263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([1.9999989 , 1.99999666, 5.39604521]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 5.099023005744494}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5905790993597078
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.92002264, 6.75082568, 2.54648833]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.2616949011562848}
episode index:264
target Thresh 18.92259505195097
target distance 5.0
model initialize at round 264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.00012237, 4.99994655, 0.59018725]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 5.099095922261526}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5918677267127164
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.11654782, 9.21723422, 2.02381616]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7913947610272734}
episode index:265
target Thresh 18.924127772614966
target distance 5.0
model initialize at round 265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([ 2.00000014, 10.0000005 ,  2.3048102 ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 5.09901997330089}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5931116250875856
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.36318714, 5.35742726, 5.73843934]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.5095676021410752}
episode index:266
target Thresh 18.92563014337636
target distance 5.0
model initialize at round 266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.81590325,  7.99706867,  4.15951395]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 5.000458670832553}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5943811146861602
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.96382158,  3.49689342,  5.59314036]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.4982087417686234}
episode index:267
target Thresh 18.927102765203493
target distance 6.0
model initialize at round 267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.00441504, 4.99688078, 0.38692563]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 7.809821347050289}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5954376068813572
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.71634366, 10.96803193,  1.25418405]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.2854520527098456}
episode index:268
target Thresh 18.928546227164734
target distance 8.0
model initialize at round 268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([3.6482636 , 4.66011393, 0.79864186]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.974336227298554}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5962953430038637
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.10012083, 10.7247982 ,  1.09952954]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9410199501066947}
episode index:269
target Thresh 18.92996110666411
target distance 14.0
model initialize at round 269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 2.4491175 , 12.61917877,  2.30221939]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 13.801685180883007}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5969967607735057
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.12562754, 10.08517123,  0.03673562]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8785108661573323}
episode index:270
target Thresh 18.931347969672288
target distance 7.0
model initialize at round 270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([1.82886221, 4.08039222, 3.70443416]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 7.611641529565695}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.598064613581412
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.69508726, 10.27453771,  2.57169245]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.7869353910350785}
episode index:271
target Thresh 18.93270737095296
target distance 3.0
model initialize at round 271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([13.12920721,  7.62950503,  4.38481176]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 3.554623081280018}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5993271706983958
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.11760309, 11.20341765,  3.81844103]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.23496644004387843}
episode index:272
target Thresh 18.93403985428477
target distance 10.0
model initialize at round 272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([2.31147217, 7.65386896, 2.38664725]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 10.250081193685013}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6000978692241206
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.8658778 , 11.10019346,  0.40434879]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.16741413999933488}
episode index:273
target Thresh 18.935345952678812
target distance 1.0
model initialize at round 273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.45150546, 9.58977688, 3.32194281]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 1.6526486944281753}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6015208697014048
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([1.80810652, 8.03536315, 5.32194257]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.1951247272346992}
episode index:274
target Thresh 18.93662618859186
target distance 8.0
model initialize at round 274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([15.00000132,  7.00000039,  1.28871554]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.9442729183211}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6023987690602504
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.98510853, 11.26507649,  3.87278864]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.2654944494120018}
episode index:275
target Thresh 18.937881074135348
target distance 11.0
model initialize at round 275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([3.        , 2.        , 3.62956178]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.045361017201547}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6002161648245248
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.86415748, 3.01291315, 0.56762766]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.135851647507742}
episode index:276
target Thresh 18.939111111280226
target distance 5.0
model initialize at round 276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 6.67577333, 11.15517904,  1.09433812]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 3.327846678022907}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6014481647688442
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.46806648, 11.3972923 ,  0.52796726]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.6639235220118666}
episode index:277
target Thresh 18.940316792057754
target distance 3.0
model initialize at round 277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.00001507, 2.99994225, 5.96968079]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 3.000057746314626}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6027400634926973
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.83864274, 5.4281011 , 1.40330981]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.0150812740416304}
episode index:278
target Thresh 18.94149859875632
target distance 3.0
model initialize at round 278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.3601274 ,  5.9387382 ,  0.83350151]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 3.0823717397745005}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6040227012937986
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([14.45089354,  8.22316516,  2.55031621]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.8982078558187864}
episode index:279
target Thresh 18.94265700411436
target distance 6.0
model initialize at round 279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([ 6.99963029, 10.99797896,  5.53345919]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 6.000370049373904}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6050954133427808
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.64902452, 11.18511226,  0.4007176 ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.3968001183519585}
episode index:280
target Thresh 18.94379247150946
target distance 3.0
model initialize at round 280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([15.60256254,  5.49036601,  0.69410151]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 2.961440457562447}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6063263551098884
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.94403634,  3.24906332,  4.41091549]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.25527332190297286}
episode index:281
target Thresh 18.94490545514372
target distance 3.0
model initialize at round 281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.62420902,  6.28904783,  1.1781184 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 3.3477563434174953}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6075485667935413
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.18054561,  3.5285774 ,  4.8949325 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.5585613551947333}
episode index:282
target Thresh 18.945996400225436
target distance 8.0
model initialize at round 282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.00002857,  3.00000945,  1.32161349]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 9.433988255438706}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6083803499263696
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.81603418, 10.2691007 ,  3.90568683]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 1.0955024262598614}
episode index:283
target Thresh 18.94706574314718
target distance 3.0
model initialize at round 283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([3.99940923, 9.00073764, 3.24808192]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 3.6058374410727465}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6096205459125442
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.1533916 , 6.27530042, 4.96489638]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.31514965106045534}
episode index:284
target Thresh 18.94811391166039
target distance 10.0
model initialize at round 284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([4.17850443, 5.67344376, 2.46652922]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 9.910677411468022}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.6093821758425594
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.89442719,  7.6017483 ,  5.08600707]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.6109391422866814}
episode index:285
target Thresh 18.949141325046437
target distance 5.0
model initialize at round 285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.9999994 , 9.0000004 , 3.55509079]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 5.385165212605847}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6103820222705879
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.77995853, 11.06401844,  0.42234825]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.22916502415049447}
episode index:286
target Thresh 18.950148394284376
target distance 2.0
model initialize at round 286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.00026978, 8.9994893 , 6.20038152]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 2.236645404888361}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6116702382208645
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.37786388, 10.70033932,  3.91719597]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.4822630340073439}
episode index:287
target Thresh 18.951135522215335
target distance 14.0
model initialize at round 287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([16.        , 11.        ,  1.65174502]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 14.866068748963954}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6120385031318972
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([2.13555389, 6.04038361, 4.81989111]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.14144148850606383}
episode index:288
target Thresh 18.952103103703642
target distance 11.0
model initialize at round 288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([14.98593507,  6.0366753 ,  2.93899691]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 11.173129178547336}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6099207228442436
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.21950798,  2.1598352 ,  6.1602413 ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.40135808358318}
episode index:289
target Thresh 18.953051525794802
target distance 5.0
model initialize at round 289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([10.69753021, 10.13350746,  6.13600802]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 4.304540689981309}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6110315663789427
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.89060411,  9.7592055 ,  1.28645008]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.2644795874580068}
episode index:290
target Thresh 18.95398116787029
target distance 2.0
model initialize at round 290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.67428636, 1.83120541, 0.90152353]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.3668528156738775}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6123682276628638
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.67428636, 1.83120541, 0.90152353]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.3668528156738775}
episode index:291
target Thresh 18.95489240179933
target distance 10.0
model initialize at round 291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([2.        , 9.        , 3.91373396]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 10.19803902723914}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6130721273544198
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.00454392, 10.84989417,  6.2146183 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.1501745915361831}
episode index:292
target Thresh 18.95578559208765
target distance 3.0
model initialize at round 292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.29330208, 1.54910038, 4.71151209]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 3.685286751613559}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6142254308443366
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.62291866, 4.27345072, 2.14513814]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.9570274351535265}
episode index:293
target Thresh 18.956661096023268
target distance 2.0
model initialize at round 293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.53667291,  9.83119484,  3.62702203]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.286127259282089}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6154365654333015
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.81104082, 11.18184344,  3.34383565]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.2622453246427876}
episode index:294
target Thresh 18.957519263819442
target distance 11.0
model initialize at round 294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.00000474,  5.00001006,  2.13258794]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.704700926496958}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6153806991317179
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.6356983 , 9.98576195, 3.60162028]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 1.1729616132498526}
episode index:295
target Thresh 18.95836043875472
target distance 13.0
model initialize at round 295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([2.        , 9.        , 3.56774485]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 13.601470508736185}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.615516775217672
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.66009967,  5.05361393,  5.88633121]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.34410272903397043}
episode index:296
target Thresh 18.959184957310303
target distance 10.0
model initialize at round 296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.00000111,  5.00000221,  2.10674893]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.0498765060776}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.613444328163067
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([13.34851273,  5.03471079,  5.32800047]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 9.398216511219465}
episode index:297
target Thresh 18.959993149304598
target distance 12.0
model initialize at round 297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([13.50821563,  1.22097699,  4.62484908]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 13.074241648197932}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.6129027151499913
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.99252243, 9.06780014, 5.54520651]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.9948354793428097}
episode index:298
target Thresh 18.96078533802519
target distance 6.0
model initialize at round 298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([3.86122847, 7.55411563, 6.2515974 ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 4.919770380207568}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6139081149236825
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.61773973, 3.16682929, 5.40204065]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.6398706055598941}
episode index:299
target Thresh 18.961561840358115
target distance 2.0
model initialize at round 299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([1.99972632, 9.00030094, 3.31080163]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 2.2364595423249596}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6151287545406035
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.02398614, 7.50284975, 1.02761536]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5034214986304795}
episode index:300
target Thresh 18.962322966914673
target distance 6.0
model initialize at round 300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.00000005,  8.0000001 ,  2.1073615 ]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 6.082762635425274}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6160596831110956
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.32424765,  1.99360496,  5.25779593]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.3243107091171537}
episode index:301
target Thresh 18.963069022155626
target distance 11.0
model initialize at round 301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([3.99999999, 5.99999999, 4.96884036]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 12.083045984413808}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6164203475244955
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.90607588, 10.65851169,  6.13698335]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.3541694567765034}
episode index:302
target Thresh 18.963800304513025
target distance 2.0
model initialize at round 302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.33400091, 9.54539066, 2.97970289]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8063649528327121}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6176862869716093
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.33400091, 9.54539066, 2.97970289]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8063649528327121}
episode index:303
target Thresh 18.964517106509557
target distance 12.0
model initialize at round 303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([14.9997711 ,  2.99932031,  5.38954055]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 12.649108442590538}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6156544241855185
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.20259004,  4.57861229,  2.32760774]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.485988839309854}
episode index:304
target Thresh 18.965219714875584
target distance 4.0
model initialize at round 304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.96278473,  6.0113515 ,  3.84753394]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 4.125275174037205}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6167853802045824
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.85909303,  2.7288584 ,  5.56434661]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.1266212380249494}
episode index:305
target Thresh 18.965908410663815
target distance 6.0
model initialize at round 305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.67422945, 10.11440506,  1.07022649]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 6.123077308718451}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6177550921891545
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.06260189,  4.81888462,  0.22066902]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8212740249996757}
episode index:306
target Thresh 18.966583469361755
target distance 11.0
model initialize at round 306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.00000048,  4.00000068,  1.96293467]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.045361430589653}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6157428606185057
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([13.28513224,  3.80402087,  5.18418633]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.354434381813347}
episode index:307
target Thresh 18.967245161001877
target distance 6.0
model initialize at round 307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([16.00000002,  3.99999979,  5.8281579 ]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 6.082762742521454}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6166506378705844
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.93570215, 10.06984188,  2.69541512]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.09493208654957452}
episode index:308
target Thresh 18.967893750269663
target distance 13.0
model initialize at round 308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([16.        ,  8.        ,  0.61271923]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6146550047383171
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.09566436,  0.53834962,  3.83395015]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.200924502376765}
episode index:309
target Thresh 18.968529496609467
target distance 14.0
model initialize at round 309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([16.        ,  2.        ,  0.16619414]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 15.231546211727817}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6126722466585162
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([12.76745075,  9.19607465,  3.38744616]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 10.833678521216223}
episode index:310
target Thresh 18.969152654328305
target distance 2.0
model initialize at round 310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.99946308,  7.00109515,  3.0286293 ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.2368076047115886}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.613885519177299
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([13.93380511,  5.69870447,  5.0286293 ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.7018330968614901}
episode index:311
target Thresh 18.96976347269757
target distance 4.0
model initialize at round 311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([3.2923124 , 8.07805339, 1.69725245]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 3.0064254094468197}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6149355019664776
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.75599356, 10.79000429,  1.13087969]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.32192754059433065}
episode index:312
target Thresh 18.970362196052754
target distance 13.0
model initialize at round 312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([2.        , 9.        , 3.29374683]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 13.000000000000094}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6151958141584364
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([15.17575398,  8.9001871 ,  6.17870678]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.20211896001412571}
episode index:313
target Thresh 18.970949063891183
target distance 13.0
model initialize at round 313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([3.09726261, 4.72396489, 0.1414172 ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 12.345345818509067}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6152223414856108
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.0488473 ,  8.33037696,  6.17682099]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.3339685539490834}
episode index:314
target Thresh 18.97152431096782
target distance 3.0
model initialize at round 314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.00005224,  6.00008324,  2.01234087]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 3.162340108115499}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6162882707186724
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.8878877 ,  3.10017459,  5.72915413]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.1503466508764081}
episode index:315
target Thresh 18.97208816738916
target distance 14.0
model initialize at round 315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([15.71290181,  1.35327221,  5.54176319]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 13.811422386344862}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6143379913809551
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.23549935,  1.85702646,  2.47981261]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.293486361849409}
episode index:316
target Thresh 18.972640858705294
target distance 7.0
model initialize at round 316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([14.53918019,  9.38137634,  5.43702865]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 5.4010708408331105}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6152529569444499
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.43170079,  4.14726351,  0.30428658]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.456127299361859}
episode index:317
target Thresh 18.97318260600012
target distance 2.0
model initialize at round 317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([1.66982852e+01, 7.93780019e+00, 1.27599796e-02]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.699001993454401}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6163087339663228
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.10506447,  7.62669575,  3.72957396]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.3878074357495152}
episode index:318
target Thresh 18.97371362597977
target distance 13.0
model initialize at round 318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([3.63198163, 7.41101511, 1.24871939]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.193805642692118}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.614376731665488
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.78570082, 3.65724432, 4.4699707 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 10.235422699960477}
episode index:319
target Thresh 18.974234131059326
target distance 2.0
model initialize at round 319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.00007977, 10.00005607,  1.614721  ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.2361538054938817}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6155196168790333
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.72541863,  8.3881893 ,  5.61472064]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8227533835697868}
episode index:320
target Thresh 18.974744329447756
target distance 13.0
model initialize at round 320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 2.99999955, 10.99999951,  4.97304964]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 15.811388391447075}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6155251325303207
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.63069391,  2.55747623,  0.442079  ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8417568256508644}
episode index:321
target Thresh 18.975244425231217
target distance 4.0
model initialize at round 321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.61506975,  3.27663779,  1.17163914]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 3.7738225890102752}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6165967812181147
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.36602619,  6.2410631 ,  2.88845324]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.9888923121854265}
episode index:322
target Thresh 18.975734618454695
target distance 14.0
model initialize at round 322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([3.21385167, 6.83429782, 0.23683375]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.787221993809002}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6146878128552103
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.79197224, 8.28558495, 3.4580872 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 11.28151651433643}
episode index:323
target Thresh 18.97621510520201
target distance 2.0
model initialize at round 323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([13.95552552,  2.0280319 ,  3.58119321]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 2.263757476331334}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6158156282476325
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.56494523,  2.51719135,  1.29800778]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.6499052582488369}
episode index:324
target Thresh 18.97668607767427
target distance 4.0
model initialize at round 324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([12.99997722, 10.99999762,  4.24785793]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 5.000011760517636}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6167887043081228
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.53104073,  7.31099953,  5.68148564]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.5627108568749529}
episode index:325
target Thresh 18.97714772426674
target distance 7.0
model initialize at round 325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.0002794 , 2.99998867, 0.96147638]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 7.000011334034381}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6176708925618059
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.19815739, 9.38493301, 2.11191963]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.6461994679156265}
episode index:326
target Thresh 18.97760022964422
target distance 10.0
model initialize at round 326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([2.52477326, 9.47894877, 6.04660809]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 9.59653680791232}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6183858371444758
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.38018636, 11.42451937,  0.34749506]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.751256039593063}
episode index:327
target Thresh 18.978043774814886
target distance 3.0
model initialize at round 327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([4.00000391, 6.99998992, 6.08469665]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 3.162288457321849}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6193998743784865
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.19792047, 10.49118582,  3.51832556]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.5295621068663512}
episode index:328
target Thresh 18.97847853720273
target distance 13.0
model initialize at round 328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([3.        , 4.        , 3.77100289]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 13.038404810405298}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6175171999882784
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.90312683, 4.57882693, 0.70906102]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.208625632223754}
episode index:329
target Thresh 18.978904690718494
target distance 14.0
model initialize at round 329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([1.99999999, 4.99999988, 5.62239099]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 14.317821099687439}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.6172069573138406
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.54114878,  8.5315923 ,  5.67549548]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.702235581490978}
episode index:330
target Thresh 18.979322405829272
target distance 12.0
model initialize at round 330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.       ,  9.       ,  0.2097432]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.416407865001869}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6173033188578121
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.21498837, 3.42016674, 4.5283317 ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.47197466778601216}
episode index:331
target Thresh 18.979731849626678
target distance 6.0
model initialize at round 331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([14.74978898,  7.87342739,  4.87701845]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 5.031234647949146}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6182513972585626
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.64731459,  3.77638271,  0.02746146]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.8527350747167329}
episode index:332
target Thresh 18.98013318589369
target distance 9.0
model initialize at round 332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([ 6.       , 11.       ,  2.3784045]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 12.727922061357857}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6184861174411181
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.38493789,  2.36390853,  5.26336409]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.5297231334078003}
episode index:333
target Thresh 18.98052657517016
target distance 7.0
model initialize at round 333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.04268204, 8.31759824, 5.73975325]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 5.403083257311318}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6193420933619795
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.90446123, 3.57759679, 0.60701106]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.0731580364316946}
episode index:334
target Thresh 18.980912174817053
target distance 11.0
model initialize at round 334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.00000016,  6.00000019,  1.86958855]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.000000162222426}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6174933109937348
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.25489236,  5.51924924,  5.09084307]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.266155003348572}
episode index:335
target Thresh 18.981290139079363
target distance 10.0
model initialize at round 335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([12.        , 11.        ,  0.10185116]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 10.770329615087908}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.61806543765215
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.95193431, 7.3387908 , 4.40273777]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.34218345833384367}
episode index:336
target Thresh 18.98166061914784
target distance 3.0
model initialize at round 336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.99993639, 7.00008701, 3.20404923]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 3.162340090435583}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6190818488460605
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.42753813, 4.24728731, 4.92086368]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.4939026873960361}
episode index:337
target Thresh 18.982023763219445
target distance 12.0
model initialize at round 337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.69893362,  5.06748477,  3.45649457]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 11.093548856626793}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.6187142654459249
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.04105035, 8.012206  , 5.22641602]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.042826602028246064}
episode index:338
target Thresh 18.98237971655665
target distance 6.0
model initialize at round 338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([2.99999713, 4.99999849, 4.62913775]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 6.082763551022205}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6195302654129243
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.18360913, 10.52033389,  1.4963951 ]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.5136067435409368}
episode index:339
target Thresh 18.98272862154554
target distance 10.0
model initialize at round 339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([12.71057265,  9.27594764,  4.16221261]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 8.714942512340082}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6202124051355183
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.79510389, 9.18924539, 4.74628584]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.8173151229501034}
episode index:340
target Thresh 18.98307061775276
target distance 10.0
model initialize at round 340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([5.13938784, 9.05793068, 0.33796614]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.733544245313144}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6183936004283761
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.80076372, 4.36196428, 3.5592147 ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.299510466225724}
episode index:341
target Thresh 18.98340584198135
target distance 2.0
model initialize at round 341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.35959017, 4.62423803, 4.3687737 ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.6635667352384447}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6194225635850183
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.7369277 , 3.44902568, 4.0855878 ]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.5204143499469538}
episode index:342
target Thresh 18.983734428325477
target distance 8.0
model initialize at round 342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([16.        , 11.        ,  1.77469366]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 8.24621125127794}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6201494535243303
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.87839264,  3.62757498,  4.6419516 ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.6392485456179589}
episode index:343
target Thresh 18.984056508224057
target distance 9.0
model initialize at round 343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 9.09563035, 10.19191124,  0.13352173]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.21098031704097}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6206306415687572
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.60839251, 2.62940116, 4.15122375]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7412841913354196}
episode index:344
target Thresh 18.984372210513346
target distance 3.0
model initialize at round 344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([16.42594089, 10.10645031,  0.44222313]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.5437060194150396}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.621644173042471
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.93716999,  8.35505348,  0.15903758]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.0021729253685787}
episode index:345
target Thresh 18.98468166147846
target distance 9.0
model initialize at round 345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.00614556, 2.00003296, 1.00736385]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 9.054675769691494}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6223083741929115
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.80987181, 10.92592767,  1.59143708]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.2040476437882706}
episode index:346
target Thresh 18.984984984903924
target distance 14.0
model initialize at round 346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([ 2.        , 11.        ,  2.82061183]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 14.317821063276353}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6227119313691485
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.57139338,  8.86101716,  0.27194335]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9617973749085903}
episode index:347
target Thresh 18.985282302123146
target distance 11.0
model initialize at round 347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([13.41327624,  8.57735623,  2.92890489]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 10.691376188287702}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6232260654443494
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.88569465, 11.59627651,  2.94660762]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.0677081477541956}
episode index:348
target Thresh 18.985573732066978
target distance 13.0
model initialize at round 348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([2.70003997, 3.47193752, 6.14397705]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 12.557074362082911}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.621440317405827
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.80200436, 7.6490074 , 3.08204698]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 10.330456935264172}
episode index:349
target Thresh 18.985859391311283
target distance 10.0
model initialize at round 349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([4.        , 5.        , 3.49830973]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 11.18033988749911}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6216545256933804
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.97336321, 10.20867448,  0.10008557]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.21036767252593858}
episode index:350
target Thresh 18.98613939412357
target distance 11.0
model initialize at round 350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([4.87206953, 3.77998545, 0.90000933]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 10.627486497205338}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.6211843126507893
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.54693033,  7.08552733,  6.10356172]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.4610716287966873}
episode index:351
target Thresh 18.986413852508697
target distance 2.0
model initialize at round 351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([2.99905238, 5.00030379, 3.83336508]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 2.236763592950013}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6222320844898497
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.19880448, 3.32930695, 5.83336508]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.8662316810800219}
episode index:352
target Thresh 18.986682876253674
target distance 5.0
model initialize at round 352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.        ,  6.        ,  6.05785608]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.830951899543589}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6230057563588832
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.55620886, 11.5333784 ,  2.92511462]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.7706236523362949}
episode index:353
target Thresh 18.98694657297159
target distance 6.0
model initialize at round 353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([3.00000774, 3.99999876, 0.84335535]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 6.08276502304136}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6238006047166514
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.13025912, 9.61280121, 1.99379907]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.4085221476712879}
episode index:354
target Thresh 18.987205048144645
target distance 2.0
model initialize at round 354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.75422838,  4.12826114,  3.4076519 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 2.2579539277465774}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6248321523089989
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.24606658,  2.52387838,  5.4076519 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.578789528590318}
episode index:355
target Thresh 18.987458405166358
target distance 4.0
model initialize at round 355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([ 6.63305607, 10.10112884,  4.78490424]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 3.3686386924473113}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6257216129749876
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([3.34476808, 8.98204386, 4.21853327]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1805672422648474}
episode index:356
target Thresh 18.987706745382912
target distance 13.0
model initialize at round 356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([ 1.99999995, 10.99999996,  4.8426156 ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 14.764823082009444}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6239688913700157
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.81552845, 5.61474096, 1.78068494]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.311685080001423}
episode index:357
target Thresh 18.987950168133708
target distance 8.0
model initialize at round 357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([ 9.        , 11.        ,  1.35588139]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 10.63014581273465}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6244877711936224
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.63251927, 3.60053718, 5.65676812]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8721958125401407}
episode index:358
target Thresh 18.98818877079109
target distance 14.0
model initialize at round 358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([15.99999878,  2.99999615,  5.4078939 ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 14.866068894852221}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6227482509396012
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.22983561,  6.20409837,  2.34596311]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.372531399272065}
episode index:359
target Thresh 18.988422648799308
target distance 4.0
model initialize at round 359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.99997996,  7.00000861,  3.73798311]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 4.00000860766665}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6236867169369912
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([13.62384532,  3.6990155 ,  5.45479744]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.7937978444426816}
episode index:360
target Thresh 18.988651895712675
target distance 3.0
model initialize at round 360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.49919266, 6.94709816, 4.17882693]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 2.989347011256512}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6246199836767778
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([2.28874817, 3.8597808 , 5.89563876]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 0.3209936567372429}
episode index:361
target Thresh 18.988876603233017
target distance 9.0
model initialize at round 361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([15.        ,  2.        ,  5.38787484]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6249793740180123
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.93340066, 10.74403215,  4.83920577]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9678617299060124}
episode index:362
target Thresh 18.989096861246338
target distance 5.0
model initialize at round 362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([1.99999999, 2.99999999, 5.14452624]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 5.099019526221289}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6257996641568826
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.00903207, 7.89092129, 2.29496722]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.9969531578690979}
episode index:363
target Thresh 18.98931275785878
target distance 5.0
model initialize at round 363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([11.        , 11.        ,  2.18894403]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 5.830951896516295}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6264670986310917
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.00482537,  5.99305203,  5.05620113]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.00845922726946481}
episode index:364
target Thresh 18.98952437943187
target distance 3.0
model initialize at round 364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.79272481, 8.18271408, 1.78032928]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 1.982660015959788}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6274091038403217
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.69851854, 9.57819713, 1.49714338]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.8159937601192717}
episode index:365
target Thresh 18.98973181061705
target distance 14.0
model initialize at round 365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([16.00000006,  9.00000019,  2.29433459]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 14.142135651307656}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6277988143374933
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.10748373, 11.41323818,  4.02884999]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.4269877603380924}
episode index:366
target Thresh 18.989935134389572
target distance 12.0
model initialize at round 366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([3.35971476, 7.99170219, 1.63215082]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.75298944319846}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.626088190865184
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.76580929, 3.8548708 , 4.85340141]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.273676843377265}
episode index:367
target Thresh 18.99013443208165
target distance 11.0
model initialize at round 367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.60571832, 9.49604947, 0.69788807]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 9.513905377746127}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6266318876939079
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.02099356, 11.88398789,  0.99877289]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.3190482151315697}
episode index:368
target Thresh 18.990329783415014
target distance 3.0
model initialize at round 368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([0.49812135, 8.75705952, 3.67669022]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 2.6993372288592004}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6275109071036805
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.48774527, 10.70661769,  1.11031448]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.5691824227762925}
episode index:369
target Thresh 18.99052126653281
target distance 7.0
model initialize at round 369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([14.        ,  3.        ,  5.02686381]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 7.280109889336208}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6282347647986941
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.60743269,  9.19344326,  1.89412103]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.8970188807088647}
episode index:370
target Thresh 18.99070895803083
target distance 10.0
model initialize at round 370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([12.82842553,  9.09859256,  3.51696599]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 8.896516334509403}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6286169976315956
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.64872286, 8.23886027, 5.25148305]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.6912999158255206}
episode index:371
target Thresh 18.99089293298818
target distance 11.0
model initialize at round 371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([2.99999998, 2.99999998, 4.85321951]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.045361033342797}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.626927166992801
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.8021855 , 3.5046866 , 1.79127204]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 9.320079044078076}
episode index:372
target Thresh 18.9910732649973
target distance 9.0
model initialize at round 372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([2.        , 3.99999997, 5.75138378]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 11.401754267247059}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6272495394521543
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.41479093, 10.72375997,  0.91952523]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.6471307529676582}
episode index:373
target Thresh 18.99125002619339
target distance 3.0
model initialize at round 373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([2.58867893, 4.91661402, 3.56758845]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 2.5164109945106268}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6281667839990737
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.10555309, 6.6597805 , 3.28440302]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9569663405900802}
episode index:374
target Thresh 18.991423287283293
target distance 6.0
model initialize at round 374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([13.13012004,  5.84979761,  3.602741  ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 6.026793188357515}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6288317286364067
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.4913138 , 11.01294067,  4.46999691]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.49148419015785577}
episode index:375
target Thresh 18.991593117573753
target distance 6.0
model initialize at round 375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([ 6.29054292, 10.05470697,  0.34210652]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 4.803390978550352}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6296382010280838
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([10.25813496, 11.4091533 ,  1.77573531]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8472131763353067}
episode index:376
target Thresh 18.991759584999148
target distance 12.0
model initialize at round 376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 5.14645782, 11.21334085,  1.81572526]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 14.23674919464106}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6298718676900736
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.14362363,  2.57941196,  4.98386695]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.0339722915134208}
episode index:377
target Thresh 18.991922756148668
target distance 2.0
model initialize at round 377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([13.9163473 ,  7.91059524,  4.96221638]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 2.827009499662565}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6307467992834861
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.84763919,  5.79958963,  0.39584374]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.2517501394068554}
episode index:378
target Thresh 18.992082696292947
target distance 2.0
model initialize at round 378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.35078904,  9.72402934,  4.30939126]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.8422138929647969}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6316946969107065
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.29402695,  8.33040754,  0.02620595]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.44229061271086473}
episode index:379
target Thresh 18.99223946941018
target distance 7.0
model initialize at round 379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([10.9999996 , 11.00000218,  2.75625867]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 8.062258477544322}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6323185156366493
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.59113049, 7.45292626, 5.62351636]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.744699570754196}
episode index:380
target Thresh 18.992393138211703
target distance 4.0
model initialize at round 380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 6.30761517, 12.61410718,  2.38447529]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 4.0297701793352925}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6330807890717971
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.98929257, 10.81055451,  5.81810313]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.18974783999219255}
episode index:381
target Thresh 18.99254376416708
target distance 3.0
model initialize at round 381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.99998183,  5.00002218,  3.25928211]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 3.000022178819887}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6339381587600909
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.80147274,  2.20807326,  4.97609061]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.2875892110208108}
episode index:382
target Thresh 18.992691407528714
target distance 12.0
model initialize at round 382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 3.99999986, 10.99999985,  4.9573245 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 15.000000022961672}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6340121636745135
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.34992542,  2.12627785,  5.27590788]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.3720133010054466}
episode index:383
target Thresh 18.992836127355904
target distance 12.0
model initialize at round 383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([15.        ,  8.        ,  0.13951939]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 13.0000000000005}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6323610903316111
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.08793485,  3.47561573,  3.36075734]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.099140548554574}
episode index:384
target Thresh 18.99297798153852
target distance 14.0
model initialize at round 384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 2.        , 10.        ,  4.48902774]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 14.560219778904296}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6326016598007703
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.60871993,  5.89970303,  5.65715297]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.4039301647017478}
episode index:385
target Thresh 18.99311702682012
target distance 4.0
model initialize at round 385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([5.84696506, 9.80162272, 5.64971673]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 4.029298736254118}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6333533256935868
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.89047936, 11.22165473,  2.80014973]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.24723589982747549}
episode index:386
target Thresh 18.993253318820678
target distance 9.0
model initialize at round 386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([ 4.68206822, 10.05425822,  1.03424567]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 7.378790758871348}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6339391268019788
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.01647555, 10.41878846,  5.90150313]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 1.1424216351340089}
episode index:387
target Thresh 18.993386912058803
target distance 14.0
model initialize at round 387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([2.        , 9.        , 3.33477855]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 14.866068747318504}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6323052630731077
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.89212191, 3.67235757, 0.27284466]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.112709171699722}
episode index:388
target Thresh 18.99351785997358
target distance 10.0
model initialize at round 388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([4.85390031, 3.02385972, 1.07523602]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 12.135483234595158}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6325623276144479
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.34761377, 10.79203148,  0.24337937]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.6847325737669893}
episode index:389
target Thresh 18.99364621494591
target distance 2.0
model initialize at round 389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.36770738, 1.13384327, 0.43745678]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 1.9703641541734676}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6334788344667185
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([3.59474219, 2.8011079 , 2.43745666]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.4514332264731602}
episode index:390
target Thresh 18.99377202831951
target distance 4.0
model initialize at round 390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.47912204,  4.74396596,  1.46802717]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.576249389702854}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6343154512839391
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.63951118,  7.25517607,  3.18484174]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.827475000531812}
episode index:391
target Thresh 18.99389535042139
target distance 11.0
model initialize at round 391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([15.00000003,  2.00000003,  1.80421894]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 11.000000027121176}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6326972996225004
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.21289064,  1.29587893,  5.02541947]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.23975868180194}
episode index:392
target Thresh 18.994016230582044
target distance 2.0
model initialize at round 392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.605575  , 4.4953931 , 1.30127817]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 1.6219010511853276}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6336064667990335
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.48529471, 5.7510461 , 3.30127817]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.571751326094035}
episode index:393
target Thresh 18.994134717155145
target distance 6.0
model initialize at round 393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 2.99999816, 11.00000061,  3.82350814]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 6.082763435129022}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6342937145356066
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.67780167, 5.31246883, 4.97395031]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.44883017930603564}
episode index:394
target Thresh 18.994250857536905
target distance 13.0
model initialize at round 394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([3.        , 7.        , 4.94414544]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 13.000000002232145}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.6339790108184267
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.56944467,  7.41896776,  0.71406415]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.6007594143116662}
episode index:395
target Thresh 18.99436469818502
target distance 9.0
model initialize at round 395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([3.00000029, 2.99999964, 0.11581772]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 12.041594598345753}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6341544367885811
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.83393742, 11.72374321,  1.00077589]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7425503454737704}
episode index:396
target Thresh 18.994476284637276
target distance 5.0
model initialize at round 396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([ 7.        , 11.        ,  1.61735934]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 5.8309518952906725}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6348123305353572
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.43178448, 6.70260408, 4.76780222]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.90361572316819}
episode index:397
target Thresh 18.994585661529733
target distance 7.0
model initialize at round 397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([3.99999999, 4.        , 3.94611681]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.062257750719533}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6352723672365177
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 7.53961384, 10.1788627 ,  6.24699984]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.9413935889873128}
episode index:398
target Thresh 18.99469287261461
target distance 9.0
model initialize at round 398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.00000003,  1.99999995,  0.0470826 ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.055385189944994}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.635857513716549
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.54320624, 10.39590105,  2.91434066]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7573612628991973}
episode index:399
target Thresh 18.994797960777774
target distance 9.0
model initialize at round 399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 7.        , 11.        ,  1.50781315]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 9.848857801796104}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6363541543358828
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([3.15087812, 2.37204227, 6.09188209]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.40147186200373686}
episode index:400
target Thresh 18.99490096805588
target distance 14.0
model initialize at round 400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([ 2.        , 10.        ,  2.47491497]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 14.317821063612962}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6366118755804026
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.68091217,  7.37065099,  5.9262441 ]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.4890799521601953}
episode index:401
target Thresh 18.995001935653224
target distance 11.0
model initialize at round 401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.41281357,  7.55958556,  3.80463505]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 10.932065262716192}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6350282639496055
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.22335173,  1.29893312,  0.7426567 ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.249957400512082}
episode index:402
target Thresh 18.99510090395819
target distance 6.0
model initialize at round 402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.0704848 ,  6.68146643,  2.53090221]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 4.4174348596359065}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6357193036109803
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([16.18930516, 10.93183739,  1.68133198]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.20120284552130804}
episode index:403
target Thresh 18.995197912559416
target distance 5.0
model initialize at round 403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.99999007, 10.00001644,  3.11592865]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 5.385176388210663}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6364297624991411
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.99216392,  5.00966934,  0.26636879]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.9922110336919493}
episode index:404
target Thresh 18.995293000261636
target distance 12.0
model initialize at round 404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.30900188,  4.29472377,  3.06303966]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 11.539466217196678}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6348583309867974
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([1.31871463e+01, 4.26302244e-01, 1.06631931e-03]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.307981082209958}
episode index:405
target Thresh 18.9953862051012
target distance 8.0
model initialize at round 405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([10.        , 11.        ,  2.07104822]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 10.000000002837945}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6352493555007306
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.11105589, 5.24533847, 4.08874475]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.26930349803969666}
episode index:406
target Thresh 18.995477564361288
target distance 6.0
model initialize at round 406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([1.99999999, 5.        , 3.96994483]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 7.211102554674912}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.63582305686994
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.31514122, 10.21651536,  0.55400983]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8444893007106724}
episode index:407
target Thresh 18.99556711458682
target distance 5.0
model initialize at round 407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.00000003, 11.        ,  0.8780424 ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 5.0000000307662695}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6365262961776802
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.24099554, 10.1993256 ,  4.31166952]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.8361568924934181}
episode index:408
target Thresh 18.99565489159908
target distance 10.0
model initialize at round 408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([14.00000002,  3.00000001,  1.25878113]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 11.180339902744038}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6349699971650209
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.15617813,  6.87910475,  4.48002397]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 9.224532732307738}
episode index:409
target Thresh 18.995740930510046
target distance 12.0
model initialize at round 409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([16.        ,  2.        ,  0.55567854]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 12.165525060596439}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6334212898548622
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([13.13525276,  2.065639  ,  3.77690446]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 9.33780463544479}
episode index:410
target Thresh 18.995825265736425
target distance 12.0
model initialize at round 410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([1.99999999, 6.99999999, 5.0980742 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 12.165525067988446}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6335916704026596
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.64948727,  9.38129024,  5.98303129]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.5179202896242567}
episode index:411
target Thresh 18.995907931013434
target distance 12.0
model initialize at round 411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([3.229256  , 2.97562878, 0.30726498]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 12.858876992265117}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.633613563440723
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'previousTarget': array([14., 10.]), 'currentState': array([13.79159269, 10.02043464,  0.34266627]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.2094067366266277}
episode index:412
target Thresh 18.99598895940829
target distance 7.0
model initialize at round 412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([14.        ,  2.        ,  5.55089688]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 7.280109892203431}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6341828909209366
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.058133  ,  8.92936981,  2.13496546]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.09147715119749415}
episode index:413
target Thresh 18.996068383333423
target distance 5.0
model initialize at round 413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 8.00001011, 11.00001531,  1.98933953]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 5.000010106694724}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.634902413763898
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.5380377 , 11.39552158,  3.42296534]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.667773832452302}
episode index:414
target Thresh 18.996146234559472
target distance 11.0
model initialize at round 414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([15.        ,  2.        ,  0.64296692]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.045361017187261}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6333725284295272
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.09272209,  1.48123773,  3.86411046]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.218689377671751}
episode index:415
target Thresh 18.996222544227958
target distance 11.0
model initialize at round 415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([15.        , 11.        ,  1.22673767]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6334581035019795
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.86502874, 7.96500165, 5.82850719]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.2959563645323895}
episode index:416
target Thresh 18.99629734286377
target distance 3.0
model initialize at round 416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([12.8341082 ,  9.08894502,  3.5311451 ]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 2.6487891624113837}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6342658754360275
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.87386441, 10.45651157,  3.24795943]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0290863300759876}
episode index:417
target Thresh 18.996370660387356
target distance 13.0
model initialize at round 417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.44964708, 6.37831724, 5.98486614]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 12.773713290838831}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6327484929589079
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.79582208, 7.69316423, 2.92292666]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.797163425109328}
episode index:418
target Thresh 18.996442526126707
target distance 9.0
model initialize at round 418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([5.92870551, 9.84435746, 5.12608814]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 8.077985453020741}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6332704721430032
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.77741659, 2.15347603, 5.71015648]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.2703669037919073}
episode index:419
target Thresh 18.996512968829077
target distance 2.0
model initialize at round 419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.59554534,  9.57219783,  2.21070009]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7332727705242865}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.63414363768552
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.59554534,  9.57219783,  2.21070009]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.7332727705242865}
episode index:420
target Thresh 18.996582016672484
target distance 8.0
model initialize at round 420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 8.99995253, 10.999964  ,  4.7923584 ]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 9.433975767663755}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.634599754042171
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.01004513,  3.83232643,  5.09324167]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.83238704298108}
episode index:421
target Thresh 18.996649697276982
target distance 7.0
model initialize at round 421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.00000003,  9.        ,  0.92526644]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 7.280109917361242}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6352176177867599
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.72778693, 10.3309258 ,  4.07570599]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.9886020973348805}
episode index:422
target Thresh 18.99671603771572
target distance 11.0
model initialize at round 422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.57580139, 4.42004439, 6.06387091]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.432658044516293}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6337159212908101
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([4.78099389, 6.8334133 , 3.00193714]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 10.604542267847096}
episode index:423
target Thresh 18.996781064525763
target distance 5.0
model initialize at round 423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.00002286, 1.99998968, 0.57788521]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 5.0990251526175605}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6344195755988672
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.33106187, 6.71955768, 2.0115109 ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.43387770208557813}
episode index:424
target Thresh 18.99684480371869
target distance 6.0
model initialize at round 424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.00000063, 9.99998057, 5.74696016]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 6.082765100502951}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6350547814798317
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.158082  , 10.8159579 ,  0.61421631]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.24261371557468978}
episode index:425
target Thresh 18.99690728079104
target distance 11.0
model initialize at round 425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.00000002,  3.00000001,  1.31499213]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 12.529964102266936}
done in step count: 74
reward sum = 0.47534004200570695
running average episode reward sum: 0.6346798642510192
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.82214927, 8.62902694, 4.80172434]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.901970304315168}
episode index:426
target Thresh 18.996968520734466
target distance 2.0
model initialize at round 426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 3.29050279, 12.46859185,  2.37750521]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 2.253695350740532}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6354888107047639
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.324307  , 11.14304689,  0.09431931]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.6906688343868899}
episode index:427
target Thresh 18.997028548045765
target distance 13.0
model initialize at round 427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([2.99998293, 3.00011481, 2.72035903]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.038413020663388}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6340040237638649
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.83586417, 5.45758864, 5.94161021]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.258885087571276}
episode index:428
target Thresh 18.997087386736666
target distance 12.0
model initialize at round 428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([4.        , 8.        , 3.89112508]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6343575764842456
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'previousTarget': array([16., 11.]), 'currentState': array([15.68295775, 11.47360915,  1.62564239]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.569931062544307}
episode index:429
target Thresh 18.997145060343424
target distance 12.0
model initialize at round 429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([16.        ,  7.        ,  2.11861383]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 13.000000001944873}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6328823263063753
{'scaleFactor': 20, 'currentTarget': array([3.75673508, 3.57763495]), 'previousTarget': array([4., 2.]), 'currentState': array([13.13225333,  1.05534308,  3.33984129]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.708877307028715}
episode index:430
target Thresh 18.997201591936253
target distance 2.0
model initialize at round 430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([4.63981753, 9.35432268, 1.21480292]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 1.684632079927278}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6336651956188896
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.30166025, 10.57406319,  0.93161153]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.5219397211847498}
episode index:431
target Thresh 18.997257004128542
target distance 10.0
model initialize at round 431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([3.48834085, 7.78558193, 1.4876582 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 9.098396839444973}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6340540044011121
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.43357671, 11.54498788,  1.50535986]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7860325221561225}
episode index:432
target Thresh 18.997311319085913
target distance 12.0
model initialize at round 432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([14.        ,  7.        ,  0.77399605]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 12.649110640673516}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6341980210608085
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.78543974, 11.18345793,  3.65895696]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8065806840216296}
episode index:433
target Thresh 18.997364558535068
target distance 4.0
model initialize at round 433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([15.00195463,  8.00001729,  1.01084655]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 4.123596902443992}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6348628751469078
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.93805811,  4.60664678,  4.44447415]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.6098008802279216}
episode index:434
target Thresh 18.997416743772497
target distance 14.0
model initialize at round 434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([15.88743242,  5.6784808 ,  2.63976121]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 14.872099575073563}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6346997540326675
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.97289589, 10.69146849,  3.54241912]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.020645930378734}
episode index:435
target Thresh 18.997467895672987
target distance 9.0
model initialize at round 435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([3.        , 2.        , 3.91708398]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 12.041594578792393}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6348901915981849
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.19253968, 10.7452245 ,  0.80203952]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.3193463369906532}
episode index:436
target Thresh 18.99751803469799
target distance 1.0
model initialize at round 436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.09104139,  4.17640777,  2.09636302]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.1799253235969258}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6356801453931548
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.86579009,  2.65977687,  6.09636242]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.3657377168039712}
episode index:437
target Thresh 18.997567180903776
target distance 6.0
model initialize at round 437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([1.28566805, 5.52381997, 3.01115513]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.793237076848988}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6363144766764665
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.68340941, 9.61022016, 2.16152565]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.5021532870692994}
episode index:438
target Thresh 18.997615353949485
target distance 13.0
model initialize at round 438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([14.99998021,  6.00005191,  2.93708909]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 13.601466856445311}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6348650131760645
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([13.22239269,  0.1128573 ,  6.15833455]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 11.379956293381658}
episode index:439
target Thresh 18.99766257310498
target distance 8.0
model initialize at round 439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([13.        , 11.        ,  0.73007363]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 8.000000000563153}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6353965604478665
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.50397994, 11.15959764,  3.59733073]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.5286465604494128}
episode index:440
target Thresh 18.997708857258548
target distance 3.0
model initialize at round 440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([14.00001401,  7.99998494,  0.18078869]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 4.2426612416672596}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6361121919432228
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.37555077, 10.1041618 ,  3.89760291]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.9713724587286212}
episode index:441
target Thresh 18.997754224924474
target distance 3.0
model initialize at round 441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([16.00002689,  5.99991611,  6.02462006]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 3.1623657483774883}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6368245852870164
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.72936819,  8.7816767 ,  3.45824765]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.3477163204494909}
episode index:442
target Thresh 18.997798694250424
target distance 9.0
model initialize at round 442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([ 5.58781647, 10.08455779,  4.71873379]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 8.488631733540824}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6371428624294816
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.80578597, 2.58483519, 4.4532462 ]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6162396402410728}
episode index:443
target Thresh 18.99784228302472
target distance 4.0
model initialize at round 443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([0.97280923, 8.68929339, 5.04967105]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.36775910113417}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6378713605095955
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.90455047, 6.64723147, 0.4832977 ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.6542317522570488}
episode index:444
target Thresh 18.99788500868346
target distance 12.0
model initialize at round 444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([2.        , 7.        , 3.82889557]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6377051443971072
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([13.81685303,  6.93521983,  4.73155873]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9529842341463933}
episode index:445
target Thresh 18.997926888317473
target distance 14.0
model initialize at round 445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([14.69206946,  3.05906334,  3.46295023]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 13.02834355079609}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.636275312234782
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.21144359,  1.25273837,  0.40102016]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.175095909366838}
episode index:446
target Thresh 18.99796793867917
target distance 10.0
model initialize at round 446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.26255241,  0.63802059,  5.10618806]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 12.478684898123445}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6348518775317958
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.2328291 ,  4.16567193,  2.04425811]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.421893353767649}
episode index:447
target Thresh 18.998008176189245
target distance 11.0
model initialize at round 447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.05104155,  8.63321726,  4.53791833]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 12.042549874262075}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6334347974480196
{'scaleFactor': 20, 'currentTarget': array([2.45591032, 5.5416683 ]), 'previousTarget': array([2.45591032, 5.5416683 ]), 'currentState': array([13.13346885,  1.08873921,  3.70606526]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 11.568873485164286}
episode index:448
target Thresh 18.998047616943236
target distance 5.0
model initialize at round 448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([15.00001786,  5.99997885,  0.13249272]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 5.830979223595532}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6340181013607382
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([12.20351798, 11.22123181,  3.28293632]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.3006045307476975}
episode index:449
target Thresh 18.998086276717977
target distance 7.0
model initialize at round 449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.32467711,  3.84039891,  4.23857188]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 7.281116479744188}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6345789164059724
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.63323847, 10.27219855,  3.10582982]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.8149901669489035}
episode index:450
target Thresh 18.99812417097788
target distance 14.0
model initialize at round 450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([2.        , 2.        , 5.27933979]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 15.65247584250062}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6346403424028195
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.29896615,  9.13732405,  5.59793056]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.7143572987680314}
episode index:451
target Thresh 18.99816131488117
target distance 11.0
model initialize at round 451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.19073272,  3.47553437,  3.07452095]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 12.100400204087304}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.6344108406341299
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.49438226, 10.4329754 ,  3.41081385]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.6571769298002521}
episode index:452
target Thresh 18.998197723285895
target distance 8.0
model initialize at round 452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([0.89045748, 4.26538612, 3.2926681 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 8.453546523496845}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6348525689361518
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.34523063, 10.7477686 ,  1.59355054]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.42755685510519004}
episode index:453
target Thresh 18.9982334107559
target distance 13.0
model initialize at round 453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([3.56567453, 7.58502674, 2.22999983]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 12.514575467347406}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6351333842344138
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.73084849,  9.08836558,  5.96451665]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.2832860966927987}
episode index:454
target Thresh 18.998268391566658
target distance 13.0
model initialize at round 454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([13.46183965, 10.68289432,  3.72576606]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 12.381569663890495}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6353308500623774
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.88493567, 6.52677439, 4.89391216]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.5391948266046932}
episode index:455
target Thresh 18.99830267971095
target distance 8.0
model initialize at round 455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([3.68109082, 3.9221132 , 0.955702  ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 9.488155498817676}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6356955424735105
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.17795378, 10.77468194,  0.97340449]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.2871163117338502}
episode index:456
target Thresh 18.998336288904497
target distance 3.0
model initialize at round 456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.9861583 , 6.01593397, 3.28803849]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 3.1730688471729294}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6363440540827741
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.98101586, 3.79866591, 4.72166645]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7988915065648884}
episode index:457
target Thresh 18.998369232591422
target distance 13.0
model initialize at round 457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([3.        , 4.        , 3.61150074]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 13.000000000000002}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.6359317815056584
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([16.18753904,  4.36637482,  0.24867858]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.41158401139804085}
episode index:458
target Thresh 18.99840152394964
target distance 11.0
model initialize at round 458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([14.        ,  2.        ,  0.32661599]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.704699910719624}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6345463092147965
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([13.12122233,  3.75488006,  3.54787134]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 10.367241923695795}
episode index:459
target Thresh 18.998433175896125
target distance 4.0
model initialize at round 459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([4.07707466, 6.27970487, 1.87316483]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 2.925764073756775}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6352551129121556
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.42518235, 8.53096911, 3.58997952]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.7418930508730585}
episode index:460
target Thresh 18.998464201092077
target distance 6.0
model initialize at round 460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([ 6.58961857, 10.09916128,  4.72580934]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 5.719053253055972}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6357806354936888
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.9659556 , 5.19947012, 5.5930656 ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.20235451472523405}
episode index:461
target Thresh 18.998494611947986
target distance 4.0
model initialize at round 461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([16.24337243,  4.33479391,  5.85951304]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 2.645229217269591}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6365047012177285
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([15.89685897,  2.74411213,  5.57632297]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 1.1653578298935832}
episode index:462
target Thresh 18.998524420628605
target distance 13.0
model initialize at round 462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([16.        ,  5.        ,  1.91833275]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 13.152946437981429}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6351299610423122
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([13.25152763,  6.14567593,  5.13958823]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 10.287064126851543}
episode index:463
target Thresh 18.9985536390578
target distance 7.0
model initialize at round 463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([3.        , 4.        , 0.07490652]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 7.071067814657412}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6356907547776924
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.74018338, 10.69746001,  3.22534965]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.39879207983580517}
episode index:464
target Thresh 18.998582278923333
target distance 5.0
model initialize at round 464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.99999995, 9.00000002, 3.71301532]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.0990195487574725}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6361364589467381
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.10801716, 4.71639793, 0.01390204]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7244954817701468}
episode index:465
target Thresh 18.99861035168153
target distance 6.0
model initialize at round 465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.99999553, 8.00000319, 3.52360654]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 6.082764943731389}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.63671209331597
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.22081927, 2.54456954, 4.67401271]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.5876369114762049}
episode index:466
target Thresh 18.998637868561875
target distance 10.0
model initialize at round 466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([5.07595957, 8.93162059, 0.2572624 ]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 9.393236758979105}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6370142544853129
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.17523357,  6.9186557 ,  6.27496096]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.9352192764231785}
episode index:467
target Thresh 18.99866484057148
target distance 12.0
model initialize at round 467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.83462916, 7.90065775, 1.55729025]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.361018610164042}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6372178637057597
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.19161851, 10.92679645,  0.72542144]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.2298098583466}
episode index:468
target Thresh 18.99869127849951
target distance 11.0
model initialize at round 468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.0009981 , 3.99903692, 0.23446053]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 12.082535925861356}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6373145305102472
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.95220658,  9.24920958,  0.83623529]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.2537511129003276}
episode index:469
target Thresh 18.998717192921493
target distance 7.0
model initialize at round 469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.47915362, 4.80274612, 1.49922627]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 8.299740982194486}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6377341033420341
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.0457126 , 10.17404098,  6.08329902]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 1.2620906233288474}
episode index:470
target Thresh 18.998742594203538
target distance 6.0
model initialize at round 470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.00000001, 6.99999998, 6.11781406]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 7.211102554749397}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6382432050822824
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.72189432, 10.55911156,  0.70187847]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5212728484210005}
episode index:471
target Thresh 18.998767492506502
target distance 9.0
model initialize at round 471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([ 9.99999998, 11.        ,  4.2035737 ]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 10.816653835910829}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6385224422448308
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.90405953,  1.78722395,  5.93806872]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.23340570355524287}
episode index:472
target Thresh 18.99879189779003
target distance 1.0
model initialize at round 472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([4.36926095, 2.99490786, 0.99828106]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 1.692547546818363}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6392445935297255
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([2.74533259, 2.90412492, 4.99827998]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.9393068498660949}
episode index:473
target Thresh 18.99881581981657
target distance 2.0
model initialize at round 473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([3.9738189 , 3.32169135, 5.69879055]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.9998615852326413}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6399430205475952
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.24300541, 2.82551951, 5.41560512]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.29915726451465025}
episode index:474
target Thresh 18.998839268155248
target distance 12.0
model initialize at round 474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([14.19518066,  3.47771858,  2.44147399]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 13.829771733985746}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.6397361617168991
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'previousTarget': array([ 2., 10.]), 'currentState': array([2.62715662, 9.76249002, 5.06095304]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.6706239035646478}
episode index:475
target Thresh 18.99886225218571
target distance 8.0
model initialize at round 475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([ 9.42110914, 10.41741917,  4.49707603]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 7.793858782572268}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6401809550139116
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.4438023 , 6.84905313, 5.08114437]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9580457691027654}
episode index:476
target Thresh 18.99888478110187
target distance 4.0
model initialize at round 476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([15.000262  ,  3.99898836,  5.96780467]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 4.0010116472255195}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6408325464078027
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.84314719,  7.457602  ,  3.40143298]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.5646223471318507}
episode index:477
target Thresh 18.998906863915607
target distance 4.0
model initialize at round 477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.        , 5.99999999, 5.4655149 ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 4.00000000519227}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6414814114778701
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.09073649, 9.01946893, 2.89913642]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.337236440738581}
episode index:478
target Thresh 18.998928509460335
target distance 11.0
model initialize at round 478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([15.00021377,  6.99958629,  6.19130778]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 11.704759432011196}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6401422018505676
{'scaleFactor': 20, 'currentTarget': array([3.57478121, 4.36826378]), 'previousTarget': array([3.57478115, 4.36826388]), 'currentState': array([13.11495127,  0.69042956,  3.67803463]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.224544456922104}
episode index:479
target Thresh 18.99894972639456
target distance 11.0
model initialize at round 479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([15.        ,  6.9999999 ,  5.70209336]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 11.401754275691337}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6403342042834923
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.22818032, 9.93130387, 4.87023898]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.23829690656213165}
episode index:480
target Thresh 18.99897052320534
target distance 12.0
model initialize at round 480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([3.57214971, 6.60053538, 1.36687916]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 10.747225033352503}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6401178508966393
{'scaleFactor': 20, 'currentTarget': array([14.,  4.]), 'previousTarget': array([14.,  4.]), 'currentState': array([13.95852292,  3.66585451,  5.9863377 ]), 'targetState': array([14,  4], dtype=int32), 'currentDistance': 0.3367098957519631}
episode index:481
target Thresh 18.998990908211674
target distance 3.0
model initialize at round 481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([3.66983768, 9.99289913, 0.9977476 ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 3.011055027355298}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6407430838810882
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.11649615, 7.66564169, 0.43137603]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.6757589875893776}
episode index:482
target Thresh 18.99901088956784
target distance 4.0
model initialize at round 482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.01402053, 9.00567255, 1.38645595]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 4.005697089123152}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6412516590111814
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.83424964, 5.43041948, 0.25371138]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.9387403176477803}
episode index:483
target Thresh 18.99903047526664
target distance 13.0
model initialize at round 483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([ 3.        , 10.        ,  4.68287206]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 14.31782106338214}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6399267588892575
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([4.81960443, 8.08367656, 1.62094223]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 11.902842489595443}
episode index:484
target Thresh 18.999049673142622
target distance 12.0
model initialize at round 484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([ 3.        , 11.        ,  3.99822342]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 13.416407864998737}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6401478832922314
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.61028322,  5.65430698,  5.44955447]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.7615752041559866}
episode index:485
target Thresh 18.999068490875192
target distance 6.0
model initialize at round 485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([10.65279085, 11.31708257,  1.19154375]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 5.468204835056467}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6406545437622394
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.77977944,  7.79762299,  0.05880062]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.29908786142863575}
episode index:486
target Thresh 18.999086935991688
target distance 10.0
model initialize at round 486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([14.4344193 ,  7.38254243,  4.51925802]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 9.17744127086131}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6409362004678593
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.08780168, 11.25550711,  4.25377473]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.2701721978553535}
episode index:487
target Thresh 18.99910501587041
target distance 9.0
model initialize at round 487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([1.77061248, 3.14731262, 3.3927139 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.673740815030085}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6412820850329277
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.38188655, 11.6828807 ,  1.4104164 ]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.9210810467554637}
episode index:488
target Thresh 18.999122737743544
target distance 5.0
model initialize at round 488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([ 1.99999996, 11.00000004,  3.43349361]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 5.000000036276612}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6418767338322612
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'previousTarget': array([2., 6.]), 'currentState': array([1.93890525, 6.90956582, 4.86711656]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.911615356163537}
episode index:489
target Thresh 18.99914010870008
target distance 11.0
model initialize at round 489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.        , 10.        ,  6.20785952]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.045361017188023}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.642219287167749
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.81134835, 9.74007859, 4.2255607 ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.7637445717620898}
episode index:490
target Thresh 18.99915713568863
target distance 12.0
model initialize at round 490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([2.        , 2.        , 6.21541786]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 13.416407864987189}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6422200861797999
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.99536188,  8.14179926,  6.25082285]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.14187508938728519}
episode index:491
target Thresh 18.99917382552022
target distance 13.0
model initialize at round 491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([16.        ,  9.        ,  5.92861223]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 13.038404810410388}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6425116675916441
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.72178951, 10.08073772,  3.6631293 ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7262910387670647}
episode index:492
target Thresh 18.999190184870997
target distance 1.0
model initialize at round 492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([13.40066486,  9.482282  ,  4.85605574]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 1.6810427733173647}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6431964309433853
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.87482475, 10.45063015,  2.57287043]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.46769260446412725}
episode index:493
target Thresh 18.99920622028493
target distance 6.0
model initialize at round 493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.        , 11.        ,  2.63213801]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 6.082762530356352}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6437068394926065
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.45250234,  4.72746105,  5.7825815 ]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.5282384376307195}
episode index:494
target Thresh 18.99922193817639
target distance 6.0
model initialize at round 494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([10.82167241, 12.43199304,  2.05186708]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 6.212378999320475}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6441970981435632
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([16.38353007,  9.26360681,  0.91912465]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.4653857196716386}
episode index:495
target Thresh 18.999237344832746
target distance 5.0
model initialize at round 495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.99999783, 8.00000356, 3.12047946]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 5.0000035588723435}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6447586860392979
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.70671723, 3.03920829, 0.27092128]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.7078040259822171}
episode index:496
target Thresh 18.999252446416868
target distance 10.0
model initialize at round 496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.00001069,  4.00002081,  2.09847523]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.198045423364537}
done in step count: 77
reward sum = 0.46122196741809546
running average episode reward sum: 0.6443893968670218
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.95860018, 6.51761708, 5.30202654]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.5192700492232996}
episode index:497
target Thresh 18.999267248969588
target distance 12.0
model initialize at round 497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([2.        , 6.        , 3.16090477]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.369316876852983}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6430954422548391
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([4.89631202, 7.64507564, 0.09897494]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 10.220267240717988}
episode index:498
target Thresh 18.999281758412124
target distance 12.0
model initialize at round 498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([3.66915224, 5.21500127, 1.13010329]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 10.699655759331561}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6432306281967577
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.72702111,  8.66202901,  0.01506491]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7161004742967203}
episode index:499
target Thresh 18.99929598054845
target distance 14.0
model initialize at round 499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([15.68018829,  4.5882093 ,  3.07079268]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 13.692828117683382}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6419441669403642
{'scaleFactor': 20, 'currentTarget': array([2., 4.]), 'previousTarget': array([2., 4.]), 'currentState': array([1.32252787e+01, 9.81387301e-01, 8.86248764e-03]), 'targetState': array([2, 4], dtype=int32), 'currentDistance': 11.624065758747468}
episode index:500
target Thresh 18.99930992106761
target distance 3.0
model initialize at round 500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([16.00315567, 10.99909237,  0.72194153]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 3.6065478950002423}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6425610249901837
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.26248718,  8.80748732,  4.4387561 ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8490790824473367}
episode index:501
target Thresh 18.999323585545994
target distance 3.0
model initialize at round 501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.00008021,  6.00003691,  1.43333739]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 3.000036916004935}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6431754254382113
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.85754782,  3.18749867,  5.15015196]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.2354747870452926}
episode index:502
target Thresh 18.999336979449577
target distance 5.0
model initialize at round 502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([16.        ,  9.        ,  6.07420588]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 5.3851648072523535}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6437128843289577
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'previousTarget': array([11., 11.]), 'currentState': array([11.26642958, 10.23088286,  5.22464972]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 0.8139569390311941}
episode index:503
target Thresh 18.999350108136102
target distance 3.0
model initialize at round 503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([4.16316578, 9.65288829, 2.47439957]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 2.5483320301047954}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6443416206894161
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.62041548, 10.54642928,  4.19121414]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.5914480565719332}
episode index:504
target Thresh 18.999362976857213
target distance 13.0
model initialize at round 504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([4.62521216, 2.56329799, 0.73949593]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 12.209427629736744}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6442281457611945
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.86854385,  7.44132406,  6.20853078]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.4604863152487665}
episode index:505
target Thresh 18.999375590760575
target distance 2.0
model initialize at round 505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.29800115, 9.05190427, 1.68305224]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7039150818423067}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6449312521924964
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.29800115, 9.05190427, 1.68305224]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.7039150818423067}
episode index:506
target Thresh 18.999387954891915
target distance 11.0
model initialize at round 506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.12059691, 4.80131537, 6.2599225 ]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 10.881217172054878}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6448525042929797
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.97050722,  5.04156398,  5.72895723]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.050964580463569074}
episode index:507
target Thresh 18.99940007419705
target distance 9.0
model initialize at round 507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.99999998, 11.00000001,  3.88903415]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 9.000000005427017}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6452932391521844
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.28991963,  2.75842534,  0.47310762]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8119497461378623}
episode index:508
target Thresh 18.999411953523868
target distance 9.0
model initialize at round 508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([3.        , 2.        , 5.31692004]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6455846360961756
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.73585606, 11.48348746,  1.05143758]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.5509375193317051}
episode index:509
target Thresh 18.99942359762425
target distance 14.0
model initialize at round 509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([16.      ,  2.      ,  6.082798]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 16.643316977093235}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6455537321650129
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.67366473, 10.51734817,  3.83501816]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.8287200712302332}
episode index:510
target Thresh 18.999435011156002
target distance 12.0
model initialize at round 510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([3.        , 3.        , 5.26223183]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 14.422205101857395}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6457091658319265
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.28998314, 10.6853288 ,  0.14719309]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.7766221145790723}
episode index:511
target Thresh 18.99944619868468
target distance 8.0
model initialize at round 511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 5.99999986, 10.99999993,  4.59437776]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 11.31370854841818}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.645794595363249
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.77999603,  3.69798518,  3.19615288]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 1.0466981972999923}
episode index:512
target Thresh 18.99945716468545
target distance 7.0
model initialize at round 512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([15.        ,  4.        ,  0.76228588]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 9.219544457379452}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6461141534000093
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 9.23186738, 10.48093535,  5.06317356]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.5684985475584786}
episode index:513
target Thresh 18.99946791354485
target distance 13.0
model initialize at round 513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([15.12959064,  3.42513494,  3.12108696]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.231400944679983}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.644857121973161
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.74829525,  9.37652574,  0.05915713]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 10.685655703876144}
episode index:514
target Thresh 18.999478449562577
target distance 5.0
model initialize at round 514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([14.44327756,  1.67000102,  4.35248351]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 5.348400150778726}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6453787921197833
{'scaleFactor': 20, 'currentTarget': array([14.,  7.]), 'previousTarget': array([14.,  7.]), 'currentState': array([13.56123904,  6.26889668,  3.50292759]), 'targetState': array([14,  7], dtype=int32), 'currentDistance': 0.8526565767790779}
episode index:515
target Thresh 18.99948877695318
target distance 9.0
model initialize at round 515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([ 7.        , 11.        ,  3.00205112]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.486832980671647}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6457616688664193
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([3.58888265, 2.46378534, 5.58612458]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.6197695677438206}
episode index:516
target Thresh 18.99949889984774
target distance 12.0
model initialize at round 516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([3.        , 7.        , 5.94363832]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 12.649110640673618}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6459578205597755
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.38814531, 11.77082006,  1.11178525]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.9841390815918235}
episode index:517
target Thresh 18.99950882229556
target distance 1.0
model initialize at round 517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 1.69212478, 11.82532834,  2.92984194]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 1.5465136458403146}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6466028826822469
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.84623072, 10.94549916,  0.64665651]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.16314205932044248}
episode index:518
target Thresh 18.99951854826575
target distance 2.0
model initialize at round 518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.58558582, 7.4251465 , 4.52962565]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 1.4841770991239278}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6472645341607012
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.71521588, 6.36099504, 0.24644035]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.8011561495070794}
episode index:519
target Thresh 18.99952808164883
target distance 9.0
model initialize at round 519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([13.00002331, 10.99996559,  0.02670019]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 9.219559750563459}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6476246288285653
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([3.77159741, 9.41752103, 4.61077354]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.4759112867266893}
episode index:520
target Thresh 18.99953742625827
target distance 7.0
model initialize at round 520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([10.21166987, 12.16061433,  1.76587981]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 7.754786003678451}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6480323711045974
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.6179847 ,  7.18194833,  0.34995315]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.42313222843143194}
episode index:521
target Thresh 18.99954658583205
target distance 6.0
model initialize at round 521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.9999973 , 11.00000187,  3.53705406]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 6.082763933720688}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6485234624913871
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.14948509,  5.32252063,  4.68749814]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.3554790453939592}
episode index:522
target Thresh 18.999555564034118
target distance 1.0
model initialize at round 522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.04758622, 10.5577049 ,  2.54225689]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5597313658411034}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.649195501760046
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.04758622, 10.5577049 ,  2.54225689]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.5597313658411034}
episode index:523
target Thresh 18.999564364455875
target distance 12.0
model initialize at round 523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([2.        , 5.        , 5.17305231]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 12.041594578793001}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6492591641517452
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.17870235,  6.1766221 ,  5.77482862]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.2512566396397443}
episode index:524
target Thresh 18.9995729906176
target distance 14.0
model initialize at round 524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 2.        , 10.        ,  5.26291633]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 14.560219778561143}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6493623804009792
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.46050295,  5.81881114,  6.14787795]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.5691102427959144}
episode index:525
target Thresh 18.999581445969884
target distance 3.0
model initialize at round 525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([ 2.99680639, 11.45753767,  1.432316  ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 3.4575391425897104}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6499358170350077
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.7851763 , 8.83922767, 5.14913069]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8662864971509507}
episode index:526
target Thresh 18.99958973389497
target distance 9.0
model initialize at round 526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.        , 11.        ,  4.16507173]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 9.055385138332033}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6503182116347418
{'scaleFactor': 20, 'currentTarget': array([15.,  2.]), 'previousTarget': array([15.,  2.]), 'currentState': array([14.74849472,  2.22372746,  4.74914519]), 'targetState': array([15,  2], dtype=int32), 'currentDistance': 0.33661384824643814}
episode index:527
target Thresh 18.99959785770815
target distance 3.0
model initialize at round 527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([16.10303297,  9.34448586,  5.77654505]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.3484279858475168}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6509242358551306
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.72200088,  7.89903832,  5.49335962]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.2957647250924505}
episode index:528
target Thresh 18.99960582065905
target distance 12.0
model initialize at round 528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([2.37638254, 4.63400325, 2.34640142]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.918324377114295}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.649693755258051
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.78706626, 6.53293672, 5.5676569 ]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.267700005486779}
episode index:529
target Thresh 18.999613625932955
target distance 11.0
model initialize at round 529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([ 5.        , 11.        ,  2.43446782]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6497819051878463
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.43189787,  3.12930171,  5.31942932]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.5826310677736719}
episode index:530
target Thresh 18.999621276652082
target distance 7.0
model initialize at round 530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([14.66114499,  4.01868627,  3.49317491]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 5.990904495523975}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6501779060342746
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.52213703,  9.99854294,  2.07724837]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.5221390648602047}
episode index:531
target Thresh 18.99962877587682
target distance 2.0
model initialize at round 531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([1.99999921, 2.99999994, 4.21536493]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 2.2360687100092016}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6507980603462402
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.00308529, 4.33672636, 1.93217963]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.0522469256603948}
episode index:532
target Thresh 18.99963612660696
target distance 13.0
model initialize at round 532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([2.        , 8.        , 5.45959234]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.341664064135935}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6510217847092026
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.61093614, 10.96409542,  0.91092458]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.3907170665046261}
episode index:533
target Thresh 18.999643331782888
target distance 7.0
model initialize at round 533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.96144832,  3.38072707,  1.96453446]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 5.619405177573079}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6514962421816738
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.06608164,  8.72627318,  3.11497854]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.9732059778642977}
episode index:534
target Thresh 18.99965039428678
target distance 11.0
model initialize at round 534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([4.        , 8.        , 3.56475711]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 12.083045973594572}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6513980361336074
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([14.92320905,  2.95372129,  5.03379219]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.08965806875550386}
episode index:535
target Thresh 18.99965731694372
target distance 3.0
model initialize at round 535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([15.84797483,  9.55476234,  6.24499941]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 2.4150148896235026}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6519930006184328
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.78192671,  8.23606034,  5.9618141 ]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.8167826269743783}
episode index:536
target Thresh 18.99966410252287
target distance 1.0
model initialize at round 536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.92916881,  1.8095445 ,  5.35833526]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 1.1925608427249468}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6526040006172811
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.29940622,  3.32527561,  3.07514995]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.4420953585150991}
episode index:537
target Thresh 18.99967075373855
target distance 11.0
model initialize at round 537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 4.        , 10.        ,  2.77394861]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.704699910719626}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6527250536507031
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.90295266,  6.61476557,  5.94209554]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.6223784161672339}
episode index:538
target Thresh 18.999677273251333
target distance 10.0
model initialize at round 538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.        ,  6.        ,  1.11659735]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6528323415423943
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.18699912e+00, 6.53000053e+00, 1.55897147e-03]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.5620224472933066}
episode index:539
target Thresh 18.999683663669117
target distance 14.0
model initialize at round 539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([3.53692654, 4.31540976, 0.58295458]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 12.576408247136248}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6529001503281844
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.45289534,  6.68990199,  5.4679162 ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8805045540245396}
episode index:540
target Thresh 18.99968992754815
target distance 3.0
model initialize at round 540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([10.10218397, 10.20591103,  0.21500748]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 2.057251314985672}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6534868395142691
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'previousTarget': array([12., 11.]), 'currentState': array([11.45737521, 10.29925188,  6.21500748]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.8862785095598369}
episode index:541
target Thresh 18.99969606739407
target distance 14.0
model initialize at round 541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([16.        , 11.        ,  2.03506092]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 14.142135623730951}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6537018880498612
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.61548517, 9.07921119, 3.76957847]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.6205613593045267}
episode index:542
target Thresh 18.99970208566289
target distance 12.0
model initialize at round 542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([15.02460419,  6.10857647,  2.34995285]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 12.981414793559948}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6538740247096801
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.65932004, 10.62024082,  3.80128508]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.5101763136810323}
episode index:543
target Thresh 18.999707984762004
target distance 12.0
model initialize at round 543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([13.9967882 ,  9.00708106,  2.99862009]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.163523401208952}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6540875708881645
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.22277078, 7.83206967, 4.73313764]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.861374918872228}
episode index:544
target Thresh 18.999713767051134
target distance 3.0
model initialize at round 544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.2528571 ,  4.47966937,  3.04038739]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.6939975599478951}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6546677753452504
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([15.64075242,  5.90639916,  2.75720208]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.3712410849161042}
episode index:545
target Thresh 18.999719434843264
target distance 13.0
model initialize at round 545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([4.01650798, 8.34125781, 1.92427319]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 11.988350089486556}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6548649821932391
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.16530347,  8.45145593,  5.65879037]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 0.9489629863031753}
episode index:546
target Thresh 18.99972499040559
target distance 9.0
model initialize at round 546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([15.        ,  2.        ,  5.66175246]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.655104128735495
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.62294298, 11.39871693,  3.39627   ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.739616893825688}
episode index:547
target Thresh 18.999730435960412
target distance 7.0
model initialize at round 547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 2.9999725 , 10.99991955,  5.38498592]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 7.000027501657766}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6555099989805013
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([ 9.98422682, 10.83404121,  6.2522447 ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.1667066703984186}
episode index:548
target Thresh 18.999735773686027
target distance 2.0
model initialize at round 548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.03434284,  4.94833604,  0.01786774]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 2.0519513744546933}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6561192703849085
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.92050096,  6.37783763,  2.01786774]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.1110391694321728}
episode index:549
target Thresh 18.999741005717592
target distance 11.0
model initialize at round 549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([4.        , 5.        , 3.51552653]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 11.045361017196912}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.656118433930707
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([15.61976444,  6.57180554,  5.83411754]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.8432493915092129}
episode index:550
target Thresh 18.999746134147987
target distance 11.0
model initialize at round 550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([ 4.66426998, 11.24996525,  1.15108078]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 12.458642852235027}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6560707119656843
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'previousTarget': array([14.,  3.]), 'currentState': array([14.81059671,  2.61911796,  5.18648648]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.8956216610977408}
episode index:551
target Thresh 18.999751161028662
target distance 2.0
model initialize at round 551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.75449827,  9.37847902,  2.07199676]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9775254311702588}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6566937722700943
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'previousTarget': array([15., 10.]), 'currentState': array([15.75449827,  9.37847902,  2.07199676]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.9775254311702588}
episode index:552
target Thresh 18.999756088370425
target distance 2.0
model initialize at round 552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([14.96516876, 10.81681534,  2.61541325]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.9736881356878762}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.657296496009208
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.48807071, 10.06311368,  4.61541325]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0563943367339064}
episode index:553
target Thresh 18.999760918144286
target distance 6.0
model initialize at round 553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.        , 3.        , 0.89574259]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 6.0827625303467165}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6577424988593878
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.59617529, 9.00652334, 2.04618667]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5962109732921487}
episode index:554
target Thresh 18.999765652282218
target distance 13.0
model initialize at round 554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([ 2.        , 11.        ,  3.30681527]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 15.264337522473747}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6577387452048198
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.79121318,  3.56268636,  5.62540627]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.9708935241836207}
episode index:555
target Thresh 18.999770292677937
target distance 13.0
model initialize at round 555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([14.47496634,  9.28826284,  4.58024931]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.250063292727173}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6578995965521701
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.23920874, 5.16906966, 6.03158154]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.29292554003570326}
episode index:556
target Thresh 18.999774841187666
target distance 4.0
model initialize at round 556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.57280333,  6.82590515,  4.26497674]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 4.195898568923554}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6584087178319705
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.19315981, 10.08971869,  3.69860613]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.2163893898117837}
episode index:557
target Thresh 18.999779299630866
target distance 3.0
model initialize at round 557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([1.60000000e+01, 2.99999999e+00, 8.81927809e-03]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 3.605551281828443}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6589330571367519
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'previousTarget': array([14.,  6.]), 'currentState': array([14.3592191 ,  5.38267461,  3.72563397]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.714233155492769}
episode index:558
target Thresh 18.99978366979098
target distance 5.0
model initialize at round 558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.11727821, 8.32820942, 5.7844255 ]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.823822603695492}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6592774662851565
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.65698004, 5.38366074, 0.08531365]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.5146438171451764}
episode index:559
target Thresh 18.99978795341612
target distance 2.0
model initialize at round 559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.82157022,  5.80505195,  4.93595052]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1502548747701777}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6598858993810758
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.82157022,  5.80505195,  4.93595052]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1502548747701777}
episode index:560
target Thresh 18.9997921522198
target distance 10.0
model initialize at round 560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([3.99999871, 1.99999482, 5.46980882]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 13.453628469272825}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.660001932244849
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'previousTarget': array([14., 11.]), 'currentState': array([13.94656289, 10.45980536,  0.35477044]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.5428312579197757}
episode index:561
target Thresh 18.9997962678816
target distance 14.0
model initialize at round 561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([15.99999999,  9.00000019,  2.64442295]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 13.999999986197727}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6601977351159528
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.81960628, 8.77308415, 4.37894049]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8504382702542047}
episode index:562
target Thresh 18.999800302047834
target distance 12.0
model initialize at round 562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([2.99999876, 4.99999933, 4.63826251]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 12.041595761253593}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6599969516475244
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'previousTarget': array([15.,  4.]), 'currentState': array([15.35326157,  4.10711042,  5.25774167]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.3691427584154201}
episode index:563
target Thresh 18.999804256332226
target distance 10.0
model initialize at round 563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([14.00002573, 10.999927  ,  6.05330801]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 10.198049945698294}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6603063786152594
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'previousTarget': array([4., 9.]), 'currentState': array([4.1939896 , 9.62099248, 4.35419617]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.6505871360737696}
episode index:564
target Thresh 18.999808132316538
target distance 2.0
model initialize at round 564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([1.99998392, 9.99999226, 4.59215546]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 2.236085822047932}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.66087238502479
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.09938457, 11.04502326,  2.30897015]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.9017401214458538}
episode index:565
target Thresh 18.99981193155122
target distance 4.0
model initialize at round 565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 7.06440793, 12.23102158,  3.22267485]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 3.30242487834351}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6614019320653822
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.90878533, 10.43312831,  4.93948954]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 1.0710902355624874}
episode index:566
target Thresh 18.999815655556016
target distance 6.0
model initialize at round 566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.10665698, 9.32342316, 5.77791929]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 4.324738551617258}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6617830944832545
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.08311587, 5.15224149, 0.36199276]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.17345235754220295}
episode index:567
target Thresh 18.999819305820573
target distance 5.0
model initialize at round 567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([16.00014498,  7.99999495,  0.96720141]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 4.999994954961681}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6622262884145932
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.8926484 ,  2.93343977,  0.11764549]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.12631163952334462}
episode index:568
target Thresh 18.999822883805056
target distance 6.0
model initialize at round 568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([11.        , 11.        ,  2.84107369]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 7.211102550927978}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.662589240126991
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.25416006,  5.91053472,  5.70833246]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.945341633991149}
episode index:569
target Thresh 18.999826390940694
target distance 11.0
model initialize at round 569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([13.77188389,  3.15066163,  3.39074361]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 11.933279130930147}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6626986981898522
{'scaleFactor': 20, 'currentTarget': array([ 4., 10.]), 'previousTarget': array([ 4., 10.]), 'currentState': array([4.48162668, 9.88132034, 4.55889054]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 0.49603339024658066}
episode index:570
target Thresh 18.999829828630393
target distance 1.0
model initialize at round 570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([11.98209985, 10.98472352,  4.8500731 ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0180147768812742}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6632719053734076
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([13.46072982, 10.29742217,  0.56688779]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.840171159207846}
episode index:571
target Thresh 18.999833198249277
target distance 3.0
model initialize at round 571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([16.77326346,  7.55939479,  1.62827223]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 2.5601738546848543}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6637917027591185
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.65180089,  9.98831462,  3.34508692]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 0.34839512976359555}
episode index:572
target Thresh 18.999836501145236
target distance 1.0
model initialize at round 572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.76248541,  2.66767697,  5.53797126]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 2.453795980423364}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6643437242202719
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([15.61582841,  4.17160813,  3.25478595]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.0322198055262748}
episode index:573
target Thresh 18.999839738639473
target distance 11.0
model initialize at round 573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([3.00000072, 2.99999856, 6.17601442]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 11.04536017181554}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6631863309725013
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([4.77467891, 7.00537077, 3.11408459]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.495727021726026}
episode index:574
target Thresh 18.999842912027027
target distance 8.0
model initialize at round 574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([15.       ,  3.       ,  5.9941709]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 11.31370849898476}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6633993602069964
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.23791749, 11.09889514,  3.72868845]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.25765282764588066}
episode index:575
target Thresh 18.9998460225773
target distance 8.0
model initialize at round 575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([1.70664465, 4.65693284, 2.74802768]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 6.473582393592425}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6637710992048992
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'previousTarget': array([ 3., 11.]), 'currentState': array([ 2.67658946, 10.74073497,  3.61528645]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.41450299863819556}
episode index:576
target Thresh 18.999849071534552
target distance 2.0
model initialize at round 576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([ 4.12027026, 10.99372121,  0.94984167]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 2.9104072308449855}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6642688790154626
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.7602786 , 8.69251004, 4.66665637]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8201058605410311}
episode index:577
target Thresh 18.999852060118403
target distance 12.0
model initialize at round 577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([13.15844487,  8.20488106,  4.75457597]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 11.186737952364659}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6645065290336695
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.89046592, 8.39605717, 4.77227882]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 1.0759537595770088}
episode index:578
target Thresh 18.99985498952433
target distance 11.0
model initialize at round 578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([3.99688482, 3.00261536, 3.44519532]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 13.60245216042284}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6645984530467345
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.25875657, 10.63190254,  0.33015694]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.8276095450695644}
episode index:579
target Thresh 18.99985786092413
target distance 7.0
model initialize at round 579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 7.99579548, 11.0004125 ,  4.04579735]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 8.066113131951104}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6649059439783503
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.78825386,  7.16468416,  0.3466855 ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.2682485817006766}
episode index:580
target Thresh 18.99986067546641
target distance 4.0
model initialize at round 580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([16.30753227,  4.63077571,  1.45148247]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 4.083672060071289}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6654148769663394
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.14065903,  7.07098306,  3.16829716]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.9396049380536008}
episode index:581
target Thresh 18.999863434277014
target distance 1.0
model initialize at round 581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.28959049, 5.35300101, 6.13523722]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9608794743412958}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6659897654938887
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([3.28959049, 5.35300101, 6.13523722]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9608794743412958}
episode index:582
target Thresh 18.999866138459513
target distance 11.0
model initialize at round 582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([16.        ,  9.        ,  0.36053437]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 11.180339887498949}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6662363145551706
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 5.17737099, 10.86617242,  4.66142252]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.22219426202153966}
episode index:583
target Thresh 18.999868789095604
target distance 4.0
model initialize at round 583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([4.00000001, 3.99999999, 0.34854668]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 4.123105637190642}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6666915012561155
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([3.2652467 , 7.76383181, 1.78217607]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.3551495877544719}
episode index:584
target Thresh 18.999871387245587
target distance 9.0
model initialize at round 584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([14.31751465,  1.96128768,  4.1665976 ]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 11.027661214323741}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6668814668255907
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.40718143, 10.651632  ,  3.90111515]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.5358702994062863}
episode index:585
target Thresh 18.999873933948756
target distance 3.0
model initialize at round 585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([4.        , 8.99999999, 5.97294927]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 3.605551279666925}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6673826861825436
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.62598994, 10.7521491 ,  1.40657865]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.44867983334700623}
episode index:586
target Thresh 18.99987643022382
target distance 6.0
model initialize at round 586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([3.        , 2.        , 6.25499582]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 6.082762530299903}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.667817715157408
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.20280348, 7.02993875, 3.4054399 ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.9910338410452646}
episode index:587
target Thresh 18.999878877069328
target distance 7.0
model initialize at round 587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([14.76455022,  7.56169034,  1.635616  ]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.491773278845653}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6681155476033714
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.19177678, 10.72880305,  4.21968946]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.3321537609424823}
episode index:588
target Thresh 18.999881275464052
target distance 4.0
model initialize at round 588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([1.99999973, 6.0000001 , 3.78309524]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 4.472136167479995}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6685013246944671
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([4.51169565, 2.65673479, 0.65035401]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.832546105689449}
episode index:589
target Thresh 18.999883626367374
target distance 10.0
model initialize at round 589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([2.65011399, 4.64616801, 2.78222549]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 12.144600501166488}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6686603778972682
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.63920549, 10.37177366,  0.23355772]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.7244591139400852}
episode index:590
target Thresh 18.999885930719692
target distance 1.0
model initialize at round 590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.00824812, 6.99156576, 0.20544546]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.008283395516981}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6691873484930427
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.62497567, 7.34595705, 4.20544546]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.5102249786367299}
episode index:591
target Thresh 18.999888189442775
target distance 9.0
model initialize at round 591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 5.99990209, 10.99980322,  5.25267482]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 9.848867359970068}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6694525195662565
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.37326454,  7.86855   ,  5.55356297]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.071063229376475}
episode index:592
target Thresh 18.999890403440148
target distance 12.0
model initialize at round 592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([13.13228463,  5.07181636,  3.46898031]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 11.51095219651413}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6695098470121811
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'previousTarget': array([2., 8.]), 'currentState': array([2.54581885, 8.62080285, 4.35394193]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.8266283335624564}
episode index:593
target Thresh 18.999892573597435
target distance 12.0
model initialize at round 593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([2.00000034, 7.99999916, 6.10167217]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 11.999999659295929}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.669692189625627
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.7480596 ,  8.74018264,  5.83618972]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.7818850975903237}
episode index:594
target Thresh 18.999894700782725
target distance 7.0
model initialize at round 594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([4.86592216, 7.98192933, 0.34488266]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 6.633022559726024}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6700563790072918
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([2.40635931, 2.71703236, 5.49532674]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 0.8241743075597247}
episode index:595
target Thresh 18.99989678584693
target distance 13.0
model initialize at round 595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([16.        , 10.        ,  0.73530596]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 13.601470508735444}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6701608202667668
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.50724846, 6.1795002 , 6.1866382 ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.5380718562988391}
episode index:596
target Thresh 18.999898829624094
target distance 5.0
model initialize at round 596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([4.57804262, 9.38609861, 5.45866156]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.735757255619141}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.67055315067672
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.93922047, 6.28629401, 0.32592033]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.9818856112945672}
episode index:597
target Thresh 18.999900832931758
target distance 13.0
model initialize at round 597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([16.        ,  3.        ,  1.48294037]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 13.928388277879028}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6705963949365409
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.87703356, 8.793027  , 4.36790199]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8025039364789542}
episode index:598
target Thresh 18.999902796571273
target distance 4.0
model initialize at round 598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.9991952 , 7.99974307, 4.45261145]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 4.123051646345872}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6710329040399974
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([4.62102367, 4.44172206, 5.88624083]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.7620949973473258}
episode index:599
target Thresh 18.99990472132812
target distance 12.0
model initialize at round 599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([14.32655525,  6.59248651,  2.37054017]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 12.559461990431382}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.671197921109606
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.59019072, 9.48602794, 4.10505771]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7645575475797171}
episode index:600
target Thresh 18.99990660797223
target distance 10.0
model initialize at round 600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([3.        , 9.        , 4.09201574]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.198039027185569}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6714696612765618
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.16562602, 11.77742472,  2.3929039 ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.1404249824976587}
episode index:601
target Thresh 18.999908457258282
target distance 3.0
model initialize at round 601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.51324529, 10.23363786,  4.61953139]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 2.286059546531333}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.671966055526933
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'previousTarget': array([15.,  8.]), 'currentState': array([14.21943008,  8.91258932,  4.33634609]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 1.2008782936989693}
episode index:602
target Thresh 18.999910269926016
target distance 7.0
model initialize at round 602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([13.99999968,  9.00000112,  2.85328048]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 7.280111053016858}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6722923901160575
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([16.58111377,  2.01788085,  5.72053926]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.5813888026975748}
episode index:603
target Thresh 18.999912046700526
target distance 13.0
model initialize at round 603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([2.00000003, 5.9999999 , 5.99441385]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 13.038404789765776}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6722432497385221
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.92003363,  7.42459808,  6.02981955]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 1.013284461556289}
episode index:604
target Thresh 18.99991378829254
target distance 1.0
model initialize at round 604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.07650932, 9.09019563, 1.86931341]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.0928770196480784}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6727521038711858
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.32469493, 7.49236213, 5.86931341]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.6025968849348914}
episode index:605
target Thresh 18.999915495398724
target distance 7.0
model initialize at round 605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([15.00000032,  1.9999995 ,  6.27628183]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 7.071068353021239}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6731194077497131
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'previousTarget': array([14.,  9.]), 'currentState': array([13.94959741,  8.61565542,  3.1435406 ]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 0.3876353662827305}
episode index:606
target Thresh 18.999917168701945
target distance 14.0
model initialize at round 606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([15.74168948,  6.18911577,  3.51162958]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 14.106895092912668}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6720104795656114
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([13.19279925,  2.58503358,  0.44969975]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 11.200488927283283}
episode index:607
target Thresh 18.999918808871534
target distance 12.0
model initialize at round 607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([13.37654486,  7.55651094,  4.41026235]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 10.660362187442479}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6721974329558114
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.81711697, 10.31224378,  2.14477989]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8747435747244665}
episode index:608
target Thresh 18.999920416563597
target distance 1.0
model initialize at round 608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([1.99243714, 6.99859118, 4.32776284]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 1.4205641607638706}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6727030200938149
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'previousTarget': array([3., 8.]), 'currentState': array([2.80615106, 8.38858676, 2.04457753]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.4342546291913741}
episode index:609
target Thresh 18.99992199242122
target distance 3.0
model initialize at round 609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.49710307, 2.87712476, 4.22517014]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 3.163108455366363}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6731592283393988
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.23634027, 5.5537619 , 1.65879952]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.5049605620777549}
episode index:610
target Thresh 18.99992353707477
target distance 1.0
model initialize at round 610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.03629466,  7.57103633,  5.73978281]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.43049638042632427}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6736941559525913
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([14.03629466,  7.57103633,  5.73978281]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 0.43049638042632427}
episode index:611
target Thresh 18.999925051142135
target distance 13.0
model initialize at round 611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([2.        , 4.        , 4.26313305]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 13.341664064126332}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6736019925293719
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.57219803,  7.22907632,  6.01535344]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.6163493694720065}
episode index:612
target Thresh 18.999926535228955
target distance 11.0
model initialize at round 612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([14.        ,  9.00000017,  2.57109237]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 13.038404902282366}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6725031312038754
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'previousTarget': array([3., 2.]), 'currentState': array([13.70699944,  4.00005646,  5.79234784]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.892201929748413}
episode index:613
target Thresh 18.999927989928885
target distance 7.0
model initialize at round 613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.00000001, 2.99999998, 6.2780838 ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 7.000000021452085}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6728660548570592
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([2.63286177, 9.66616523, 3.14534257]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.49622186258591977}
episode index:614
target Thresh 18.99992941582383
target distance 13.0
model initialize at round 614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([16.        ,  7.        ,  1.65010136]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 13.000000000154941}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6728274151391914
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.82699731, 7.04054702, 5.96869237]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8279907111323274}
episode index:615
target Thresh 18.99993081348416
target distance 11.0
model initialize at round 615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([13.94208371,  4.29201036,  3.25890446]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 10.611042158965978}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6727995446934689
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.85676369, 7.85813433, 5.57749546]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 0.8684295492689725}
episode index:616
target Thresh 18.999932183468964
target distance 10.0
model initialize at round 616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([14.70099641,  3.52426928,  2.14174826]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 10.983649976140809}
done in step count: 93
reward sum = 0.39271102835780486
running average episode reward sum: 0.6723455924789865
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'previousTarget': array([4., 6.]), 'currentState': array([4.10542297, 6.80068245, 5.92937436]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.8075929551274295}
episode index:617
target Thresh 18.999933526326252
target distance 2.0
model initialize at round 617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([3.9991794 , 7.00126737, 3.14739609]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 2.8287434296222855}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.672812017102807
{'scaleFactor': 20, 'currentTarget': array([2., 5.]), 'previousTarget': array([2., 5.]), 'currentState': array([2.12707399, 4.45590526, 4.86421078]), 'targetState': array([2, 5], dtype=int32), 'currentDistance': 0.558736865658768}
episode index:618
target Thresh 18.99993484259318
target distance 12.0
model initialize at round 618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([15.        , 10.        ,  0.48658388]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 12.041594578792294}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6729943533285007
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([2.63955778, 8.65534475, 4.50428673]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.4987041528403817}
episode index:619
target Thresh 18.99993613279628
target distance 12.0
model initialize at round 619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([14.99999946,  9.00000153,  2.91124141]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 11.999999462729134}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6731508836389469
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'previousTarget': array([3., 9.]), 'currentState': array([3.015664  , 8.87032489, 4.64575895]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.1306177429949121}
episode index:620
target Thresh 18.999937397451646
target distance 8.0
model initialize at round 620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([ 9.04088841, 10.95445199,  0.16274613]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 9.988275087924928}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6732461372396159
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([3.7828228 , 3.30726569, 5.61407836]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.840965952605411}
episode index:621
target Thresh 18.999938637065156
target distance 1.0
model initialize at round 621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.00027588, 10.00059241,  2.13696976]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 1.414437515332835}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6737394714241182
{'scaleFactor': 20, 'currentTarget': array([16.,  9.]), 'previousTarget': array([16.,  9.]), 'currentState': array([15.66687945,  8.50623426,  6.13696976]), 'targetState': array([16,  9], dtype=int32), 'currentDistance': 0.5956289982393209}
episode index:622
target Thresh 18.999939852132677
target distance 2.0
model initialize at round 622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([16.00000018,  2.99999994,  0.68459099]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 2.8284272903902936}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6741999153062624
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.70951375,  5.3389465 ,  2.40140568]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 0.7863170369824476}
episode index:623
target Thresh 18.999941043140247
target distance 1.0
model initialize at round 623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.69831993,  8.93776637,  4.82804275]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 2.0841829138442676}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6746901398009639
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'previousTarget': array([15., 11.]), 'currentState': array([14.74472696, 10.59788305,  2.54485744]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.47630071104820215}
episode index:624
target Thresh 18.99994221056429
target distance 12.0
model initialize at round 624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([4.76740602, 6.28106187, 1.87702387]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 11.832230069707471}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.674747520740918
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.19440018, 10.71009908,  0.76198549]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0738862879825137}
episode index:625
target Thresh 18.999943354871785
target distance 6.0
model initialize at round 625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([15.99999997,  4.99999996,  5.0513804 ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 8.485281380536037}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6750435444372445
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'previousTarget': array([10., 11.]), 'currentState': array([10.90366717, 10.58752062,  3.63545386]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9933547174539014}
episode index:626
target Thresh 18.999944476520472
target distance 7.0
model initialize at round 626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.88586903,  9.9201345 ,  4.7541647 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 7.009247410442976}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6753249068402072
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.17011576,  2.55645626,  5.33823816]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.4750478086091673}
episode index:627
target Thresh 18.999945575959032
target distance 12.0
model initialize at round 627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([3.        , 2.        , 4.20680213]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 12.999999999999998}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6752324947925991
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([15.79038113,  6.97511124,  5.95902252]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.7907729054989309}
episode index:628
target Thresh 18.999946653627248
target distance 1.0
model initialize at round 628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([2.39623055, 2.89992622, 5.21243334]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.9447978700801425}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6757171808104169
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([3.11546236, 4.34442608, 2.92924803]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.9492292431071442}
episode index:629
target Thresh 18.9999477099562
target distance 8.0
model initialize at round 629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([15.        , 11.        ,  1.94484871]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.000000000001618}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6760375043694463
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.98732247, 11.72797967,  2.81210748]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.226686618420949}
episode index:630
target Thresh 18.999948745368442
target distance 12.0
model initialize at round 630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([4.87710883, 4.74293448, 0.86416214]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 11.25862018535448}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6759642652677565
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'previousTarget': array([16.,  3.]), 'currentState': array([15.11132268,  3.14780485,  4.89956784]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.9008849276651263}
episode index:631
target Thresh 18.999949760278145
target distance 10.0
model initialize at round 631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([ 5.48598657, 10.2104466 ,  0.51360148]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 8.796277560073484}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6761759165382526
{'scaleFactor': 20, 'currentTarget': array([14.,  8.]), 'previousTarget': array([14.,  8.]), 'currentState': array([13.01724863,  8.81337185,  4.81448963]), 'targetState': array([14,  8], dtype=int32), 'currentDistance': 1.2756857060789613}
episode index:632
target Thresh 18.99995075509129
target distance 1.0
model initialize at round 632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.63375197,  3.20269797,  3.63810873]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 1.2572271209937402}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.676671689181952
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'previousTarget': array([16.,  2.]), 'currentState': array([15.50810835,  1.55876222,  5.63810873]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.6607935946629513}
episode index:633
target Thresh 18.999951730205815
target distance 11.0
model initialize at round 633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([15.25355927,  2.15008308,  1.53644627]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 11.254560019205174}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6756043836785105
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'previousTarget': array([4., 2.]), 'currentState': array([13.16199467,  1.56211938,  4.75770175]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.172452548942225}
episode index:634
target Thresh 18.99995268601178
target distance 3.0
model initialize at round 634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([3.99999002, 7.99999996, 4.1471684 ]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 3.605556843535856}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6760380618930325
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.95908941, 10.43053642,  1.58079778]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.570931205688712}
episode index:635
target Thresh 18.999953622891518
target distance 11.0
model initialize at round 635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.99153047,  4.00611336,  3.518381  ]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 11.037481875882106}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6749751089655278
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'previousTarget': array([3., 3.]), 'currentState': array([13.19106561,  4.56939084,  0.45645117]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 10.311198083462257}
episode index:636
target Thresh 18.999954541219793
target distance 1.0
model initialize at round 636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.70239757, 6.34267413, 3.2879163 ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7815292129860087}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6754853521225678
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([3.70239757, 6.34267413, 3.2879163 ]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.7815292129860087}
episode index:637
target Thresh 18.99995544136395
target distance 6.0
model initialize at round 637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.00000003, 11.00000003,  1.93198651]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.000000025596459}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6758441244154929
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.31952647, 11.69830107,  3.08243059]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.7679332956540149}
episode index:638
target Thresh 18.99995632368406
target distance 12.0
model initialize at round 638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([14.32656028,  2.82140469,  4.24991369]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 13.17297493262821}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6759324800418449
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.87634356, 10.78420529,  3.41806062]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.24871322951006095}
episode index:639
target Thresh 18.99995718853306
target distance 12.0
model initialize at round 639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([16.        ,  5.        ,  0.93792885]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6748763355417795
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'previousTarget': array([4., 4.]), 'currentState': array([12.8899962 ,  8.99403451,  4.15918433]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.19668638028181}
episode index:640
target Thresh 18.999958036256903
target distance 11.0
model initialize at round 640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([3.67627814, 8.85343415, 0.91478651]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 9.567629500273936}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6751123609525342
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.77476356, 11.20594003,  1.21567467]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.30519297085825103}
episode index:641
target Thresh 18.999958867194692
target distance 5.0
model initialize at round 641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.82218056,  2.53068506,  1.57517737]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 4.544310393250567}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6755125992499711
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([15.05666267,  6.43311581,  3.00880675]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 1.100564861361926}
episode index:642
target Thresh 18.999959681678806
target distance 5.0
model initialize at round 642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([3.09337924, 5.26263012, 1.85911291]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 3.8940225730431437}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6759115926382402
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.78082126, 8.97333524, 3.2927423 ]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.22079477043950416}
episode index:643
target Thresh 18.999960480035057
target distance 4.0
model initialize at round 643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 3.99991689, 10.99999505,  4.20312309]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 3.99999504762546}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6763536491869386
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.86780176, 7.89026793, 5.91993779]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.2432445008360835}
episode index:644
target Thresh 18.999961262582794
target distance 13.0
model initialize at round 644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([ 3.90064712, 12.42110799,  2.0079196 ]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 12.339210024007716}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.676547877001438
{'scaleFactor': 20, 'currentTarget': array([16., 10.]), 'previousTarget': array([16., 10.]), 'currentState': array([15.0508335 , 10.48623544,  2.02562245]), 'targetState': array([16, 10], dtype=int32), 'currentDistance': 1.0664623492559944}
episode index:645
target Thresh 18.999962029635046
target distance 3.0
model initialize at round 645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([4.00038384, 4.00005775, 1.15133446]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 3.162344277131295}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6769875799936959
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([2.60088406, 6.98156957, 2.86814915]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.39954125765885695}
episode index:646
target Thresh 18.99996278149865
target distance 6.0
model initialize at round 646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([ 2.99999998, 10.00000003,  3.14063621]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 6.0000000255445975}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6773674209742743
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.8062128 , 4.8940632 , 0.29108028]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 1.2038804294113223}
episode index:647
target Thresh 18.999963518474352
target distance 13.0
model initialize at round 647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([15.        ,  9.        ,  1.00897854]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.152946437965907}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6774636138020738
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([1.96570206, 7.34601304, 4.46031078]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.3477087430358275}
episode index:648
target Thresh 18.999964240856958
target distance 13.0
model initialize at round 648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([2.        , 5.        , 3.60080004]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 13.152946437965905}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.6772134959340025
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.14049138,  3.80054365,  3.65390859]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.812777928375524}
episode index:649
target Thresh 18.999964948935435
target distance 11.0
model initialize at round 649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([4.86402042, 4.71187229, 1.4325816 ]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 11.005731294481986}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.677232319918518
{'scaleFactor': 20, 'currentTarget': array([15.,  9.]), 'previousTarget': array([15.,  9.]), 'currentState': array([14.95936604,  8.80002576,  0.03435791]), 'targetState': array([15,  9], dtype=int32), 'currentDistance': 0.20406081403202966}
episode index:650
target Thresh 18.999965642993015
target distance 14.0
model initialize at round 650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([3.58243014, 7.42710597, 0.65464466]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.652544588536628}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6772725893118836
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'previousTarget': array([16.,  5.]), 'currentState': array([16.03065564,  5.95369683,  1.53960629]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9541894026240468}
episode index:651
target Thresh 18.999966323307333
target distance 12.0
model initialize at round 651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([15.        ,  5.        ,  5.60675931]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 12.165525060596561}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6772806905476175
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.84518111, 7.19141504, 6.20853562]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8665857253238638}
episode index:652
target Thresh 18.999966990150526
target distance 11.0
model initialize at round 652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([15.        , 11.        ,  1.32475727]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 11.40175425099138}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6774835193036262
{'scaleFactor': 20, 'currentTarget': array([4., 8.]), 'previousTarget': array([4., 8.]), 'currentState': array([4.94766325, 8.58884271, 5.62564543]), 'targetState': array([4, 8], dtype=int32), 'currentDistance': 1.1157066717551032}
episode index:653
target Thresh 18.99996764378934
target distance 2.0
model initialize at round 653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([2.86384367, 5.97667866, 5.58211255]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 1.4982497834391437}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6779312493964341
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.8224962 , 5.0661736 , 5.29892724]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.18943744583735128}
episode index:654
target Thresh 18.999968284485238
target distance 3.0
model initialize at round 654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([15.99993989,  7.99923571,  5.63589716]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 4.243138655437192}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.678305010381215
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'previousTarget': array([13., 11.]), 'currentState': array([12.77136699, 11.02887154,  2.78634124]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.2304487374979441}
episode index:655
target Thresh 18.999968912494506
target distance 1.0
model initialize at round 655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([3.06934791, 7.1180598 , 5.7928586 ]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 1.0758452783269719}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6787501231702681
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.95452474, 6.98557035, 5.50967329]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9546337991912367}
episode index:656
target Thresh 18.99996952806836
target distance 7.0
model initialize at round 656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([2.       , 2.       , 5.7633388]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 7.000000000000001}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6790797854702505
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'previousTarget': array([2., 9.]), 'currentState': array([1.35924781, 8.54828533, 2.63059758]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.7839703531822604}
episode index:657
target Thresh 18.999970131453033
target distance 11.0
model initialize at round 657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([4.00000398, 1.99999709, 0.37108057]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 11.401751172883818}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6789399024861581
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'previousTarget': array([15.,  5.]), 'currentState': array([14.33929812,  5.96445639,  5.84011566]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 1.1690607778694528}
episode index:658
target Thresh 18.999970722889888
target distance 7.0
model initialize at round 658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([ 9.68293903, 11.0032068 ,  1.00390547]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 7.300905119554708}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6792147408050582
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'previousTarget': array([15.,  6.]), 'currentState': array([14.90553275,  5.52448104,  5.87116424]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.4848116584215062}
episode index:659
target Thresh 18.999971302615513
target distance 8.0
model initialize at round 659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([ 7.99795547, 10.95265581,  5.67123151]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 9.410712838703013}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6794373982035893
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'previousTarget': array([16.,  6.]), 'currentState': array([16.88516074,  6.49704501,  5.97211967]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 1.0151666222462903}
episode index:660
target Thresh 18.999971870861796
target distance 5.0
model initialize at round 660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([15.        , 10.        ,  1.31317252]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 5.099019513592785}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6796468831345933
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'previousTarget': array([14.,  5.]), 'currentState': array([14.86987885,  5.67463611,  3.61406067]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 1.1008283672819161}
episode index:661
target Thresh 18.999972427856054
target distance 4.0
model initialize at round 661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([1.99996805, 6.9999882 , 4.49735022]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 3.999988200765101}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6799194080160234
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'previousTarget': array([2., 3.]), 'currentState': array([2.30624909, 3.77127688, 3.08142368]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 0.8298533203027041}
episode index:662
target Thresh 18.99997297382108
target distance 7.0
model initialize at round 662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([13.65560406, 10.56872239,  2.78890699]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 6.66956263633645}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.680257964074836
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.7365822 , 10.54854402,  3.93935107]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.8639246739813333}
episode index:663
target Thresh 18.99997350897528
target distance 13.0
model initialize at round 663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([16.        ,  3.        ,  5.68993282]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 13.038404810405314}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.680009288100964
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.32279665, 3.88516861, 5.74304137]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.3426133750927019}
episode index:664
target Thresh 18.999974033532716
target distance 6.0
model initialize at round 664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([3.54377155, 5.27153461, 1.17610949]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 5.91066129735594}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6803062982286301
{'scaleFactor': 20, 'currentTarget': array([ 5., 11.]), 'previousTarget': array([ 5., 11.]), 'currentState': array([ 4.29207477, 11.56239002,  2.04336826]), 'targetState': array([ 5, 11], dtype=int32), 'currentDistance': 0.9041242558943997}
episode index:665
target Thresh 18.999974547703218
target distance 5.0
model initialize at round 665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([3.46801596, 8.44420988, 5.38491416]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.4850521813038995}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6806843148197387
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'previousTarget': array([4., 5.]), 'currentState': array([4.82273037, 5.33291143, 0.53535824]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8875332550927723}
episode index:666
target Thresh 18.99997505169246
target distance 9.0
model initialize at round 666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([12.9669178 ,  9.31740484,  5.69471502]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 10.968829420689469}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6807184428260054
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([4.50407807, 3.06643727, 0.29649134]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.5084374177187511}
episode index:667
target Thresh 18.99997554570205
target distance 12.0
model initialize at round 667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([14.        ,  8.        ,  0.39709824]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 12.041594578792296}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6808067391292424
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.23283583, 7.46778532, 3.84843047]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.522528116086573}
episode index:668
target Thresh 18.99997602992959
target distance 9.0
model initialize at round 668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([ 8.        , 11.        ,  3.72188413]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 10.81665382655361}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6808727684219608
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'previousTarget': array([14.,  2.]), 'currentState': array([14.82331743,  2.60223833,  4.89003106]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 1.0200698956531205}
episode index:669
target Thresh 18.999976504568785
target distance 4.0
model initialize at round 669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([ 2.        , 11.        ,  2.72739667]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 4.123105625617661}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6812337712965966
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.72383487, 7.36170983, 6.16102605]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.8091791656741428}
episode index:670
target Thresh 18.999976969809495
target distance 13.0
model initialize at round 670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([3.        , 9.        , 5.16114783]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 13.15294643796801}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6812668769951107
{'scaleFactor': 20, 'currentTarget': array([16.,  7.]), 'previousTarget': array([16.,  7.]), 'currentState': array([16.37411694,  6.81415695,  6.04610945]), 'targetState': array([16,  7], dtype=int32), 'currentDistance': 0.4177333152543957}
episode index:671
target Thresh 18.99997742583782
target distance 13.0
model initialize at round 671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([15.        ,  4.        ,  5.57841587]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 13.341664064126334}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6812687932421573
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'previousTarget': array([2., 7.]), 'currentState': array([2.95432863, 7.14191403, 6.18019218]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.9648226390455622}
episode index:672
target Thresh 18.999977872836183
target distance 4.0
model initialize at round 672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.        ,  4.        ,  0.64055365]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 4.123105625664449}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6816838411125256
{'scaleFactor': 20, 'currentTarget': array([16.,  8.]), 'previousTarget': array([16.,  8.]), 'currentState': array([15.23563139,  7.31628984,  2.35736834]), 'targetState': array([16,  8], dtype=int32), 'currentDistance': 1.0255334995358185}
episode index:673
target Thresh 18.99997831098338
target distance 7.0
model initialize at round 673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([13.99999841, 10.00000096,  3.6007719 ]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 7.0710689862424045}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.682000835790784
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'previousTarget': array([15.,  3.]), 'currentState': array([15.50254486,  3.6646354 ,  0.46803068]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.8332415928311289}
episode index:674
target Thresh 18.999978740454683
target distance 8.0
model initialize at round 674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([3.        , 3.        , 4.39449215]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 10.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6821661890468623
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.52216675, 10.45015315,  0.12900969]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.7284615152374302}
episode index:675
target Thresh 18.999979161421887
target distance 12.0
model initialize at round 675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([16.        ,  6.        ,  6.18708611]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 12.36931687685298}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.681157067465432
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'previousTarget': array([4., 3.]), 'currentState': array([13.10211339,  6.63233798,  3.12515628]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 9.800119762592022}
episode index:676
target Thresh 18.99997957405338
target distance 10.0
model initialize at round 676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([12.54372483, 11.08440053,  3.96068287]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 11.318242509614421}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6812217990289363
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'previousTarget': array([3., 5.]), 'currentState': array([3.24236762, 4.8947323 , 5.1288298 ]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 0.2642410875136737}
episode index:677
target Thresh 18.999979978514222
target distance 4.0
model initialize at round 677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([ 6.30827909, 10.03019664,  6.07918596]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 3.809231421541102}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6816056609026414
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'previousTarget': array([4., 7.]), 'currentState': array([4.85973643, 7.98290361, 5.51281535]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 1.3058507702967095}
episode index:678
target Thresh 18.999980374966203
target distance 1.0
model initialize at round 678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.99973143, 3.99868342, 5.51315761]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9997322947618199}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6820745774550675
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'previousTarget': array([3., 4.]), 'currentState': array([3.99973143, 3.99868342, 5.51315761]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.9997322947618199}
episode index:679
target Thresh 18.99998076356791
target distance 14.0
model initialize at round 679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([2.        , 9.        , 5.18565154]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 14.866068747318506}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6820454707837865
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'previousTarget': array([16.,  4.]), 'currentState': array([15.13532019,  4.95578931,  5.50424255]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.2888771796299108}
episode index:680
target Thresh 18.999981144474784
target distance 2.0
model initialize at round 680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.        , 4.        , 4.52505612]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 2.2360679778635872}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6824831426328559
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'previousTarget': array([3., 6.]), 'currentState': array([2.54576908, 5.58732015, 2.24187082]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.6137021987336174}
episode index:681
target Thresh 18.9999815178392
target distance 8.0
model initialize at round 681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([ 7.04919914, 10.77398038,  5.9287219 ]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 8.801031881974865}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6826938251566136
{'scaleFactor': 20, 'currentTarget': array([15.,  7.]), 'previousTarget': array([15.,  7.]), 'currentState': array([14.93530632,  6.91544101,  6.22961006]), 'targetState': array([15,  7], dtype=int32), 'currentDistance': 0.10646828160713237}
episode index:682
target Thresh 18.999981883810502
target distance 8.0
model initialize at round 682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([11.        , 11.        ,  0.33556574]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 8.94427191021299}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6829161091043345
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'previousTarget': array([3., 7.]), 'currentState': array([3.17946907, 6.70277574, 4.91963921]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.34720514410489034}
episode index:683
target Thresh 18.99998224253509
target distance 1.0
model initialize at round 683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([ 3.00001363, 11.00002136,  2.00472212]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 1.0000213573798222}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6833505884769889
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'previousTarget': array([ 3., 10.]), 'currentState': array([3.46560645, 9.44168245, 6.00472212]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.726985450771962}
episode index:684
target Thresh 18.999982594156453
target distance 14.0
model initialize at round 684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 2.]), 'previousTarget': array([2., 2.]), 'currentState': array([16.        ,  3.        ,  0.80292672]), 'targetState': array([2, 2], dtype=int32), 'currentDistance': 14.035668847618197}
Process finished with exit code 0
